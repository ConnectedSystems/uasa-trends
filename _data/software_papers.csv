,id,year,title,keywords,abstract,copyright,AU,SO,DE,DOI
0,WOS:000245063400008,2007,Parameter estimation and uncertainty analysis for a watershed model,RAINFALL-RUNOFF MODELS|SHUFFLED COMPLEX EVOLUTION|GROUNDWATER-FLOW MODEL|MONTE-CARLO METHODS|BAYESIAN-APPROACH|METROPOLIS ALGORITHM|PREDICTION INTERVALS|CATCHMENT MODELS|MARKOV-CHAINS|CALIBRATION,"Where numerical models are employed as an aid to environmental management, the uncertainty associated with predictions made by such models must be assessed. A number of different methods are available to make such an assessment. This paper explores the use of three such methods, and compares their performance when used in conjunction with a lumped parameter model for surface water flow (HSPF) in a large watershed. Linear (or first-order) uncertainty analysis has the advantage that it can be implemented with virtually no computational burden. While the results of such an analysis can be extremely useful for assessing parameter uncertainty in a relative sense, and ascertaining the degree of correlation between model parameters, its use in analyzing predictive uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods are far more robust, and can produce reliable estimates of parameter and predictive uncertainty. As well as this, they can provide the modeler with valuable qualitative information on the shape of parameter and predictive probability distributions; these shapes can be quite complex, especially where local objective function optima lie within those parts of parameter space that are considered probable after calibration has been undertaken. Nonlinear calibration-constrained optimization can also provide good estimates of parameter and predictive uncertainty, even in situations where the objective function surface is complex. Furthermore, they can achieve these estimates using far fewer model runs than MCMC methods. However, they do not provide the same amount of qualitative information on the probability structure of parameter space as do MCMC methods, a situation that can be partially rectified by combining their use with an efficient gradient-based search method that is specifically designed to locate different local optima. All methods of parameter and predictive uncertainty analysis discussed herein are implemented using freely-available software. Hence similar studies, or extensions of the present study, can be easily undertaken in other modeling contexts by other modelers. (c) ", Elsevier Ltd. All rights reserved.,"Gallagher, Mark|Doherty, John",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty analysis|parameter estimation|mathematical modeling|markov chain monte carlo|model calibration,10.1016/j.envsoft.2006.06.007
1,WOS:000089556100004,2000,Estimating labor productivity using probability inference neural network,ESTIMATING CONSTRUCTION PRODUCTIVITY,"This paper discusses the derivation of a probabilistic neural network classification model and its application in the construction industry. The probability inference neural network (PINN) model is based on the same concepts as those of the learning vector quantization method combined with a probabilistic approach. The classification and prediction networks are combined in an integrated network, which required the development of a different training and recall algorithm. The topology and algorithm of the developed model was presented and explained in detail. Portable computer software was developed to implement the training, testing, and recall for PINN. The PINN was tested on real historical productivity data at a local construction company and compared to the classic feedforward back-propagation neural network model. This showed marked improvement in performance and accuracy. In addition, the effectiveness of PINN for estimating labor production rates in the context of the application domain was validated through sensitivity analysis.",,"Lu, M|AbouRizk, SM|Hermann, UH",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2000)14:4(241)
2,WOS:000241686100002,2006,Application of the Morris algorithm for sensitivity analysis of the REALM model for the Goulburn irrigation system,2ND-ORDER SCREENING METHOD,"The REALM modelling shell is widely used in Australia as a water allocation modelling tool. It has been used to develop the Goulburn System Model (GSM) of the Goulburn, Broken, Loddon and Campaspe Rivers in northeastern Victoria. REALM represents the river and irrigation system as a network of storages and carriers. The model has been optimised to best represent the water harvesting and allocation for use by water management authorities. The model is analysed to assess the sensitivity of a subset of the model outputs, to a subset of the system parameters. The New Morris algorithm uses sampling paths generated in the space of the parameters, to generate points at which the model is run (to generate the model outputs). These model runs are then used to estimate the first and second-order effects of the parameters on the outputs. The results illustrate the mild linkage of the Goulburn and Broken systems, and the Broken system also shows differences between minimum and average outflows. The Goulburn is more sensitive to some of the numerical convergence parameters used in the allocation software, while the Broken is less sensitive to these factors. The numerical convergence factors also lead to important second-order effects.",,"Braddock, R. D.|Schreider, S. Yu.",ENVIRONMENTAL MODELING & ASSESSMENT,sensitivity analysis|water allocation model|realm,10.1007/s10666-005-9029-z
3,WOS:000378360600027,2016,Operational snow mapping with simplified data assimilation using the seNorge snow model,WATER EQUIVALENT|COVERED AREA|SWISS ALPS|DEPTH|PREDICTION|CALIBRATION|CHALLENGES|RADIATION|NORWAY|SCHEME,"Frequently updated maps of snow conditions are useful for many applications, e.g., for avalanche and flood forecasting services, hydropower energy situation analysis, as well as for the general public. Numerical snow models are often applied in snow map production for operational hydrological services. However, inaccuracies in the simulated snow maps due to model uncertainties and the lack of suitable data assimilation techniques to correct them in near-real time may often reduce the usefulness of the snow maps in operational use. In this paper the revised seNorge snow model (v...) for snow mapping is described, and a simplified data assimilation procedure is introduced to correct detected snow model biases in near real-time. The data assimilation procedure is theoretically based on the Bayesian updating paradigm and is meant to be pragmatic with modest computational and input data requirements. Moreover, it is flexible and can utilize both point-based snow depth and satellite-based areal snow-covered area observations, which are generally the most common data-sources of snow observations. The model and analysis codes as well as the ""R"" statistical software are freely available. All these features should help to lower the challenges and hurdles hampering the application of data-assimilation techniques in operational hydrological modeling. The steps of the data assimilation procedure (evaluation, sensitivity analysis, optimization) and their contribution to significantly increased accuracy of the snow maps are demonstrated with a case from eastern Norway in winter /.", (C) 2016 Elsevier B.V. All rights reserved.,"Saloranta, Tuomo M.",JOURNAL OF HYDROLOGY,snow|modeling|snow mapping|data assimilation,10.1016/j.jhydrol.2016.03.061
4,WOS:000319162300007,2013,Soil organic matter accounting in the carbon footprint analysis of the wine chain,LIFE-CYCLE ASSESSMENT|GREENHOUSE-GAS EMISSIONS|NITROUS-OXIDE EVOLUTION|CROPPING SYSTEMS|RAINFALL EVENTS|ASSESSMENT LCA|DYNAMICS|SEQUESTRATION|AGRICULTURE|MODEL,"Concerns about global warming led to the calculation of the carbon footprint (CF) left by human activities. The agricultural sector is a significant source of greenhouse gas (GHG) emissions, though cropland soils can also act as sinks. So far, most LCA studies on agricultural products have not considered changes in soil organic matter (SOM). This paper aimed to: () integrate the H,nin-Dupuis SOM model into the CF study and () outline the impacts of different vineyard soil management scenarios on the overall CF. A representative wine chain in the Maremma Rural District, Tuscany (Italy), made up of a cooperative winery and nine of its associated farms, was selected to investigate the production of a non-aged, high-quality red wine. The system boundary was established from vineyard planting to waste management after use. The functional unit (FU) chosen for this study was a .-L bottle of wine, and all data refer to the year . The SOM balance, based on H,nin-Dupuis' equation, was integrated and run using GaBi software. A sensitivity analysis was performed, and four scenarios were developed to assess the impact of vineyard soil management types with decreasing levels of organic matter inputs. SOM accounting reduced the overall CF of one wine bottle from . to . kg CO-eq/FU. The vineyard planting sub-phase produced a loss of SOM while, in the pre-production and production sub-phases, the loss/accumulation of SOM was related to the soil management practices. On average, soil management in the production sub-phase led to a net accumulation of SOM, and the overall vineyard phase was a sink of CO. Residue incorporation and grassing were identified as the main factors affecting changes in SOM in vineyard soils. Our results showed that incorporating SOM accounting into the wine chain's CF analysis changed the vineyard phase from a GHG source to a modest net GHG sink. These results highlighted the need to include soil C dynamics in the CF of the agricultural product. Here, the SOM balance method proposed was sensitive to changes in management practices and was site specific. Moreover, we were also able to define a minimum data set for SOM accounting. The EU recognises soil carbon sequestration as one of the major European strategies for mitigation. However, specific measures have yet to be included in the CAP . It would be desirable to include soil in the new ISO -Carbon Footprint of Products.",,"Bosco, Simona|Di Bene, Claudia|Galli, Mariassunta|Remorini, Damiano|Massai, Rossano|Bonari, Enrico",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,agriculture mitigation potential|food lca|greenhouse gas emissions|soil organic matter model|vineyard soil management|wine,10.1007/s11367-013-0567-3
5,WOS:000358997900004,2015,Sensitivity Analysis for Bayesian Hierarchical Models,LINEAR MIXED MODELS|LOCAL INFLUENCE|MARGINAL DENSITIES|INFERENCE|APPROXIMATIONS|PERTURBATION,"Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.",,"Roos, Malgorzata|Martins, Thiago G.|Held, Leonhard|Rue, Havard",BAYESIAN ANALYSIS,base prior|formal local sensitivity measure|bayesian robustness|calibration|hellinger distance|bayesian hierarchical models|identifiability|overparametrisation,10.1214/14-BA909
6,WOS:000321439500015,2013,Application of Bayesian Networks in Quantitative Risk Assessment of Subsea Blowout Preventer Operations,OFFSHORE SAFETY ASSESSMENT|RELIABILITY-ANALYSIS|ORGANIZATIONAL-FACTORS|FAULT-TREES|SYSTEMS|METHODOLOGY|FACILITIES|ACCIDENTS|SECURITY|PLATFORM,"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.",,"Cai, Baoping|Liu, Yonghong|Liu, Zengkai|Tian, Xiaojie|Zhang, Yanzhen|Ji, Renjie",RISK ANALYSIS,bayesian networks|quantitative risk assessment|subsea blowout preventer,10.1111/j.1539-6924.2012.01918.x
7,WOS:000368869200001,2016,A GUI platform for uncertainty quantification of complex dynamical models,RAINFALL-RUNOFF MODELS|GLOBAL SENSITIVITY MEASURES|AUTOMATIC CALIBRATION|OPTIMIZATION|INDEXES|DESIGN|MACHINE|OUTPUT,"Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications. (C)  The Authors.", Published by Elsevier Ltd.,"Wang, Chen|Duan, Qingyun|Tong, Charles H.|Di, Zhenhua|Gong, Wei",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification|design of experiments|sensitivity analysis|surrogate modeling|parameter optimization|uq-pyl,10.1016/j.envsoft.2015.11.004
8,WOS:000172361300016,2001,Including optimisation in the conception of fabric structures,SENSITIVITY ANALYSIS|VARIATIONAL APPROACH|ADJOINT SYSTEMS|OPTIMIZATION|DOMAIN,"For the conception of fabric structure, many specialised software have been developed in the last  years. Therefore, designing a fabric structure remains difficult, as textile behaviour is complex. To improve the static analysis stage, two new CAD tools have been developed and are presented in this paper. The first is a design sensitivity analysis module to give designers a better structural behaviour understanding. The second is an optimisation module, which determine the optimal adjustments leading to a structure satisfying the different requirements. All of these are illustrated by some examples.", (C) 2001 Civil-Comp Ltd. and Elsevier Science Ltd. All rights reserved.,"Sindel, F|Nouri-Baranger, T|Trompette, P",COMPUTERS & STRUCTURES,fabric structures|static analysis|design sensitivity analysis|optimisation,10.1016/S0045-7949(01)00079-7
9,WOS:000346334900006,2015,Context-Aware Framework for Highway Bridge Inspections,,"Bridge inspections are tedious, time consuming, and complex tasks in the field that require highly specific information pertinent to the decisions at hand. The use of a centralized inspection database and bridge inspection reporting software has been explored by several state DOTs in recent years. During an inspection routine, the inspector visually assesses the condition of a particular bridge component. Based on a priori knowledge of the bridge components' taxonomic hierarchy and ontology, the inspector navigates to the form corresponding to the component. The inspector then reports the component's condition to the database. Context-aware computing offers the possibility to make inspections more efficient by reducing the time required to navigate the software and the effort spent by inspectors to learn, remember, and recall the taxonomic hierarchy and ontology of bridge components. Context-aware computing leverages environmental variables, which define the inspector's context, and delivers streamlined information, pertinent to the task at hand, to assist decision making. This paper presents a computing framework that identifies the component of interest to the inspector and automatically queries the inspection database to retrieve information relevant to the component being assessed. The framework's run-time and space complexity are analyzed and presented. The uncertainty in sensing the inspector's location and line of sight are translated into errors in identifying the component of interest. Using a case study bridge, sensitivity analysis is performed to evaluate and characterize the errors in identifying the component of interest due to the errors in tracking technologies through simulation studies and field testing. The sensitivity analysis is used to evaluate the feasibility of employing global positioning systems (GPS) and magnetic compass technologies for location and line-of-sight tracking. Finally, the authors suggest a workflow design for integrating the framework into bridge inspection reporting software.", (C) 2014 American Society of Civil Engineers.,"Akula, Manu|Sandur, Atul|Kamat, Vineet R.|Prakash, Atul",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,bridge inspection|context-aware computing|contextual object identification|gps coordinates|simulation|uncertainty,10.1061/(ASCE)CP.1943-5487.0000292
10,WOS:000330916900008,2014,Detecting the causes of ill-conditioning in structural finite element models,,"In , version . of the finite element-based structural analysis package Oasys GSA was released. A new feature in this release was the estimation of the -norm condition number kappa() (K)= parallel to K parallel to()parallel to K- parallel to() of the stiffness matrix K of structural models by using a -norm estimation algorithm of Higham and Tisseur to estimate parallel to K- parallel to(). The condition estimate is reported as part of the information provided to engineers when they carry out linear/static analysis of models and a warning is raised if the condition number is found to be large. The inclusion of this feature prompted queries from users asking how the condition number impacted the analysis and, in cases where the software displayed an ill conditioning warning, how the ill conditioning could be ""fixed"". We describe a method that we have developed and implemented in the software that enables engineers to detect sources of ill conditioning in their models and rectify them. We give the theoretical background and illustrate our discussion with real-life examples of structural models to which this tool has been applied and found useful. Typically, condition numbers of stiffness matrices reduce from O(()) for erroneous models to O(()) or less for the corrected model. (C) ", Elsevier Ltd. All rights reserved.,"Kannan, Ramaseshan|Hendry, Stephen|Higham, Nicholas J.|Tisseur, Francoise",COMPUTERS & STRUCTURES,ill-conditioning|condition number|stiffness matrix|structural analysis|modelling errors,10.1016/j.compstruc.2013.11.014
11,WOS:000340977000075,2014,Groundwater fluxes in a shallow seasonal wetland pond: The effect of bathymetric uncertainty on predicted water and solute balances,MASS-BALANCE|DEPENDENT ECOSYSTEMS|LAKES|MODEL|DISCHARGE|STORAGE|RN-222|VOLUME|CALIBRATION|AUSTRALIA,"The successful management of groundwater dependent shallow seasonal wetlands requires a sound understanding of groundwater fluxes. However, such fluxes are hard to quantify. Water volume and solute mass balance models can be used in order to derive an estimate of groundwater fluxes within such systems. This approach is particularly attractive, as it can be undertaken using measurable environmental variables, such as; rainfall, evaporation, pond level and salinity. Groundwater fluxes estimated from such an approach are subject to uncertainty in the measured variables as well as in the process representation and in parameters within the model. However, the shallow nature of seasonal wetland ponds means water volume and surface area can change rapidly and non-linearly with depth, requiring an accurate representation of the wetland pond bathymetry. Unfortunately, detailed bathymetry is rarely available and simplifying assumptions regarding the bathymetry have to be made. However, the implications of these assumptions are typically not quantified. We systematically quantify the uncertainty implications for eight different representations of wetland bathymetry for a shallow seasonal wetland pond in South Australia. The predictive uncertainty estimation methods provided in the Model-Independent Parameter Estimation and Uncertainty Analysis software (PEST) are used to quantify the effect of bathymetric uncertainty on the modelled fluxes. We demonstrate that bathymetry can be successfully represented within the model in a simple parametric form using a cubic Sexier curve, allowing an assessment of bathymetric uncertainty due to measurement error and survey detail on the derived groundwater fluxes compared with the fixed bathymetry models. Findings show that different bathymetry conceptualisations can result in very different mass balance components and hence process conceptualisations, despite equally good fits to observed data, potentially leading to poor management decisions for the wetlands. Model predictive uncertainty increases with the crudity of the bathymetry representation, however, approximations that capture the general shape of the wetland pond such as a power law or Bezier curve show only a small increase in prediction uncertainty compared to the full dGPS surveyed bathymetry, implying these may be sufficient for most modelling purposes.", (C) 2014 Elsevier B.V. All rights reserved.,"Trigg, Mark A.|Cook, Peter G.|Brunner, Philip",JOURNAL OF HYDROLOGY,wetland ponds|bathymetry|uncertainty|pest|solute balance|bezier curve,10.1016/j.jhydrol.2014.06.020
12,WOS:000369512800012,2016,Integration of a Three-Dimensional Process-Based Hydrological Model into the Object Modeling System,JGRASS-NEWAGE SYSTEM|DISTRIBUTED MODEL|FRAMEWORK|ENERGY|BUDGETS|BASIN|WATER|TIME|FLOW|TERRAIN,"The integration of a spatial process model into an environmental modeling framework can enhance the model's capabilities. This paper describes a general methodology for integrating environmental models into the Object Modeling System (OMS) regardless of the model's complexity, the programming language, and the operating system used. We present the integration of the GEOtop model into the OMS version . and illustrate its application in a small watershed. OMS is an environmental modeling framework that facilitates model development, calibration, evaluation, and maintenance. It provides innovative techniques in software design such as multithreading, implicit parallelism, calibration and sensitivity analysis algorithms, and cloud-services. GEOtop is a physically based, spatially distributed rainfall-runoff model that performs three-dimensional finite volume calculations of water and energy budgets. Executing GEOtop as an OMS model component allows it to: () interact directly with the open-source geographical information system (GIS) uDig-JGrass to access geo-processing, visualization, and other modeling components; and () use OMS components for automatic calibration, sensitivity analysis, or meteorological data interpolation. A case study of the model in a semi-arid agricultural catchment is presented for illustration and proof-of-concept. Simulated soil water content and soil temperature results are compared with measured data, and model performance is evaluated using goodness-of-fit indices. This study serves as a template for future integration of process models into OMS.",,"Formetta, Giuseppe|Capparelli, Giovanna|David, Olaf|Green, Timothy R.|Rigon, Riccardo",WATER,watershed model|environmental modeling framework|automatic calibration|software integration,10.3390/w8010012
13,WOS:000227883000033,2005,Application of the RICEWQ-VADOFT model for simulating the environmental fate of pretilachlor in rice paddies,WATER|SOIL|DEGRADATION|DISSIPATION|PESTICIDES|PARAMETERS|SOFTWARE|SURFACE,"No validated models in Europe are capable of simulating the environmental fate of pesticides under the specific conditions of rice fields. Rice water quality-vadose zone flow and transport (RICEWQ-VADOFT) is a model developed from the coupling of a surface runoff model (RICEWQ) and a vadose zone flow and transport model (VADOFT) for determining predicted environmental concentrations in paddy water and sediment, runoff, and groundwater. This study is intended to evaluate the capability of this model to simulate effectively the environmental fate of the herbicide pretilachlor in the paddy environment. A two-year field study conducted in a representative rice-cultivated area of northern Italy provided measured concentrations of pretilachlor in paddy water and sediment and also a limited number of observations on runoff losses. The model successfully predicted the water balance in the paddy field in both years. After limited calibration, the model predicted the fate of pretilachlor in paddy water and sediment with high accuracy. Agreement between predicted and measured concentrations of pretilachlor in both years was assessed statistically using several statistical indicators. For example, modeling efficiency (EF) values of . to . and . to . in paddy water and sediment, respectively, document the strong agreement between predicted and measured pesticide concentrations. The model predictions showed high agreement with the limited amount of measured runoff data in . The model predicted that no significant amounts of pretilachlor would leach below the top  cm of the soil, although no measured data were available to evaluate the predicted results. A sensitivity analysis of the model to variables controlling pesticide partitioning to paddy sediment (VBIND, depth for direct partitioning of pesticide to bed sediment; VMIX, mixing velocity by molecular diffusion) revealed that the predictions of pesticide leaching were influenced strongly by those variables. Generally the RICEWQ-VADOFT model is a useful modeling fool for pesticide risk assessment in rice paddies.",,"Karpouzas, DG|Ferrero, A|Vidotto, F|Capri, E",ENVIRONMENTAL TOXICOLOGY AND CHEMISTRY,risk assessment|pretilachlor|rice paddies|sensitivity analysis,10.1897/04-180R.1
14,WOS:000330720400013,2014,Effect of thermal-hydrogeological and borehole heat exchanger properties on performance and impact of vertical closed-loop geothermal heat pump systems,ENERGY-STORAGE|SENSITIVITY-ANALYSIS|POROUS-MEDIA|GROUND-WATER|TRANSPORT|SIMULATION|AQUIFER|CANADA|MODEL,"Ground-source geothermal systems are drawing increasing attention and popularity due to their efficiency, sustainability and being implementable worldwide. Consequently, design software and regulatory guidelines have been developed. Interaction with the subsurface significantly affects the thermal performance, sustainability, and impacts of such systems. Reviewing the related guidelines and the design software, room for improvement is evident, especially in regards to interaction with groundwater movement. In order to accurately evaluate the thermal effect of system and hydrogeological properties on a borehole heat exchanger, a fully discretized finite-element model is used. Sensitivity of the loop outlet temperatures and heat exchange rates to hydrogeological, system and meteorological factors (i.e. groundwater flux, thermal conductivity and volumetric heat capacity of solids, porosity, thermal dispersivity, grout thermal conductivity, background and inlet temperatures) are analyzed over -month and -year operation periods. Furthermore, thermal recovery during  years after system decommissioning has been modeled. The thermal plume development, transport and dissipation are also assessed. This study shows the importance of subsurface thermal conductivity, groundwater flow (flux > (-) m/s), and background and inlet temperature on system performance and impact. It also shows the importance of groundwater flow (flux > (-) m/s) on thermal recovery of the ground over other factors.",,"Dehkordi, S. Emad|Schincariol, Robert A.",HYDROGEOLOGY JOURNAL,groundwater flow|temperature|thermal conditions|thermal conductivity|thermal recovery,10.1007/s10040-013-1060-6
15,WOS:000414818000049,2017,"An optimization model for collection, haul, transfer, treatment and disposal of infectious medical waste: Application to a Greek region",SOLID-WASTE|TRANSPORTATION|MANAGEMENT|SYSTEM|ALGORITHM|ROUTES|SITE,"The objective of this work was to develop an optimization model to minimize the cost of a collection, haul, transfer, treatment and disposal system for infectious medical waste (IMW). The model calculates the optimum locations of the treatment facilities and transfer stations, their design capacities (t/d), the number and capacities of all waste collection, transport and transfer vehicles and their optimum transport path and the minimum IMW management system cost. Waste production nodes (hospitals, healthcare centers, peripheral health offices, private clinics and physicians in private practice) and their IMW production rates were specified and used as model inputs. The candidate locations of the treatment facilities, transfer stations and sanitary landfills were designated, using a GIS-based methodology. Specifically, Mapinfo software with exclusion criteria for non-appropriate areas was used for siting candidate locations for the construction of the treatment plant and calculating the distance and travel time of all possible vehicle routes. The objective function was a non-linear equation, which minimized the total collection, transport, treatment and disposal cost. Total cost comprised capital and operation costs for: () treatment plant, () waste transfer stations, () waste transport and transfer vehicles and () waste collection bins and hospital boxes. Binary variables were used to decide whether a treatment plant and/or a transfer station should be constructed and whether a collection route between two or more nodes should be followed. Microsoft excel software was used as installation platform of the optimization model: For the execution of the optimization routine, two completely different software were used and the results were compared, thus, resulting in higher reliability and validity of the results. The first software was Evolver, which is based on the use of genetic algorithms. The second one was Crystal Ball, which is based on Monte Carlo simulation. The model was applied to the Region of East Macedonia Thrace in Greece. The optimum solution resulted in one treatment plant located in the sanitary landfill area of Chrysoupolis, required no transfer stations and had a total management cost of , (sic)/month or  (sic)/t. If a treatment plant is sited in the most eastern part of the Region, i.e., the industrial area of Alexandroupolis, the optimum solution would result in a transfer station of  m(), located near Kavala General Hospital, and a total cost of , (sic)/month or  (sic)/t. A sensitivity analysis was conducted and two alternative scenarios were optimized. In the first scenario, a % rise in fuel cost and in the second scenario a % rise in IMW production were considered. At the end, a cost calculation in (sic)/t/km for every type of vehicle used for haul and transfer was conducted. Also, the cost of the whole system was itemized and calculated in (sic)/t/km and EX The results showed that the higher percentage of the total cost was due to the construction of the treatment plant. (C) ", Elsevier Ltd. All rights reserved.,"Mantzaras, Gerasimos|Voudrias, Evangelos A.",WASTE MANAGEMENT,infectious medical waste management|optimization model|cost minimization|transfer stations|infectious medical waste treatment,10.1016/j.wasman.2017.08.037
16,WOS:000315974500021,2013,Distributed computation of large scale SWAT models on the Grid,PERSPECTIVES|SYSTEMS|TOOL,"The increasing interest in larger spatial and temporal scale models and high resolution input data processing comes at a price of higher computational demand. This price is evidently even higher when common modeling routines such as calibration and uncertainty analysis are involved. Likewise, methods and techniques for reducing computation time in large scale socio-environmental modeling software is growing. Recent advancements in distributed computing such as Grid infrastructure have provided further opportunity to this effort. In the interest of gaining computational efficiency, we developed generic tools and techniques for enabling the Soil and Water Assessment Tool (SWAT) model application to run on the EGEE (Enabling Grids for E-science projects in Europe) Grid. Various program components/scripts were written to split a large scale hydrological model of the Soil and Water Assessment Tool (SWAT), to submit the split models to the Grid, and to collect and merge results into single output format. A three-step procedure was applied to take advantage of the Grid. Firstly, a python script was run in order to split the SWAT model into several sub-models. Then, individual sub-models were submitted in parallel for execution on the Grid. Finally, the outputs of the sub-basins were collected and the reach routing process was performed with another script executing a modified SWAT program. We conducted experimental simulations with multiple temporal and spatial scale hydrological models on the Grid infrastructure. Results showed that, in spite of computing overheads, parallel computation of socio-environmental models on the Grid is beneficial for model applications especially with large spatial and temporal scales. In the end, we conclude by recommending methods for further reducing computational overheads while running large scale model applications on the Grid. (c) ", Elsevier Ltd. All rights reserved.,"Yalew, S.|van Griensven, A.|Ray, N.|Kokoszkiewicz, L.|Betrie, G. D.",ENVIRONMENTAL MODELLING & SOFTWARE,distributed computing|grid computing|hydrological models|swat|gridification,10.1016/j.envsoft.2012.08.002
17,WOS:000391079300073,2016,"Cradle-to-gate Life Cycle Assessment of traditional gravel ballasted, white reflective, and vegetative roofs: A Lebanese case study",EXTENSIVE GREEN ROOFS|STORMWATER RETENTION|SYSTEMS|ENVIRONMENT|SURFACE|RUNOFF|DEPTH|SLOPE,"Lebanon, a Mediterranean country, lacks a clear sustainability plan as well as an infrastructure update and management, leading to road flooding, especially in urban areas. Therefore, the installation of Vegetative Roofs (VRs) could be an interesting option for Lebanon. To evaluate if VRs are truly superior to Traditional Gravel Ballasted Roofs (TGBRs) and White Reflective Roofs (WRRs), a cradle-to-gate Life Cycle Assessment (LCA) was performed. Potential environmental impacts of an existing Extensive Green Roof (EGR) were compared to three fictitious roofs of the same area: TGBR, WRR, and Intensive Green Roof (IGR). The functional unit used for comparison was: ""providing a cover for a surface area of  sqm and for  years"". Specifications of TGBRs and WRRs were provided by local technicians and civil engineers. Furthermore, specifications of VRs were provided by the United Nations Development Program (UNDP), Country Energy Efficiency and Renewable Energy Demonstration for the recovery of Lebanon (CEDRO) project The SimaPro software and Ecoinvent library were used to model the systems considered. Results clearly indicated that EGR was the best option for all environmental impact categories. Rebar, concrete, and thermal insulation were the main contributors to the environmental impacts for TGBR, while rebar, concrete, thermal insulation, and waterproof membrane were the highest contributors for WRR. Sensitivity and uncertainty analysis were also performed to verify the robustness of the results. (C) ", Elsevier Ltd. All rights reserved.,"El Bachawati, Makram|Manneh, Rima|Belarbi, Rafik|Dandres, Thomas|Nassab, Carla|El Zakhem, Henri",JOURNAL OF CLEANER PRODUCTION,life cycle assessment|vegetative roofs|reflective roofs|traditional roofs|lebanon,10.1016/j.jclepro.2016.07.170
18,WOS:000187422700003,2004,MULINO-DSS: a computer tool for sustainable use of water resources at the catchment scale,EXAMPLE|DESIGN|MODELS,"MULINO, an ongoing project financed by the European Commission, has released the prototype of a Decision Support System software (mDSS) for the sustainable management of water resources at the catchment scale. The software integrates socio-economic and environmental modelling, with geo-spatial information and multi-criteria analysis. The policy background refers to the EU Water Framework Directive. The challenging multi-disciplinary context was approached by developing an innovative and dynamic implementation of the DPSIR framework, originally proposed by the European Environmental Agency. In mDSS integrated assessment modelling provides the values of quantitative indicators to be used for transparent and participated decisions, through the application of value functions, weights and decision rules chosen by the end user. Simple routines for the sensitivity analysis and comparison of alternative weight vectors also provides effective decision support by exploring and finding compromises between conflicting interests/perspectives in a multi-stakeholder context.", (C) 2003 Published by Elsevier B.V on behalf of IMACS.,"Giupponi, C|Mysiak, J|Fassio, A|Cogan, ",MATHEMATICS AND COMPUTERS IN SIMULATION,dss software|water resources|sustainable use|catchment|modelling,10.1016/j.matcom.2003.07.003
19,WOS:000188167100002,2003,Uncertainty analysis of hydrologic and water quality predictions for a small watershed using SWAT2000,SOIL HYDRAULIC-PROPERTIES|RECHARGE RATE ESTIMATION|SPATIAL VARIABILITY|ARID ENVIRONMENTS|SOLUTE TRANSPORT|RISK-ASSESSMENT|PART 1|MODEL|CONSTRAINTS|VARIABLES,"Hydrologic and water quality (H/WQ) models are being used with increasing frequency to devise alternative pollution control strategies. It has been recognized that such models may have a large degree of uncertainty associated with their predictions, and that this uncertainty can significantly impact the utility of the model. In this study, ARRAMIS (Advanced Risk & Reliability Assessment Model) software package was used to analyze the uncertainty of the SWAT (Soil and Water Assessment Tool) outputs concerning nutrients and sediment losses from agricultural lands. ARRAMIS applies Monte Carlo simulation technique connected with Latin hypercube sampling (LHS) scheme. This technique is applied to the Warner Creek watershed located in the Piedmont physiographic region of Maryland, and it provides an interval estimate of a range of values with an associated probability instead of a point estimate of a particular pollutant constituent. Uncertainty of model outputs was investigated using LHS scheme with restricted pairing for the model input sampling. Probability distribution functions (pdfs) for each of the  model simulations were constructed from these results. Model output distributions of interest in this analysis were stream flow, sediment, organic nitrogen (organic-N), organic phosphorus (organic-P), nitrate, ammonium, and mineral phosphorus (mineral-P) transported with water. Developed probability distribution functions for the model provided information with desirable probability. Results indicate that consideration of input parameter uncertainty produces % less mean stream flow along with approximately .% larger sediment loading than obtained using mean input parameters. On the contrary, mean of outputs regarding nutrients such as nitrate, ammonia, organic-N, and organic-P (but not mineral-P) were almost the same as the one using mean input parameters. The uncertainty in predicted stream flow and sediment loading is large, but that for nutrient loadings is the same as that of the corresponding input parameters. This study concluded that using a best possible distribution for the input parameters to reflect the impact of soils and land use diversity in a small watershed on SWAT model outputs may be more accurate than using average values for each input parameter.",,"Sohrabi, TM|Shirmohammadi, A|Chu, TW|Montas, H|Nejadhashemi, AP",ENVIRONMENTAL FORENSICS,uncertainty|swat2000|latin hypercube sampling|monte carlo|nonpoint pollution|nutrient,10.1080/714044368
20,WOS:000269046900012,2009,IPH-TRIM3D-PCLake: A three-dimensional complex dynamic model for subtropical aquatic ecosystems,,"This paper presents IPH-TRIMD-PCLake, a three-dimensional complex dynamic model for subtropical aquatic ecosystems. It combines a spatially explicit hydrodynamic model with a water-quality and biotic model of ecological interactions. The software, which is freely available for research purposes, has a graphical user-friendly interface and a flexible design that allows the user to vary the complexity of the model. It also has built-in analysis tools such as Monte Carlo sensitivity analysis, a genetic algorithm for calibration, and plotting tools. (C) ", Elsevier Ltd. All rights reserved.,"Fragoso, Carlos R.|van Nes, Egbert H.|Janse, Jan H.|Marques, David da Motta",ENVIRONMENTAL MODELLING & SOFTWARE,aquatic ecosystem model|cascading trophic effects|subtropical ecosystems|3d model,10.1016/j.envsoft.2009.05.006
21,WOS:000178643700005,2002,A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data,SYSTEMS,"In the construction of mathematical models from experimental data, it is possible to determine equations that model relations using several methodologies [Un nuevo algoritmo para la modelizacion de sistemas altamente estructurados (); Env. Model. Software  () ; Guide to Statistics , (); Regression models (); Ecosystems and Sustainable Development II ()]. These methodologies build equations that are in line with the experimental data and they analyse a dimension of adjustment and a dimension of error or distance between the experimental data and the data that is produced by the model. There are studies of sensitivity of the sample of data, as found by Bolado and Alonso [Proceedings SAMO ]. The authors consider that it is useful to obtain new parameters that relate the sensitivity of the equations to the variations that are produced by the experimental data. This will allow the selection of the model according to new criteria. On the one hand, the authors present a theoretical study of sensitivity of the models according to different points of view. On the other hand, they discuss a computing algorithm that allows the analysis of sensitivity (and stability) of the mathematical equations, which are built from any methodology. An interface has been incorporated into this algorithm to allow a graphic visualisation of the effects that are produced when modifications of the model are carried out.", (C) 2002 Elsevier Science Ltd. All rights reserved.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,sensitivity analysis|environmental modelling|monte carlo,10.1016/S0965-9978(02)00023-6
22,WOS:000281772900010,2010,INTEGRATION OF UNCERTAINTIES INTO INTERNAL CONTAMINATION MONITORING,IDEAS GUIDELINES|DOSIMETRY,"Potential internal contaminations of workers are monitored by periodic bioassays interpreted in terms of intake and committed effective dose through biokinetic and dosimetric models. After a prospective evaluation of exposure at a workplace, a suitable monitoring program can be defined by the choice of measurement techniques and frequency of measurements. However, the actual conditions of exposure are usually not well defined and the measurements are subject to errors. In this study we took into consideration the uncertainties associated with a routine monitoring program in order to evaluate the minimum intake and dose detectable for a given level of confidence. Major sources of uncertainty are the contamination time, the size distribution and absorption into blood of the incorporated particles, and the measurement errors. Different assumptions may be applied to model uncertain knowledge, which lead to different statistical approaches. The available information is modeled here by classical or Bayesian probability distributions. These techniques are implemented in the OPSCI software under development. This methodology was applied to the monitoring program of workers in charge of plutonium purification at the AREVA NC reprocessing facility (La Hague, France). A sensitivity analysis was carried out to determine the important parameters for the minimum detectable dose. The methods presented here may be used for assessment of any other routine monitoring program through the comparison of the minimum detectable dose for a given confidence level with dose constraints. Health Phys. ():-; ",,"Davesne, E.|Casanova, P.|Chojnacki, E.|Paquet, F.|Blanchardon, E.",HEALTH PHYSICS,computer calculations|contamination|internal|effective dose|plutonium,10.1097/HP.0b013e3181cd3d47
23,WOS:000307721100006,2012,Evolutionary topology optimization of periodic composites for extremal magnetic permeability and electrical permittivity,LEVEL-SET|STRUCTURAL OPTIMIZATION|DESIGN|HOMOGENIZATION|METAMATERIALS|SHAPE|MICROSTRUCTURES,"This paper presents a bidirectional evolutionary structural optimization (BESO) method for designing periodic microstructures of two-phase composites with extremal electromagnetic permeability and permittivity. The effective permeability and effective permittivity of the composite are obtained by applying the homogenization technique to the representative periodic base cell (PBC). Single or multiple objectives are defined to maximize or minimize the electromagnetic properties separately or simultaneously. The sensitivity analysis of the objective function is conducted using the adjoint method. Based on the established sensitivity number, BESO gradually evolves the topology of the PBC to an optimum. Numerical examples demonstrate that the electromagnetic properties of the resulting D and D microstructures are very close to the theoretical Hashin-Shtrikman (HS) bounds. The proposed BESO algorithm is computationally efficient as the solution usually converges in less than  iterations. The proposed BESO method can be implemented easily as a post-processor to standard commercial finite element analysis software packages, e.g. ANSYS which has been used in this study. The resulting topologies are clear black-and-white solutions (with no grey areas). Some interesting topological patterns such as Vigdergauz-type structure and Schwarz primitive structure have been found which will be useful for the design of electromagnetic materials.",,"Huang, X.|Xie, Y. M.|Jia, B.|Li, Q.|Zhou, S. W.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,topology optimization|bidirectional evolutionary structural optimization (beso)|homogenization|effective permeability|effective permittivity,10.1007/s00158-012-0766-8
24,WOS:000347153700005,2015,Carbon footprint of milk production in Brazil: a comparative case study,LIFE-CYCLE ASSESSMENT|DAIRY FARMS|ENVIRONMENTAL ASSESSMENT|NEW-ZEALAND|SYSTEMS|METHANE|PERFORMANCE|EFFICIENCY|SCENARIOS|EMISSION,"Livestock production is a recognized source of environmental impact, and this sector indirectly involves approximately  million people in Brazil. Livestock production includes nearly . million milk producers that use several different production systems. We chose the southern region of Brazil to evaluate the carbon footprint (CF) per  kg of energy-corrected milk (ECM) at the farm gate for different dairy production systems with the use of a good level of technology. The dairy production systems were confined feedlot system, semi-confined feedlot system (including some grazing), and pasture-based grazing system. A sensitivity analysis of the dry matter intake (DMI) in each farming system and an uncertainty analysis based on a Monte Carlo (MC) simulation were performed to complement the discussion. The standards ISO :  and ISO :  were used for the comparative life cycle assessment (LCA) focused on the CF. The LCA software tool SimaPro .. was used. Sensitivity analyses were conducted on input data for total digestible nutrients (TDN) and crude protein (CP) based on values from the literature. The comparative LCA showed that the confined feedlot system had a lower CF than the other systems studied. Total greenhouse gas emissions were . kg CO()e kg ECM- for the confined feedlot system, . kg CO()e kg ECM- for the semi-confined feedlot system, and . kg CO()e kg ECM- for the pasture-based system without considering the impact from direct land use change (dLUC). When considering these emissions, the CFs for grain and cottonseed production showed CF increases of ., ., and . % for the confined feedlot, semi-confined feedlot, and pasture-based systems, respectively. The results from the MC simulations showed low uncertainty through variations in TDN and CP. The coefficient of variation was . % for the confined feedlot, . % for the semi-confined feedlot, and . % for the pasture systems. The uncertainties were due mainly to variations in NO emissions from manure for the three systems. The CF in Brazilian systems was lower than almost all the results found in the literature, even when impacts from the dLUC were considered. The lowest CF in this case study was due mainly to the emission factor used for enteric fermentation.",,"de Leis, Cristiane Maria|Cherubini, Edivan|Ruviaro, Clandio Favarini|da Silva, Vamilson Prudencio|Lampert, Vinicius do Nascimento|Spies, Airton|Soares, Sebastiao Roberto",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,brazilian milk production|carbon footprint|confined feedlot system|direct land use change|life cycle assessment|pasture system|semi-confined feedlot system,10.1007/s11367-014-0813-3
25,WOS:000321425100048,2013,Life cycle assessment of the production of hydrogen and transportation fuels from corn stover via fast pyrolysis,LAND-USE CHANGE|BIOMASS FAST PYROLYSIS|BIO-OIL|TECHNOECONOMIC ANALYSIS|ETHANOL-PRODUCTION|AQUEOUS FRACTION|COMBUSTION|CATALYSTS|BIOFUELS|SWITCHGRASS,"This life cycle assessment evaluates and quantifies the environmental impacts of the production of hydrogen and transportation fuels from the fast pyrolysis and upgrading of corn stover. Input data for this analysis come from Aspen Plus modeling, a GREET (Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation) model database and a US Life Cycle Inventory Database. SimaPro . software is employed to estimate the environmental impacts. The results indicate that the net fossil energy input is . MJ and . MJ per km traveled for a light-duty vehicle fueled by gasoline and diesel fuel, respectively. Bio-oil production requires the largest fossil energy input. The net global warming potential (GWP) is . kg CO()eq and . kg CO()eq per km traveled for a vehicle fueled by gasoline and diesel fuel, respectively. Vehicle operations contribute up to % of the total positive GWP, which is the largest greenhouse gas footprint of all the unit processes. The net GWPs in this study are % and % lower than for petroleum-based gasoline and diesel fuel ( baseline), respectively. Biomass transportation has the largest impact on ozone depletion among all of the unit processes. Sensitivity analysis shows that fuel economy, transportation fuel yield, bio-oil yield, and electricity consumption are the key factors that influence greenhouse gas emissions.",,"Zhang, Yanan|Hu, Guiping|Brown, Robert C.",ENVIRONMENTAL RESEARCH LETTERS,life cycle assessment|fast pyrolysis|bio-oil upgrading|greenhouse gas emission|energy demand,10.1088/1748-9326/8/2/025001
26,WOS:000306076900004,2012,A GENERAL TECHNIQUE FOR COUPLING TWO ARBITRARY METHODS IN STRESS ANALYSIS,ELEMENT-FREE GALERKIN|FINITE-ELEMENT|BOUNDARY|BEM|FEM,"In this paper, a general technique for coupling two arbitrary methods is presented. The problem domain is decomposed into two sub-domains. Afterwards, a sensitivity analysis at the interface of each sub-domain is carried out. Sensitivity matrices of the two sub-domains are used to find the coupling matrix equation. Unknowns at the interface are then found by solving the equations. The size of the matrix equation is very small in comparison with coefficient matrices of each sub-domain. The present method allows the black box coupling of different methods, even commercial software without having access to the matrices created by the methods.",,"Hematiyan, Mohammad Rahim|Khosravifard, Amir|Mohammadi, Mehrdad",INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS,fem|bem|mesh-free methods|radial point interpolation method|coupling,10.1142/S0219876212400270
27,WOS:000251683400002,2007,Anisotropic finite element modeling for patient-specific mandible,AUTOMATIC MESH GENERATION|MEDICAL IMAGES|QUALITY ASSESSMENT|CANCELLOUS BONE|CORTICAL BONE|CT|ALGORITHM|CAD|SEGMENTS|NUMBERS,"This paper presents an ad hoc modular software tool to quasi-automatically generate patient-specific three-dimensional (D) finite element (FE) model of the human mandible. The main task is taking into account the complex geometry of the individual mandible, as well as the inherent highly anisotropic material law. At first, by computed tomography data (CT), the individual geometry of the complete range of mandible was well reproduced, also the separation between cortical and cancellous bone. Then, taking advantage of the inherent shape nature as 'curve' long bone, the algorithm employed a pair of B-spline curves running along the entire upper and lower mandible borders as auxiliary baselines, whose directions are also compatible with that of the trajectory of maximum material stiffness throughout the cortical bone of the mandible. And under the guidance of this pair of auxiliary baselines, a sequence of B-spline surfaces were interpolated adaptively as curve cross-sections to cut the original geometry. Following, based on the produced curve contours and the corresponding curve cross-section surfaces, quite well structured FE volume meshes were constructed, as well as the inherent trajectory vector fields of the anisotropic material (orthotropic for cortical bone and transversely isotropic for cancellous bone). Finally, a sensitivity analysis comprising various D FE simulations was carried out to reveal the relevance of elastic anisotropy for the load carrying behavior of the mandible.", (C) 2007 Elsevier Ireland Ltd. All rights reserved.,"Liao, Sheng-Hui|Tong, Ruo-Feng|Dong, Jin-Xiang",COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,anisotropic material|automatic mesh generator (amg)|complete mandible model|patient-specific|finite element method,10.1016/j.cmpb.2007.09.009
28,WOS:000233938100033,2005,Utility of dynamic-landscape metapopulation models for sustainable forest management,BIRDS|INDICATORS|VIABILITY|FRAGMENTATION|BIODIVERSITY|FIRE,"We evaluated the utility of combining metapopulation models with landscape-level forest-dynamics models to assess the sustainability of forest management practices. We used the Brown Creeper (Certhia americana) in the boreal forests of northern Ontario as a case study. We selected the Brown Creeper as a potential indicator of sustainability because it is relatively common in the region but is dependent on snags and old trees for nesting and foraging; hence, it may be sensitive to timber harvesting. For the modeling we used RAMAS Landscape, a software package that integrates RAMAS GIS, population-modeling software, and LANDIS, forest-dynamics modeling software. Predictions about the future floristic composition and structure of the landscape tinder a variety of management and natural disturbance scenarios were derived using LANDIS. We modeled eight alternative forest management scenarios, ranging in intensity from no timber harvesting and a natural fire regime to intensive timber harvesting with salvage logging after fire. We predicted the response of The Brown Creeper metapopulation over a -year period and used future population size and expected minimum population size to compare the sustainability of the various management scenarios. The modeling methods were easy to apply and model predictions were sensitive to the differences among management scenarios, indicating that these methods may be useful for assessing and ranking the sustainability of forest management options. Primary concerns about the method are the practical difficulties associated with incorporating fire stochasticity in prediction uncertainty and the number of model assumptions that must be made and tested with sensitivity analysis. We wrote new software to bell) quantify the contribution of landscape stochasticity to model prediction uncertainty.",,"Wintle, BA|Bekessy, SA|Venier, LA|Pearce, JL|Chisholm, RA",CONSERVATION BIOLOGY,brown creeper|landscape ecology|population model|population viability analysis|succession model,10.1111/j.1523-1739.2005.00276.x
29,WOS:000366034200088,2015,Comprehensive Sensitivity Analysis in NLP Models in PSE Applications Using Space-Filling DOE Strategy,OPTIMIZATION|DESIGN,"Sensitivity analysis is an integral step in the interpretation of the solutions of optimization models, particularly when there are uncertainties in the numerical values of model parameters. Conventional approaches to sensitivity analysis rely on the use of shadow prices in linear models and Lagrange multipliers in non-linear models. Modern commercial optimization software packages are able to automatically generate such sensitivity coefficients to allow rapid post-optimality analysis. However, in the case of non-linear models, Lagrange multipliers have two distinct limitations. First, they represent only changes in the optimal value of an objective function with respect to small changes in parameter values, and thus remain valid only near the immediate vicinity of the nominal design point. Secondly, each Lagrange multiplier gives only the effect of the change of one parameter, assuming that all other parameters remain at their nominal values. Hence, they provide no information about joint effects or interactions caused by simultaneous changes in parameter values. In this paper, we present a strategy based on design of experiments (DOE) to generate a sensitivity surface, which we define as the mapping of the optimal model solution against a range of values of the optimization model parameters. Space-filling designs are used as a basis to generate proxy regression models with quadratic and interaction terms, in order to capture curvature of the sensitivity surface. The resulting proxy model contains more information than is available in conventional sensitivity analysis. In particular, this approach shows curvature and interaction effects that are not reflected when Lagrange multipliers are used. We present case studies based on problems drawn from process systems engineering (PSE) literature to illustrate this comprehensive sensitivity analysis strategy.",,"Tan, Raymond R.|Aviso, Kathleen B.|Uy, Oscar M.|Varbanov, PS|Klemes, JJ|Alwi, SRW|Yong, JY|Liu, ","PRES15: PROCESS INTEGRATION, MODELLING AND OPTIMISATION FOR ENERGY SAVING AND POLLUTION REDUCTION",,10.3303/CET1545088
30,WOS:000382269000125,2016,Irrigation water demand of selected agricultural crops in Germany between 1902 and 2010,LEAF-AREA INDEX|CLIMATE-CHANGE|IMPACT ASSESSMENT|NORTHERN GERMANY|HUMID CLIMATE|REQUIREMENTS|EUROPE|AVAILABILITY|ENGLAND|MODEL,"Irrigation water demand (IWD) is increasing worldwide, including in regions such as Germany that are characterized with low precipitation levels, yet grow water-demanding crops such as sugar beets, potatoes, and vegetables. This study aimed to calculate and analyze the spatial and temporal changes in the IWD of four crops spring barley, oat, winter wheat, and potato between  and  in Germany by using the modeling software AgroHyd Farmmodel. Climatic conditions in Germany continued to change over the investigation period, with an increase in temperature of . K/yr and an increase in precipitation of  mm/yr. Nevertheless, no significant increasing or decreasing trend in IWD was noted in the analysis. The IWD for the investigated crops in the area of the current ""Federal Republic of Germany"" over the  years was  mm/yr, varying between  and  mm/yr. Changes in cropping pattern and cultivated area over the last century caused large differences in the IWD calculated for each administrative district. The mean annual IWD of over the study period (which was divided into  parts) varied between , Mm()/yr in the earliest period (-) and  Mm()/yr in the latest period (-). Policy and management measures to adapt to climate change are currently being debated in Germany. The presented results suggest that the effects of the choice of crops (in this case, changes in cropping pattern in the German nation states) had a stronger influence on regional water resources than those of climate variability. Thus, the influence of climate change on water resources is relativized which brings an important input into the debate.", (C) 2016 Elsevier BM. All rights reserved.,"Drastig, Katrin|Prochnow, Annette|Libra, Judy|Koch, Hagen|Rolinski, Susanne",SCIENCE OF THE TOTAL ENVIRONMENT,in igation water demand|inigation trend|agrohyd fanninodel,10.1016/j.scitotenv.2016.06.206
31,WOS:000184050100008,2003,Adaptive algorithms for optimal control of time-dependent partial differential-algebraic equation systems,SENSITIVITY ANALYSIS|SOFTWARE,"This paper describes an adaptive algorithm for optimal control of time-dependent partial differential-algebraic equation (PDAE) systems. A direct method based on a modified multiple shooting type technique and sequential quadratic programming (SQP) is used for solving the optimal control problem, while an adaptive mesh refinement (AMR) algorithm is employed to dynamically adapt the spatial integration mesh. Issues of coupling the AMR solver to the optimization algorithm are addressed. For time-dependent PDAEs which can benefit from the use of an adaptive mesh, the resulting method is shown to be highly efficient."," Copyright (C) 2003 John Wiley Sons, Ltd.","Serban, R|Li, ST|Petzold, LR",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,adaptive algorithm|partial differential-algebraic equation,10.1002/nme.786
32,WOS:000286101300010,2011,Application of life cycle assessment in the mining industry,CRADLE-TO-GATE|IMPACT ASSESSMENT|ENVIRONMENTAL-MANAGEMENT|UNCERTAINTY|BIODIVERSITY|INVENTORY|FRAMEWORK|LCA,"In spite of the increasing application of life cycle assessment (LCA) for engineering evaluation of systems and products, the application of LCA in the mining industry is limited. For example, a search in the Engineering Compendex database using the keywords ""life cycle assessment"" results in , results, but only  are related to the mining industry. Also, mining companies are increasingly adopting ISO  certified environmental management systems (EMSs). A key requirement of ISO certified EMSs is continual improvement, which can be better managed with life cycle thinking. This paper presents a review of the current application of LCA in the mining industry. It discusses the current application, the issues, and challenges and makes relevant recommendations for new research to improve the current situation. The paper reviews the major published articles in the literature pertaining to LCA methodology as applied in the mining industry. The challenges associated with LCA applications in mining are discussed next. Finally, the authors present recommended research areas to increase the application of LCA in the mining industry. The literature review shows a limited number of published mining LCA studies. The paper also shows the variation in functional unit definition for mining LCA studies. The challenges and research needed to address the problems are highlighted in the discussions. The limited number of mining LCAs may be due to the lack of life cycle thinking in the industry. The paper, however, highlights the major contributions in the literature to LCA practice in the mining industry. This paper discusses the lack of LCA awareness and tools for mining LCAs, issues relating to functional unit and scoping of mining product systems, defining adequate and appropriate impact categories, and challenges with uncertainty and sensitivity analysis. The authors recommend that future research focus on the development of a mining-specific LCA framework, data uncertainty characterization, and software development to increase the application of LCA in mining. LCA presents beneficial insights to the mining industry as it seeks to develop world-class EMSs and environmentally sustainable projects. However, to take full advantage of this technique, further research is necessary to improve the level of LCA application in mining. Major challenges have been identified, and recommended research areas have been proposed to improve the situation. The paper outlines the benefits of increased application of LCA in the mining industry to LCA databases and all practitioners. It is recommended that additional research be undertaken through industry-academia partnerships to develop a more rigorous mining-specific LCA framework. Such a framework should allow for sensitivity and uncertainty analysis while allowing for suitable data collection that still covers the temporal and spatial dimensions of mining. Research should also be carried out to develop objective ways of characterizing the uncertainty introduced in a LCA study due to the use of secondary data (emissions factors) from prior studies. Finally, new software or GUIs that address the peculiarities of mining should be developed to help mining professionals with basic LCA knowledge to undertake LCA studies of their systems and mines.",,"Awuah-Offei, Kwame|Adekpedjou, Akim",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,lca methodology|life cycle assessment|mining|sensitivity analysis|uncertainty analysis,10.1007/s11367-010-0246-6
33,WOS:000338763900008,2014,Development of a zoning-based environmental-ecological coupled model for lakes: a case study of Baiyangdian Lake in northern China,WATER-QUALITY|SENSITIVITY-ANALYSIS|ECOSYSTEM MODEL|WETLANDS|MANAGEMENT|RIVERS,"Environmental/ecological models are widely used for lake management as they provide a means to understand physical, chemical, and biological processes in highly complex ecosystems. Most research has focused on the development of environmental (water quality) and ecological models, separately. Limited studies were developed to couple the two models, and in these limited coupled models, a lake was regarded as a whole for analysis (i.e. considering the lake to be one well-mixed box), which is appropriate for small-scale lakes but is not sufficient to capture spatial variations within middle-scale or large-scale lakes. In response to this problem, this paper seeks to establish a zoning-based environmental-ecological coupled model for a lake. Hierarchical cluster analysis was adopted to determine the number of zones in a given lake based on hydrological, water quality, and ecological data analysis. The MIKE  model was used to construct -D hydrodynamics and water quality simulations. STELLA software was used to create a lake ecological model that can simulate the spatial variations of ecological condition based on flow field distribution results generated by MIKE . Baiyangdian Lake, the largest freshwater lake in northern China, was adopted as the study case. The results showed that the new model is promising for predicting spatial variations of ecological conditions in response to changes in lake water quantity and quality, and could be useful for lake management.",,"Zhao, Y. W.|Xu, M. J.|Xu, F.|Wu, S. R.|Yin, X. A.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-18-2113-2014
34,WOS:000349876500012,2015,"Understanding the Day Cent model: Calibration, sensitivity, and identifiability through inverse modeling",NITROUS-OXIDE EMISSIONS|EVALUATING PARAMETER IDENTIFIABILITY|DAYCENT ECOSYSTEM MODEL|SOIL ORGANIC-MATTER|ERROR REDUCTION|2 STATISTICS|AUTOMATIC CALIBRATION|PRODUCTION SYSTEMS|N2O EMISSIONS|WATER-FLOW,"The ability of biogeochemical ecosystem models to represent agro-ecosystems depends on their correct integration with field observations. We report simultaneous calibration of  DayCent model parameters using multiple observation types through inverse modeling using the PEST parameter estimation software. Parameter estimation reduced the total sum of weighted squared residuals by % and improved model fit to crop productivity, soil carbon, volumetric soil water content, soil temperature, NO, and soil NO- compared to the default simulation. Inverse modeling substantially reduced predictive model error relative to the default model for all model predictions, except for soil NO- and NH+. Post-processing analyses provided insights into parameter-observation relationships based on parameter correlations, sensitivity and identifiability. Inverse modeling tools are shown to be a powerful way to systematize and accelerate the process of biogeochemical model interrogation, improving our understanding of model function and the underlying ecosystem biogeochemical processes that they represent. (C)  The Authors. Published by", Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).,"Necpalova, Magdalena|Anex, Robert P.|Fienen, Michael N.|Del Grosso, Stephen J.|Castellano, Michael J.|Sawyer, John E.|Iqbal, Javed|Pantoja, Jose L.|Barker, Daniel W.",ENVIRONMENTAL MODELLING & SOFTWARE,daycent model|inverse modeling|pest|sensitivity analysis|parameter identifiability|parameter correlations,10.1016/j.envsoft.2014.12.011
35,WOS:000251865000019,2008,Synthetic microarray data generation with RANGE and NEMO,RECONSTRUCTION,"Motivation: For testing and sensitivity analysis purposes, it is beneficial to have known transcription networks of sufficient size and variability during development of microarray data and network deconvolution algorithms. Description of such networks in a simple language translatable to Systems Biology Markup Language would allow generation of model data for the networks. Results: Described herein is software (RANGE: RAndom Network GEnerator) to generate large random transcription networks in the NEMO (NEtwork MOtif) language. NEMO is recognized by a grammar for transcription network motifs using lex and yacc to output Systems Biology Markup Language models for either specified or randomized gene input functions. These models of known networks may be input to a biochemical simulator, allowing the generation of synthetic microarray data.",,"Long, James|Roth, Mitchell",BIOINFORMATICS,,10.1093/bioinformatics/btm529
36,WOS:000335707200019,2014,FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module,LATIN HYPERCUBE SAMPLES|REINFORCEMENT CORROSION|INPUT VARIABLES|CONCRETE|SIZE|SIMULATION|CARBONATION|DURABILITY|FRACTURE|MODEL,"The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of ""randomizing"" computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique - simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper. (C) ", Elsevier Ltd. All rights reserved.,"Novak, Drahomir|Vorechovsky, Miroslav|Teply, Bretislav",ADVANCES IN ENGINEERING SOFTWARE,statistical analysis|sensitivity|reliability|monte carlo simulation|latin hypercube sampling|simulated annealing|random fields|material degradation,10.1016/j.advengsoft.2013.06.011
37,WOS:000087626100009,2000,ECOP: an economic model to assess the willow short rotation coppice global profitability in a case of small scale gasification pathway in Belgium,WOOD,"This paper presents a software package developed to assess the economic profitability of an original way to produce renewable energy: the small scale gasification of willow cultivated as short rotation coppice (SRC) in Belgium. The theoretical bases of the model (process hypotheses and economic indicators) are firstly presented together with the most relevant characteristics of the energy production route (SRC management and wood production, storage and conversion). A reference case is then defined which combines the most influencing parameters (reference interest rate, rotation length, subsidies, harvest mode, SRC yield, power of the electricity generator and annual production of electricity). A sensitivity analysis on these parameters highlighted that the project profitability, from the net present value point of view, is very sensitive to the reference interest rate, to the subsidies (of the conversion unit but probably also of any other kind of subsidies), to the SRC yield and to the generator power, all other parameters remaining constant. The rotation length has only a low influence, at least in the range of classic values ( to  years). To harvest the wood in stems (with delayed chipping) seems also to be the most interesting option.", (C) 2000 Elsevier Science Ltd. All rights reserved.,"Goor, F|Jossart, JM|Ledent, JF",ENVIRONMENTAL MODELLING & SOFTWARE,,10.1016/S1364-8152(00)00014-1
38,WOS:000256840200073,2007,"MEDOR, a didactic tool to support interpretation of bioassay data after internal contamination by actinides",UNCERTAINTIES|INHALATION|(PUO2)-PU-239|WORKERS|MODEL|LUNG,"A didactic software, MEthodes DOsimetriques de REference (MEDOR), is being developed to provide help in the interpretation of biological data. Its main purpose is to evaluate the pertinence of the application of different models. This paper describes its first version that is focused on inhalation exposure to actinide aerosols. With this tool, sensitivity analysis on different parameters of the ICRP models can be easily done for aerosol deposition, in terms of activity and particle number, actinide biokinetics and doses. The user can analyse different inhalation cases showing either that dose per unit intake cannot be applied if the aerosol contains a low number of particles or that an inhibition of the late pulmonary clearance by particle transport can occur which contributes to a - fold increase in effective dose as compared with application of default parameters. This underlines the need to estimate systematically the number of deposited particles, as well as to do chest monitoring as long as possible.",,"Miele, A.|Blanchin, N.|Raynaud, P.|Quesne, B.|Giraud, J. M.|Fottorino, R.|Berard, P.|Ansoborlo, E.|Franck, D.|Blanchardon, E.|Vathaire, C. Challeton-de|Lebaron-Jacobs, L.|Poncy, J. L.|Piechowski, J.|Fritsch, P.",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm288
39,WOS:000287437100013,2011,"NCNA: Integrated platform for constructing, visualizing, analyzing and sharing human-mediated nitrogen biogeochemical networks",,"Human alterations to the nitrogen (N) cycle are closely associated with global environmental and climate change. New tools are necessary to model and analyze the highly complex N cycles emerging from human-mediated ecosystems. We developed a new software. NCNA, to provide three functions: a) rigorous reconstruction of quasi-empirical models (QEMs), b) computer-aided interface for data collection and automatic sensitivity analysis, and c) automatic generation, visualization and network environ analysis (NEA) of N cycling networks. (c) ", Elsevier Ltd. All rights reserved.,"Min, Yong|Gong, Wei|Jin, Xiaogang|Chang, Jie|Gu, Baojing|Han, Zhen|Ge, Ying",ENVIRONMENTAL MODELLING & SOFTWARE,quasi-empirical model|reconstruction|visualization|network environ analysis|urbanization,10.1016/j.envsoft.2010.11.002
40,WOS:000385907200039,2016,MINFIT: A Spreadsheet-Based Tool for Parameter Estimation in an Equilibrium Speciation Software Program,SURFACE COMPLEXATION MODELS|URANIUM(VI) ADSORPTION|CONSISTENT MODEL|TITRATION DATA|LAYER MODEL|SORPTION|OXIDE|MONTMORILLONITE|PHOSPHATE|HEMATITE,"Determination of equilibrium constants describing chemical reactions in the aqueous phase and at solid-water interface relies on inverse modeling and parameter estimation. Although there are existing tools available, the steep learning curve prevents the wider community of environmental engineers and chemists to adopt those tools. Stemming from classical chemical equilibrium codes, MINEQL+ has been one of the most widely used chemical equilibrium software programs. We developed a spreadsheet-based tool, which we are calling MINFIT, that interacts with MINEQL+ to perform parameter estimations that optimize model fits to experimental data sets. MINFIT enables automatic and convenient screening of a large number of parameter sets toward the optimal solutions by calling MINEQL+ to perform iterative forward calculations following either exhaustive equidistant grid search or randomized search algorithms. The combined use of the two algorithms can securely guide the searches for the global optima. We developed interactive interfaces so that the optimization processes are transparent. Benchmark examples including both aqueous and surface complexation problems illustrate the parameter estimation and associated sensitivity analysis. MINFIT is accessible at http://minfit.strikingly.com.",,"Xie, Xiongfei|Giammar, Daniel E.|Wang, Zimeng",ENVIRONMENTAL SCIENCE & TECHNOLOGY,,10.1021/acs.est.6b03399
41,WOS:000295845900012,2011,Groundwater drawdown at Nankou site of Beijing Plain: model development and calibration,FLOW,"Water shortage and groundwater pollution have become two primary environmental concerns to Beijing since the s. The local aquifers, as the dominant sources for domestic and agricultural water supply, are depleting due to groundwater abstraction and continuous drought in recent years with rapid urbanization and increasing water consumption. Therefore, understanding the hydrogeological system is fundamental for a sustainable water resources management. In this article, the numerical analysis of a -D regional groundwater flow model for the Nankou area is presented. The hydrogeological system is reproduced according to sparsely distributed boreholes data. The numerical analysis is carried out using the scientific software OpenGeoSys, which is based on the finite element method. The model calibration and sensitivity analysis are accomplished with inverse methods by applying a model independent parameter estimation system (PEST). The results of the calibrated model show reasonable agreements with observed water levels. The transient groundwater flow simulations reflect the observed drawdown of the last  years and show the formation of a depression cone in an intensively pumped area.",,"Sun, Feng|Shao, Haibing|Kalbacher, Thomas|Wang, Wenqing|Yang, Zhongshan|Huang, Zhenfang|Kolditz, Olaf",ENVIRONMENTAL EARTH SCIENCES,groundwater modeling|opengeosys|pest|nankou,10.1007/s12665-011-0957-4
42,WOS:000295845300022,2011,Modeling the effects of completion techniques and formation heterogeneity on CO2 sequestration in shallow and deep saline aquifers,STORAGE,"This work studied the effect of completion techniques and reservoir heterogeneity on CO storage and injectivity in saline aquifers using a compositional reservoir simulator, CMG-GEM. Two reservoir models were built based on the published data to represent a deep saline aquifer and a shallow aquifer. The effect of various completion conditions on CO storage was then discussed, including partial perforation of the reservoir net pay (partial completion), well geometry, orientation, location, and length. The heterogeneity effect was addressed by considering three parameters: mean permeability, the vertical to horizontal permeability ratio, and permeability variation. Sensitivity analysis was carried out using iSIGHT software (design of experiments) to determine the dominant factors affecting CO storage capacity and injectivity. Simulation results show that the most favorable option is the perforation of all layers with horizontal wells - m long set in the upper layers. Mean permeability has the most effect on CO storage capacity and injectivity; k(v)/k(h) affects CO injectivity storage capacity more than permeability variation, V-k. More CO can be stored in the heterogeneous reservoirs with low mean permeability; however, high injectivity can be achieved in the uniform reservoirs with high mean permeability.",,"Yang, Fang|Bai, Baojun|Dunn-Norman, Shari",ENVIRONMENTAL EARTH SCIENCES,co2 sequestration|saline aquifer|completion|heterogeneity|reservoir simulation,10.1007/s12665-011-0908-0
43,WOS:000358060800012,2015,A software development framework for structural optimization considering non linear static responses,DYNAMIC TOPOLOGY OPTIMIZATION|EQUIVALENT LOADS,"In the real world, structural systems may not have linear static characteristics. However, structural optimization has been developed based on static responses because sensitivity analysis regarding static finite element analysis is developed quite well. Analyses other than static analysis are heavily required in the engineering community these days. Techniques for such analyses have been extensively developed and many software systems using the finite element method are easily available in the market. On the other hand, development of structural optimization using such analyses is fairly slow due to many obstacles. One obstacle is that it is very difficult and expensive to consider the nonlinearities or dynamic effects in the way of conventional optimization. Recently, the equivalent static loads method for non linear static response structural optimization (ESLSO) has been proposed for structural optimization with various responses: linear dynamic response, nonlinear static response, and nonlinear dynamic response. In ESLSO, finite element analysis other than static analysis is performed, equivalent static loads (ESLs) are generated, linear static response structural optimization is carried out with the ESLs and the process iterates. A software system for the automatic use of ESLSO is developed and described. One of the advantages of ESLSO is that it can use well developed commercial software systems for structural analysis and linear static response structural optimization. Various analysis and optimization systems are integrated in the developed system. The structure of the system is systematically defined and the software is developed by the C++ language on the Windows operating system.",,"Lee, Hyun-Ah|Park, Gyung-Jin",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization|equivalent static loads (esls)|equivalent static loads method for non linear static response structural optimization (eslso),10.1007/s00158-015-1228-x
44,WOS:000333446100014,2014,Study of MEPDG Sensitivity Using Nonparametric Regression Procedures,,"Because the new Mechanistic-Empirical Pavement Design Guide (MEPDG) includes numerous inputs, a sensitivity analysis using the Monte Carlo approach is not practical because it requires thousands of MEPDG runs. Instead, nonparametric regression procedures can be very useful to perform MEPDG sensitivity analysis. In this study, nonparametric regression procedures such as multivariate adaptive regression splines and gradient boosting machine are employed to identify inputs that contribute significantly to the outputs. Thirty inputs are used to randomly generate  input combinations by using Latin hypercube sampling. Using four-layer pavement geometry [two asphalt concrete (AC), base, and subgrade layers], simulations are run in MEPDG software to produce a time series of predicted distresses such as roughness, rutting, and cracking. Sensitivity analysis resulted in three groups of inputs to which the output is ()highly sensitive, ()moderately sensitive, and ()minimally sensitive. Results show that roughness is highly sensitive to traffic input variables such as annual average daily truck traffic (AADTT), percentage of trucks in the design lane, and thickness of bottom AC layer. AC rutting is highly affected by AADTT, percentage of trucks in design direction, and tire pressure. Three major factors for total rutting, longitudinal cracking, and alligator cracking are AADTT, percentage of trucks in design direction, and thickness of bottom AC layer. In addition to these, alligator cracking is highly sensitive to percentage of air voids in the bottom AC layer. Transverse cracking is highly sensitive to the percentage of trucks in the design lane, percentage of Class  vehicles, plastic limit, thickness of base layer, effective binder content of top AC layer, and climate. Among all of the inputs, the thickness of the AC layer is highly interactive with other input variables.",,"Tarefder, Rafiqul A.|Sumee, Nasrin|Storlie, Curtis",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,sensitivity analysis|statistics|sampling|pavements|mechanistic-empirical pavement design guide|sensitivity analysis|advanced statistical analysis|latin hypercube sampling|nonparametric regression|confidence intervals,10.1061/(ASCE)CP.1943-5487.0000239
45,WOS:000357125300022,2015,A 2-D process-based model for suspended sediment dynamics: a first step towards ecological modeling,SAN-FRANCISCO BAY|JOAQUIN DELTA|SEA-LEVEL|PABLO BAY|CALIFORNIA|TRANSPORT|SACRAMENTO|SENSORS|MARSH|FLUX,"In estuaries suspended sediment concentration (SSC) is one of the most important contributors to turbidity, which influences habitat conditions and ecological functions of the system. Sediment dynamics differs depending on sediment supply and hydrodynamic forcing conditions that vary over space and over time. A robust sediment transport model is a first step in developing a chain of models enabling simulations of contaminants, phytoplankton and habitat conditions. This works aims to determine turbidity levels in the complex-geometry delta of the San Francisco estuary using a process-based approach (DelftD Flexible Mesh software). Our approach includes a detailed calibration against measured SSC levels, a sensitivity analysis on model parameters and the determination of a yearly sediment budget as well as an assessment of model results in terms of turbidity levels for a single year, water year (WY) . Model results show that our process-based approach is a valuable tool in assessing sediment dynamics and their related ecological parameters over a range of spatial and temporal scales. The model may act as the base model for a chain of ecological models assessing the impact of climate change and management scenarios. Here we present a modeling approach that, with limited data, produces reliable predictions and can be useful for estuaries without a large amount of processes data.",,"Achete, F. M.|van der Wegen, M.|Roelvink, D.|Jaffe, B.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-19-2837-2015
46,WOS:000321813200001,2013,Modelling of groundwater infiltration into sewer systems,,"Groundwater infiltration into urban sewers represents a problem that influences costs and management of technical systems. The hydrodynamic groundwater software MODFLOW is used to analyse the influencing variables of the infiltration processes. Besides the hydraulic conductivity of the soil and the piezometric head in the vicinity of the sewer pipe, properties of the sewer trench, the shape and the size of leaks are important influencing factors. A non-linear-regression method is applied to develop a one-dimensional approach in accordance with the MODFLOW results and Darcy's law. Monte Carlo simulations and the developed one-dimensional model are used to assess the leak area and the range of pressure loss in the vicinity of the pipe leaks. By additional sensitivity analysis it was found that the infiltration factor and the conductivity of the backfill are very important for the calculation of the leak area.",,"Karpf, Christian|Krebs, Peter",URBAN WATER JOURNAL,infiltration|sewer|modelling|parameter|sensitivity|monte carlo simulations,10.1080/1573062X.2012.724077
47,WOS:000309496000040,2012,Estimation of surface shortwave radiation components under all sky conditions: Modeling and sensitivity analysis,PHOTOSYNTHETICALLY ACTIVE RADIATION|DISCRETE-ORDINATE-METHOD|SIMPLE PHYSICAL MODEL|SOLAR-RADIATION|INDEPENDENT PIXEL|GLOBAL IRRADIANCE|CLIMATE RESEARCH|MODIS DATA|SATELLITE|CLOUDS,"Clouds are the most important modulator of the amount of solar energy absorbed by the earth-atmosphere system. Traditional one-dimensional (D) plane-parallel atmospheric radiative transfer models which use the independent pixel approximation (IPA) can only consider two extreme conditions, i.e., either cloud-free or overcast cases. In this paper, two cloud fraction related factors (hemispherical effective cloud fraction and regional cloud fraction) are calculated and incorporated into MODTRAN  (one of the most popular radiative transfer packages) to simulate the surface shortwave radiation components and the top-of-atmosphere (TOA) radiance for all possible solar-cloud-viewing geometries. The accuracy of this modified solar radiative transfer model (named as MODTRAN-CF) is consistent with its prototype (MODTRAN ) which has been widely used and validated in radiative transfer modeling. Some field measurements are used to validate the superiority of MODTRAN-CF. For further understanding and simplifying of this physical model, a global sensitivity analysis (GSA) method is employed to analyze the effect of model parameters on each surface shortwave radiation component. Five parameters including solar zenith angle, surface albedo, hemispherical effective cloud fraction, ground altitude and atmospheric visibility show non-negligible impacts on almost all surface shortwave fluxes, which indicates that these five parameters should be carefully considered in the future modeling of the surface shortwave radiation fluxes. Two cloud optical thickness related parameters (cloud extinction coefficient and cloud thickness) exhibit obvious importance only under cloudy illumination condition especially with optically thin clouds. These findings on the improved model will enhance our knowledge on how to accurately model the surface shortwave radiation fluxes under all sky conditions.", (C) 2012 Elsevier Inc. All rights reserved.,"Chen, Ling|Yan, Guangjian|Wang, Tianxing|Ren, Huazhong|Calbo, Josep|Zhao, Jing|McKenzie, Richard",REMOTE SENSING OF ENVIRONMENT,modtran-cf|hemispherical effective cloud fraction|global sensitivity analysis,10.1016/j.rse.2012.04.006
48,WOS:000243060300017,2007,Neural network prediction of peptide separation in strong anion exchange chromatography,,"Motivation: The still emerging combination of technologies that enable description and characterization of all expressed proteins in a biological system is known as proteomics. Although many separation and analysis technologies have been employed in proteomics, it remains a challenge to predict peptide behavior during separation processes. New informatics tools are needed to model the experimental analysis method that will allow scientists to predict peptide separation and assist with required data mining steps, such as protein identification. Results: We developed a software package to predict the separation of peptides in strong anion exchange (SAX) chromatography using artificial neural network based pattern classification techniques. A multi-layer perceptron is used as a pattern classifier and it is designed with feature vectors extracted from the peptides so that the classification error is minimized. A genetic algorithm is employed to train the neural network. The developed system was tested using  protein digests, and the sensitivity analysis was carried out to investigate the significance of each feature.",,"Oh, Cheolhwan|Zak, Stanislaw H.|Mirzaei, Hamid|Buck, Charles|Regnier, Fred E.|Zhang, Xiang",BIOINFORMATICS,,10.1093/bioinformatics/btl561
49,WOS:000368731900010,2016,A pseudo-statistical approach to treat choice uncertainty: the example of partitioning allocation methods,LIFE-CYCLE ASSESSMENT|LCA|PROPAGATION|INVENTORY|SCENARIOS|SYSTEMS,"Purpose Despite efforts to treat uncertainty due to methodological choices in life cycle assessment (LCA) such as standardization, one-at-a-time (OAT) sensitivity analysis, and analytical and statistical methods, no method exists that propagate this source of uncertainty for all relevant processes simultaneously with data uncertainty through LCA. This study aims to develop, implement, and test such a method, for the particular example of the choice of partitioning methods for allocation in LCA, to be used in LCA calculations and software. Methods Monte Carlo simulations were used jointly with the CMLCA software for propagating into distributions of LCA results, uncertainty due to the choice of allocation method together with uncertainty of unit process data. In this study, a methodological preference is assigned to each partitioning method, applicable to multi-functional processes in the system. The allocation methods are sampled per process according to these preferences. A case study on rapeseed oil focusing on three greenhouse gas (GHG) emissions and their global warming impacts is presented to illustrate the method developed. The results of the developed method are compared with those for the same case similarly quantifying uncertainty of unit process data but accompanied by separate scenarios for the different partitioning choices. Results and discussion The median of the inventory flows (emissions) for separate scenarios varies due to the partitioning choices and unit process data uncertainties. Inventory variations are reflected in the global warming results. Results for the approach of this study vary with the methodological preference assigned to the different allocation methods per multi-functional process and with the continuous distribution of unit process data. The method proved feasible and implementable. However, absolute uncertainties only further increased. Therefore, it should be further researched to reflect relative uncertainties, more relevant for comparative LCAs. Conclusions Propagation of uncertainties due to the choice of partitioning methods and to unit process data into LCA results is enabled by the proposed method, while capturing variability due to both sources. It is a practical proposal to tackle unresolved debates about partitioning choices increasing robustness and transparency of LCA results. Assigning a methodological preference to each allocation method of multi-functional processes in the system enables pseudo-statistical propagation of uncertainty due to allocation. Involving stakeholders in determining these methodological preferences allows for participatory approaches. Eventually, this method could be expanded to also cover other ways of dealing with allocation and to other methodological choices in LCA.",,"Beltran, Angelica Mendoza|Heijungs, Reinout|Guinee, Jeroen|Tukker, Arnold",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,allocation|lca|methodological choices|monte carlo|uncertainty,10.1007/s11367-015-0994-4
50,WOS:000337157100001,2014,Life cycle assessment of commodity chemical production from forest residue via fast pyrolysis,TECHNOECONOMIC ANALYSIS|BIO-OIL|ENERGY|INDUSTRY|BIOTECHNOLOGY|BIOPOLYMERS|FEEDSTOCKS|PATHWAYS|CORN|GAS,"This life cycle assessment evaluates and quantifies the environmental impacts of renewable chemical production from forest residue via fast pyrolysis with hydrotreating/fluidized catalytic cracking (FCC) pathway. The assessment input data are taken from Aspen Plus and greenhouse gases, regulated emissions, and energy use in transportation (GREET) model. The SimaPro . software is employed to evaluate the environmental impacts. The results indicate that the net fossil energy input is . MJ to produce  kg of chemicals, and the net global warming potential (GWP) is -. kg CO eq. per kg chemicals produced under the proposed chemical production pathway. Sensitivity analysis indicates that bio-oil yields and chemical yields play the most important roles in the greenhouse gas footprints. Fossil energy consumption and greenhouse gas (GHG) emissions can be reduced if commodity chemicals are produced via forest residue fast pyrolysis with hydrotreating/FCC pathway in place of conventional petroleum-based production pathways.",,"Zhang, Yanan|Hu, Guiping|Brown, Robert C.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,bio-oil upgrading|commodity chemicals|fast pyrolysis|fossil energy|greenhouse gas emission|life cycle assessment,10.1007/s11367-014-0745-y
51,WOS:000387850900012,2016,Two-scale topology design optimization of stiffened or porous plate subject to out-of-plane buckling constraint,STRUCTURAL OPTIMIZATION|HOMOGENIZATION METHOD|HIERARCHICAL-OPTIMIZATION|OPTIMUM STRUCTURE|MICROSTRUCTURES|PENALIZATION|STIFFNESS|LAYOUT|BEAM,"This paper studies maximum out-of-plane buckling load design of thin bending plates for a given amount of material. Two kinds of plates are considered. One is made of periodic homogeneous porous material. Another is uniformly stiffened solid plate. The plate material, thickness, design domain of its middle plane and boundary conditions are given. The pattern of prescribed in-plane external load or displacements along the part of boundaries, which move freely, is given. Both plate topology and micro-structural topology of porous material or stiffener layout are concurrently optimized. The artificial element material densities in both macro and micro-scale are chosen as design variables. The volume preserving nonlinear density filter is applied to obtain the black-white optimum topology and comparison of its different sensitivities is made to show the reason for oscillation during optimization process in Appendix. The new numerical implementation of asymptotic homogenization method (NIAH, Cheng (Acta Mech Sinica (): -, ) and Cai (Int J Solids Struct (), -, ) is applied to homogenization of periodic plate structures and analytic sensitivity analysis of effective stiffness with respect to the topological design variables in both macro-scale and micro-scale. On basis of that, this paper implements the sensitivity analysis of out-of-plane buckling load by using commercial FEA software and enables the application of gradient-based search algorithm in optimization. Several numerical implementation details are discussed. Three numerical examples are given to show the validity of this method.",,"Cheng, Gengdong|Xu, Liang",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,two-scale analysis|concurrent topology optimization|buckling constraints|niah method,10.1007/s00158-016-1542-y
52,WOS:000352642900010,2015,"A computational framework for dynamic data-driven material damage control, based on Bayesian inference and model selection",APPLICATIONS SYSTEMS|CREEP DAMAGE|SIMULATIONS|MECHANICS|GROWTH,"In the present study, a general dynamic data-driven application system (DDDAS) is developed for real-time monitoring of damage in composite materials using methods and models that account for uncertainty in experimental data, model parameters, and in the selection of the model itself. The methodology involves (i) data data from uniaxial tensile experiments conducted on a composite material; (ii) continuum damage mechanics based material constitutive models; (iii) a Bayesian framework for uncertainty quantification, calibration, validation, and selection of models; and (iv) general Bayesian filtering, as well as Kalman and extended Kalman filters. A software infrastructure is developed and implemented in order to integrate the various parts of the DDDAS. The outcomes of computational analyses using the experimental data prove the feasibility of the Bayesian-based methods for model calibration, validation, and selection. Moreover, using such DDDAS infrastructure for real-time monitoring of the damage and degradation in materials results in results in an improved prediction of failure in the system."," Copyright (C) 2014 John Wiley & Sons, Ltd.","Prudencio, E. E.|Bauman, P. T.|Faghihi, D.|Ravi-Chandar, K.|Oden, J. T.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,bayesian model selection|extended kalman filter|dynamic data-driven application systems|material damage,10.1002/nme.4669
53,WOS:000405522600004,2017,Estimation of vertical water fluxes from temperature time series by the inverse numerical computer program FLUX-BOT,FLOW|DIFFUSIVITY|HEAT,"The application of heat as a hydrological tracer has become a standard method for quantifying water fluxes between groundwater and surface water. The typical application is to estimate vertical water fluxes in the shallow subsurface beneath streams or lakes. For this purpose, time series of temperatures in the surface water and in the sediment are measured and evaluated by a vertical D representation of heat transport by advection and conduction. Several analytical solutions exist to calculate the vertical water flux from the measured temperatures. Although analytical solutions can be easily implemented, they are restricted to specific boundary conditions such as a sinusoidal upper temperature boundary. Numerical solutions offer higher flexibility in the selection of the boundary conditions. This, in turn, reduces the effort of data preprocessing, such as the extraction of the diurnal temperature variation from the raw data. Here, we present software to estimate water fluxes based on temperaturesFLUX-BOT. FLUX-BOT is a numerical code written in MATLAB that calculates vertical water fluxes in saturated sediments based on the inversion of measured temperature time series observed at multiple depths. FLUX-BOT applies a centred Crank-Nicolson implicit finite difference scheme to solve the one-dimensional heat advection-conduction equation. FLUX-BOT includes functions for the inverse numerical routines, functions for visualizing the results, and a function for performing uncertainty analysis. We present applications of FLUX-BOT to synthetic and to real temperature data to demonstrate its performance.",,"Munz, Matthias|Schmidt, Christian",HYDROLOGICAL PROCESSES,heat tracing|numerical solution|surface water groundwater interaction|temperature time series|vertical water flux,10.1002/hyp.11198
54,WOS:000407370700097,2017,Modeling Nitrogen Dynamics in a Waste Stabilization Pond System Using Flexible Modeling Environment with MCMC,SENSITIVITY-ANALYSIS|CONSTRUCTED WETLAND|WATER TREATMENT|PREDICTIVE UNCERTAINTY|NUTRIENT RECOVERY|GLUE METHODOLOGY|BAYESIAN METHOD|UNITED-STATES|REMOVAL|PERFORMANCE,"This study presents an approach for obtaining realization sets of parameters for nitrogen removal in a pilot-scale waste stabilization pond (WSP) system. The proposed approach was designed for optimal parameterization, local sensitivity analysis, and global uncertainty analysis of a dynamic simulation model for the WSP by using the R software package Flexible Modeling Environment (R-FME) with the Markov chain Monte Carlo (MCMC) method. Additionally, generalized likelihood uncertainty estimation (GLUE) was integrated into the FME to evaluate the major parameters that affect the simulation outputs in the study WSP. Comprehensive modeling analysis was used to simulate and assess nine parameters and concentrations of ON-N, NH-N and NO-N. Results indicate that the integrated FME-GLUE-based model, with good Nash-Sutcliffe coefficients (.-.) and correlation coefficients (.-.), successfully simulates the concentrations of ON-N, NH-N and NO-N. Moreover, the Arrhenius constant was the only parameter sensitive to model performances of ON-N and NH-N simulations. However, Nitrosomonas growth rate, the denitrification constant, and the maximum growth rate at  degrees C were sensitive to ON-N and NO-N simulation, which was measured using global sensitivity.",,"Mukhtar, Hussnain|Lin, Yu-Pin|Shipin, Oleg V.|Petway, Joy R.",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,flexiblemodeling environment|waste stabilization pond|nitrogen dynamic|parameterization|sensitivity|mcmc|glue|global uncertainty,10.3390/ijerph14070765
55,WOS:000347153700004,2015,Impact of maintenance on life cycle impact and cost assessment for residential flooring options,SUSTAINABILITY ASSESSMENT|LCA|COVERINGS,"Most life cycle assessment (LCA) studies for flooring exclude the environmental and economic impacts incurred from the maintenance required due to uncertainty in average cleaning procedures, although some studies indicate that it may be the most significant component of the life cycle. This study investigates the impacts of maintenance on types of flooring and develops a single scoring system to compare floors based on both environmental and economic impacts. The focus of this study was on the impact of maintenance on the life cycle of flooring choices. Maintenance data was collected from trade association studies and manufacturer's recommendations. This data was compiled, along with data from previous flooring studies, to create a comprehensive life cycle inventory which was analyzed with the LCA software, SimaPro. A number of maintenance techniques and frequencies were tested in order to do a sensitivity analysis. An uncertainty analysis was completed using Monte Carlo simulations. A life cycle costing (LCC) analysis was used to evaluate the total present value cost of flooring including maintenance. Environmental and economic impacts were normalized to create a single score in order to compare the overall performance of flooring choices. Maintenance procedures may account for a significant portion of environmental and economic impacts of floorings. In the case of environmental scores, adding high maintenance to the life cycle can increase scores by anywhere from  % (hardwood) to  % (carpet). A sensitivity analysis of these scores shows that most of the score increase can be attributed to vacuuming. Maintenance costs considerably increase the total cost, accounting for  % of hardwood's total cost and about  % of costs for linoleum, vinyl, and carpet. The expected service life of the home greatly influences which flooring is best, as costs and environmental scores change dynamically over time. For the expected service life of a home of  years, carpet has the worst and linoleum has the best overall performance. Although averages for maintenance techniques are not currently known, ignoring maintenance as a part of the use phase presents a significant error in the comparison of flooring options environmentally and economically. Due in part to yearly maintenance effects, the flooring choice with the best overall performance changes dynamically depending on the expected service life remaining for the home.",,"Minne, Elizabeth|Crittenden, John C.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,flooring|life cycle costing|life cycle impact assessment|lifetime|maintenance phase|residential buildings|single scoring|uncertainty analysis,10.1007/s11367-014-0809-z
56,WOS:000342532200014,2014,Natural ventilation design: An analysis of predicted and measured performance,,"We present a study of natural ventilation design during the early (conceptual) stage of a building's design, based on a field study in a naturally ventilated office in California where we collected data on occupants' window use, local weather conditions, indoor environmental conditions, and air change rates based on tracer-gas decay. We performed uncertainty and sensitivity analyses to determine which design parameters have most impact on the uncertainty associated with ventilation performance predictions. Using the results of the field study along with wind-tunnel measurements and other detailed analysis, we incrementally improved our early-design-stage model. The improved model's natural ventilation performance predictions were significantly more accurate than those of the first draft early-stage-design model that employed model assumptions typical during initial design. This process highlighted significant limitations in the EnergyPlus software's models of occupant-driven window control. We conclude with recommendations on key design parameters including window control, wind pressure coefficients and weather data resolution to help improve early-design-stage predictions of natural ventilation performance using EnergyPlus. (C) ", Elsevier Ltd. All rights reserved.,"Belleri, Annamaria|Lollini, Roberto|Dutton, Spencer M.",BUILDING AND ENVIRONMENT,natural ventilation|early-design-stage|uncertainty analysis|air change rates|occupant behavior|airflow network,10.1016/j.buildenv.2014.06.009
57,WOS:000330487700021,2014,"A modified (S-1,S) inventory system for deteriorating items with Poisson demand and non-zero lead time",OPTIMAL REPLENISHMENT POLICY|PRODUCTION QUANTITY MODELS|STOCK-DEPENDENT DEMAND|OPTIMAL SELLING PRICE|VARYING DEMAND|LOT-SIZE|EOQ MODEL|SHORTAGES,"An inventory system is considered for continuous decaying items with non-zero lead time and stochastic demand when shortages are allowed and all unsatisfied demands are back-logged. In this research we consider orders as separate packages where replenishment is one-for-one and a modified base stock policy is applied. In this paper, a penalty cost is introduced for stochastic inventory models with decaying items when less than one unit of the product is delivered to the customers. The objective of the warehouse is to maximize his average profit. Since the concavity analysis of the model is extremely complicated, an upper bound is introduced and an algorithm is presented for finding the optimal solution. Finally, a numerical example is presented and sensitivity analysis is carried out for a number of important parameters.", (C) 2013 Elsevier Inc. All rights reserved.,"Alizadeh, M.|Eskandari, H.|Sajadifar, S. M.",APPLIED MATHEMATICAL MODELLING,inventory|deteriorating items|poisson demand|upper bound,10.1016/j.apm.2013.07.014
58,WOS:000394352200005,2017,Energy and environmental life cycle assessment of a high concentration photovoltaic power plant in Morocco,PAYBACK|MODULES|SYSTEMS|TIME|SI,"High concentrated photovoltaic (HCPV) technology transforms solar radiation into electricity at efficiencies far higher than conventional PV cells. The aim of this paper is to evaluate the environmental impact of a commercial HCPV plant located in Morocco by determining the impact of this technology on a wide range of environmental categories. The results are expected to contribute to a better environmental design and performance of the power plant. A complete life cycle inventory was gathered for a . MW HCPV power plant located in Casablanca (Morocco). The system was evaluated using a cradle to gate approach, considering  MWh as functional unit. ReCiPe Midpoint (World) evaluation method and Simapro Software were used for calculations. A sensitivity analysis on the life expectancy for ,  and  years was also performed. Cumulative energy demand and energy payback time were determined for each scenario. The results showed an EPBT of . years and the following main environmental impacts: climate change . kg CO eq/MWh, freshwater eutrophication . g P eq/MWh, human toxicity . kg .-DB eq/MWh, freshwater ecotoxicity . kg .-DB eq/MWh and marine ecotoxicity . g .-DB eq/kWh. Most of the impacts were associated with the extraction of raw materials and manufacturing of components, being aluminum and steel the materials with higher impacts. Normalization assigned the highest impacts to the toxicity categories, due mainly to the materials employed in the electronic devices and the aluminum used in the module manufacturing. The end-of-life stage had a significant positive effect on the performance of the plant, reducing the impact on each category by between  % (in ozone depletion) and  % (in particulate matter formation), due mainly to the recycling of steel and aluminum. The power plant components manufacturing and the electricity consumption from the grid presented a high impact in the life cycle of the plant, implying a significant importance of the local electricity mix. An adequate recycling of the materials is recommendable, since it reduces considerably the impact of the system. The sensitivity analysis revealed a significant improvement in the environmental performance when increasing lifetime expectancy from  to  years.",,"Corona, Blanca|Escudero, Lidia|Quemere, Goulven|Luque-Heredia, Ignacio|San Miguel, Guillermo",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,cumulative energy demand (ced)|energy payback time (epbt)|high concentration photovoltaic (hcpv)|lifetime expectancy,10.1007/s11367-016-1157-y
59,WOS:000326061300005,2013,"A methodology for optimal MSW management, with an application in the waste transportation of Attica Region, Greece",MUNICIPAL SOLID-WASTE|SYSTEM|GENERATION|COLLECTION|MODEL,"The paper describes a software system capable of formulating alternative optimal Municipal Solid Wastes (MSWs) management plans, each of which meets a set of constraints that may reflect selected objections and/or wishes of local communities. The objective function to be minimized in each plan is the sum of the annualized capital investment and annual operating cost of all transportation, treatment and final disposal operations involved, taking into consideration the possible income from the sale of products and any other financial incentives or disincentives that may exist. For each plan formulated, the system generates several reports that define the plan, analyze its cost elements and yield an indicative profile of selected types of installations, as well as data files that facilitate the geographic representation of the optimal solution in maps through the use of GIS. A number of these reports compare the technical and economic data from all scenarios considered at the study area, municipality and installation level constituting in effect sensitivity analysis. The generation of alternative plans offers local authorities the opportunity of choice and the results of the sensitivity analysis allow them to choose wisely and with consensus. The paper presents also an application of this software system in the capital Region of Attica in Greece, for the purpose of developing an optimal waste transportation system in line with its approved waste management plan. The formulated plan was able to: (a) serve  Municipalities and Communities that generate nearly  million t/y of comingled MSW with distinctly different waste collection patterns, (b) take into consideration several existing waste transfer stations (WTS) and optimize their use within the overall plan, (c) select the most appropriate sites among the potentially suitable (new and in use) ones, (d) generate the optimal profile of each WTS proposed, and (e) perform sensitivity analysis so as to define the impact of selected sets of constraints (limitations in the availability of sites and in the capacity of their installations) on the design and cost of the ensuing optimal waste transfer system. The results show that optimal planning offers significant economic savings to municipalities, while reducing at the same time the present levels of traffic, fuel consumptions and air emissions in the congested Athens basin. (C) ", Elsevier Ltd. All rights reserved.,"Economopoulou, M. A.|Economopoulou, A. A.|Economopoulos, A. P.",WASTE MANAGEMENT,municipal solid wastes|msw|optimal msw transportation|optimal msw treatment|optimal msw management|software for optimal msw management|regional msw management plans|national msw management plans,10.1016/j.wasman.2013.06.016
60,WOS:000285122400004,2010,Efficient solution for Galerkin-based polynomial chaos expansion systems,STOCHASTIC PROJECTION METHOD|FINITE-ELEMENT SYSTEMS|ITERATIVE SOLUTION|LINEAR-SYSTEMS|FLUID-FLOW|MODELS,"Iterative solvers and preconditioners are widely used for handling the linear system of equations arising from stochastic finite element method (SFEM) formulations, e.g. galerkin-based polynomial chaos (G-P-C) Expansion method. Especially, Preconditioned Conjugate Gradient (PCG) solver and the Incomplete Cholesky (IC) preconditioner are shown to be adequate choices within this context. In this study, approaches for the automated adjustment of the input parameters for these tools are to be introduced. The proposed algorithms aim to enable the use of the PCG solver and IC preconditioner in a black-box fashion. As a result, the requirement of the expertise for using these tools is removed to a certain extend. Furthermore, these algorithms can be used also for the implementation purposes of SFEM's within general purpose software by increasing the ease of the use of these tools and hence leading to an improved user-comfort. (C) ", Elsevier Ltd. All rights reserved.,"Panayirci, H. M.",ADVANCES IN ENGINEERING SOFTWARE,stochastic finite elements|polynomial chaos expansion|computational efficiency|iterative solvers|preconditioners|uncertainty quantification,10.1016/j.advengsoft.2010.09.004
61,WOS:000362135900019,2015,Evaluation and comparison of open source program solutions for automatic seed counting on digital images,SOFTWARE|SHAPE|IDENTIFICATION|YIELD|SIZE,"Seed number quantification is an essential agronomic parameter conducted mostly manually or by mechanical counters, both with obvious limitations. Digital image analysis provides a reliable and robust alternative to accurately calculate many biological features. This study presents and evaluates the performance of four open-source image-analysis programs i.e. ImageJ, CellProfiler, P-TRAP and SmartGrain to count crop seeds from digital images captured by camera and scanner. It also evaluates ImageJ program for automated seed counting using macro containing RenyiEntropy threshold algorithm. Digital images of cereal crop seeds were acquired i.e. wheat, barley, maize, rye, oat, sorghum, triticale and rice. All images contained  seeds per image present in an area of approx.  cm(). RenyiEntropy threshold increased the seed count accuracy of ImageJ from digital camera images. Generally, seed counts from digital camera images of all crops were accurate, but software-crop combination had significant (p < .) difference from reference value. Among image analysis programs, ImageJ produced mostly higher seed count across all observed crops than other programs. Mean seed counts from scanned images of maize were observed only by CellProfiler and P-TRAP, with other programs inappropriate due to high inaccuracy. These results suggest CellProfiler as a reliable image analysis program for seed counting from digital images. Benchmark test was also performed to compare speed of analysis. The automated seed count produced by image analysis programs described here allows faster, reliable and reproducible analysis, compared to standard manual method. To our knowledge this is the first study on using CellProfiler program for crop seed counting from digital images.", (C) 2015 Elsevier B.V. All rights reserved.,"Mussadiq, Zohaib|Laszlo, Baranyai|Helyes, Lajos|Gyuricza, Csaba",COMPUTERS AND ELECTRONICS IN AGRICULTURE,cellprofiler|imagej|maize|p-trap|smartgrain|wheat,10.1016/j.compag.2015.08.010
62,WOS:000309627100022,2012,Life cycle assessment of TV sets in China: A case study of the impacts of CRT monitors,POLYBROMINATED DIPHENYL ETHERS|MOBILE PHONE NETWORKS|E-WASTE|PERSONAL-COMPUTER|POLLUTION|METALS|FLOW|END|AIR,"Along with the rapid increase in both production and use of TV sets in China, there is an increasing awareness of the environmental impacts related to the accelerating mass production, electricity use, and waste management of these sets. This paper aims to describe the application of life cycle assessment (LCA) to investigate the environmental performance of Chinese TV sets. An assessment of the TV set device (focusing on the Cathode Ray Tube (CRT) monitor) was carried out using a detailed modular LCA based on the international standards of the ISO  series. The LCA was constructed using SimaPro software version . and expressed with the Eco-indicator'  life cycle impact assessment method. For a sensitivity analysis of the overall LCA results, the CML method was used in order to estimate the influence of the choice of the assessment method on the results. Life cycle inventory information was compiled by Ecoinvent . databases, combined with literature and field investigations on the current Chinese situation. The established LCA study shows that the use stage of such devices has the highest environmental impact, followed by the manufacturing stage. In the manufacturing stage, the CRT and the Printed Circuit Board (PCB) are those components contributing the most environmental impacts. During the use phase, the environmental impacts are due entirely to the methods of electricity generation used to run them, since no other aspects were taken into account for this phase. The final processing step-the end-of-life stage can lead to a clear environmental benefit when the TV sets are processed through the formal dismantling enterprises in China. (C) ", Elsevier Ltd. All rights reserved.,"Song, Qingbin|Wang, Zhishi|Li, Jinhui|Zeng, Xianlai",WASTE MANAGEMENT,tv sets|crt monitor|life cycle assessment|environmental impacts|china,10.1016/j.wasman.2012.05.007
63,WOS:000395607700003,2017,Infiltration under snow cover: Modeling approaches and predictive uncertainty,ENERGY-BALANCE|HYDRAULIC CONDUCTIVITY|WATER EQUIVALENT|MELT SIMULATIONS|PILOT POINTS|TEMPERATURE|INDEX|RADIATION|PARAMETER|SURFACE,"Groundwater recharge from snowmelt represents a temporal redistribution of precipitation. This is extremely important because the rate and timing of snowpack drainage has substantial consequences to aquifer recharge patterns, which in turn affect groundwater availability throughout the rest of the year. The modeling methods developed to estimate drainage from a snowpack, which typically rely on temporally dense point-measurements or temporally-limited spatially-dispersed calibration data, range in complexity from the simple degree-day method to more complex and physically-based energy balance approaches. While the gamut of snowmelt models are routinely used to aid in water resource management, a comparison of snowmelt models' predictive uncertainties had previously not been done. Therefore, we established a snowmelt model calibration dataset that is both temporally dense and represents the integrated snowmelt infiltration signal for the Vers Chez le Brandt research catchment, which functions as a rather unique natural lysimeter. We then evaluated the uncertainty associated with the degree-day, a modified degree-day and energy balance snowmelt model predictions using the null space Monte Carlo approach. All three melt models underestimate total snowpack drainage, underestimate the rate of early and midwinter drainage and overestimate spring snowmelt rates. The actual rate of snowpack water loss is more constant over the course of the entire winter season than the snowmelt models would imply, indicating that mid-winter melt can contribute as significantly as springtime snow melt to groundwater recharge in low alpine settings. Further, actual groundwater recharge could be between  and % greater than snowmelt models suggest, over the total winter season. This study shows that snowmelt model predictions can have considerable uncertainty, which may be reduced by the inclusion of more data that allows for the use of more complex approaches such as the energy balance method. Further, our study demonstrated that an uncertainty analysis of model predictions is easily accomplished due to the low computational demand of the models and efficient calibration software and is absolutely worth the additional investment. Lastly, development of a systematic instrumentation that evaluates the distributed, temporal evolution of snowpack drainage is vital for optimal understanding and management of cold-climate hydrologic systems.", (C) 2017 Elsevier B.V. All rights reserved.,"Meeks, Jessica|Moeck, Christian|Brunner, Philip|Hunkeler, Daniel",JOURNAL OF HYDROLOGY,uncertainty|snowmelt|energy balance|day degree|recharge|karst|groundwater,10.1016/j.jhydrol.2016.12.042
64,WOS:000263990600027,2009,Life cycle assessment study of a Chinese desktop personal computer,POLYBROMINATED DIPHENYL ETHERS|MOBILE PHONE NETWORKS|E-WASTE|IMPACT ASSESSMENT|FLOW,"Associated with the tremendous prosperity in world electronic information and telecommunication industry, there continues to be an increasing awareness of the environmental impacts related to the accelerating mass production, electricity use, and waste management of electronic and electric products (e-products). China's importance as both a consumer and supplier of e-products has grown at an unprecedented pace in recent decade. Hence, this paper aims to describe the application of life cycle assessment (LCA) to investigate the environmental performance of Chinese e-products from a global level. A desktop personal computer system has been selected to carry out a detailed and modular LCA which follows the ISO  series. The LCA is constructed by SimaPro, software version . and expressed with the Eco-indicator' life cycle impact assessment method. For a sensitivity analysis of the overall LCA results, the so-called CML method is used in order to estimate the influence of the choice of the assessment method on the result Life cycle inventory information is complied by ecoinvent . databases, combined with literature and field investigations on the present Chinese situation. The established LCA study shows that that the manufacturing and the use of such devices are of the highest environmental importance. In the manufacturing of such devices, the integrated circuits (Is) and the Liquid Crystal Display (LCD) are those parts contributing most to the impact. As no other aspects are taken into account during the use phase, the impact is due to the way how the electricity is produced. The final process steps - i.e. the end of life phase - lead to a clear environmental benefit if a formal and modem, up-to-date technical system is assumed, like here in this study.", Crown Copyright (C) 2008 Published by Elsevier B.V. All rights reserved.,"Duan, Huabo|Eugster, Martin|Hischier, Roland|Streicher-Porte, Martin|Li, Jinhui",SCIENCE OF THE TOTAL ENVIRONMENT,lca|personal computer|environmental impact|electronics|china,10.1016/j.scitotenv.2008.10.063
65,WOS:000336668600061,2014,A New Algorithm for Small-Signal Analysis of DC-DC Converters,POWER CONVERTERS|STABILITY ANALYSIS|PWM CONVERTERS|A SURVEY|IMPLEMENTATION|SYSTEMS,"This paper presents a new approach for small-signal analysis of all types of dc-dc converters with any number of topological modes within a switching cycle. So far, sampled-data modeling and sensitivity analysis are mostly used for such a purpose. In both cases, the switching conditions implicitly appear in the small-signal matrices which increases the complexity of the computation for the system with a larger number of topological modes. Here, we propose an alternative approach based on Filippov's method, which studies the effects of each switching separately. It uses a shooting method with an event detector to locate a periodic steady-state, and, when the Newton-Raphson search process of the shooting method converges, it also gives the Jacobian matrix. The algorithm can be easily implemented in a software program as an analytical tool which is expected to be useful for fast and accurate frequency-domain analysis (by small-signal transfer functions) to facilitate controller design. Moreover, since it uses the Filippov's method, this algorithm can also predict the slow-and fast-timescale instabilities.",,"Mandal, Kuntal|Banerjee, Soumitro|Chakraborty, Chandan",IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS,dc-dc converters|filippov's method|resonant converters|small-signal analysis,10.1109/TII.2013.2277942
66,WOS:000269569700009,2009,Using a Monte Carlo approach to evaluate seawater intrusion in the Oristano coastal aquifer: A case study from the AQUAGRID collaborative computing platform,,"Uncertainties in the physical parameters of a groundwater system, due to the lack of direct access to the subsurface, strongly affect the design of water management policies, so that the risk of mismanagement becomes a critical factor in complex ecological and economic analyses. Stochastic modeling may help provide uncertainty quantification and also add robustness to the analysis by means of probabilistic forecasts. in this study a stochastic approach has been employed to model hydraulic conductivity of a confining formation in a multi-layered coastal aquifer system, under conditions of uncertainty. A Monte Carlo simulation, based on a coupled flow and transport groundwater D model, has been carried out to propagate the hydraulic conductivity parameter uncertainty to groundwater model outputs, namely pressure head and salt concentration. The aim of the study is to assess the risk of seawater intrusion into the aquifer by means of probabilistic threshold analysis on the simulated groundwater concentrations for different aquifer exploitation schemes. Maximum difference for nodal concentrations, with reference to homogeneous aquitard configuration, was found equal to %, proving how important can be the impact of the spatial variability of the hydraulic conductivity of the confining layer on the simulated salt concentrations. Such analysis enables to take better decisions about the management of the groundwater resource and to make additional field investigations consistent with environmental protection. The application workflow, based on the integration of both in-house developed and public domain software tools with hydrogeological data, has been deployed on a problem solving Grid platform (http://grida.crs.it). Further developments will include the planning of cost-effective additional field data acquisition based on the outcome of the stochastic model. (C) ", Elsevier Ltd. All rights reserved.,"Lecca, Giuditta|Cau, Pierluigi",PHYSICS AND CHEMISTRY OF THE EARTH,3d groundwater model|monte carlo simulation|grid computing|collaborative web platform|optimal management of coastal aquifers,10.1016/j.pce.2009.03.002
67,WOS:000383298800020,2016,Designing and implementing a multi-core capable integrated urban drainage modelling Toolkit:Lessons from CityDrain3,WASTE-WATER SYSTEM|CLIMATE-CHANGE|IDENTIFIABILITY ANALYSIS|SENSITIVITY-ANALYSIS|STORMWATER MODELS|STORAGE TANK|SEWER SYSTEM|CITY DRAIN|MANAGEMENT|UNCERTAINTY,"Integrated urban drainage modelling combines different aspects of the urban water system into a common framework. With increasing pressures of a changing climate, urban growth and economic constraints, the need for wider spread integration is necessary in the interest of a sustainable future. Greater complexity results in greater computational burden but modelling packages will, likewise, need to be flexible enough to allow incorporation of new algorithms. With advancements in modern information technology, a parallel implementation of such a modelling toolkit is mandatory while still leaving its users the flexibility of extensions. The design and implementation of the integrated modelling framework CityDrain shows that it is possible to write research code that is high-performance and extensible by many research projects. Three use case scenarios are presented to showcase the application of CityDrain. The performance advantage of parallelization (up to  times compared to its predecessor) and the scalability of the framework are also demonstrated. (C) ", Elsevier Ltd. All rights reserved.,"Burger, Gregor|Bach, Peter M.|Urich, Christian|Leonhardt, Gunther|Kleidorfer, Manfred|Rauch, Wolfgang",ADVANCES IN ENGINEERING SOFTWARE,integrated urban drainage|modelling|simulation framework|object-oriented design|multi-core|parallel computing,10.1016/j.advengsoft.2016.08.004
68,WOS:000300251600008,2012,Coupling hydrogeological with surface runoff model in a Poltva case study in Western Ukraine,HETEROGENEOUS POROUS-MEDIA|WATER-GROUNDWATER INTERACTIONS|URBAN DRAINAGE SYSTEMS|SENSITIVITY-ANALYSIS|HYDROLOGIC-RESPONSE|STREAM DEPLETION|AQUIFER RESPONSE|OVERLAND-FLOW|MASS-TRANSFER|STORM RUNOFF,"This paper presents the hydrological coupling of the software framework OpenGeoSys (OGS) with the EPA Storm Water Management Model (SWMM). Conceptual models include the Saint Venant equation for river flow, the D Darcy equations for confined and unconfined groundwater flow, a two-way hydrological coupling flux in a compartment coupling approach (conductance concept), and Lagrangian particles for solute transport in the river course. A SWMM river-OGS aquifer inter-compartment coupling flux is examined for discharging groundwater in a systematic parameter sensitivity analysis. The parameter study involves a small perturbation (first-order) sensitivity analysis and is performed for a synthetic test example base-by-base through a comprehensive range of aquifer parametrizations. Through parametrization, the test cases enables to determine the leakance parameter for simulating streambed clogging and non-ocillatory river-aquifer water exchange rates with the sequential (partitioned) coupling scheme. The implementation is further tested with a hypothetical but realistic D river-D aquifer model of the Poltva catchment, where discharging groundwater in the upland area affects the river-aquifer coupling fluxes downstream in the river course (propagating feedbacks). Groundwater contribution in the moving river water is numerically determined with Lagrangian particles. A numerical experiment demonstrates that the integrated river-aquifer model is a serviceable and realistic constituent in a complete compartment model of the Poltva catchment.",,"Delfs, Jens-Olaf|Blumensaat, Frank|Wang, Wenqing|Krebs, Peter|Kolditz, Olaf",ENVIRONMENTAL EARTH SCIENCES,integrated surface-subsurface flow modelling|urban water|conductance concept|sensitivity analysis|random walk particle tracking (rwpt)|poltva basin (western ukraine),10.1007/s12665-011-1285-4
69,WOS:000299324200005,2012,Yield improvement analysis with parameter-screening factorials,SENSITIVITY-ANALYSIS|SYSTEM|MODEL,"This paper presents a technique for the critical parameter analysis of the disk drive manufacturing process. The objective of the work is to improve the manufacturing yield by tuning the parameters that significantly affect the yield. Several techniques were studied including the sensitivity analysis framework, which is currently used at several disk drive plants. From our initial experiments, we found that the sensitivity analysis results were not sufficiently good and the interactions between parameters were not identified. We then designed a new technique based on factorial designs, the parameter-screening factorials algorithm. Our method can work with a large number of inputs within reasonable computing time, and can identify both the parameter and the interaction effects. The results can be obtained more quickly and are better in comparison with the currently used technique. Moreover, by applying the technique to the full list instead of the pre-selected list of the manufacturing parameters, we discovered that the parameters watch list previously identified by the experts should be adjusted to include some extra parameters. After the results were validated by the experts, we designed software that automates the critical parameter analysis process. The software should greatly benefit the daily yield analysis at the disk drive manufacturing plant greatly.", (C) 2011 Elsevier B.V. All rights reserved.,"Yamwong, Worraluk|Achalakul, Tiranee",APPLIED SOFT COMPUTING,critical parameter identification|yield improvement analysis|factorial designs,10.1016/j.asoc.2011.11.021
70,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, Jean-Luc",WATER SCIENCE AND TECHNOLOGY,calibration|epistemology|field data|modelling|sensitivity analysis|separate and combined sewers|stormwater|verification,10.2166/wst.2007.090
71,WOS:000414081000003,2017,Multi-scale equation of state computations for confined fluids,EQUILIBRIUM,"Fluid properties of five binary mixtures relevant to shale gas and light tight oil in confined nano-channels are studied. Canonical (NVT) Monte Carlo simulations are used to determine internal energies of departure of pure fluids using the RASPA software system (Dubbeldam et al., ). The linear mixing rule proposed by Lucia et al. () is used to determine internal energies of departure for mixtures, U-M(D), in confined spaces and compared to U-M(D) from direct NVT Monte Carlo simulation. The sensitivity of the mixture energy parameter, a(M), for the Gibbs-Helmholtz constrained (GHC) equation, confined fluid molar volume, V-M, and bubble point pressure are studied as a function of uncertainty in U-M(D). Results show that the sensitivity of confined fluid molar volume to % uncertainty in U-M(D) is less than % and that the GHC equation predicts physically meaningful reductions in bubble point pressure for light tight oils. (C) ", Elsevier Ltd. All rights reserved.,"Thomas, Edward|Lucia, Angelo",COMPUTERS & CHEMICAL ENGINEERING,confined fluids|monte carlo simulation|ghc equation of state|sensitivity analysis|bubble point pressure,10.1016/j.compchemeng.2017.05.028
72,WOS:000403211900001,2017,Epidemic model formulation and analysis for diarrheal infections caused by salmonella,SENSITIVITY-ANALYSIS|ECONOMIC BURDEN|UNCERTAINTY|DISEASE,"Epidemic modeling can be used to gain better understanding of infectious diseases, such as diarrhea. In the presented research, a continuous mathematical model has been formulated for diarrhea caused by salmonella. This model has been analyzed and simulated to be established in a functioning form. Elementary model analysis, such as working out the disease-free state and basic reproduction number, has been done for this model. The basic reproduction number has been calculated using the next generation matrix method. Stability analysis of the model has been done using the Routh-Hurwitz method. Sensitivity analysis and parameter estimation have been completed for the system too using MATLAB packages that work on the Latin Hypercube Sampling and Partial Rank Correlation Coefficient methods. It was established that as long as R- < , there will be no epidemic. Upon simulation using assumed parameter values, the results produced comprehended the epidemic theory and practical situations. The system was proven stable using the Routh-Hurwitz criterion and parameter estimation was successfully completed. Salmonella diarrhea has been successfully modeled and analyzed in this research. This model has been flexibly built and it can be integrated onto certain platforms to be used as a predictive system to prevent further infections of salmonella diarrhea.",,"Chaturvedi, Ojaswita|Jeffrey, Mandu|Lungu, Edward|Masupe, Shedden",SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL,salmonella|epidemic modeling|model analysis|routh-hurwitz|sensitivity analysis|parameter estimation,10.1177/0037549716685409
73,WOS:000388889800001,2016,A New Software Reliability Growth Model: Multigeneration Faults and a Power-Law Testing-Effort Function,ERROR-DETECTION|OPTIMIZATION|DEPENDENCY|COST,"Software reliability growth models (SRGMs) based on a nonhomogeneous Poisson process (NHPP) are widely used to describe the stochastic failure behavior and assess the reliability of software systems. For these models, the testing-effort effect and the fault interdependency play significant roles. Considering a power-law function of testing effort and the interdependency of multigeneration faults, we propose a modified SRGM to reconsider the reliability of open source software (OSS) systems and then to validate the model's performance using several real-world data. Our empirical experiments show that the model well fits the failure data and presents a high-level prediction capability. We also formally examine the optimal policy of software release, considering both the testing cost and the reliability requirement. By conducting sensitivity analysis, we find that if the testing-effort effect or the fault interdependency was ignored, the best time to release software would be seriously delayed and more resources would be misplaced in testing the software.",,"Li, Fan|Yi, Ze-Long",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2016/9276093
74,WOS:000381545200029,2016,Life Cycle Analysis applied to acrylic acid production process with different fuels for steam generation,PETROCHEMICAL INDUSTRY|CATALYSTS|EMISSION|DESIGN|CO2,"Acrylic acid, one of the most important monomers, has a wide range of uses e.g. in the paints adhesives, textile finishing, leather processing or super absorbents. The production process from propylene is well known and used at industrial scale in the USA, Europe, and Asia. The present paper focuses on the evaluation of the environmental impact of the acrylic acid production process from propylene. Steam is a major raw material in the process under study, consequently different paths to obtain steam are investigated. The process was simulated using commercial software (Aspen Plus, PROII). A productivity capacity of , tones/year of acrylic acid has been considered. The environmental assessment, evaluated using Life Cycle Analysis method, depends on the material and energy balances generated by simulations. The present work uses a cradle-to-grave approach. The functional unit, to which all the environmental results for the cases under study are reported, is one tone of acrylic acid produced. The boundaries of the system cover: i) acrylic acid production; ii) upstream processes for example catalyst and molten salt production; steam production using various fuels (e.g. natural gas, anthracite, lignite, heavy fuel oil, light fuel oil and biomass) and iii) downstream processes for example: acrylic acid and acetic acid transport to other chemical plants for further processing. The CML (An LCA method developed by the Center of Environmental Science of Leiden University)  impact assessment method was used for comparison between different case studies. The best value for Global Warming Potential (GWP) e.g. . kg CO-Equiv./tone is obtained when biomass is used to generate the steam required by the process. For other indicators such as Acidification Potential (AP), Photochemical Oxidation Potential (PCOP) and Eutrophication Potential (EP), the best value is obtained when steam is generated from natural gas. A sensitivity analysis of the environmental impact categories using the different combination of natural gas and biomass was also investigated. The study investigates also the association of the post-combustion amine based carbon capture technologies with the conventional acrylic acid production process. Such association decreases the value of some environmental impact categories (e.g. GWP) while other impact categories are increasing e.g. AP, Human Toxicity Potential (HIP), Terrestrial Ecotoxicity Potential (TETP). (C) ", Elsevier Ltd. All rights reserved.,"Petrescu, Letitia|Fermeglia, Maurizio|Cormos, Calin-Cristian",JOURNAL OF CLEANER PRODUCTION,acrylic acid production|life cycle analysis (lca)|environmental impact,10.1016/j.jclepro.2016.05.088
75,WOS:000280543800002,2010,An evolutionary optimization of diffuser shapes based on CFD simulations,STRAIGHT 2-DIMENSIONAL DIFFUSERS|HEAT-TRANSFER OPTIMIZATION|INTERNALLY FINNED TUBES|SENSITIVITY ANALYSIS|ADJOINT FORMULATION|PARALLEL COMPUTERS|LAMINAR-FLOW|DESIGN|PERFORMANCE|CHANNELS,"An efficient and robust algorithm is presented for the optimum design of plane symmetric diffusers handling incompressible turbulent flow. The indigenously developed algorithm uses the CFD software: Fluent for the hydrodynamic analysis and employs a genetic algorithm (GA) for optimization. For a prescribed inlet velocity and outlet pressure, pressure recovery coefficient C-p* (the objective function) is estimated computationally for various design options. The CFD software and the GA have been combined in a monolithic platform for a fully automated operation using some special control commands. Based on the developed algorithm, an extensive exercise has been made to optimize the diffuser shape. Different methodologies have been adopted to create a large number of design options. Interestingly, not much difference has been noted in the optimum C-p* values obtained through different approaches. However, in all the approaches, a better design has been obtained through a proper selection of the number of design variables. Finally, the effect of diffuser length on the optimum shape has also been studied."," Copyright (C) 2009 John Wiley & Sons, Ltd.","Ghosh, S.|Pratihar, D. K.|Maiti, B.|Das, P. K.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,incompressible flow|optimization|genetic algorithm|2d planar diffuser|2d planar duct,10.1002/fld.2124
76,WOS:000404133200071,2017,Addressing Large-Scale Energy Retrofit of a Building Stock via Representative Building Samples: Public and Private Perspectives,COST-OPTIMALITY|MULTIOBJECTIVE OPTIMIZATION|RESIDENTIAL BUILDINGS|PERFORMANCE|IMPACT|CONSUMPTION|METHODOLOGY|CATEGORY|ENVELOPE|POLICIES,"Scientific literature about energy retrofit focuses on single buildings, but the investigation of whole building stocks is particularly worthy because it can yield substantial energy, environmental and economic benefits. Hence, how to address large-scale energy retrofit of existing building stocks? The paper handles this issue by employing a methodology that provides a robust energy analysis of building categories. This is denoted as SLABE, ""Simulation-based Large-scale uncertainty/sensitivity Analysis of Building Energy performance"". It was presented by the same authors and is here enhanced to investigate a whole and heterogeneous building stock that includes various categories. Each category is represented via a Representative Building Sample (RBS), which is defined through Latin hypercube sampling and uncertainty analysis. Hence, optimal retrofit packages are found in function of building location, intended use and construction type. Two families of optimal solutions are achieved. The first one collects the most energy-efficient (and thus sustainable) solutions, among the ones that produce global cost savings, thereby addressing the public perspective. The second one collects cost-optimal solutions thereby addressing the private perspective. EnergyPlus is employed as a simulation tool and coupled with MATLAB (R) for data analysis and processing. The methodology is applied to a significant share of the Italian public administration building stock, which includes several building categories depending on location, use destination and construction type. The outcomes show huge potential energy and economic savings, and could support a deep energy renovation of the Italian building stock.",,"Ascione, Fabrizio|Bianco, Nicola|De Stasio, Claudio|Mauro, Gerardo Maria|Vanoli, Giuseppe Peter",SUSTAINABILITY,building energy performance|energy simulations|building stock|retrofit|building sampling|representative building sample|large-scale analysis|cost-optimal|public incentives,10.3390/su9060940
77,WOS:000331916100026,2014,Life cycle assessment of corn-based ethanol production in Argentina,TILLAGE SYSTEMS|BIOENERGY PRODUCTION|UNITED-STATES|LAND-USE|EMISSIONS|BIOFUELS|IMPACTS|PAMPAS|ENERGY|DYNAMICS,"The promotion of biofuels as energy for transportation in the world is mainly driven by the perspective of oil depletion, the concerns about energy security and global warming. In Argentina, the legislation has imposed the use of biofuels in blend with fossil fuels ( to %) in the transport sector. The aim of this paper is to assess the environmental impact of corn-based ethanol production in the province of Santa Fe in Argentina based on the life cycle assessment methodology. The studied system includes from raw materials production to anhydrous ethanol production using dry milling technology. The system is divided into two subsystems: agricultural system and refinery system. The treatment of stillage is considered as well as the use of co-products (distiller's dried grains with solubles), but the use and/or application of the produced biofuel is not analyzed: a cradle-to-gate analysis is presented. As functional unit,  MJ of anhydrous ethanol at biorefinery is chosen. Two life cycle impact assessment methods are selected to perform the study: Eco-indicator  and ReCiPe. SimaPro is the life cycle assessment software used. The influence of the perspectives on the model is analyzed by sensitivity analysis for both methods. The two selected methods identify the same relevant processes. The use of fertilizers and resources, seeds production, harvesting process, corn drying, and phosphorus fertilizers and acetamide-anillide-compounds production are the most relevant processes in agricultural system. For refinery system, corn production, supplied heat and burned natural gas result in the higher contributions. The use of distiller's dried grains with solubles has an important positive environmental impact.", (C) 2013 Elsevier B.V. All rights reserved.,"Pieragostini, Carla|Aguirre, Pio|Mussati, Miguel C.",SCIENCE OF THE TOTAL ENVIRONMENT,life cycle assessment|corn-based ethanol|eco-indicator 99|recipe|sensitivity analysis|perspectives analysis,10.1016/j.scitotenv.2013.11.012
78,WOS:000377452800009,2016,The Influence of Model Structure Uncertainty on Water Quality Assessment,RUNOFF|APPLICABILITY|TRANSPORT|SUPPORT,"Physically-based mathematical water quality models are known as potentially effective tools to simulate the temporal and spatial variations of water quality variables along rivers. Each model relies on specific sets of assumptions and equations to simulate the physico-biochemical processes, which influence on its simulation results. This paper aims to improve the insight in the uncertainties related to state-of-the-art river physico-biochemical water quality modelling. Sensitivity analysis is applied to the processes implemented in three most popular commercial software packages: MIKE, InfoWorks RS and InfoWorks ICM. This is done for the Molse Neet river case study. Firstly, the physico-biochemical processes are screened to obtain a preliminary assessment on the critical processes and to determine the processes that require more detailed comparison. Then, local sensitivity analysis is carried out to specify the sensitive parameters and processes. Results show that the hydrodynamic results, heat transfer rate and reaeration simulations cause large differences in model simulation outputs for water temperature and dissolved oxygen concentrations. The ignorance of processes related to sediment transport, phytoplankton and bacteria has a significant influence on the higher values of organic matter and lower values of dissolved oxygen concentrations. The three models show consensus on the main pollutant sources explaining organic matter and nitrate concentrations, but disagree on the main factors explaining the DO concentrations.",,"Thanh Thuy Nguyen, |Willems, Patrick",WATER RESOURCES MANAGEMENT,infoworksicm|infoworksrs|mike11|model structure uncertainty|river water quality model|sensitivity analysis,10.1007/s11269-016-1330-x
79,WOS:000259363300011,2008,Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment,EXPERT JUDGMENT|PROBABILITY-DISTRIBUTIONS|COMPREHENSIVE REALISM|INHALED FORMALDEHYDE|PHARMACOKINETIC DATA|CLIMATE-CHANGE|LUNG-CANCER|F344 RAT|INFORMATION|ELICITATION,"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure.",,"Small, Mitchell J.",RISK ANALYSIS,bayesian belief network|cancer risk assessment|distributional method|expert judgment|genotoxicity|mode of action|uncertainty analysis|weight of evidence,10.1111/j.1539-6924.2008.01134.x
80,WOS:000353971300004,2015,Enhancing the Characterization of Epistemic Uncertainties in PM2.5 Risk Analyses,PARTICULATE AIR-POLLUTION|LONG-TERM EXPOSURE|UNITED-STATES|FOLLOW-UP|6 CITIES|MORTALITY|FINE|COHORT|VETERANS|QUALITY,"The Environmental Benefits Mapping and Analysis Program (BenMAP) is a software tool developed by the U.S. Environmental Protection Agency (EPA) that is widely used inside and outside of EPA to produce quantitative estimates of public health risks from fine particulate matter (PM.). This article discusses the purpose and appropriate role of a risk analysis tool to support risk management deliberations, and evaluates the functions of BenMAP in this context. It highlights the importance in quantitative risk analyses of characterization of epistemic uncertainty, or outright lack of knowledge, about the true risk relationships being quantified. This article describes and quantitatively illustrates sensitivities of PM. risk estimates to several key forms of epistemic uncertainty that pervade those calculations: the risk coefficient, shape of the risk function, and the relative toxicity of individual PM. constituents. It also summarizes findings from a review of U.S.-based epidemiological evidence regarding the PM. risk coefficient for mortality from long-term exposure. That review shows that the set of risk coefficients embedded in BenMAP substantially understates the range in the literature. We conclude that BenMAP would more usefully fulfill its role as a risk analysis support tool if its functions were extended to better enable and prompt its users to characterize the epistemic uncertainties in their risk calculations. This requires expanded automatic sensitivity analysis functions and more recognition of the full range of uncertainty in risk coefficients.",,"Smith, Anne E.|Gans, Will",RISK ANALYSIS,benmap|epidemiology|health risk|pm2|5|risk analysis|uncertainty,10.1111/risa.12236
81,WOS:000227978000007,2005,"Investigating uncertainty and sensitivity in integrated, multimedia environmental models: tools for FRAMES-3MRA",FRAMEWORK|SYSTEM,"Elucidating uncertainty and sensitivity structures in environmental models can be a difficult task, even for low-order, single-medium constructs driven by a unique set of site-specific data. Quantitative assessment of integrated, multimedia models that simulate hundreds of sites, spanning multiple geographical and ecological regions, will ultimately require a comparative approach using several techniques, coupled with sufficient computational power. The Framework for Risk Analysis in Multimedia Environmental Systems - Multimedia, Multipathway, and Multireceptor Risk Assessment (FRAMES-MRA) is an important software model being developed by the United States Environmental Protection Agency for use in risk assessment of hazardous waste management facilities. The MRA modeling system includes a set of  science modules that collectively simulate release, fate and transport, exposure, and risk associated with hazardous contaminants disposed of in land-based waste management units (WMU). The MRA model encompasses  multi-dimensional input variables, over  of which are explicitly stochastic. Design of SuperMUSE, a  GHz PC-based, Windows-based Supercomputer for Model Uncertainty and Sensitivity Evaluation is described. Developed for MRA and extendable to other computer models, an accompanying platform-independent, Java-based parallel processing software toolset is also discussed. For MRA, comparison of stand-alone PC versus SuperMUSE simulation executions showed a parallel computing overhead of only . seconds/simulation, a relative cost increase of .% over average model runtime. Parallel computing software tools represent a critical aspect of exploiting the capabilities of such modeling systems. The Java toolset developed here readily handled machine and job management tasks over the Windows cluster, and is currently capable of completing over  million MRA model simulations per month on SuperMUSE. Preliminary work is reported for an example uncertainty analysis of Benzene disposal that describes the relative importance of various exposure pathways in driving risk levels for ecological receptors and human health. Incorporating landfills, waste piles, aerated tanks, surface impoundments, and land application units, the site-based data used in the analysis included  facilities across the United States representing  site-WMU combinations.", Published by Elsevier Ltd.,"Babendreier, JE|Castleton, KJ",ENVIRONMENTAL MODELLING & SOFTWARE,multimedia model|parallel computing|pc-based supercomputing|uncertainty analysis|sensitivity analysis|benzene disposal|java,10.1016/j.envsoft.2004.09.013
82,WOS:000389389400004,2016,Simulation of environmental impact of an existing natural gas dehydration plant using a combination of thermodynamic models,SENSITIVITY-ANALYSIS|MIXING RULES|EQUATIONS|STATE,"A new approach was presented to improve the simulation results of an existing TEG based natural gas dehydration plant, using Aspen Plus software. Furthermore, the environmental impact of the plant was investigated. The plant consists of four main unit operations including an absorber, a flash tank, a stripper and a regenerator. Twelve thermodynamic models were assigned to these units. In the first step of the study, only one thermodynamic model was assigned to all of the units while in other steps, combinations of thermodynamic models were employed. The most accurate model combination was found to be RKSMHV for the absorber and stripper and PSRK for the flash tank and regenerator. It was found that a proper combination of thermodynamic models may improve the simulation results. As solvent circulation rate increased, BTEX, VOC and greenhouse gas emissions enhanced. (C)  Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Torkmahalleh, Mehdi Amouei|Magazova, Galiya|Magazova, Aliya|Rad, Seyed Jamal Hassani",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,natural gas dehydration|btex|voc|greenhouse gas|aspen plus|thermodynamic models,10.1016/j.psep.2016.08.008
83,WOS:000317821300006,2013,Sensitivity Analysis of Multiple Informant Models When Data Are Not Missing at Random,MARITAL SATISFACTION|DROP-OUT|SYMPTOMS|ADOPTION|PARENT|CHILD,"Missing data are common in studies that rely on multiple informant data to evaluate relationships among variables for distinguishable individuals clustered within groups. Estimation of structural equation models using raw data allows for incomplete data, and so all groups can be retained for analysis even if only  member of a group contributes data. Statistical inference is based on the assumption that data are missing completely at random or missing at random. Importantly, whether or not data are missing is assumed to be independent of the missing data. A saturated correlates model that incorporates correlates of the missingness or the missing data into an analysis and multiple imputation that might also use such correlates offer advantages over the standard implementation of SEM when data are not missing at random because these approaches could result in a data analysis problem for which the missingness is ignorable. This article considers these approaches in an analysis of family data to assess the sensitivity of parameter estimates and statistical inferences to assumptions about missing data, a strategy that could be easily implemented using SEM software.",,"Blozis, Shelley A.|Ge, Xiaojia|Xu, Shu|Natsuaki, Misaki N.|Shaw, Daniel S.|Neiderhiser, Jenae M.|Scaramella, Laura V.|Leve, Leslie D.|Reiss, David",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,auxiliary variables|missing data|missing not at random|multiple imputation|multiple informant data,10.1080/10705511.2013.769393
84,WOS:000236055400006,2006,Simulation of yield decline as a result of water stress with a robust soil water balance model,DEFICIT IRRIGATION|CROP MODELS|GROWTH|WHEAT|MANAGEMENT,"The relative yield decline that is expected under specific levels of water stress at different moments in the growing period is estimated by integrating the FAO K, approach [Doorenbos, J., Kassam, A.H., . Yield response to water. FAO Irrigation and Drainage Paper No. . Rome, Italy] in the soil water balance model BUDGET. The water stored in the root zone is determined in the soil water balance model on a daily basis by keeping track of incoming and outgoing water fluxes at its boundary. Given the simulated soil water content in the root zone, the corresponding crop water stress is deter-mined. Subsequently, the yield decline is estimated with the Ky approach. In the Ky approach the relation between water stress in a particular growth stage and the corresponding expected yield is described by a linear function. To account for the effect of water stresses in the various growth stages, the multiplicative, seasonal and minimal approach are integrated in the model. To evaluate the model, the simulated yields for two crops under various levels of water stress in two different environments were compared with observed yields: winter wheat under three different water application levels in the North of Tunisia, and maize in three different farmers' fields in different years in the South West of Burkina Faso. Simulated crop yields agreed well with observed yields for both locations using the multiplicative approach. The correlation value (R ) between observed and simulated yields ranged from . to . with very high modeling efficiencies. The root mean square error values are relatively small and ranged between  and %. The minimal and seasonal approaches performed significantly less accurately in both of the study areas. Estimation of yields on basis of relative transpiration performed significantly better than estimations on basis of relative evapotranspiration in Burkina Faso. A sensitivity analysis showed that the model is robust and that good estimates can be obtained in both regions even by using indicative values for the required crop and soil parameters. The minimal input requirement, the robustness of the model and its ability to describe the effect on seasonal yield of water stress occurring at particular moments in the growing period, make the model very useful for the design of deficit irrigation strategies. BUDGET is public domain software and hence freely available. An installation disk and manual can be downloaded from the web.", (c) 2005 Elsevier B.V. All rights reserved.,"Raes, D|Geerts, S|Kipkorir, E|Wellens, J|Sahli, A",AGRICULTURAL WATER MANAGEMENT,water productivity|maize|winter wheat|k-y approach|yield estimation|soil water balance,10.1016/j.agwat.2005.04.006
85,WOS:000256623900025,2008,Development of a open-vessel single-stage respirometer,ACTIVATED-SLUDGE|WASTE-WATER|PARAMETERS|MODEL,"This paper describes the development and accuracy analysis of a single-stage respirometer which can be used both in the laboratory for wastewater characterization and in the plant as a process instrument. It is based on an accurate model of parasitic aeration, making the two-stage assumption unnecessary. Its operation is supervised by a real-time software, written in Lab View, managing the various measurement procedures and estimating the wastewater characteristics. Its accuracy is assessed through sensitivity and error propagation analysis, proving superior to the conventional model. A laboratory implementation of the instrument was tested with readily degradable substrate, yielding consistent and accurate respirograms.",,"Marsili-Libelli, S.|D'Ardes, V.|Bondi, C.",WATER SCIENCE AND TECHNOLOGY,on-line process control|parameter estimation|respirometry|sensitivity analysis|sensors,10.2166/wst.2008.149
86,WOS:000414958700005,2017,Spatial analysis and health risk assessment of heavy metals concentration in drinking water resources,MONTE-CARLO-SIMULATION|SURFACE-WATER|SENSITIVITY-ANALYSIS|GROUNDWATER|CONTAMINATION|POLLUTION|CHINA|RIVER|GIS|EXPOSURE,"The heavy metals available in drinking water can be considered as a threat to human health. Oncogenic risk of such metals is proven in several studies. Present study aimed to investigate concentration of the heavy metals including As, Cd, Cr, Cu, Fe, Hg, Mn, Ni, Pb, and Zn in  water supply wells and  water reservoirs within the cities Ardakan, Meibod, Abarkouh, Bafgh, and Bahabad. The spatial distribution of the concentration was carried out by the software ArcGIS. Such simulations as non-carcinogenic hazard and lifetime cancer risk were conducted for lead and nickel using Monte Carlo technique. The sensitivity analysis was carried out to find the most important and effective parameters on risk assessment. The results indicated that concentration of all metals in  wells (except iron in  cases) reached the levels mentioned in EPA, World Health Organization, and Pollution Control Department standards. Based on the spatial distribution results at all studied regions, the highest concentrations of metals were derived, respectively, for iron and zinc. Calculated HQ values for non-carcinogenic hazard indicated a reasonable risk. Average lifetime cancer risks for the lead in Ardakan and nickel in Meibod and Bahabad were shown to be . x (-), . x (-), and  x (-), respectively, demonstrating high carcinogenic risk compared to similar standards and studies. The sensitivity analysis suggests high impact of concentration and BW in carcinogenic risk.",,"Fallahzadeh, Reza Ali|Ghaneian, Mohammad Taghi|Miri, Mohammad|Dashti, Mohamad Mehdi",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,groundwater|metals|health risk assessment|monte carlo simulation|sensitivity analysis|geographic information systems,10.1007/s11356-017-0102-3
87,WOS:000356610500013,2015,Environmental and economic analysis of residual woody biomass transport for energetic use in Chile,LIFE-CYCLE ASSESSMENT|SOLID-WASTE MANAGEMENT|CORN STOVER|UNCERTAINTY|OPTIONS|SYSTEMS|STRATEGIES|MINNESOTA|SPAIN|MODEL,"This study compares transport performance of residual biomass using different pre-treatment options. Life cycle inventory data was obtained from forestry companies in southern Chile, databases, scientific and technological literature, as well as equipment operational manuals. Three different scenarios were evaluated: residual biomass transport without pre-treatment (scenario ), chipped residual biomass (scenario ), and compacted residual biomass (scenario ) transport. The truck's loading capacity was considered as a function of the residual biomass density. Impact assessment was performed using software SimaPro .. using the ReCiPe midpoint methodology. Moreover, an uncertainty analysis was performed using Monte Carlo simulation with a  % confidence. Transport costs evaluation variables considered were machine cost, machine residual value, amortization, personnel costs, fuel consumption, machine maintenance, and operational yield. All variables are based on local conditions of La Araucania Region in Chile. Regarding greenhouse gas (GHG) emissions, optimum transport distance ranges were identified for the different scenarios. For a distance up to  km, scenario  is the most favorable; for distances between  and  km, scenario  is the most favorable one; and for distances longer than  km, compacted residual biomass (scenario ) presents the lowest GHG emissions balance. When looking the other impact categories, it was established that the benefits are not only related to GHG emission savings but also to other impact categories. Transport impacts are only relevant for large distances, while for short distances biomass pre-treatment and loading stages provoke a higher environmental load. In fact, for scenario  where chipped biomass is transported, only for distances longer than  km, the transport stage accounts for more than  % of the environmental load of all impact categories. For the case of scenario  (compacted biomass transport), this situation occurs for a distance of at least  km. Most probable optimal transport distances were determined for pre-treated and unpretreated biomass. In this sense, for determining the best transport option of residual biomass, transport distance, loading capacity, and pre-treatment processes efficiency, including chipping and compacting, as well as data uncertainty, should be taken into account. From these variables, biomass loading and pre-treatment stages account for a relevant percentage of the environmental impacts generated for transport distances of less than  km. In this sense, biomass loading and pre-treatment efficiency coupled with the effective supplies demand should be carefully studied in future research works.",,"Munoz, Edmundo|Vargas, Sebastian|Navia, Rodrigo",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,cost|lca|residual woody biomass|transport,10.1007/s11367-015-0891-x
88,WOS:000278898800009,2010,Sensitivity coefficients for matrix-based LCA,LIFE-CYCLE ASSESSMENT|UNCERTAINTY|PRODUCT,"Background, aim, and scope Matrix-based life cycle assessment (LCA) is part of the standard ingredients of modern LCA tools. An important aspect of matrix-based LCA that is straightforward to carry out, but that requires a careful mathematical handling, is the inclusion of sensitivity coefficients based on differentiating the matrix-based formulas. Materials and methods We briefly review the basic equations for LCA and the basic theory of sensitivity coefficients. Results We present the complete set of sensitivity coefficients from inventory to weighting through characterization and normalization. We show the specific formulas for perturbation analysis, uncertainty analysis, and key issue analysis. We also provide an example using the ecoinvent data. Discussion The limitations of the present approach include the restriction to small changes and uncertainties and the ignorance of correlation between input uncertainties. In contrast to common thinking, there is no restriction to normally distributed uncertainties: Every uncertainty distribution for which a variance can be defined can be submitted to the analytical uncertainty analysis. Conclusions This paper provides a useful set of tables for a number of purposes related to uncertainty and sensitivity analysis. Recommendations and perspectives Although the formulas derived are not simple, they are straightforward to implement in software for LCA. Once this is done, the use of these formulas can become routine practice, enabling a key issue analysis and speeding up perturbation and uncertainty analysis.",,"Heijungs, Reinout",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,derivatives|life cycle interpretation|matrix-based lca|sensitivity|taylor series expansion|uncertainty,10.1007/s11367-010-0158-5
89,WOS:000355131600020,2015,Effect of bacteria density and accumulated inert solids on the effluent pollutant concentrations predicted by the constructed wetlands model BIO_PORE,WASTE-WATER TREATMENT|SENSITIVITY-ANALYSIS|SIMULATION|GROWTH,"Constructed wetlands are a widely adopted technology for the treatment of wastewater in small communities. The understanding of their internal functioning has increased at an unprecedented pace over recent years, in part thanks to the use of mathematical models. BIO_PORE model is one of the most recent models developed for constructed wetlands. This model was built in the COMSOL Multiphysics (TM) software and implements the biokinetic expressions of Constructed Wetlands Model  (CWM) to describe the fate and transport of organic matter, nitrogen and sulphur in horizontal subsurface-flow constructed wetlands. In previous studies, CWM was extended with the inclusion of two empirical parameters (M-bio_max and M-cap) that proved to be essential to provide realistic bacteria growth rates and dynamics. The aim of the current work was to determine the effect of these two parameters on the effluent pollutant concentrations predicted by the model. To that end, nine simulations, each with a different M-bio_max-M-cap pair, were launched on a high-end multi-processor computer and the effluent COD and ammonia nitrogen concentrations obtained on each simulation were qualitatively compared among them. Prior to this study, a finite element mesh optimization procedure was carried out to reduce computational cost. Results of the mesh optimization procedure indicated that among the  tested meshes of different element size, the mesh utilized for this model in previous studies represented a fair compromise between output accuracy and computation time. Results of the sensitivity analysis showed that the value of M-cap has a dramatic effect on the simulated effluent concentrations of COD and ammonia nitrogen, which clearly decreased for increasing values of this parameter. On the other hand, the model output was also sensitive to the values of M-bio_max, but its effects were less important and no clear relation could be established between its value and the simulated effluent concentration of COD and ammonia nitrogen.", (C) 2014 Elsevier B.V. All rights reserved.,"Samso, Roger|Blazquez, Jordi|Agullo, Nuria|Grau, Joan|Torres, Ricardo|Garcia, Joan",ECOLOGICAL ENGINEERING,local sensitivity|mesh optimization|bacteria|growth|parallel computing|batch,10.1016/j.ecoleng.2014.09.069
90,WOS:000301688100024,2012,Analyzing longitudinal clinical trial data with nonignorable missingness and unknown missingness reasons,QUALITY-OF-LIFE|LOCAL SENSITIVITY|DROP-OUT|BAYESIAN-INFERENCE|NONRANDOM DROPOUT|REPEATED OUTCOMES|BINARY DATA|MODELS|NONRESPONSE|REGRESSION,"Longitudinal clinical trials are often plagued by nonmonotone missingness due to both patient dropout and intermittent missingness. Standard analysis assumes that missingness is ignorable. Because the assumption can be questionable, the sensitivity of inferences to alternative assumptions about missingness needs to be evaluated. This need arises in the analysis of a longitudinal prostate cancer quality-of-life (QoL) clinical trial dataset, in which nonmonotone missingness occurs. The choice of the missing data model is studied in the analysis. A local sensitivity analysis method is then applied to analyze the dataset and to investigate the changes in parameter estimates in the neighborhood of the ignorable model. One advantage of the method is that it surmounts computational difficulty and completely avoids evaluating the high-dimensional integrals in the likelihood due to nonmonotone missingness. Another is that it can be implemented using the standard software without excessive additional computation. The method is especially advantageous for large clinical datasets for which alternative approaches can become computationally prohibitive. In addition, the analysis demonstrates the importance of exploiting information on reasons for missingness. When such information is unavailable for some missingness and therefore the missingness types (i.e., dropout versus intermittent missingness) are unknown, a bound analysis is proposed, combined with genetic algorithms, to account for unknown missingness types. The analysis demonstrates the usefulness of the method as a general approach to evaluating the sensitivity of standard analysis to nonignorable nonmonotone missingness in clinical trials.", (C) 2010 Elsevier B.V. All rights reserved.,"Xie, Hui",COMPUTATIONAL STATISTICS & DATA ANALYSIS,bound analysis|clinical trial|genetic algorithm|missing data|multinomial logit model|sensitivity analysis,10.1016/j.csda.2010.11.021
91,WOS:000257960600006,2008,Life cycle assessment of commercial furniture: a case study of Formway LIFE chair,WASTE MANAGEMENT,"Background, aims and scope The environmental aspects of companies and their products are becoming more significant in delivering competitive advantage. Formway Furniture, a designer and manufacturer of office furniture products, is a New Zealand-based company that is committed to sustainable development. It manufactures two models of the light, intuitive, flexible and environmental (LIFE) office chair: one with an aluminium base and one with a glass-filled nylon (GFN) base. It was decided to undertake a life cycle assessment (LCA) study of these two models in order to: () determine environmental hotspots in the life cycle of the two chairs (goal ); () compare the life cycle impacts of the two chairs (goal ); and () compare alternative potential waste-management scenarios (goal ). The study also included sensitivity analysis with respect to recycled content of aluminium in the product. Materials and methods The LIFE chair models consist of a mix of metal and plastic components manufactured by selected Formway suppliers according to design criteria. Hence, the research methodology included determining the specific material composition of the two chair models and acquisition of manufacturing data from individual suppliers. These data were compiled and used in conjunction with pre-existing data, specifically from the ecoinvent database purchased in conjunction with the SimaPro LCA software, to develop the life cycle inventory of the two chair models. The life cycle stages included in the study extended from raw-material extraction through to waste management. Impact assessment was carried out using CML  baseline , the methodology developed by Leiden University's Institute for Environmental Sciences. Results This paper presents results for global warming potential (GWP). The study showed a significant impact contribution from the raw-material extraction/refinement stage for both chair models; aluminium extraction and refining made the greatest contribution to GWP. The comparison of the two LIFE chair models showed that the model with the aluminium base had a higher GWP impact than the model with the GFN base. The waste-management scenario compared the GWP result when () both chair models were sent to landfill and () steel and aluminium components were recycled with the remainder of the chair sent to landfill. The results showed that the recycling scenario contributed to a reduced GWP result. Since production and processing of aluminium was found to be significant, a sensitivity analysis was carried out to determine the impact of using aluminium with different recycled contents (%, % and %) in both waste-management scenarios; this showed that increased use of recycled aluminium was beneficial. The recycling at end-of-life scenarios was modelled using two different end-of-life allocation approaches, i.e. consequential and attributional, in order to illustrate the variation in results caused by choice of allocation approach. The results using the consequential approach showed that recycling at end-of-life was beneficial, while use of the attributional method led to a similar GWP as that seen for the landfill scenario. Discussion The results show that the main hotspot in the life cycle is the raw-material extraction/refinement stage. This can be attributed to the extraction and processing of aluminium, a material that is energy intensive. The LIFE chair model with the aluminium base has a higher GWP as it contains more aluminium. Sensitivity analysis pertaining to the recycled content of aluminium showed that use of aluminium with high recycled content was beneficial; this is because production of recycled aluminium is less energy intensive than production of primary aluminium. The waste-management scenario showed that recycling at end-of-life resulted in a significantly lower GWP than landfilling at end-of-life. However, this result is dependent upon the modelling approach used for recycling. Conclusions With respect to goal , the study found that the raw-material extraction/refinement stage of the life cycle was a significant factor for both LIFE chair models. This was largely due to the use of aluminium in the product. For goal , it was found that the LIFE chair model with the aluminium base had a higher GWP than the GFN model, again due to the material content of the two models. Results for goal  illustrated that recycling at end-of-life is beneficial when using a system expansion (consequential) approach to model recycling; if an attributional 'cut-off' approach is used to model recycling at end-of-life, there is virtually no difference in the results between landfilling and recycling. Sensitivity analysis pertaining to the recycled content of aluminium showed that use of higher recycled contents leads to a lower GWP impact. Recommendation and perspectives Most of the GWP impact was contributed during the raw-material extraction/refinement stage of the life cycle; thus, the overall impact of both LIFE chair models may be reduced through engaging in material choice and supply chain environmental management with respect to environmental requirements. The study identified aluminium components as a major contributor to GWP for both LIFE chair models and also highlighted the sensitivity of the results to its recycled content. Thus, it is recommended that the use of aluminium in future product designs be limited unless it is possible to use aluminium with a high recycled content. With respect to waste management, it was found that a substantial reduction in the GWP impact would occur if the chairs are recycled rather than landfilled, assuming an expanding market for aluminium. Thus, recycling the two LIFE chair models at end-of-life is highly recommended.",,"Gamage, Gayathri Babarenda|Boyle, Carol|McLaren, Sarah J.|McLaren, Jake",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,carbon footprint|case study|commercial furniture|life chair|lca|life cycle assessment|office furniture,10.1007/s11367-008-0002-3
92,WOS:000265171300023,2009,Artificial neural networks to predict daylight illuminance in office buildings,CONTROL-SYSTEMS|COST ESTIMATION|PERFORMANCE|COEFFICIENT|SAVINGS|DESIGN|MODELS|FUZZY,"A prediction model was developed to determine daylight illuminance for the office buildings by using artificial neural networks (ANNs). Illuminance data were collected for  months by applying a field measuring method. Utilizing weather data from the local weather station and building parameters from the architectural drawings, a three-layer ANN model of feed-forward type (with one output node) was constructed. Two variables for time (date, hour),  weather determinants (outdoor temperature, solar radiation, humidity, UV index and UV dose) and  building parameters (distance to windows. number of windows, orientation of rooms, floor identification, room dimensions and point identification) were considered as input variables. Illuminance was used as the output variable. In ANN modeling, the data were divided into two groups; the first  of these data sets were used for training and the remaining  for testing. Microsoft Excel Solver used simplex optimization method for the optimal weights. The model's performance was then measured by using the illuminance percentage error. As the prediction power of the model was almost %, predicted data had close matches with the measured data. The prediction results were successful within the sample measurements. The model was then subjected to sensitivity analysis to determine the relationship between the input and output variables. NeuroSolutions Software by NeuroDimensions Inc., was adopted for this application. Researchers and designers will benefit from this model in daylighting performance assessment of buildings by making predictions and comparisons and in the daylighting design process by determining illuminance. (C) ", Elsevier Ltd. All rights reserved.,"Kazanasmaz, Tugce|Gunaydin, Murat|Binol, Selcen",BUILDING AND ENVIRONMENT,modeling|building|daylighting|artificial neural networks,10.1016/j.buildenv.2008.11.012
93,WOS:000391079300041,2016,Electric vehicle transformation in Beijing and the comparative eco-environmental impacts: A case study of electric and gasoline powered taxis,ENERGY|BATTERIES|HYBRID,"Tailpipe emissions of gasoline vehicles are one of a main cause of atmospheric environmental problems such as global warming and haze. The substitution of electric vehicles for conventional gasoline vehicles is a promising new way to reduce urban air pollution in many countries such as America, Japan, the EU countries and China. In , Beijing launched a plan that was the substitution of electric vehicles (such as Midi taxis) for gasoline vehicles (such as Hyundai taxis) to low carbon transformations. Our study used local data and LCA based analysis to compare environmental impacts of the transition from Hyundai gasoline vehicles to Midi electric vehicles. We established a life.cycle analysis (LCA) model with GaBi software and a life cycle assessment integrated model by CML (Problem oriented) and EI (Damage oriented) models, which evaluated the comparative environmental impacts of the full life cycle, production stage, use stage and end of life. Finally, we analyzed the key haze-induced factors, key life cycle processes and the sensitivities of lifetime and electric power mbc. Our results indicated that in light of the full life cycle assessment, electric vehicles could play significant role in decreasing the potential of Global Warming, Abiotic Depletion and Ozone Layer Depletion; whereas, electric vehicles also exhibited the impacts for increases in the potential of Acidification, Eutrophication, Human Toxicity and Eco-toxicity (marine aquatic and terrestrial). On the basis of inventory data analysis and  Beijing electricity mix, the comparative results of haze-induced pollutants emissions showed that the full life cycle emission of VOCs from a Midi electric vehicle was lower than a Hyundai gasoline vehicle, but the emissions of PM., NOx, SOX of a Midi electric vehicle were higher than a Hyundai gasoline vehicle. These differences are mainly the result of different emissions during the use stages. In addition, the results of sensitivity analysis indicated that the decreased rate of Green House Gas per kilometer (Midi relative to Hyundai) gradually improved with the increase of lifetime and use of cleaner energy; haze-induced pollutants and carbon emissions from EVs could be reduced significantly with the increased use of cleaner energy. (C) ", Elsevier Ltd. All rights reserved.,"Shi, Xiaoqing|Wang, Xue|Yang, Jianxin|Sun, Zhaoxin",JOURNAL OF CLEANER PRODUCTION,electric taxi|transformation|environmental impacts|life cycle assessment|low carbon,10.1016/j.jclepro.2016.07.096
94,WOS:000255505900032,2008,A sensitivity analysis of typical life insurance contracts with respect to the technical basis,,"In [Christiansen, M.C., . A sensitivity analysis concept for life insurance with respect to a valuation basis of infinite dimension. Insurance: Math. Econom. doi: ./j.insmatheco...] a sensitivity analysis concept was introduced for the prospective reserve of individual life insurance contracts as functional of the technical basis parameters such as interest rate, mortality probability, disability probability, et cetera. On the basis of that concept, the present paper gives in addition the sensitivities of the premium level. Applying these approaches, an extensive sensitivity analysis is carried out: A study of the basic life insurance contract types 'pure endowment insurance', 'temporary life insurance', 'annuity insurance' and 'disability insurance' identifies their diverse characteristics, in particular their weakest points concerning fluctuations of the technical basis. An investigation of combinations of these insurance contract types shows what synergy effects can be expected by creating insurance packages.", (C) 2007 Elsevier B.V. All rights reserved.,"Christiansen, Marcus C.",INSURANCE MATHEMATICS & ECONOMICS,life insurance|variations in the technical basis|prospective reserve|premium level|sensitivity analysis|insurance packages,10.1016/j.insmatheco.2007.08.005
95,WOS:000231058700001,2005,A user's guide to the brave new world of designing simulation experiments,COMPUTER EXPERIMENTS|SENSITIVITY-ANALYSIS|SAMPLING CRITERIA|ROBUST DESIGN|OUTPUT|MODEL|OPTIMIZATION|METHODOLOGY|METAMODELS|MANAGEMENT,"Many simulation practitioners can get more from their analyses by using the statistical theory on design of experiments (DOE) developed specifically for exploring computer models. We discuss a toolkit of designs for simulators with limited DOE expertise who want to select a design and an appropriate analysis for their experiments. Furthermore, we provide a research agenda listing problems in the design of simulation experiments-as opposed to real-world experiments-that require more investigation. We consider three types of practical problems: () developing a basic understanding of a particular simulation model or system, () finding robust decisions or policies as opposed to so-called optimal solutions, and () comparing the merits of various decisions or policies. Our discussion emphasizes aspects that are typical for simulation, such as having many more factors than in real-world experiments, and the sequential nature of the data collection. Because the same problem type may be addressed through different design types, we discuss quality attributes of designs, such as the ease of design construction, the flexibility for analysis, and efficiency considerations. Moreover, the selection of the design type depends on the metamodel (response surface) that the analysts tentatively assume; for example, complicated metamodels require more simulation runs. We present several procedures to validate the metamodel estimated from a specific design, and we summarize a case study illustrating several of our major themes. We conclude with a discussion of areas that merit more work to achieve the potential benefits-either via new research or incorporation into standard simulation or statistical packages.",,"Kleijnen, JPC|Sanchez, SM|Lucas, TW|Cioppa, TM",INFORMS JOURNAL ON COMPUTING,simulation|design of experiments|metamodels|latin hypercube|sequential bifurcation|robust design,10.1287/ijoc.1050.0136
96,WOS:000233651000004,2005,A review of probabilistic risk assessment of contaminated land,MONTE-CARLO-SIMULATION|POLYCYCLIC AROMATIC-HYDROCARBONS|SOIL REMEDIATION GOALS|NON-IONIZED CHEMICALS|SUPERFUND SITES|UNCERTAINTY ANALYSIS|EXPOSURE ASSESSMENT|CLEANUP LEVELS|HEALTH-RISKS|HETEROGENEOUS AQUIFER,"Background, Aims and Scope. The management and decisions concerning restoration of contaminated land often require in-depth risk analyses. An environmental risk assessment is generally described as proceeding in four separate steps: hazard identification, dose-response assessment, exposure assessment, and risk characterization. The risk assessment should acknowledge and quantify the uncertainty in risk predictions. This can be achieved by applying probabilistic methods which, although they have been available for many years, are still not generally used. Risk assessment of contaminated land is an area where probabilistic methods have proved particularly useful. Many reports have appeared in the literature, mostly by North American researchers. The aim of this review is to summarize the experience gained so far, provide a number of useful examples, and suggest what may be done to promote probabilistic methods in Europe and the rest of the world. Methods. The available literature has been explored through searches in the major scientific and technical databases, WWW resources, textbooks and direct contacts with active researchers. A calculation example was created using standard simulation software. Results and Discussion. Uncertainty and variability are part of every risk assessment. Much work on risks from contaminated soil has focussed on exposure, and choice and structure of the exposure model is then a basic uncertainty factor. Other factors, e.g. parameter uncertainty, are easier to characterize. Variability can be separated into inter-individual, spatial and temporal components. Both uncertainty and variability in the exposure variables can be investigated using Monte Carlo simulation methods. These simulations enable not only the estimation of the probability for a given risk or exposure, but also add information on the sensitivity of the various input variables. This will assist the assessor in further refining the risk analysis. The large number of applications published encompasses soil contamination by lead, arsenic, chromium, uranium, polychlorinated bipheryls (PCB), polycyclic aromatic hydrocarbons (PAH), hexachloro benzene, pentachlorophenol and chlorinated solvents. Probabilistic risk assessments have been used in widely different settings, such as the metallurgical industry (mining and smelting operations), manufacturing, gas plants, wood impregnation, infrastructure, and waste landfills. Site-specific remediation goals can be specified using probabilistic methods, and a guideline document has been issued within the US Superfund programme. The usability of probabilistic risk assessment is illustrated by a calculation example. The current Swedish generic guideline value for benzo[a]pyrene in contaminated soil, with ingestion of vegetables as the major route of exposure, is compared with a probabilistic estimate. The toxicological reference value corresponds well with the upper th percentile of the estimated variability in intake, but does not account for uncertainty in the partition coefficients. Conclusions and Outlook. The probabilistic approach to risk assessment has proved its value in characterizing variability and uncertainty, and thereby contributing to a more informed and transparent decision-in a king process. The management of contaminated land is a major environmental application for probabilistic risk assessments. A substantial number of studies have been published and the method is now well established in the scientific community. This development has progressed further in the United States than elsewhere, but similar applications are now being reported from Europe and Asia. Probabilistic risk assessment is used to derive soil guideline values in the United Kingdom, and other countries may be anticipated to follow. However, efficient use of probabilistic methods for risk assessment of contaminated land requires certain components. There is a requirement for quality assurance and transparency that can be met by guidelines specifying data requirements and which items to report on. Both federal and state governments in the United States have issued such guidelines, and we see a similar need from a European perspective. A second component, necessary for a successful implementation of probabilistic methods, is education. We have ourselves developed undergraduate curricula, but we also see a need for continuous education of risk assessors and decision makers. The third component required is case studies, showing how probabilistic risk assessment can be implemented successfully in the cleanup of contaminated land. Most published studies originate from the United States, so here too there is a need for the rest of the world to catch LIP. In addition to the three components mentioned, there is an obvious need to develop and improve methods and practice of risk communication.",,"Oberg, T|Bergback, B",JOURNAL OF SOILS AND SEDIMENTS,"exposure assessment|monte carlo simulation|multimedia model|point estimate|probabilistic risk assessment, probability distributions|risk analysis|sensitivity analysis|uncertainty|variability",10.1065/jss2005.08.143
97,WOS:000247276500009,2007,Application of non-linear automatic optimization techniques for calibration of HSPF,GLOBAL OPTIMIZATION|MODELS,"Development of TMDLs (total maximum daily loads) is often facilitated by using the software system BASINS (Better Assessment Science Integrating point and Nonpoint Sources). One of the key elements of BASINS is the watershed model HSPF (Hydrological Simulation Program Fortran) developed by USEPA. Calibration of HSPF is a very tedious and time consuming task, more than  parameters are involved in the calibration process. In the current research, three non-linear automatic optimization techniques are applied and compared, as well an efficient way to calibrate HSPF is suggested. Parameter optimization using local and global optimization techniques for the watershed model is discussed. Approaches to automatic calibration of HSPF using the nonlinear parameter estimator PEST (Parameter Estimation Tool) with its Gauss-Marquardt-L evenberg (GMI-) method, Random multiple Search Method (RSM), and Shuffled Complex Evolution method developed at the University of Arizona (SCE-UA) are presented. Sensitivity analysis was conducted and the most and the least sensitive parameters were identified. It was noted that sensitivity depends on number of adjustable parameters. As more parameters were optimized simultaneously - a wider range of parameter values can maintain the model in the calibrated state. Impact of GML, RSM, and SCE-UA variables on ability to find the global minimum of the objective function (OF) was studied and the best variables are suggested. All three methods proved to be more efficient than manual HSPF calibration. Optimization results obtained by these methods are very similar, although in most cases RSM out performs GML and SCE-UA outperforms RSM. GML is a very fast method, it can perform as well as SCE-UA when the variables are properly adjusted, initial guess is good and insensitive parameters are eliminated from the optimization process. SCE-UA is very robust and convenient to use. Logical definition of key variables in most cases leads to the global minimum.",,"Iskra, Igor|Droste, Ronald",WATER ENVIRONMENT RESEARCH,calibration|hspf|pest|gauss-marquardt-levenberg method|sce-ua,10.2175/106143007X156862
98,WOS:000223741600032,2004,Technical and economic survey of low enthalpy solar installations for heating sanitary water,,"For the last decade, the energy supplies has become a very preoccupying problem, not only because of the increasing difficulties bound to oil production, but also because it is necessary today to admit that at the scale of our planet the energy resources, fossil-fuel or other, are limited. The solar energy is the only outside energy whose contribution is permanent to the global scale. However, real progress of solar field depends widely on its economic cost. In this context, this investigation focuses on a techno-economic study of a solar power unit for heating sanitary water at low enthalpy. A software program, based on a mathematical model, was elaborated, in order to identify criteria of an optimum use. These criteria take into account, in particular, the technical models, the recent technological innovations and an accurate analysis of the commercial balance (investment costs, actualised values of energy savings, profitability ratio, etc.). The solar installation production is determined by climate conditions, energy demand, collector area, material type and load capacity A sensitivity analysis is carried out to determine the influence of these parameters, and many others, on the yearly capacity. This paper presents the results of a program concerning water heating in a partial solar central power station. The study aims, on the one hand, to help the fitters and the industrials in the calculation and the design of such plants, and on the other hand, the economists will be allowed to compute the different financial and economic parameters in deal with the solar systems.",,"Khiari, B|Mabrouk, SB",DESALINATION,heating sanitary water|solar central power installation|economic cost|mathematical model|software survey|profitability,10.1016/j.desal.2004.06.030
99,WOS:000403557600004,2017,Comparative attributional life cycle assessment of European cellulase enzyme production for use in second-generation lignocellulosic bioethanol production,ENVIRONMENTAL ASSESSMENT|CELLULOSIC ETHANOL|ENERGY|BIOFUELS|CORN|CONVERSION|BIOMASS|ISSUES,"The production of cellulase enzymes (CE) has been identified as one major contributor towards the life cycle environmental and economic impacts of second-generation lignocellulosic bioethanol (LCB) production. Despite this knowledge, the literature lacks consistent and transparent life cycle assessments (LCA) which compare CE production based on the three more commonly proposed carbon sources: cornstarch glucose, sugar cane molasses and pre-treated softwood. Furthermore, numerous LCAs of LCB omit CE production from their system boundaries, with several authors citing the lack of available production data. In this article, we perform a comparative attributional LCA for the on-site production of  kg CE in full broth via submerged aerobic fermentation (SmF) based on the three alternative carbon sources, cases A, B and C, respectively. We determine life cycle inventory (LCI) material consumption using stoichiometric equations and volume flow, supplemented with information from the literature. All LCIs are provided in a consistent and transparent manner, filling the existing data gaps towards performing representative LCAs of LCB production with on-site CE production. Life cycle impact assessment (LCIA) results are determined with SimaPro  software using CML A baseline and non-baseline methods along with cumulative energy demand and are compared to results of similar studies. Sensitivity analysis is performed both for all major assumptions and for market changes with the application of advanced attributional LCA (AALCA). We find that CE production from pre-treated softwood (case C) provides the lowest environmental impacts, followed by sugar cane molasses (case B) and then cornstarch glucose (case A), with global warming potentials of ., . and . kg CO eq./kg enzyme, respectively. These findings compare well with those of similar studies, though great variation exists in the literature. Through sensitivity analysis, we determine that results are sensitive to assumptions made concerning carbon source origin, applied allocation, market changes, process efficiency and electricity supply. Furthermore, we find that the contribution of CE production towards the overall life cycle impacts of LCB is significant and that the omission of this sub-process in LCAs of LCB production can compromise their representativeness.",,"Gilpin, Geoffrey S.|Andrae, Anders S. G.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,biomass|cellulase enzyme|glucose|life cycle assessment|lignocellulosic bioethanol|molasses,10.1007/s11367-016-1208-4
100,WOS:000259464100004,2008,Improving user assessment of error implications in digital elevation models,ACCURACY ASSESSMENT|SPATIAL DATABASES|FUZZY-SETS|SLOPE|UNCERTAINTY|DEM|CONSEQUENCES|PREDICTION|SIMULATION|PROGRAM,"A digital representation of a terrain Surface is an approximation of reality and is inherently prone to some degree of error and uncertainty. Research in uncertainty analysis has produced a vast range of methods for investigating error and its propagation. However, the complex and varied methods proposed by researchers and academics create ambiguity for the dataset user. In this study, existing methods are combined and simplified to present a prototype tool to enable any digital elevation model (DEM) user to access and apply uncertainty analysis. The effect of correlated gridded DEM error is investigated, using stochastic conditional simulation to generate multiple equally likely representations of an actual terrain surface. Propagation of data uncertainty to the slope derivative, and the impact on a landslide susceptibility model are assessed. Two frameworks are developed to examine the probable and possible uncertainties in classifying the landslide hazard: probabilistic and fuzzy. The entire procedure is automated using publicly available software and user requirements are minimised. A case study example shows the resultant code can be used to quantify, visualise and demonstrate the propagation of error in a DEM. As a tool for uncertainty analysis the method can improve user assessment of error and its implications. (C) ", Elsevier Ltd. All rights reserved.,"Darnell, Amii R.|Tate, Nicholas J.|Brunsdon, Chris",COMPUTERS ENVIRONMENT AND URBAN SYSTEMS,error|uncertainty|stochastic simulation|digital elevation models|propagation,10.1016/j.compenvurbsys.2008.02.003
101,WOS:000383683800015,2016,Scalable subsurface inverse modeling of huge data sets with an application to tracer concentration breakthrough data from magnetic resonance imaging,COMPONENT GEOSTATISTICAL APPROACH|GENERALIZED COVARIANCE FUNCTIONS|HETEROGENEOUS POROUS-MEDIA|PARAMETER-ESTIMATION|UNCERTAINTY QUANTIFICATION|HYDRAULIC CONDUCTIVITY|TEMPORAL MOMENTS|EQUATIONS|TRANSPORT|SYSTEMS,"Characterizing subsurface properties is crucial for reliable and cost-effective groundwater supply management and contaminant remediation. With recent advances in sensor technology, large volumes of hydrogeophysical and geochemical data can be obtained to achieve high-resolution images of subsurface properties. However, characterization with such a large amount of information requires prohibitive computational costs associated with ""big data'' processing and numerous large-scale numerical simulations. To tackle such difficulties, the principal component geostatistical approach (PCGA) has been proposed as a ""Jacobian-free'' inversion method that requires much smaller forward simulation runs for each iteration than the number of unknown parameters and measurements needed in the traditional inversion methods. PCGA can be conveniently linked to any multiphysics simulation software with independent parallel executions. In this paper, we extend PCGA to handle a large number of measurements (e.g.,  or more) by constructing a fast preconditioner whose computational cost scales linearly with the data size. For illustration, we characterize the heterogeneous hydraulic conductivity (K) distribution in a laboratory-scale -D sand box using about  million transient tracer concentration measurements obtained using magnetic resonance imaging. Since each individual observation has little information on the K distribution, the data were compressed by the zeroth temporal moment of breakthrough curves, which is equivalent to the mean travel time under the experimental setting. Only about  forward simulations in total were required to obtain the best estimate with corresponding estimation uncertainty, and the estimated K field captured key patterns of the original packing design, showing the efficiency and effectiveness of the proposed method.",,"Lee, Jonghyun|Yoon, Hongkyu|Kitanidis, Peter K.|Werth, Charles J.|Valocchi, Albert J.",WATER RESOURCES RESEARCH,,10.1002/2015WR018483
102,WOS:000276271600005,2010,Application of generic data assimilation tools (DATools) for flood forecasting purposes,,"This paper describes the generic data assimilation software tool DATools. DATools can be used as standalone or within Delft-FEWS. DATools is completely configurable via XML configuration. DATools is built up of three components: a Filter, a Stochastic Modeler, and a Stochastic Observer. Configuration of all these three parts is explained in detail. At the moment two data assimilation filters are available within DATools: () ensemble Kalman Filter and () the residual resampling filter. Results of a twin experiment with both filters with DATtools show similar results as a previous study performed with custom implementations. It is also shown that DATools can function inside Delft-FEWS software used for operational flood forecasting. Applying EnKF to a D hydrodynamic SOBEK-RE model of the river Rhine within the operational system FEWS-NL Rhine and Meuse improves the forecasts at the Lobith gaugin station and downstream of Lobith. DATools has been coupled with the HBV-, SOBEK, and REW models and will be coupled to MODFLOW, Delft-D, and the geotechnical model MSetlle in the near future. Uncertainty analysis with this tool is also possible and calibration will be added later this year. (C) ", Elsevier Ltd. All rights reserved.,"Weerts, Albrecht H.|El Serafy, Ghada Y.|Hummel, Stef|Dhondia, Juzer|Gerritsen, Herman",COMPUTERS & GEOSCIENCES,data assimilation|hydrology|ensemble kalman filter|residual resampling filter|operational system|delft-fews,10.1016/j.cageo.2009.07.009
103,WOS:000361583900032,2015,Comparison of different methods for calculating thermal bridges: Application to wood-frame buildings,SENSITIVITY-ANALYSIS|MATERIAL SELECTION|WALLS|MODEL|SIMULATION,"Nowadays thermal bridges losses in building design (standard or dynamic simulations) are generally evaluated using heat transmission coefficients from a database of usual cases. Numerous studies exist on the thermal bridges of classic constructions (concrete, brick). The originality of this paper is to deal with the particular case of the wood-frame construction. In this case, catalogs with heat transmission coefficients have been integrated into building energy simulation software used by building engineers. First, this paper proposes a scientific hindsight and a critical overview of existing calculation methods of thermal bridges. Simulations are made in steady state conditions according to the European standard and with dynamic conditions. A series of calculations was performed in order to validate the presented method. The results for wood stud thermal bridges showed that the values that are mainly used by engineering offices often lead to important errors due to the standard method and rounding choice. Secondly different models are proposed to correctly apply the thermal behavior of thermal bridges to some examples of wood-frame structure. These are based on recent articles covering the methods for thermal bridges calculation in dynamic conditions. This section shows that the most accurate models depend on the consideration of the inertia of the wood stud that concentrates all the mass of the wall unlike more conventional configurations.", (C) 2015 Published by Elsevier Ltd.,"Viot, H.|Sempey, A.|Pauly, M.|Mora, L.",BUILDING AND ENVIRONMENT,thermal bridge|energy consumption|dynamic heat flow|inertia|thermal simulation,10.1016/j.buildenv.2015.07.017
104,WOS:000377729800008,2016,Kinematic calibration of a 3-DoF rotational parallel manipulator using laser tracker,SENSITIVITY-ANALYSIS|ACCURACY|ROBOT|PLATFORM|DESIGN|MODEL,"This paper proposes a laser tracker based kinematic calibration of a -degree-of-freedom (DoF) rotational parallel manipulator that would be applied in tracking and positioning fields. The process is implemented in this paper by four steps: ) formulation of the geometric error model of this manipulator by means of screw theory considering all possible geometric source errors, which is followed by the verification of this error model employing SolidWorks (R) software. ) sensitivity analysis of all geometric source errors based upon Monte Carlo method and remove some errors that have little influence on the pose accuracy of the moving platform in order to decrease the difficulty and complexity of the kinematic calibration. ) error parameter identification and kinematic calibration experiment using laser tracker. ) error compensation by amending controller model. Kinematic calibration experiment results of this -DoF rotational parallel manipulator show that three angular deviations are improved from . degrees, . degrees and . degrees to . degrees, . degrees and . degrees respectively within the prescribed workspace. (C) ", Elsevier Ltd. All rights reserved.,"Sun, Tao|Zhai, Yapu|Song, Yimin|Zhang, Jiateng",ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING,rotational parallel manipulator|kinematic calibration|geometric error model|sensitivity analysis|calibration experiment,10.1016/j.rcim.2016.02.008
105,WOS:000168591400008,2001,MCE-RISK: integrating multicriteria evaluation and CIS for risk decision-making in natural hazards,GEOGRAPHICAL INFORMATION-SYSTEMS,"During the past two decades there have been a wide range of applications for decision-making linking multicriteria evaluation (MCE) and geographic information systems (GIS). However, limited literature reports the development of MCE-GIS software, and the comparison of various MCE-GIS approaches. This paper introduces an MCE-GIS program called MCE-RISK for risk-based decision-making. It consists of a series of modules for data standardisation, weighting, MCE-GIS methods. and sensitivity analysis. The program incorporates different MCE-GIS methods. including weighted linear combination (WLC), the technique for order preference by similarity to ideal solution (TOPSIS), and compromise programming (CP), enabling comparisons between different methods for the same decision problem to be made. An example of decision-making for determining priority areas for a bushfire hazard reduction burning is examined. After implementing the alternative MCE-GIS methods, and comparing final outputs and the computational difficulty involved in the analysis, WLC is recommended. Some caveats on using MCE-GIS methods art: also dis cussed. Although the development of MCE-RISK and its application reported in this paper are specific to risk-based decisionmaking in natural hazards, the program can be used for other environmental decision applications. such as environmental impact assessment and land-use planning."," (C) 2001 Elsevier Science Ltd, All rights reserved.","Chen, KP|Blong, R|Jacobson, C",ENVIRONMENTAL MODELLING & SOFTWARE,risk decision-making|multicriteria evaluation|cis|bushfire|prescribed burning,10.1016/S1364-8152(01)00006-8
106,WOS:000296919500013,2012,Modeling and quantitatively predicting software security based on stochastic Petri nets,RELIABILITY|SYSTEMS|TOOLS,"To quantitatively predict software security in the design phase, hierarchical software security modeling and evaluation methods are proposed based on Stochastic Petri Nets (SPNs). Hierarchical methods mitigate the state-space explosion problem in SPNs. An isomorphic Markov Chain (MC) is obtained from the component SPN model. The security prediction value is calculated based on the probability distribution of the MC in the steady state. A sensitivity analysis method is proposed through evaluating the derivative of the security evaluation prediction equation. It provides a means to identify and trace back to the critical components for security enhancing. Security prediction and sensitivity analysis in the design phase provide the possibility to investigate and compare different solutions to the target system before realization. A case study shows the applicability and feasibility of our method. (C) ", Elsevier Ltd. All rights reserved.,"Yang, Nianhua|Yu, Huiqun|Qian, Zhilin|Sun, Hua",MATHEMATICAL AND COMPUTER MODELLING,model|quantify|software security|sensitivity analysis|hierarchical|stochastic petri net,10.1016/j.mcm.2011.01.055
107,WOS:000384333000001,2016,"Gulf war contamination assessment for optimal monitoring and remediation cost-benefit analysis, Kuwait",RAUDHATAIN,"Site characterization was performed on an area of  km() around the strategically vital freshwater aquifers of the Al-Rawdhatain and Umm Al-Aish to assess the status of groundwater pollution as the result of Iraq invasion to Kuwait in . Advanced data analysis and visualization software (EVS-Pro) was used for groundwater contamination assessment analytes: total petroleum hydrocarbon (TPH) and total dissolved solids (TDS). This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. Based on the ""minimum-maximum plume technology'' analysis, the nominal plume area with a threshold of . mg/kg TPH is estimated at about . km . This is the difference between the maximum and minimum predicted plume sizes. EVS-Pro also computed . x () and . x () for the plume volumes and masses (dollars per volume and mass), respectively. Also, new sampling locations were determined for further detailed site assessments based on the confidence and uncertainty analysis, which is more defensible and cost-optimized approach. This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. These tools prove to be effective in assessing remediation costs of clean-up versus benefits obtained and in developing a cost-effective monitoring programme for insights into processes controlling subsurface contaminant transport that impact water quality.",,"Yihdego, Yohannes|Al-Weshah, Radwan A.",ENVIRONMENTAL EARTH SCIENCES,visualization|data analysis|site investigation|plume|monitoring|remediation|hydrocarbon|clean-up cost|pollution,10.1007/s12665-016-6025-3
108,WOS:000303035400007,2012,Comparison of different uncertainty techniques in urban stormwater quantity and quality modelling,FORMAL BAYESIAN METHOD|SENSITIVITY-ANALYSIS|STREAMFLOW SIMULATION|HYDRAULIC-PROPERTIES|GLUE APPROACH|WATER|OPTIMIZATION|CALIBRATION|PARAMETER|QUANTIFICATION,"Urban drainage models are important tools used by both practitioners and scientists in the field of stormwater management. These models are often conceptual and usually require calibration using local datasets. The quantification of the uncertainty associated with the models is a must, although it is rarely practiced. The International Working Group on Data and Models, which works under the IWA/IAHR Joint Committee on Urban Drainage, has been working on the development of a framework for defining and assessing uncertainties in the field of urban drainage modelling. A part of that work is the assessment and comparison of different techniques generally used in the uncertainty assessment of the parameters of water models. This paper compares a number of these techniques: the Generalized Likelihood Uncertainty Estimation (GLUE), the Shuffled Complex Evolution Metropolis algorithm (SCEM-UA), an approach based on a multi-objective auto-calibration (a multialgorithm, genetically adaptive multi-objective method, AMALGAM) and a Bayesian approach based on a simplified Markov Chain Monte Carlo method (implemented in the software MICA). To allow a meaningful comparison among the different uncertainty techniques, common criteria have been set for the likelihood formulation, defining the number of simulations, and the measure of uncertainty bounds. Moreover, all the uncertainty techniques were implemented for the same case study, in which the same stormwater quantity and quality model was used alongside the same dataset. The comparison results for a well-posed rainfall/runoff model showed that the four methods provide similar probability distributions of model parameters, and model prediction intervals. For ill-posed water quality model the differences between the results were much wider; and the paper provides the specific advantages and disadvantages of each method. In relation to computational efficiency (i.e. number of iterations required to generate the probability distribution of parameters), it was found that SCEM-UA and AMALGAM produce results quicker than GLUE in terms of required number of simulations. However, GLUE requires the lowest modelling skills and is easy to implement. All non-Bayesian methods have problems with the way they accept behavioural parameter sets, e.g. GLUE, SCEM-UA and AMALGAM have subjective acceptance thresholds, while MICA has usually problem with its hypothesis on normality of residuals. It is concluded that modellers should select the method which is most suitable for the system they are modelling (e.g. complexity of the model's structure including the number of parameters), their skill/knowledge level, the available information, and the purpose of their study. (C) ", Elsevier Ltd. All rights reserved.,"Dotto, Cintia B. S.|Mannina, Giorgio|Kleidorfer, Manfred|Vezzaro, Luca|Henrichs, Malte|McCarthy, David T.|Freni, Gabriele|Rauch, Wolfgang|Deletic, Ana",WATER RESEARCH,urban drainage models|uncertainties|parameter probability distributions|bayesian inference|glue|scem-ua|mica|amalgam|mcmc|multi-objective auto-calibration,10.1016/j.watres.2012.02.009
109,WOS:000244170000035,2007,Economic modelling of price support mechanisms for renewable energy: Case study on Ireland,,"The Irish Government is considering its future targets, policy and programmes for renewable energy for the period beyond . This follows a review in  of policy options that identified a number of different measures to stimulate increased deployment of renewable energy generation capacity. This paper expands this review with an economic analysis of renewable energy price support mechanisms in the Irish electricity generation sector. The focus is on three primary price support mechanisms quota obligations, feed in tariffs and competitive tender schemes. The Green-X computer model is utilised to characterise the RES-E potential and costs in Ireland up until, and including, . The results from this dynamic software tool are used to compare the different support mechanisms in terms of total costs to society and the average premium costs relative to the market price for electricity. The results indicate that in achieving a % RES-E proportion of gross electricity consumption by , a tender scheme provides the least costs to society over the period - but only in case there is limited or no strategic bidding. Considering, however, strategic bidding, a feed-in tariff can be the more efficient solution. Between the other two support mechanisms, the total costs to society are highest for feed-in-tariffs (FIT) until , at which point the costs for the quota system begin to rise rapidly and overtake FIT in -. The paper also provides a sensitivity analysis of the support mechanism calculations by varying default parameters such as the interim () target, the assumed investment risk levels and the amount of biomass co-firing. This analysis shows that a  target of % rather than .% generates lower costs for society over the whole period -, but higher costs for the RES-E strategy over the period -.", (c) 2006 Published by Elsevier Ltd.,"Huber, Claus|Ryan, Lisa|O Gallachoir, Brian|Resch, Gustav|Polaski, Katrina|Bazilian, Morgan",ENERGY POLICY,renewable energy policy|modelling|energy economics,10.1016/j.enpol.2006.01.025
110,WOS:000283094800003,2010,Uncertainty analysis for estimation of landfill emissions and data sensitivity for the input variation,,"Results of research and practical experience confirm that stabilization of GHG concentrations will require a tremendous effort. One of the sectors identified as a significant source of methane (CH()) emissions are solid waste disposal sites (SWDS). Landfills are the key source of CH() emissions in the emissions inventory of Slovakia, and the actual emission factors are estimated with a high uncertainty level. The calculation of emission uncertainty of the landfills using the more sophisticated Tier  Monte Carlo method is evaluated in this article. The software package that works with the probabilistic distributions and their combination was developed with this purpose in mind. The results, sensitivity analysis, and computational methodology of the CH() emissions from SWDS are presented in this paper.",,"Szemesova, J.|Gera, M.",CLIMATIC CHANGE,,10.1007/s10584-010-9919-1
111,WOS:000231782200001,2005,Scenario-based simulation of runoff-related pesticide entries into small streams on a landscape level,SURFACE|WATER|FIELD,"The prediction of runoff-related pesticide entry into surface waters on a landscape level usually requires considerable efforts with regard to input data, time, and personnel. Therefore, the need for an easy to use simulation tool with easily accessible input data, for example from already existing public sources, is obvious. In this paper, we present a simulation tool for the simulation of pesticide entry from arable land into adjacent streams. Our aim was to develop a tool applicable on the landscape level using ""real world data"" from numerous sites and for the simulation of parameter case studies concerning particular parameters at single sites. We used the ratio of exposure to toxicity (REXTOX) model proposed by the OECD, which had been successfully validated in the study area as part of a previous study and which was extended to calculate pesticide concentrations in adjacent streams. We simulated the pesticide entry on the landscape level at  sites in small streams situated in the central lowland of Germany with winter wheat, barley, and sugar beat as the main agricultural. crops. A sensitivity analysis indicated that the most significant model parameters were the width of the no-application zone and the degree of plant interception. The simulation was carried out for the  most frequently detected substances found in the study area using eight different environmental scenarios, covering variation of the width of the no-application zone, climate, and seasonal scenarios. The highest in-stream concentrations were predicted for a scenario using no ( m) buffer zone in conjunction with increased precipitation. According to the predicted concentrations, the risk for the aquatic communities was estimated based on standard toxicity tests and the application of a safety factor. The simulation results are presented both by means of risk maps for the study area showing the simulated pesticide concentration and the resulting ecological risk for numerous sites under varying scenarios and by case study diagrams with focus on the model behavior under the influence of single parameters. Risk maps confirmed the importance of no-application (buffer) zones for the levels of pesticide input. They also indicated the importance of the existing no-application zones for certain compounds and in some cases the need for a further evaluation of these regulations. The simulation tool was implemented as a standard PC software combining the REXTOX model with a geographical information system and can be used on any current personal computer. All input data was taken from public sources of German authorities. With little effort the tool should be applicable for other areas with similar data quality.", (C) 2005 Elsevier Inc. All rights reserved.,"Probst, M|Berenzen, N|Lentzen-Godding, A|Schulz, R",ECOTOXICOLOGY AND ENVIRONMENTAL SAFETY,risk assessment|pesticides|runoff|buffer zones|simulation|modeling|landscape level|climate change|risk mitigation,10.1016/j.ecoenv.2005.04.012
112,WOS:000168416500008,2001,A computational methodology for shape optimization of structures in frictionless contact,SUPERCONVERGENT PATCH RECOVERY|SENSITIVITY ANALYSIS|FORMULATION|ALGORITHMS|SOLIDS,"This paper presents a computational methodology for shape optimization of structures in frictionless contact. which provides a basis for developing user-friendly and efficient shape optimization software. For evaluation it has been implemented as a subsystem of a general finite element software. The overall design and main principles of operation of this software are outlined. The parts connected to shape optimization are described in more detail. The key building blocks are: analytic sensitivity analysis, an adaptive finite element method, an accurate contact solver. and a sequential convex programing optimization algorithm. Results for three model application examples are presented, in which the contact pressure and the effective stress are optimized.", (C) 2001 Elsevier Science B.V. All rights reserved.,"Hilding, D|Torstenfelt, B|Klarbring, A",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,shape optimization|frictionless contact|finite element method|sensitivity analysis|adaptive meshing,10.1016/S0045-7825(00)00310-8
113,WOS:000392285600091,2016,Identifying critical architectural components with spectral analysis of fault trees,SOFTWARE ARCHITECTURE|RELIABILITY-ANALYSIS|SYSTEMS|MODELS|TOOLS,"We increasingly rely on software-intensive embedded systems. Increasing size and complexity of these hardware/software systems makes it necessary to evaluate reliability at the system architecture level. One aspect of this evaluation is sensitivity analysis, which aims at identifying critical components of the architecture. These are the components of which unreliability contributes the most to the unreliability of the system. In this paper, we propose a novel approach for sensitivity analysis based on spectral analysis of fault trees. We show that measures obtained with our approach are both consistent and complementary with respect to the recognized metrics in the literature.", (C) 2016 Elsevier B.V. All rights reserved.,"Ayav, Toga|Sozer, Hasan",APPLIED SOFT COMPUTING,hardware/software architecture evaluation|reliability analysis|fault trees|fourier analysis|sensitivity analysis|importance analysisa,10.1016/j.asoc.2016.06.042
114,WOS:000241177900002,2006,Design parameterization and tool integration for CAD-based mechanism optimization,,"This paper presents an open and integrated tool environment that enables engineers to effectively search, in a CAD solid model form, for a mechanism design with optimal kinematic and dynamic performance. In order to demonstrate the feasibility of such an environment, design parameterization that supports capturing design intents in product solid models must be available, and advanced modeling, simulation, and optimization technologies implemented in engineering software tools must be incorporated. In this paper, the design parameterization capabilities developed previously have been applied to support design optimization of engineering products, including a High Mobility Multi-purpose Wheeled Vehicle (HMMWV). In the proposed environment, Pro/ENGINEER and SolidWorks are supported for product model representation, DADS (Dynamic Analysis and Design System) is employed for dynamic simulation of mechanical systems including ground vehicles, and DOT (Design Optimization Tool) is included for a batch mode design optimization. In addition to the commercial tools, a number of software modules have been implemented to support the integration; e.g., interface modules for data retrieval, and model update modules for updating CAD and simulation models in accordance with design changes. Note that in this research, the overall finite difference method has been adopted to support design sensitivity analysis. (C) ", Elsevier Ltd. All rights reserved.,"Chang, Kuang-Hua|Joo, Sung-Hwan",ADVANCES IN ENGINEERING SOFTWARE,design optimization|design parameterization|computer-aided design|dynamic simulation|tool integration,10.1016/j.advengsoft.2006.05.005
115,WOS:000223572800017,2004,Radionuclide migration modeling through the soil-plant system as adapted for Hungarian environment,FOOD-CHAIN MODEL|DYNAMIC-MODEL|FALLOUT,"The migration of radionuclides released as fallout through the food-chain to humans was modelled using the MODELMAKER software. In the established dynamic environmental transfer model ETM- with compartmental structure, the principal pathways of vegetable contamination were studied specifically for the Hungarian environment. These pathways were: direct deposition on plant surface, root uptake and deposition after resuspension from the soil surface. As result of the modeling the variation of activity-concentration with time was obtained in the compartments. The validation of the model was done by comparing the calculated results with those obtained in field experiments. A sensitivity analysis of the input parameters was also carried out and the parameters were categorized by their sensitivity index (SI). According to this study, the most sensitive parameters are the daily human intake of vegetable, the distribution coefficient, the transfer factor from soil to plant and the weathering half-time. The most probable distribution types for the parameter values were also determined based on Monte Carlo simulations.", (C) 2004 Elsevier B.V. All rights reserved.,"Kabai, E|Zagyvai, P|Lang-Lazi, M|Oncsik, MB",SCIENCE OF THE TOTAL ENVIRONMENT,environmental model|plants|sensitivity analysis|sensitivity index,10.1016/j.scitotenv.2004.03.039
116,WOS:000227298500003,2005,Numerical approaches to life cycle interpretation - The case of the ecoinvent'96 database,,"Goal, Scope and Background. To strengthen the evaluative power of LCA, life cycle interpretation should be further developed. A previous contribution '(Heijungs & Kleijn ) elaborated five examples of concrete methods within the subset of numerical approaches towards interpretation. These methods were: contribution analysis, perturbation analysis, uncertainty analysis, comparative analysis, and discernibility analysis. Developments in software have enabled the possibility to apply the five example methods to explore the much-used ecoinvent ' database. Discussion of Methods. The numerical approaches implemented in this study include contribution analysis, perturbation analysis, uncertainty analysis, comparative analysis, discernibility analysis and the newly developed key issue analysis. The data used comes from a very large process database: ecoinvent ', containing  processes,  economic flows and  environmental flows. Conclusions. Results are twofold: they serve as a benchmark to the usefulness and feasibility of these numerical approaches, and they shed light on the question of stability and structure in an often-used large system of interconnected processes. Most of the approaches perform quite well: computation time on a moderate PC is between a few seconds and a few minutes. Only Monte Carlo analyses may require much longer, but even then it appears that most questions can be answered within a few hours. Moreover, analytical expressions for error propagation are much faster than Monte Carlo analyses, while providing almost identical results. Despite the fact that many processes are connected to each other, leading to the possibility of a very unstable system and very sensitive coefficients, the overall results show that most results are not extremely uncertain. There are, however, some exceptions to this positive message.",,"Heijungs, R|Suh, S|Kleijn, R",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,contribution analysis|discernibility analysis|ecoinvent'96|key issue analysis|life cycle interpretation|perturbation analysis|sensitivity analysis|uncertainty analysis,10.1065/lca2004.06.161
117,WOS:000400267300014,2017,Hierarchical approach to hydrological model calibration,WATER-QUALITY MODELS|UNCERTAINTY ANALYSIS|RIVER-BASIN|SWAT|OPTIMIZATION|FLOW|EQUIFINALITY|PREDICTIONS|SENSITIVITY|VALIDATION,"Hydrological models have been widely used for water resources management. Successful application of hydrological models depends on careful calibration and uncertainty analysis. Spatial unit of water balance calculations may differ widely in different models from grids to hydrological response units (HRU). The Soil and Water Assessment Tool (SWAT) software uses HRU as the spatial unit. SWAT simulates hydrological processes at sub-basin level by deriving HRUs by thresholding areas of soil type, land use, and slope combinations. This may ignore some important areas, which may have great impact on hydrological processes in the watershed. In this study, a hierarchical HRU approach was developed in order to increase model performance and reduce computational complexity simultaneously. For hierarchical optimization, HRUs are first divided into two-HRU types and are optimized with respect to some relevant influence parameters. Then, each HRU is further divided into two. Each child HRU inherits the optimum parameter values of the parent HRU as its initial value. This approach decreases the total calibration time while obtaining a better result. The performance of the hierarchical methodology is demonstrated on two basins, namely Sarisu-Eylikler and Namazgah Dam Lake Basins in Turkey. In Sarisu-Eylikler, we obtained good results by a combination of curve number (CN), soil hydraulic conductivity, and slope for generating HRUs, while in Namazgah use of only CN gave better results.",,"Ozdemir, Ayfer|Leloglu, Ugur Murat|Abbaspour, Karim C.",ENVIRONMENTAL EARTH SCIENCES,swat|calibration|hydrological response unit|sufi2|optimization,10.1007/s12665-017-6560-6
118,WOS:000413245200004,2017,A user-friendly software package for VIC hydrologic model development,GLOBAL SENSITIVITY-ANALYSIS|PRECIPITATION PRODUCTS|MULTISITE CALIBRATION|SOIL-MOISTURE|RUNOFF-MODEL|WATER|BASIN|SIMULATION|DATASET|FLUXES,"The Variable Infiltration Capacity (VIC) hydrologic and river routing model simulates the water and energy fluxes that occur near the land surface and provides useful information regarding the quantity and timing of available water within a watershed system. However, despite its popularity, wider adoption is hampered by the considerable effort required to prepare model inputs and calibrate the model parameters. This study presents a user-friendly software package, named VIC-Automated Setup Toolkit (VIC-ASSIST), accessible through an intuitive MATLAB graphical user interface. VIC-ASSIST enables users to navigate the model building process through prompts and automation, with the intention to promote the use of the model for practical, educational, and research purposes. The automated processes include watershed delineation, climate and geographical input set-up, model parameter calibration, sensitivity analysis, and graphical output generation. We demonstrate the package's utilities in various case studies. (C) ", Elsevier Ltd. All rights reserved.,"Wi, Sungwook|Ray, Patrick|Demaria, Eleonora M. C.|Steinschneider, Scott|Brown, Casey",ENVIRONMENTAL MODELLING & SOFTWARE,vic hydrologic model|vic setup assistant tool|matlab graphic user interface|automatic calibration|sensitivity analysis,10.1016/j.envsoft.2017.09.006
119,WOS:000353715400015,2015,LCI Databases Sensitivity Analysis of the Environmental Impact of the Injection Molding Process,LIFE-CYCLE ASSESSMENT,"During the last decades, society's concern for the environment has increased. Specific tools like the Life Cycle Assessment (LCA), and software and databases to apply this method have been developed to calculate the environmental burden of products or processes. Calculating the environmental impact of plastic products is relevant as the global plastics production rose to  million tons in . Among the different ways of processing plastics, the injection molding process is one of the most used in the industry worldwide. In this paper, a sensitivity analysis of the environmental impact of the injection molding process has been carried out. In order to perform this study, the EcoInvent database inventory for injection molding, and the data from which this database is created, have been studied. Generally, when an LCA of a product is carried out, databases such as EcoInvent, where materials, processes and transports are characterized providing average values, are used to quantify the environmental impact. This approach can be good enough in some cases but in order to assess a specific production process, like injection molding, a further level of detail is needed. This study shows how the final results of environmental impact differ for injection molding when using the PVC's, PP's or PET's data. This aspect suggests the necessity of studying, in a more precise way, this process, to correctly evaluate its environmental burden. This also allows us to identify priority areas and thereby actions to develop a more sustainable way of manufacturing plastics.",,"Elduque, Ana|Javierre, Carlos|Elduque, Daniel|Fernandez, Angel",SUSTAINABILITY,,10.3390/su7043792
120,WOS:000388155500003,2016,GTApprox: Surrogate modeling for industrial design,SENSITIVITY-ANALYSIS|GAUSSIAN-PROCESSES|REGRESSION|REGULARIZATION|ALGORITHM|SELECTION|MACHINE|SPLINES|SAMPLES|EXPERTS,"We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. (C) ", Elsevier Ltd. All rights reserved.,"Belyaev, Mikhail|Burnaev, Evgeny|Kapushev, Ermek|Panov, Maxim|Prikhodko, Pavel|Vetrov, Dmitry|Yarotsky, Dmitry",ADVANCES IN ENGINEERING SOFTWARE,approximation|surrogate model|surrogate-based optimization,10.1016/j.advengsoft.2016.09.001
121,WOS:000230722000011,2005,"Assessing the potential of thermal infrared satellite surveys for monitoring seismically active areas: The case of Kocaell (Izmit) earthquake, August 17, 1999",AFTERSHOCK DISTRIBUTION|IMPENDING EARTHQUAKES|SENSITIVITY-ANALYSIS|SURFACE-TEMPERATURE|AUTOMATED DETECTION|TURKEY|GREECE|FAULT|PREDICTION|PRECURSOR,"Space-time anomalies of Earth's emitted radiation in the thermal infrared spectral range (TIR) measured from satellite months to weeks before the occurrence of earthquakes, have been interpreted, by several authors, as pre-seismic signals. The claimed connection of TIR emission with seismic activity has been considered, up to now, with some caution by the scientific community mainly for the insufficiency of the validation data-sets and the scarce importance attached by those authors to other causes (e.g. meteorological) that, rather than seismic activity, could be responsible for the observed TIR signal fluctuations. In this paper, a robust satellite data analysis technique is described which pen-nits us to identify anomalous space-time TIR signal transients even in very variable observational (satellite view angle, land topography and coverage, etc.) and natural (e.g. meteorological) conditions. A statistically well-founded definition of TIR anomaly is given and proposed as a suitable tool for satellite TIR surveys in seismically active regions. Eight years of Meteosat TIR observations have been analyzed in order to characterize the TIR signal behavior at each specific observation time and location. Space-time TIR signal transients have then been analyzed, both in the presence (validation) and in the absence of (confutation) seismic events, looking for possible space-time relationships. The devastating earthquake which occurred in Turkey (Izmit.. August , M(S)similar to .) in  has been considered as a test case for validation, relatively unperturbed periods (no earthquakes with M > ) were taken for confutation purposes. Quite intense (S/N > .) and rare, spatially extensive and time persistent, TIR signal transients were identified appearing eight days before the Izmit main shock in Greece, moving to Turkey on August  and disappearing, moving back to Greece, some days after. Possible implications of such results, together with present limitations of the proposed technique, will also be discussed in the light of the improved performances expected by its extension to other existing or future satellite packages.", (c) 2005 Elsevier Inc. All rights reserved.,"Tramutoli, |Cuomo, |Filizzola, C|Pergola, N|Pietrapertosa, C",REMOTE SENSING OF ENVIRONMENT,earthquake|thermal anomalies|thermal infrared satellite|avhrr|meteosat|msg|seviri|north anatolian fault|kocaeli-izmit,10.1016/j.rse.2005.04.006
122,WOS:000303384500008,2012,Life cycle assessment of polychlorinated biphenyl contaminated soil remediation processes,SITE REMEDIATION|ENVIRONMENTAL ASSESSMENT|INVENTORY MODELS|LCA|BIOREMEDIATION|FRAMEWORK|OPTIONS,"Purpose A life-cycle assessment (LCA) was performed to evaluate the environmental impacts of the remediation of industrial soils contaminated by polychlorobiphenyl (PCB). Two new bioremediation treatment options were compared with the usual incineration process. In this attributional LCA, only secondary impacts were considered. The contaminated soil used for the experiments contained  mg of PCB per kilogram. Methods Three off-site treatment scenarios were studied: ) bioremediation with mechanical aeration, ) bioremediation with electric aeration and ) incineration with natural gas. Bioremediation processes were designed from lab-scale, scale-up and pilot experiments. The incineration technique was inspired by a French plant. A semi-quantitative uncertainty analysis was performed on the data. Environmental impacts were evaluated with the CML  method using the SimaPro software. Results and discussion In most compared categories, the bioremediation processes are favorable. Of the bioremediation options, the lowest environmental footprint was observed for electric aeration. The uncertainty analysis supported the results that compared incineration and bioremediation but decreased the difference between the options of aeration. The distance of transportation was one of the most sensitive parameters, especially for bioremediation. At equal distances between the polluted sites and the treatment plant, bioremediation had fewer impacts than incineration in eight out of  categories. Conclusions The use of natural gas for the incineration process generated the most impacts. Irrespective of the aeration option, bioremediation was better than incineration. The time of treatment should be taken into account. More precise and detailed data are required for the incineration scenario. More parameters of biological treatments should be measured. LCA results should be completed using ecological and health risk assessment and an acceptability evaluation.",,"Busset, Guillaume|Sangely, Matthieu|Montrejaud-Vignoles, Mireille|Thannberger, Laurent|Sablayrolles, Caroline",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,attributional lca|cml-method|environmental evaluation|midpoint category|polychlorinated biphenyl,10.1007/s11367-011-0366-7
123,WOS:000267670900009,2009,Environmental performance assessment of hardboard manufacture,LIFE-CYCLE ASSESSMENT|FOREST OPERATIONS|WOOD FIBERS|PRODUCTS|FIBERBOARD|INVENTORY|LCA|INDUSTRY|IMPACTS|DENSITY,"The forest-based and related industries comprise one of the most important industry sectors in the European Union, representing some % of the EU's manufacturing industries. Their activities are based on renewable raw material resources and efficient recycling. The forest-based industries can be broken down into the following sectors: forestry, woodworking, pulp and paper manufacturing, paper and board converting and printing and furniture. The woodworking sector includes many sub-sectors; one of the most important is that of wood panels accounting for % of total industry production. Wood panels are used as intermediate products in a wide variety of applications in the furniture and building industries. There are different kinds of panels: particleboard, fibreboard, veneer, plywood and blockboard. The main goal of this study was to assess the environmental impacts during the life cycle of wet-process fibreboard (hardboard) manufacturing to identify the processes with the largest environmental impacts. The study covers the life cycle of hardboard production from a cradle-to-gate perspective. A hardboard plant was analysed in detail, dividing the process chain into three subsystems: wood preparation, board forming and board finishing. Ancillary activities such as chemicals, wood chips, thermal energy and electricity production and transport were included within the system boundaries. Inventory data came from interviews and surveys (on-site measurements). When necessary, the data were complemented with bibliographic resources. The life cycle assessment procedure followed the ISO series. The life cycle inventory (LCI) and impact assessment database for this study were constructed using SimaPro Version . software. Abiotic depletion (AD), global warming (GW), ozone layer depletion (OLD), human toxicity (HT), ecotoxicity, photochemical oxidant formation (PO), acidification (AC) and eutrophication (EP) were the impact categories analysed in this study. The wood preparation subsystem contributed more than % to all impact categories, followed by board forming and board finishing, which is mainly due to chemicals consumption in the wood preparation subsystem. In addition, thermal energy requirements (for all subsystems) were fulfilled by on-site wood waste burning and, accordingly, biomass energy converters were considered. Several processes were identified as hot spots in this study: phenol-formaldehyde resin production (with large contribution to HT, fresh water aquatic ecotoxicity and PO), electricity production (main contributor to marine aquatic ecotoxicity), wood chips production (AD and OLD) and finally, biomass burning for heat production (identified as the largest contributor to AC and EP due to NO (X) emissions). In addition, uncontrolled formaldehyde emissions from manufacturing processes at the plant such as fibre drying should be controlled due to relevant contributions to terrestrial ecotoxicity and PO. A sensitivity analysis of electricity profile generation (strong geographic dependence) was carried out and several European profiles were analysed. Novel binding agents for the wood panel industry as a substitute for the currently used formaldehyde-based binders have been extensively investigated. Reductions of toxic emissions during drying, mat forming and binder production are desirable. The improved method would considerably reduce the contributions to all impact categories. The results obtained in this work allow forecasting the importance of the wood preparation subsystem for the environmental burdens associated with hardboard manufacture. Special attention was paid to the inventory analysis stage for each subsystem. It is possible to improve the environmental performance of the hardboard manufacturing process if some alternatives are implemented regarding the use of chemicals, electricity profile and emission sources in the production processes located inside the plant. This study provides useful information for forest-based industries related to panel manufacture with the aim of increasing their sustainability. Our research continues to assess the use phase and final disposal of panels to complete the life cycle assessment. Future work will focus on analysing the environmental aspects associated with plywood, another type of commonly used wood panel.",,"Gonzalez-Garcia, Sara|Feijoo, Gumersindo|Widsten, Petri|Kandelbauer, Andreas|Zikulnig-Rusch, Edith|Moreira, Ma Teresa",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,fibreboard|hardboard|life cycle assessment (lca)|life cycle inventory (lci)|wet-process fibreboard|wood boards|wood panels,10.1007/s11367-009-0099-z
124,WOS:000334298800035,2014,Direct Production of Gasoline and Diesel Fuels from Biomass via Integrated Hydropyrolysis and Hydroconversion Process-A Techno- economic Analysis,LIGNOCELLULOSIC BIOMASS|FAST PYROLYSIS|TECHNOECONOMICS|GASIFICATION,"A techno-economic analysis (TEA) is performed to investigate the production of gasoline and diesel range hydrocarbon fuels by conversion of woody biomass via Gas Technology Institute (GTI)'s integrated hydropyrolysis plus hydroconversion (IH) process. The processing capacity is  dry metric tonnes ( dry US tons) of woody biomass per day. Major process areas include catalytic hydropyrolysis, catalytic hydroconversion, on-site hydrogen production, feedstock handling and storage, hydrocarbon absorber, sour water stripper, hydrogen sulfide scrubber, distillation tower, and all other operations support utilities. The TEA incorporates applicable commercial technologies, process modeling using Aspen HYSYS software, equipment cost estimation, and discounted cash flow analysis. The resulting minimum fuel selling price is $. per gallon (or $. per gallon of gasoline equivalent) in  US dollars. The process yields  gallons of liquid fuels per dry US ton of woody biomass feedstock, for an annual fuel production rate of  million gallons at % on-stream time. The estimated total capital investment for an nth-plant is $ million. A sensitivity analysis captures uncertainties in costs and plant performance. Results from this TEA can serve as the baseline for future comparison and as a basis for comparing this process to other biomass-to-liquid fuel pathways."," (c) 2013 American Institute of Chemical Engineers Environ Prog, 33: 609-617, 2014","Tan, Eric C. D.|Marker, Terry L.|Roberts, Michael J.",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,biomass to fuels|hydropyrolysis|gasoline|diesel|techno-economic analysis|process modeling,10.1002/ep.11791
125,WOS:000272519900022,2009,Predicting physical properties of emerging compounds with limited physical and chemical data: QSAR model uncertainty and applicability to military munitions,OCTANOL/WATER PARTITION-COEFFICIENTS|RISK-ASSESSMENT|LOG-P|SOLUBILITY,"Reliable, up-front information on physical and biological properties of emerging materials is essential before making a decision and investment to formulate, synthesize, scale-up, test, and manufacture a new material for use in both military and civilian applications. Multiple quantitative structure-activity relationships (QSARs) software tools are available for predicting a material's physical/chemical properties and environmental effects. Even though information on emerging materials is often limited, QSAR software output is treated without sufficient uncertainty analysis. We hypothesize that uncertainty and variability in material properties and uncertainty in model prediction can be too large to provide meaningful results. To test this hypothesis, we predicted octanol water partitioning coefficients (log P) for multiple, similar compounds with limited physical-chemical properties using six different commercial log P calculators (KOWWIN, MarvinSketch, ACD/Labs, ALogP, CLogP, SPARC). Analysis was done for materials with largely uncertain properties that were similar, based on molecular formula, to military compounds (RDX, BTTN, TNT) and pharmaceuticals (Carbamazepine, Gemfibrizol). We have also compared QSAR modeling results for a well-studied pesticide and pesticide breakdown product (Atrazine, DDE). Our analysis shows variability due to structural variations of the emerging chemicals may be several orders of magnitude. The model uncertainty across six software packages was very high ( orders of magnitude) for emerging materials while it was low for traditional chemicals (e.g. Atrazine). Thus the use of QSAR models for emerging materials screening requires extensive model validation and coupling QSAR output with available empirical data and other relevant information.", Published by Elsevier Ltd.,"Bennett, Erin R.|Clausen, Jay|Linkov, Eugene|Linkov, Igor",CHEMOSPHERE,qsar|epi suite (tm)|explosives|rdx|tnt,10.1016/j.chemosphere.2009.09.003
126,WOS:000270759400017,2009,Quantifying predictive uncertainty for a mountain-watershed model,HYDROLOGIC-MODELS|AUTOMATIC CALIBRATION|GLOBAL OPTIMIZATION|SWAT MODEL|VALIDATION|SENSITIVITY,"Watershed models require calibration before they are utilized as a decision-making tool. This paper describes a rigorous sensitivity analysis, automated parameter estimation and evaluation of prediction uncertainty for a Watershed Analysis Risk Management Framework (WARMF) model of the Turkey Creek Watershed. Sensitivity analysis was conducted using UCODE calibration and uncertainty-analysis software. Simulated stream flow is strongly sensitive to  of the  parameters evaluated: hydraulic conductivity, field capacity, total porosity, precipitation weighting factor, evaporation magnitude, evaporation skewness and snow melting rates; and parameter sensitivity is dependent on site-specific climate and soil conditions. Simulated stream flow matched observed stream flow fairly well with an R() value of ., Nash-Sutcliffe coefficient of efficiency (NSE) value of . and Root Mean Squared Error (RMSE) of . m()/s. The calibrated model was used to predict changes in stream flow that would result from changes in land use, including development of forested areas in parts of the watershed to commercial and residential areas. As expected, new development resulted in increased peak flows and reduced low flows. Uncertainty associated with all model parameters, including those not estimated by calibration by enhancing the parameter variance/covariance matrix, was considered when evaluating prediction uncertainties. Seventy percent of the time, predicted flows had uncertainties less than % with more of the uncertainty during low flow conditions.", (C) 2009 Elsevier B.V. All rights reserved.,"Geza, Mengistu|Poeter, Eileen P.|McCray, John E.",JOURNAL OF HYDROLOGY,sensitivity analysis|automatic calibration|prediction uncertainty|ucode|warmf,10.1016/j.jhydrol.2009.07.025
127,WOS:000405513900006,2017,Bayesian inference of earthquake parameters from buoy data using a polynomial chaos-based surrogate,NUMERICAL TIDAL MODEL|UNCERTAINTY QUANTIFICATION|DIFFERENTIAL-EQUATIONS|FRICTION COEFFICIENTS|TSUNAMI|SIMULATIONS,"This work addresses the estimation of the parameters of an earthquake model by the consequent tsunami, with an application to the Chile  event. We are particularly interested in the Bayesian inference of the location, the orientation, and the slip of an Okada-based model of the earthquake ocean floor displacement. The tsunami numerical model is based on the GeoClaw software while the observational data is provided by a single DARTa""c buoy. We propose in this paper a methodology based on polynomial chaos expansion to construct a surrogate model of the wave height at the buoy location. A correlated noise model is first proposed in order to represent the discrepancy between the computational model and the data. This step is necessary, as a classical independent Gaussian noise is shown to be unsuitable for modeling the error, and to prevent convergence of the Markov Chain Monte Carlo sampler. Second, the polynomial chaos model is subsequently improved to handle the variability of the arrival time of the wave, using a preconditioned non-intrusive spectral method. Finally, the construction of a reduced model dedicated to Bayesian inference is proposed. Numerical results are presented and discussed.",,"Giraldi, Loic|Le Maitre, Olivier P.|Mandli, Kyle T.|Dawson, Clint N.|Hoteit, Ibrahim|Knio, Omar M.",COMPUTATIONAL GEOSCIENCES,uncertainty quantification|bayesian inference|polynomial chaos expansion|noise model|low-rank representation|shallow water equation|tsunami|earthquake inversion,10.1007/s10596-017-9646-z
128,WOS:000283842100010,2010,The Nested Event Tree Model with Application to Combating Terrorism,MIXED-INTEGER MODELS|PROGRAMMING-PROBLEMS|GLOBAL OPTIMIZATION|RESOURCES|ALLOCATION|RISKS,"In this paper, we model and solve the strategic problem of minimizing the expected loss inflicted by a hostile terrorist organization. An appropriate allocation of certain capability-related, intent-related, vulnerability-related, and consequence-related resources is used to reduce the probabilities of success in the respective attack-related actions and to ameliorate losses in case of a successful attack. We adopt a nested event tree optimization framework and formulate the problem as a specially structured nonconvex factorable program. We develop two branch-and-bound schemes based, respectively, on utilizing a convex nonlinear relaxation and a linear outer approximation, both of which are proven to converge to a global optimal solution. We also design an alternative direct mixed-integer programming model representation for this case, and we investigate a fundamental special-case variant for this scheme that provides a relaxation and affords an optimality gap measure. Several range reduction, partitioning, and branching strategies are proposed, and extensive computational results are presented to study the efficacy of different compositions of these algorithmic ingredients, including comparisons with the commercial software BARON. A sensitivity analysis is also conducted to explore the effect of certain key model parameters.",,"Lunday, Brian J.|Sherali, Hanif D.|Glickman, Theodore S.",INFORMS JOURNAL ON COMPUTING,combating terrorism|outer approximation|branch and bound|global optimization|factorable programs,10.1287/ijoc.1100.0377
129,WOS:000294704000006,2011,Multi-parametric sensitivity analysis of CCHE2D for channel flow simulations in Nile River,MODEL,"Multi-Parametric Sensitivity Analysis (MPSA) is proposed to determine the relative importance of the different empirical parameters controlling flow field. CCHED which is one of the famous public unsteady-flow simulation software is applied to simulate the flow field at Elbogdady reach, at the south of Luxor staff gauge, . km upstream of Roda's staff gauge on the Nile River. The main purpose of the paper is to apply the model in the considered river reach and to assess the capabilities of CCHED. Moreover, the paper presents findings of the authors in determining the model's sensitivity analysis. Multi-Parametric Sensitivity Analysis results show that flow field in Nile River is controlled mainly by the bed roughness coefficient. In addition, the empirical formula of Van Rijn () is found to be more efficient to calculate bed roughness. (C)  International Association of Hydro-environment Engineering and Research, Asia Pacific Division.", Published by Elsevier B.V. All rights reserved.,"Nassar, M. A.",JOURNAL OF HYDRO-ENVIRONMENT RESEARCH,2d-modling|cche2d|nile river|numerical and sensitivity analysis,10.1016/j.jher.2010.12.002
130,WOS:000345142000010,2015,Geometric sensitivity of patient-specific finite element models of the spine to variability in user-selected anatomical landmarks,SOFT-TISSUE PROPERTIES|LUMBAR SPINE|INTERVERTEBRAL DISC|MATERIAL PROPERTY|SCOLIOSIS|COMPRESSION|DEFORMITY|SEGMENT|FORCES|TRUNK,"Software to create individualised finite element (FE) models of the osseoligamentous spine using pre-operative computed tomography (CT) data-sets for spinal surgery patients has recently been developed. This study presents a geometric sensitivity analysis of this software to assess the effect of intra-observer variability in user-selected anatomical landmarks. User-selected landmarks on the osseous anatomy were defined from CT data-sets for three scoliosis patients and these landmarks were used to reconstruct patient-specific anatomy of the spine and ribcage using parametric descriptions. The intra-observer errors in landmark co-ordinates for these anatomical landmarks were calculated. FE models of the spine and ribcage were created using the reconstructed anatomy for each patient and these models were analysed for a loadcase simulating clinical flexibility assessment. The intra-observer error in the anatomical measurements was low in comparison to the initial dimensions, with the exception of the angular measurements for disc wedge and zygapophyseal joint (z-joint) orientation and disc height. This variability suggested that CT resolution may influence such angular measurements, particularly for small anatomical features, such as the z-joints, and may also affect disc height. The results of the FE analysis showed low variation in the model predictions for spinal curvature with the mean intra-observer variability substantially less than the accepted error in clinical measurement. These findings demonstrate that intra-observer variability in landmark point selection has minimal effect on the subsequent FE predictions for a clinical loadcase.",,"Little, J. P.|Adam, C. J.",COMPUTER METHODS IN BIOMECHANICS AND BIOMEDICAL ENGINEERING,patient-specific|finite element|scoliosis|thoracolumbar|spine|ribcage,10.1080/10255842.2013.843673
131,WOS:000387350400006,2016,Efficient uncertainty quantification of a fully nonlinear and dispersive water wave model with random inputs,DYNAMICALLY BIORTHOGONAL METHOD|SENSITIVITY-ANALYSIS|DIFFERENTIAL-EQUATIONS|QUADRATURE-RULES|POLYNOMIAL CHAOS|NUMERICAL-METHODS|OCEAN WAVES|PROPAGATION|SIMULATIONS|FIELDS,"A major challenge in next-generation industrial applications is to improve numerical analysis by quantifying uncertainties in predictions. In this work we present a formulation of a fully nonlinear and dispersive potential flow water wave model with random inputs for the probabilistic description of the evolution of waves. The model is analyzed using random sampling techniques and nonintrusive methods based on generalized polynomial chaos (PC). These methods allow us to accurately and efficiently estimate the probability distribution of the solution and require only the computation of the solution at different points in the parameter space, allowing for the reuse of existing simulation software. The choice of the applied methods is driven by the number of uncertain input parameters and by the fact that finding the solution of the considered model is computationally intensive. We revisit experimental benchmarks often used for validation of deterministic water wave models. Based on numerical experiments and assumed uncertainties in boundary data, our analysis reveals that some of the known discrepancies from deterministic simulation in comparison with experimental measurements could be partially explained by the variability in the model input. Finally, we present a synthetic experiment studying the variance-based sensitivity of the wave load on an offshore structure to a number of input uncertainties. In the numerical examples presented the PC methods exhibit fast convergence, suggesting that the problem is amenable to analysis using such methods.",,"Bigoni, Daniele|Engsig-Karup, Allan P.|Eskilsson, Claes",JOURNAL OF ENGINEERING MATHEMATICS,free surface water waves|generalized polynomial chaos|high-performance computing|sensitivity analysis|uncertainty quantification,10.1007/s10665-016-9848-8
132,WOS:000411869000009,2017,A Scenario Based Impact Assessment of Trace Metals on Ecosystem of River Ganges Using Multivariate Analysis Coupled with Fuzzy Decision-Making Approach,WATER-QUALITY MANAGEMENT|HEAVY-METALS|INDIA|BASIN|FISH,"The growing consciousness about the health risks associated with environmental pollutants has brought a major shift in global concern towards prevention of hazardous/trace metals discharge in water bodies. Majority of these trace metals gets accumulated in the body of aquatic lives, which are considered as potential indicators of hazardous content. This results in an ecological imbalance in the form of poisoning, diseases and even death of fish and other aquatic lives, and ultimately affect humans through food chain. Trace metals such as Cd, Cr, Cu, Mn, Ni, Pb and Zn originated from various industrial operations containing metallic solutions and agricultural practices, have been contributing significantly to cause aquatic pollution. The present study develops a novel approach of expressing sustainability of river's ecosystem based on health of the fish by coupling fuzzy sensitivity analysis into multivariate analysis. A systematic methodology has been developed by generating monoplot, two dimensional biplot and rotated component matrix (using 'Analyze it' and 'SPSS' software), which can simultaneously identify critical trace metals and their industrial sources, critical sampling stations, and adversely affected fish species along with their interrelationships. A case study of assessing the impact of trace metals on the aquatic life of river Ganges, India has also been presented to demonstrate effectiveness of the model. The clusters pertaining to various water quality parameters have been identified using Principal Component Analysis (PCA) to determine actual sources of pollutants and their impact on aquatic life. The fuzzy sensitivity analysis reveals the cause-effect relationship of these critical parameters. The study suggests pollution control agencies to enforce appropriate regulations on the wastewater dischargers responsible for polluting river streams with a particular kind of trace metal(s).",,"Srinivas, R.|Singh, Ajit Pratap|Sharma, Rishikesh",WATER RESOURCES MANAGEMENT,aquatic ecosystem|fuzzy decision-making|impact assessment|multivariate analysis|river ecosystem|water quality,10.1007/s11269-017-1738-y
133,WOS:000241572400011,2006,Optimal control of open-channel flow using adjoint sensitivity analysis,MANNINGS ROUGHNESS COEFFICIENTS|CONTAMINANT RELEASES|HAZARD MITIGATION|WAVE CONTROL|IDENTIFICATION|OPTIMIZATION|RIVERS|MODEL,"An optimal flow control methodology based on adjoint sensitivity analysis for controlling nonlinear open channel flows with complex geometries is presented. The adjoint equations, derived from the nonlinear Saint-Venant equations, are generally capable of evaluating the time-dependent sensitivities with respect to a variety of control variables under complex flow conditions and cross-section shapes. The internal boundary conditions of the adjoint equations at a confluence (junction) derived by the variational approach make the flow control model applicable to solve optimal flow control problems in a channel network over a watershed. As a result, an optimal flow control software package has been developed, in which two basic modules, i.e., a hydrodynamic module and a bound constrained optimization module using the limited-memory quasi-Newton algorithm, are integrated. The effectiveness and applicability of this integrated optimal control tool are demonstrated thoroughly by implementing flood diversion controls in rivers, from one reach with a single or multiple floodgates (with or without constraints), to a channel network with multiple floodgates. This new optimal flow control model can be generally applied to make optimal decisions in real-time flood control and water resource management in a watershed.",,"Ding, Yan|Wang, Sam S. Y.",JOURNAL OF HYDRAULIC ENGINEERING-ASCE,open channel flow|sensitivity analysis|floods|geometry|optimization,10.1061/(ASCE)0733-9429(2006)132:11(1215)
134,WOS:000395529100004,2017,Monte Carlo Approach for Uncertainty Analysis of Acoustic Doppler Current Profiler Discharge Measurement by Moving Boat,ADCP VELOCITY|FRAMEWORK|VARIANCE,"This paper presents a method using Monte Carlo simulations for assessing uncertainty of moving-boat acoustic Doppler current profiler (ADCP) discharge measurements using a software tool known as QUant, which was developed for this purpose. Analysis was performed on  data sets from four Water Survey of Canada gauging stations in order to evaluate the relative contribution of a range of error sources to the total estimated uncertainty. The factors that differed among data sets included the fraction of unmeasured discharge relative to the total discharge, flow nonuniformity, and operator decisions about instrument programming and measurement cross section. As anticipated, it was found that the estimated uncertainty is dominated by uncertainty of the discharge in the unmeasured areas, highlighting the importance of appropriate selection of the site, the instrument, and the user inputs required to estimate the unmeasured discharge. The main contributor to uncertainty was invalid data, but spatial inhomogeneity in water velocity and bottom-track velocity also contributed, as did variation in the edge velocity, uncertainty in the edge distances, edge coefficients, and the top and bottom extrapolation methods. To a lesser extent, spatial inhomogeneity in the bottom depth also contributed to the total uncertainty, as did uncertainty in the ADCP draft at shallow sites. The estimated uncertainties from QUant can be used to assess the adequacy of standard operating procedures. They also provide quantitative feedback to the ADCP operators about the quality of their measurements, indicating which parameters are contributing most to uncertainty, and perhaps even highlighting ways in which uncertainty can be reduced. Additionally, QUant can be used to account for self-dependent error sources such as heading errors, which are a function of heading. The results demonstrate the importance of a Monte Carlo method tool such as QUant for quantifying random and bias errors when evaluating the uncertainty of moving-boat ADCP measurements.",,"Moore, Stephanie A.|Jamieson, Elizabeth C.|Rainville, Francois|Rennie, Colin D.|Mueller, David S.",JOURNAL OF HYDRAULIC ENGINEERING,moving-boat acoustic doppler current profiler (adcp)|uncertainty|monte carlo|probabilistic|stream gauging procedures,10.1061/(ASCE)HY.1943-7900.0001249
135,WOS:000417943600002,2017,Parameter sensitivity analysis of a 1-D cold region lake model for land-surface schemes,DIURNAL MIXED-LAYER|GLOBAL SENSITIVITY|CLIMATE MODELS|WATER-QUALITY|ENVIRONMENTAL SYSTEMS|UNCERTAINTY ANALYSES|HYDROLOGICAL MODELS|GENERAL-CIRCULATION|ORGANIC-MATTER|GREAT-LAKES,"Lakes might be sentinels of climate change, but the uncertainty in their main feedback to the atmosphere heat- exchange fluxes - is often not considered within climate models. Additionally, these fluxes are seldom measured, hindering critical evaluation of model output. Analysis of the Canadian Small Lake Model (CSLM), a one-dimensional integral lake model, was performed to assess its ability to reproduce diurnal and seasonal variations in heat fluxes and the sensitivity of simulated fluxes to changes in model parameters, i.e., turbulent transport parameters and the light extinction coefficient (K-d). A C++ open-source software package, Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), was used to perform sensitivity analysis (SA) and identify the parameters that dominate model behavior. The generalized likelihood uncertainty estimation (GLUE) was applied to quantify the fluxes' uncertainty, comparing daily-averaged eddy-covariance observations to the output of CSLM. Seven qualitative and two quantitative SA methods were tested, and the posterior likelihoods of the modeled parameters, obtained from the GLUE analysis, were used to determine the dominant parameters and the uncertainty in the modeled fluxes. Despite the ubiquity of the equifinality issue - different parameter-value combinations yielding equivalent results-the answer to the question was unequivocal: K-d, a measure of how much light penetrates the lake, dominates sensible and latent heat fluxes, and the uncertainty in their estimates is strongly related to the accuracy with which K-d is determined. This is important since accurate and continuous measurements of K-d could reduce modeling uncertainty.",,"Guerrero, Jose-Luis|Pernica, Patricia|Wheater, Howard|Mackay, Murray|Spence, Chris",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-21-6345-2017
136,WOS:000398771100014,2017,Life cycle energy and costs of sprawling and compact neighborhoods,CITY METROPOLITAN-AREA|URBAN FORM|NATURAL VENTILATION|MEXICO|CONSUMPTION|BUILDINGS|EMISSIONS|MANAGEMENT,"Purpose The aim of this study is to compare the life cycle energy and costs derived from the production and occupation of social interest housing models located in two different types of neighborhoods: compact and sprawling. Two neighborhood development alternatives in Mexico City were established and evaluated including the potential impacts analysis of the built environment/infrastructure and the commuting of the occupants. Methods The study includes the conventional phases of a building life cycle (LC)-preoccupation, occupation, and post-occupation-but it was expanded to include a fourth phase, ""occupant transportation,"" to cover the commuting potential impacts. The methodology consists of four main stages: () definition of function, functional unit, and scope; () data collection-divided in three main steps: architectural, land costs and transformations, and commuting data; () impact assessment-we used software SimaPro v.. to manage the LC inventory data; and () interpretation of results and sensitivity analysis. Results and discussion In the preoccupation phase, the sprawling neighborhood cell (NC) cumulative energy demand (CED) is  % larger than the compact NC ones. Regarding the LC costs, land costs strongly impact the compact NC, but when aggregated in the preoccupation phase, the LC costs for the sprawling NC are only  % above those of the compact NC. For the occupation phase, results show that the compact NC has lower CED (by  %) and LC costs ( %) than the sprawling NC. The occupant transportation phase plays a highly important role, since it represents up to  % of total LC CED and up to  % of total LC costs. This phase affects significantly the sprawling NC, which has a % higher CED and doubles LC costs, when compared with the compact NC. Post-occupation phase contributes just in a small proportion of the total CED and LC costs for both NC, since it accounts for  % or less of the total energy and LC costs. Overall results show that the compact NC has lower CED and LC costs than the sprawling NC. Conclusions The results show that occupant transportation phase plays a highly important role in the neighborhood performance. Neighborhood development assessment should consider a number of variables beyond CED and costs. However, in order to improve the sector's energy efficiency and household's economy, we recommend to consider house location as it can be as important as other energy or cost-reduction actions in neighborhood development.",,"Ochoa Sosa, Ricardo|Hernandez Espinoza, Andrea|Garfias Royo, Margarita|Morillon Galvez, David",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,compact|costs|energy|neighborhood|sprawling,10.1007/s11367-016-1100-2
137,WOS:000340951200003,2014,Numerical and intelligent modeling of triaxial strength of anisotropic jointed rock specimens,FUZZY INFERENCE SYSTEM|INTACT ROCKS|CONTINUUM|MASSES,"The strength of anisotropic rock masses can be evaluated through either theoretical or experimental methods. The latter is more precise but also more expensive and time-consuming especially due to difficulties of preparing high-quality samples. Numerical methods, such as finite element method (FEM), finite difference method (FDM), distinct element method (DEM), etc. have been regarded as precise and low-cost theoretical approaches in different fields of rock engineering. On the other hand, applicability of intelligent approaches such as fuzzy systems, neural networks and decision trees in rock mechanics problems has been recognized through numerous published papers. In current study, it is aimed to theoretically evaluate the strength of anisotropic rocks with through-going discontinuity using numerical and intelligent methods. In order to do this, first, strength data of such rocks are collected from the literature. Then FlAC, a commercially well-known software for FDM analysis, is applied to simulate the situation of triaxial test on anisotropic jointed specimens. Reliability of this simulation in predicting the strength of jointed specimens has been verified by previous researches. Therefore, the few gaps of the experimental data are filled by numerical simulation to prevent unexpected learning errors. Furthermore, a sensitivity analysis is carried out based on the numerical process applied herein. Finally, two intelligent methods namely feed forward neural network and a newly developed fuzzy modeling approach are utilized to predict the strength of above-mentioned specimens. Comparison of the results with experimental data demonstrates that the intelligent models result in desirable prediction accuracy.",,"Asadi, Mojtaba|Bagheripour, Mohammad Hossein",EARTH SCIENCE INFORMATICS,numerical modeling|artificial neural networks|fuzzy systems|strength anisotropy|jointed rock,10.1007/s12145-013-0137-z
138,WOS:000235218600050,2005,Comparison of deterministic and Monte Carlo methods in shielding design,,"In shielding calculation, deterministic methods have some advantages and also some disadvantages relative to other kind of codes, such as Monte Carlo. The main advantage is the short computer time needed to find solutions while the disadvantages are related to the often-used build-up factor that is extrapolated from high to low energies or with unknown geometrical conditions, which can lead to significant errors in shielding results. The aim of this work is to investigate how good are some deterministic methods to calculating low-energy shielding, using attenuation coefficients and build-up factor corrections. Commercial software MicroShield . has been used as the deterministic code while MCNP has been used as the Monte Carlo code. Point and cylindrical sources with slab shield have been defined allowing comparison between the capability of both Monte Carlo and deterministic methods in a day-by-day shielding calculation using sensitivity analysis of significant parameters, such as energy and geometrical conditions.",,"Oliveira, AD|Oliveira, C",RADIATION PROTECTION DOSIMETRY,,10.1093/prd/nci187
139,WOS:000168569900007,2001,COOPT - a software package for optimal control of large-scale differential-algebraic equation systems,,"This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential-algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals, The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering. (C)  IMACS.", Published by Elsevier Science B.V. All rights reserved.,"Serban, R|Petzold, LR",MATHEMATICS AND COMPUTERS IN SIMULATION,differential-algebraic equations|sensitivity analysis|optimal control|sequential quadratic programming methods,10.1016/S0378-4754(01)00289-0
140,WOS:000246812000012,2007,Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation,SURFACE PARAMETERIZATION SCHEMES|DISTRIBUTED MODEL|MULTIOBJECTIVE OPTIMIZATION|ENVIRONMENTAL SYSTEMS|HYDROLOGICAL MODELS|UNCERTAINTY|CALIBRATION|EFFICIENT|PROJECT|OUTPUT,"This study seeks to identify sensitivity tools that will advance our understanding of lumped hydrologic models for the purposes of model improvement, calibration efficiency and improved measurement schemes. Four sensitivity analysis methods were tested: () local analysis using parameter estimation software (PEST), () regional sensitivity analysis (RSA), () analysis of variance (ANOVA), and () Sobol's method. The methods' relative efficiencies and effectiveness have been analyzed and compared. These four sensitivity methods were applied to the lumped Sacramento soil moisture accounting model (SAC-SMA) coupled with SNOW-. Results from this study characterize model sensitivities for two medium sized watersheds within the Juniata River Basin in Pennsylvania, USA. Comparative results for the  sensitivity methods are presented for a -year time series with  h,  h, and  h time intervals. The results of this study show that model parameter sensitivities are heavily impacted by the choice of analysis method as well as the model time interval. Differences between the two adjacent watersheds also suggest strong influences of local physical characteristics on the sensitivity methods' results. This study also contributes a comprehensive assessment of the repeatability, robustness, efficiency, and ease-of-implementation of the four sensitivity methods. Overall ANOVA and Sobol's method were shown to be superior to RSA and PEST. Relative to one another, ANOVA has reduced computational requirements and Sobol's method yielded more robust sensitivity rankings.",,"Tang, Y.|Reed, P.|Wagener, T.|van Werkhoven, K.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-11-793-2007
141,WOS:000273938000054,2009,"Optimal testing resource allocation during module testing considering cost, testing effort and reliability",SOFTWARE-RELIABILITY|ERROR-DETECTION|MODELS|PROGRAMS,"Software reliability is one of the most important quality attributes of commercial software. During software testing, software reliability growth models (SRGMs) are commonly used to describe the phenomenon of failure occurrence and/or fault removal which consequently enhancements software reliability. Large software systems are developed by integrating a number of relatively small and independent modules, which are tested independently during module testing phase. The amount of testing resource available is limited which is desired to be consumed judiciously so as to optimize the testing process. In this paper we formulate a resource allocation problem of minimizing the cost of software testing under available amount of testing resource, given a reliability constraint. We use a flexible SRGM considering testing effort which, depending upon the values of parameters, can describe either exponential or S-shaped failure pattern of software modules. A systematic and sequential Algorithm is proposed to solve the optimization problem formulated. Numerical examples are given to illustrate the formulation and solution procedures. Sensitivity analysis is performed to examine the behavior of some parameters of SRGM with most significant influence. (C) ", Elsevier Ltd. All rights reserved.,"Jha, P. C.|Gupta, Deepali|Yang, Bo|Kapur, P. K.",COMPUTERS & INDUSTRIAL ENGINEERING,software reliability growth models|modular software|testing resource|optimal allocation|sensitivity analysis,10.1016/j.cie.2009.05.001
142,WOS:000237124500022,2006,Automatic calibration and predictive uncertainty analysis of a semidistributed watershed model,RAINFALL-RUNOFF MODELS|GLOBAL OPTIMIZATION|HYDROLOGIC-MODELS|PARAMETER-ESTIMATION|ALGORITHM|MULTIPLE|SCHEME,"Semidistributed models are commonly calibrated manually, but software for automatic calibration is now available. We present a two-stage routine for automatic calibration of the semidistributed watershed model Soil and Water Assessment Tool ( SWAT) that finds the best values for the model parameters, preserves spatial variability in essential parameters, and leads to a measure of the model prediction uncertainty. In the first stage, a modified global Shuffled Complex Evolution (SCE-UA) method was employed to find the ""best'' values for the lumped model parameters. In the second stage, the spatial variability of the original model parameters was restored and a local search method ( a variant of Levenberg - Marquart method) was used to find a more distributed set of parameters using the results of the previous stage as starting values. A method called ""regularization'' was adopted to prevent the parameters from taking extreme values. In addition, we applied a nonlinear calibration-constrained method to develop confidence intervals for annual and -d average flow predictions. We calibrated stream flow in the Etowah River measured at Canton, GA ( a watershed area of  km()) for the years  to  and used the years  to  for validation. The Parameter Estimator ( PEST) software was used to conduct the two-stage automatic calibration and prediction uncertainty analysis. Calibration for daily and monthly flow produced a very good fit to the measured data. Nash-Sutcliffe coefficients for daily and monthly flow over the calibration period were . and ., respectively. They were . and ., respectively, for the validation period. The nonlinear prediction uncertainty analysis worked well for long-term ( annual) flow in that our prediction confidence intervals included or were very near to the observed flow for most years. It did not work well for short-term (-d average) flows in that the prediction confidence intervals did not include the observed flow, especially for low and high flow conditions.",,"Lin, ZL|Radcliffe, DE",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0025
143,WOS:000283561900013,2010,"A model for the development of a power production system in Greece, Part II: Where RES meet EU targets",,"This paper studies the electricity production system of the Greek Interconnected Electric System using a development model created with the software package WASP-IV. The period of study is from  till . It consists of three scenarios using three different criteria: energy, environmental, and economic. The three scenarios are the Renewable Energy Source (RES), the lignite-RES, and the natural gas-RES. Subsequently, a sensitivity analysis is carried out for annual growth rate of electricity consumption and load demand. It is considered that there are no other energy sources beyond those already in use (lignite, oil, natural gas, biomass, solar, wind, and hydropower), no CO capture policies are implemented, and electricity production from RES meets targets of the European Union in . The present paper completes the study started with the paper ""A model for the development of a power production system in Greece, Part : RES do not meet EU targets"". It is shown that with regard to fossil fuels, the use of natural gas is the best choice. The use of RES, though environmentally friendly, is an expensive solution. (C) ", Elsevier Ltd. All rights reserved.,"Kalampalikas, Nikolaos G.|Pilavachi, Petros A.",ENERGY POLICY,electricity production system|electricity consumption|co2 emissions,10.1016/j.enpol.2010.05.037
144,WOS:000281060100019,2010,Prediction of performance of sea water reverse osmosis units,SEAWATER RO|DESALINATION|REGRESSION|OPTIMIZATION|SIMULATION|DESIGN,"This paper presents a methodology for the prediction of the performance of sea water reverse osmosis units acquired by salient mathematical correlations which involve the important aspects of membrane input and output variables. Feed water conditions pertinent to flow rate, TDS (total dissolved solids), pressure, pH and temperature are correlated with recovery ratio, permeate TDS and power consumption via regression analysis. Satisfactory results are obtained by comparing the resulting correlations against the software ROSA predictions. The mathematical correlations and regression assessment led to the derivation of simple polynomial expressions which were used for sensitivity analysis and may be useful for the design, monitoring and control of this important desalination process.", (C) 2010 Elsevier B.V. All rights reserved.,"Alahmad, M.",DESALINATION,correlations|performance|predictions|reverse osmosis|rosa,10.1016/j.desal.2010.05.018
145,WOS:000295716700008,2011,Uncertainty propagation or box propagation,ORDINARY DIFFERENTIAL-EQUATIONS|INITIAL-VALUE PROBLEMS|TAYLOR-SERIES METHOD|VALIDATED SOLUTIONS|LORENZ MODEL|ODES|ODES/DAES,This paper discusses the use of recently developed techniques and software in the numerical propagation of uncertainties in initial coordinates and/or parameters for initial value problems. We present an approach based on several validated numerical integration techniques but focusing on the propagation of boxes. The procedure uses a multivariable high order Taylor series development of the solution of the system whose Taylor coefficients are calculated via extended automatic differentiation rules for all the basic operations. These techniques are implemented in the recent free-software TIDES. The classical two-body and Lorenz problems are chosen as examples to show the benefits of the approach. The results show that the solution of uncertainties can be approximated in an analytic form by means of a Taylor series and that these techniques can be extremely useful in different practical applications. (C) , Elsevier Ltd. All rights reserved.,"Barrio, R.|Rodriguez, M.|Abad, A.|Serrano, S.",MATHEMATICAL AND COMPUTER MODELLING,taylor series method|automatic differentiation|uncertainty propagation|freeware software,10.1016/j.mcm.2011.06.036
146,WOS:000282087300021,2010,TAMkin: A Versatile Package for Vibrational Analysis and Chemical Kinetics,MOLECULAR-ORBITAL METHODS|FREE-RADICAL POLYMERIZATIONS|BLOCK HESSIAN APPROACH|FREQUENCY NORMAL-MODES|AB-INITIO CALCULATION|PHASE N-ALKANES|HARMONIC-ANALYSIS|LARGE SYSTEMS|BASIS-SET|THERMOCHEMICAL KINETICS,"TAMkin is a program for the calculation and analysis of normal modes, thermochemical properties and chemical reaction rates. At present, the output from the frequently applied software programs ADF, CHARMM, CPMD, CPK, Gaussian, Q-Chem, and VASP can be analyzed. The normal-mode analysis can be performed using a broad variety of advanced models, including the standard full Hessian, the Mobile Block Hessian, the Partial Hessian Vibrational approach, the Vibrational Subsystem Analysis with or without mass matrix correction, the Elastic Network Model, and other combinations. TAMkin is readily extensible because of its modular structure. Chemical kinetics of unimolecular and bimolecular reactions can be analyzed in a straightforward way using conventional transition state theory, including tunneling corrections and internal rotor refinements. A sensitivity analysis can also be performed, providing important insight into the theoretical error margins on the kinetic parameters. Two extensive examples demonstrate the capabilities of TAMkin: the conformational change of the biological system adenylate kinase is studied, as well as the reaction kinetics of the addition of ethene to the ethyl radical. The important feature of batch processing large amounts of data is highlighted by performing an extended level of theory study, which TAMkin can automate significantly.",,"Ghysels, An|Verstraelen, Toon|Hemelsoet, Karen|Waroquier, Michel|Van Speybroeck, Veronique",JOURNAL OF CHEMICAL INFORMATION AND MODELING,,10.1021/ci100099g
147,WOS:000403512500013,2017,"The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control",REAL-TIME MANAGEMENT|COMPUTATIONAL SCIENCE|CO2 STORAGE|FLOW|ASSIMILATION|VERIFICATION|ALGORITHMS|COMPLEXITY|TRANSPORT|SELECTION,"In the past decades, simulation frameworks have greatly increased in complexity, due to coupling of models from various disciplines into so-called integrated models. Recently, the combination with tools for uncertainty quantification, inverse modelling, optimization and control started a development towards what we call extended simulation frameworks. While there is an ongoing discussion on quality assurance and reproducibility for simulation frameworks, we have not observed a similar discussion for the extended case. Particularly for extended frameworks, the need for quality assurance is high: The overwhelming range of options and algorithms is unmanageable by a domain expert and opaque to decision makers or the public. The resulting demand for 'intelligent software' with automated configuration can lead to a blind trust in simulation results even if they are incorrect. This is a threatening scenario due to potential consequences in simulation-based engineering or political decisions. In this paper, we analyze the increasing complexity of scientific computing workflows, and discuss the corresponding problems of extended scientific simulation frameworks. We propose a paradigm that regulates the allowable properties of framework components, supports the framework configuration for complex simulations, enforces automatic self-tests of configured frameworks, and communicates automated algorithm choices, potentially critical user settings or convergence issues with adaptive detail level and urgency to the end-user. Our goal is to start transferring the quality assurance discussion in the field of integrated modeling and conventional software frameworks to the area of extended simulation frameworks. With this, we hope to increase the reliability and transparency of (extended) frameworks, framework use and of the corresponding simulation results. (C) ", Elsevier Ltd. All rights reserved.,"Wirtz, Daniel|Nowak, Wolfgang",ENVIRONMENTAL MODELLING & SOFTWARE,scientific computing|extended software frameworks|no free lunch|reproducibility|software trust,10.1016/j.envsoft.2016.10.003
148,WOS:000255770300011,2008,Tools to support a model-based methodology for emission/immission and benefit/cost/risk analysis of wastewater systems that considers uncertainty,NO. 1 RWQM1|TREATMENT PLANTS|WWTP DESIGN|QUALITY|COSTS|BENCHMARKING,"This paper presents a set of tools developed to support an innovative methodology to design and upgrade wastewater treatment systems in a probabilistic way. For the first step, data reconstruction, two different tools were developed, one for situations where data are available and another one where no data are available. The second step, modelling and simulation, implied the development of a new simulation platform and of distributed computation software to deal with the simulation load generated by the third step, uncertainty analysis, with Monte Carlo simulations of the system over one year, important dynamics and stiff behaviour. For the fourth step, evaluation of alternatives, the evaluator tool processes the results of the simulations and plots the relevant information regarding the robustness of the process against input and parameters uncertainties, as well as concentration-duration curves for the risk of non-compliance with effluent and receiving water quality limits. This paper illustrates the merits of these tools to make the innovative methodology of practical interest. The design practice should move from conventional procedures suited for the relatively fixed context of emission limits, to more advanced, transparent and cost-effective procedures appropriate to cope with the flexibility and complexity introduced by integrated water management approaches. (c) ", Elsevier Ltd. All rights reserved.,"Benedetti, Lorenzo|Bixio, Davide|Claeys, Filip|Vanrolleghem, Peter A.",ENVIRONMENTAL MODELLING & SOFTWARE,wastewater treatment plant design|cost-benefit analysis|risk|modelling and simulation|software tools|grid computing,10.1016/j.envsoft.2008.01.001
149,WOS:000250159500009,2007,Simiyu River catchment parameterization using SWAT model,INFORMATION-SYSTEM APPROACH|RAINFALL-RUNOFF MODELS|SURFACE SOIL-MOISTURE|MONSOON 90,"The paper presents advances in hydrologic modelling of the Simiyu River catchment using the soil and water assessment tool (SWAT). In this study, the SWAT model set-up and subsequent application to the catchment was based on high-resolution data such as land use from  in LandSat TM Satellite,  in Digital Elevation Model and Soil from Soil and Terrain Database for Southern Africa (SOTERSAF). The land use data were reclassified based on some ground truth maps using IDRISI Kilimanjaro software. The Soil data were also reclassified manually to represent different soil hydrologic groups, which are important for the SWAT model set-up and simulations. The SWAT application first involved analysis of parameter sensitivity, which was then used for model auto-calibration that followed hierarchy of sensitive model parameters. The analysis of sensitive parameters and auto-calibration was achieved by sensitivity analysis and auto-calibration options, which are new in the recent version of SWAT, SWAT . The paper discusses the results of sensitivity and auto -calibration analyses, and present optimum model parameters, which are important for operation and water/land management studies (e.g. rain-fed agriculture and erosion/sediment and pollutant transport) in the catchment using SWAT. The river discharge estimates from this and a previous study were compared so as to evaluate performances of the recent hydrologic simulations in the catchment. Results showed that surface water model parameters are the most sensitive and have more physical meaning especially CN (the most sensitive) and SOL-K. Simulation results showed more or less same estimate of river flow at Ndagalu gauging station. The model efficiencies (R-%) in this and in the pervious study during calibration and validation periods were, respectively, ., . and ., .. The low level of model performance achieved in these studies showed that other factors than the spatial land data are greatly important for improvement of flow estimation by SWAT in Simiyu.", (c) 2007 Published by Elsevier Ltd.,"Mulungu, Deogratlas M. M.|Munishi, Subira E.",PHYSICS AND CHEMISTRY OF THE EARTH,auto-calibration analysis|high-resolution data|parameter sensitivity analysis|simiyu river catchment|swat 2005,10.1016/j.pce.2007.07.053
150,WOS:000257487300002,2008,Materials integrity in microsystems: a framework for a petascale predictive-science-based multiscale modeling and simulation system,STRESSED GRAIN-GROWTH|CHAOS REPRESENTATIONS|POLYCRYSTALLINE MATERIALS|ELASTIC PROPERTIES|ADAPTIVE ANALYSIS|DESIGN|UNCERTAINTY|FORMULATION|METALS|ERRORS,"Microsystems have become an integral part of our lives and can be found in homeland security, medical science, aerospace applications and beyond. Many critical microsystem applications are in harsh environments, in which long-term reliability needs to be guaranteed and repair is not feasible. For example, gyroscope microsystems on satellites need to function for over  years under severe radiation, thermal cycling, and shock loading. Hence a predictive-science-based, verified and validated computational models and algorithms to predict the performance and materials integrity of microsystems in these situations is needed. Confidence in these predictions is improved by quantifying uncertainties and approximation errors. With no full system testing and limited sub-system testings, petascale computing is certainly necessary to span both time and space scales and to reduce the uncertainty in the prediction of long-term reliability. This paper presents the necessary steps to develop predictive-science-based multiscale modeling and simulation system. The development of this system will be focused on the prediction of the long-term performance of a gyroscope microsystem. The environmental effects to be considered include radiation, thermo-mechanical cycling and shock. Since there will be many material performance issues, attention is restricted to creep resulting from thermal aging and radiation-enhanced mass diffusion, material instability due to radiation and thermo-mechanical cycling and damage and fracture due to shock. To meet these challenges, we aim to develop an integrated multiscale software analysis system that spans the length scales from the atomistic scale to the scale of the device. The proposed software system will include molecular mechanics, phase field evolution, micromechanics and continuum mechanics software, and the state-of-the-art model identification strategies where atomistic properties are calibrated by quantum calculations. We aim to predict the long-term (in excess of  years) integrity of the resonator, electrode base, multilayer metallic bonding pads, and vacuum seals in a prescribed mission. Although multiscale simulations are efficient in the sense that they focus the most computationally intensive models and methods on only the portions of the space-time domain needed, the execution of the multiscale simulations associated with evaluating materials and device integrity for aerospace microsystems will require the application of petascale computing. A component-based software strategy will be used in the development of our massively parallel multiscale simulation system. This approach will allow us to take full advantage of existing single scale modeling components. An extensive, pervasive thrust in the software system development is verification, validation, and uncertainty quantification (UQ). Each component and the integrated software system need to be carefully verified. An UQ methodology that determines the quality of predictive information available from experimental measurements and packages the information in a form suitable for UQ at various scales needs to be developed. Experiments to validate the model at the nanoscale, microscale, and macroscale are proposed. The development of a petascale predictive-science-based multiscale modeling and simulation system will advance the field of predictive multiscale science so that it can be used to reliably analyze problems of unprecedented complexity, where limited testing resources can be adequately replaced by petascale computational power, advanced verificaion, validation, and UQ methodologies.",,"To, Albert C.|Liu, Wing Kam|Olson, Gregory B.|Belytschko, Ted|Chen, Wei|Shephard, Mark S.|Chung, Yip-Wah|Ghanem, Roger|Voorhees, Peter W.|Seidman, David N.|Wolverton, Chris|Chen, J. S.|Moran, Brian|Freeman, Arthur J.|Tian, Rong|Luo, Xiaojuan|Lautenschlager, Eric|Challoner, A. Dorian",COMPUTATIONAL MECHANICS,multiscale modeling|petascale computing|microsystems|mems,10.1007/s00466-008-0267-1
151,WOS:000356347800059,2015,Integrated regional ecological risk assessment of multi-ecosystems under multi-disasters: a case study of China,SAMPLING-BASED METHODS|SENSITIVITY-ANALYSIS|UNCERTAINTY|MODEL,"Using China as a case study, this paper explores the integrated regional ecological risk assessment of multiple stressors and multiple receptors on a large spatial scale. The objective is to provide scientific data to support ecological risk identification and prevention. To carry out this assessment, ten natural disasters were chosen as risk sources, and twenty-two ecosystems were chosen as risk receptors. The vulnerability of environment where these ecosystems existed was taken into consideration. Using the software platform GIS, the ecological risk of each disaster was evaluated, the integrated assessment for all disasters was compiled, and the integrated risk of different ecosystems was obtained. All results were shown in assessment maps. The results show that forty-five percent of the ecosystems' areas in China face high or medium ecological risks. This result indicates that the establishment of ecosystem protection and ecological risk prevention mechanisms in China is still a long-term, difficult task, requiring the rational use and conservation of forests, meadows, farmland, wetlands, and other ecosystems alike is of great necessity. The uncertainty analysis of risk assessment using the Monte Carlo Simulation method demonstrated the results to be reliable and credible.",,"Xu, Xuegong|Xu, Lifen|Yan, Lei|Ma, Luyi|Lu, Yaling",ENVIRONMENTAL EARTH SCIENCES,ecological risk|integrated assessment|natural disaster|uncertainty analysis|china,10.1007/s12665-015-4079-2
152,WOS:000282320800018,2010,Effect of fracture zone on DNAPL transport and dispersion: a numerical approach,MULTIPHASE FLOW|GROUNDWATER|DISSOLUTION|MIGRATION|AQUIFER|POOLS|MEDIA,"Two numerical simulation techniques have been used to identify a suitable method to assist in the characterization of DNAPL movement within fractured porous rock aquifers. Both MODFLOW and UTCHEM software modeling suites were used to simulate different scenarios in fracture dip and hydraulic conductivities. The complexity and the physical structure of fracture characterization were shown to have a significant effect on modeling results, to the extent that fracture zone should be characterized fully before simulation models are used for DNAPL simulations. Sensitivity analysis was conducted on both the hydraulic conductivity and fracture dip values. DNAPL movement in the subsurface showed a high sensitivity to fracture dip variation.",,"Dennis, I.|Pretorius, J.|Steyl, G.",ENVIRONMENTAL EARTH SCIENCES,dnapl|fracture dip|numerical simulation|hydraulic conductivity,10.1007/s12665-010-0468-8
153,WOS:000242724500019,2006,Uncertainty analysis for regional-scale reserve selection,SITE SELECTION|SPECIES DISTRIBUTION|CONSERVATION|DESIGN|BIODIVERSITY|NETWORKS|MODELS|PERSISTENCE|PROBABILITIES|CONNECTIVITY,"Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, ""distribution discounting,"" in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.",,"Moilanen, Atte|Wintle, Brendan A.|Elith, Jane|Burgman, Mark",CONSERVATION BIOLOGY,distribution discounting|distribution smoothing|information-gap decision theory|reserve-network design|site-selection algorithm|spatial reserve design|zonation,10.1111/j.1523-1739.2006.00560.x
154,WOS:000366074400041,2016,Soil solution concentrations and chemical species of copper and zinc in a soil with a history of pig slurry application and plant cultivation,DISSOLVED ORGANIC-MATTER|SANDY TYPIC HAPLUDALF|CONTAMINATED SOILS|HEAVY-METALS|VINEYARD SOILS|DEEP-LITTER|ZN|CU|COMPLEXATION|PHOSPHORUS,"Successive pig slurry applications may increase soil copper(Cu) and zinc (Zn) concentrations and change the proportions of free chemical species in solution when combined with plant cultivation. The aim of this study was to assess the soluble, available,,and total Cu and Zn concentrations and the distribution of their chemical species in the solution in a Hapludalf soil with a history of pig slurry application and plant cultivation. The study was conducted in undisturbed soil columns that originated from an -year-long experiment conducted at the experimental unit of the Federal University of Santa Maria in Santa Maria, southern Brazil. The soil was a Typic Hapludalf soil fertilized with pig slurry at rates of , , , and  m() ha(-). The soil was collected from depth intervals of -., .-., .-., .-., -., and .-. m before and after cultivation with black oat and maize in a greenhouse to assess the total and available Cu and Zn concentrations and to extract the solution. The soil solution concentrations of the main cations, anions, and dissolved organic carbon (DOC) and pH were assessed. The distribution of Cu and Zn chemical species was assessed using the Visual Minteq software. The history of  pig slurry applications increased the concentration of Cu and Zn in surface soil intervals, but the concentration of Cu also increased in the soil solution at depth. The phytotoxicity caused by Cu and Zn may not occur even after several years of pig slurry application because the plants provide soil conditions in which chemical species complexed with dissolved organic carbon predominate and Cu and Zn in free forms are present only in small amounts.", (C) 2015 Elsevier B.V. All rights reserved.,"De Conti, Lessandro|Ceretta, Carlos A.|Ferreira, Paulo A. A.|Lourenzi, Cledimar R.|Girotto, Eduardo|Lorensini, Felipe|Tiecher, Tadeu L.|Marchezan, Carina|Anchieta, Mylena G.|Brunetto, Gustavo",AGRICULTURE ECOSYSTEMS & ENVIRONMENT,poaceae|pig slurry|speciation modeling|dissolved organic carbon,10.1016/j.agee.2015.09.040
155,WOS:000390183000039,2016,Identifiability of sorption parameters in stirred flow-through reactor experiments and their identification with a Bayesian approach,NONEQUILIBRIUM SOLUTE TRANSPORT|CHAIN MONTE-CARLO|POROUS-MEDIA|MODELS|ADSORPTION|EQUILIBRIUM|KINETICS|VALUES,"This paper addresses the methodological conditions particularly experimental design and statistical inference ensuring the identifiability of sorption parameters from breakthrough curves measured during stirred flow-through reactor experiments also known as continuous flow stirred-tank reactor (CSTR) experiments. The equilibrium-kinetic (EK) sorption model was selected as nonequilibrium parameterization embedding the Kd approach. Parameter identifiability was studied.formally on the equations governing outlet concentrations. It was also studied numerically on  simulated CSTR experiments on a soil with known equilibrium-kinetic sorption parameters. EK sorption parameters can not be identified from a single breakthrough curve of a CSTR experiment, because K-d,K- and k(-) were diagnosed collinear. For pairs of CSTR experiments, Bayesian inference allowed to select the correct models of sorption and error among sorption alternatives. Bayesian inference was conducted with SAMCAT software (Sensitivity Analysis and Markov Chain simulations Applied to Transfer models) which launched the simulations through the embedded simulation engine GNU-MCSim, and automated their configuration and post-processing. Experimental designs consisting in varying flow rates between experiments reaching equilibrium at contamination stage were found optimal, because they simultaneously gave accurate sorption parameters and predictions. Bayesian results were comparable to maximum likehood method but they avoided convergence problems, the marginal likelihood allowed to compare all modeK and credible interval gave directly the uncertainty of sorption parameters theta. Although these findings are limited to the specific conditions studied here, in particular the considered sorption model, the chosen parameter values and error structure, they help in the conception and analysis of future CSTR experiments with radionuclides whose kinetic behaviour is suspected. (C) ", Elsevier Ltd. All rights reserved.,"Nicoulaud-Gouin, V.|Garcia-Sanchez, L.|Giacalone, M.|Attard, J. C.|Martin-Garin, A.|Bois, F. Y.",JOURNAL OF ENVIRONMENTAL RADIOACTIVITY,bayesian inference|sorption parameters|identifiability|mcmc|convergence monitoring|equilibrium kinetic model,10.1016/j.jenvrad.2016.06.008
156,WOS:000404559900070,2017,"Two-Dimensional Dam-Break Flood Analysis in Data-Scarce Regions: The Case Study of Chipembe Dam, Mozambique",DIGITAL ELEVATION MODELS|INUNDATION MODELS|SHUTTLE RADAR|SIMULATION|RESOLUTION|UNCERTAINTY|PREDICTIONS|HYDRAULICS|TOPOGRAPHY|PARAMETERS,"This paper presents the results of a modeling study of the hypothetical dam break of Chipembe dam in Mozambique. The modeling approach is based on the software Iber, a freely available dam break and two-dimensional finite volume shallow water model. The shuttle radar topography mission (SRTM) online digital elevation model (DEM) is used as main source of topographic data. Two different DEMs are considered as input for the hydraulic model: a DEM based on the original SRTM data and a hydrologically-conditioned DEM. A sensitivity analysis on the Manning roughness coefficient is performed. The results demonstrate the relevant impact of the DEM used on the predicted flood wave propagation, and a lower influence of the roughness value. The low cost modeling approach proposed in this paper can be an attractive option for modeling exceptional flood caused by dam break, when limited data and resources are available, as in the presented case. The resulting flood-inundation and hazard maps will enable the Regional Water Management Administration of Mozambique (ARA) to develop early warning systems.",,"Alvarez, Manuel|Puertas, Jeronimo|Pena, Enrique|Bermudez, Maria",WATER,dam-break|flood modeling|hazard mapping|iber model|mozambique,10.3390/w9060432
157,WOS:000337157100006,2014,Error propagation methods for LCA-a comparison,LIFE-CYCLE ASSESSMENT|INPUT-OUTPUT MODELS|UNCERTAINTY ANALYSIS|DECISION-MAKING|ASSESSMENTS|INVENTORIES|PRODUCTS|IMPACT,"The analysis of uncertainty in life cycle assessment (LCA) studies has been a topic for more than  years, and many commercial LCA programs now feature a sampling approach called Monte Carlo analysis. Yet, a full Monte Carlo analysis of a large LCA system, for instance containing the , unit processes of ecoinvent v., is rarely carried out by LCA practitioners. One reason for this is computation time. An alternative faster than Monte Carlo method is analytical error propagation by means of a Taylor series expansion; however, this approach suffers from being explained in the literature in conflicting ways, hampering implementation in most software packages for LCA. The purpose of this paper is to compare the two different approaches from a theoretical and practical perspective. In this paper, we compare the analytical and sampling approaches in terms of their theoretical background and their mathematical formulation. Using three case studies-one stylized, one real-sized, and one input-output (IO)-based-we approach these techniques from a practical perspective and compare them in terms of speed and results. Depending on the precise question, a sampling or an analytical approach provides more useful information. Whenever they provide the same indicators, an analytical approach is much faster but less reliable when the uncertainties are large. For a good analysis, analytical and sampling approaches are equally important, and we recommend practitioners to use both whenever available, and we recommend software suppliers to implement both.",,"Heijungs, Reinout|Lenzen, Manfred",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,analytical methods|gaussian error propagation|ioa|lca|monte carlo|sampling methods|uncertainty,10.1007/s11367-014-0751-0
158,WOS:000266820000004,2009,Finite element response sensitivity analysis of multi-yield-surface J(2) plasticity model by direct differentiation method,ELASTOPLASTIC SEISMIC RESPONSE|FOUNDATION-GROUND SYSTEM|STRESS-STRAIN BEHAVIOR|3-D EARTH DAMS|DYNAMIC-RESPONSE|SOFTWARE|RELIABILITY,"Finite element (FE) response sensitivity analysis is an essential tool for gradient-based optimization methods used in various sub-fields of civil engineering such as structural optimization, reliability analysis, system identification, and finite element model updating. Furthermore, stand-alone sensitivity analysis is invaluable for gaining insight into the effects and relative importance of various system and loading parameters on system response. The direct differentiation method (DDM) is a general, accurate and efficient method to compute FE response sensitivities to FE model parameters. In this paper, the DDM-based response sensitivity analysis methodology is applied to a pressure independent multi-yield-surface J() plasticity material model, which has been used extensively to simulate the nonlinear undrained shear behavior of cohesive soils subjected to static and dynamic loading conditions. The complete derivation of the DDM-based response sensitivity algorithm is presented. This algorithm is implemented in a general-purpose nonlinear finite element analysis program. The work presented in this paper extends significantly the framework of DDM-based response sensitivity analysis, since it enables numerous applications involving the use of the multi-yield-surface J() plasticity material model. The new algorithm and its software implementation are validated through two application examples, in which DDM-based response sensitivities are compared with their counterparts obtained using forward finite difference (FFD) analysis. The normalized response sensitivity analysis results are then used to measure the relative importance of the soil constitutive parameters on the system response.", (C) 2009 Elsevier B.V. All rights reserved.,"Quan Gu, |Conte, Joel P.|Elgamal, Ahmed|Yang, Zhaohui",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,nonlinear finite element analysis|response sensitivity analysis|multi-yield-surface plasticity model|direct differentiation method|soil material model,10.1016/j.cma.2009.02.030
159,WOS:000223528100004,2004,POLCAGE 1.0 - a possibilistic life-cycle assessment model for evaluating alternative transportation fuels,FUZZY OUTRANKING|SYSTEMS|ENERGY,"A composite software model for the comparative life-cycle assessment (LCA) of  different fuel options for the Philippine automotive transport sector was developed. It is based on the GREET fuel-cycle inventory model developed by the Argonne National Laboratory for the United States Department of Energy. GREET .a is linked to an impact assessment submodel using the Danish environmental design of industrial products (EDIP) method. This combined inventory-impact assessment model is enhanced further with possibilistic uncertainty propagation (PUP) and possibilistic compromise programming (PCP) features that allow the  alternatives to be ranked in the presence of multiple criteria and uncertain data. Sensitivity and scenario analysis can also be performed within the composite model. Some current and anticipated Philippine conditions, including electricity generation mix, are incorporated in the prototype's built-in database. The software model, designated as POLCAGE . (possibilistic LCA using GREET and EDIP), is coded in Microsoft Excel and Visual Basic. The model's capabilities and features are demonstrated using a case study based on its default scenario. (C) ", Elsevier Ltd. All rights reserved.,"Tan, RR|Culaba, AB|Purvis, MRI",ENVIRONMENTAL MODELLING & SOFTWARE,life-cycle assessment (lca)|decision support system (dss)|alternative fuels,10.1016/j.envsoft.2003.10.004
160,WOS:000378366700024,2016,Thermal comfort and energy performance: Sensitivity analysis to apply the Passive House concept to the Portuguese climate,SIMULATION,"The need to apply the Passive House concept to Mediterranean countries climate is regarded as being of great importance to support countries such as Portugal to reduce its primary energy demand associated to buildings consumption and thus, devising a cost-efficient strategy to meet the targets pointed out by the recast of the EPBD //EU. In this sense, the present research intends to contribute to the implementation of the Passive House concept in Portugal, by means of a detailed study for the Aveiro region and a more broad analysis examination for different district capitals of Portugal mainland. A detached two-storey lightweight steel structure of contemporary architecture was modelled as case study for the Portuguese climate, based on its original design solutions and resorting to the EnergyPlus((R)) software. From this original model, sensitivity analyses were carried out in order to meet the parameters defined by PH standards. The improved results from the climate region of Aveiro, in Portugal, have led to a reduction of the %, % and .% for the heating demand, cooling demand and overheating rate, respectively (comparing the improved solution with the original as reference). It was therefore possible to meet the PH requirements, proving its applicability to the Portuguese climate and for this particular building technology.", (c) 2016 Elsevier Ltd.,"Figueiredo, Antonio|Figueira, Jose|Vicente, Romeu|Maio, Rui",BUILDING AND ENVIRONMENT,energy efficiency|passive house|dynamic building simulation,10.1016/j.buildenv.2016.03.031
161,WOS:000408861800155,2017,Improving Thermal Comfort of Low-Income Housing in Thailand through Passive Design Strategies,TROPICAL HUMID REGION|BUILDINGS|STANDARDS|ADAPTATION|HOUSES,"In Thailand, the delivery of adequate low-income housing has historically been overshadowed by politics with cost and quantity being prioritised over quality, comfort and resilience. In a country that experiences hot and humid temperatures throughout the year, buildings need to be adaptable to the climate to improve the thermal comfort of inhabitants. This research is focused on identifying areas for improving the thermal performance of these housing designs. Firstly, dynamic thermal simulations were run on a baseline model using the adaptive thermal comfort model CIBSE TM for assessment. The three criteria defined in CIBSE TM were used to assess the frequency and severity of overheating in the buildings. The internal temperature of the apartments was shown to exceed the thermal comfort threshold for these criteria throughout the year. The internal operating daily temperatures of the apartment remain high, ranging from a maximum of . degrees C to a minimum of . degrees C. Based on these findings, five criteria were selected to be analysed for sensitivity to obtain the key parameters that influence the thermal performance and to suggest possible areas for improvement. The computer software package Integrated Environmental SolutionsVirtual Environment (IES-VE) was used to perform building energy simulations. Once the baseline conditions were identified, the software packages SimLab. and RStudio were used to carry out the sensitivity analysis. These results indicated that roof material and the presence of a balcony have the greatest influence on the system. Incorporating insulation into the roof reduced the mean number of days of overheating by .%. Removing the balcony increased the number of days of overheating by .% due to significant reductions in internal ventilation.",,"Bhikhoo, Nafisa|Hashemi, Arman|Cruickshank, Heather",SUSTAINABILITY,thermal comfort|low income housing|thailand|tropical climates|dynamic thermal simulations|sensitivity analysis,10.3390/su9081440
162,WOS:000186310600006,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: Part I - theory and software tools,VARIATIONAL DATA ASSIMILATION|ATMOSPHERIC CHEMISTRY PROBLEMS|GREENS-FUNCTION METHOD|NON-LINEAR SYSTEMS|STIFF ODE SOLVERS|ROSENBROCK METHOD|MODELS|IMPLEMENTATION|IMPLICIT|EXPLICIT,"The analysis of comprehensive chemical reactions mechanisms, parameter estimation techniques, and variational chemical data assimilation applications require the development of efficient sensitivity methods for chemical kinetics systems. The new release (KPP-.) of the kinetic preprocessor (KPP) contains software tools that facilitate direct and adjoint sensitivity analysis. The direct-decoupled method, built using BDF formulas, has been the method of choice for direct sensitivity studies. In this work, we extend the direct-decoupled approach to Rosenbrock stiff integration methods. The need for Jacobian derivatives prevented Rosenbrock methods to be used extensively in direct sensitivity calculations; however, the new automatic and symbolic differentiation technologies make the computation of these derivatives feasible. The direct-decoupled method is known to be efficient for computing the sensitivities of a large number of output parameters with respect to a small number of input parameters. The adjoint modeling is presented as an efficient tool to evaluate the sensitivity of a scalar response function with respect to the initial conditions and model parameters. In addition, sensitivity with respect to time-dependent model parameters may be obtained through a single backward integration of the adjoint model. KPP software may be used to completely generate the continuous and discrete adjoint models taking full advantage of the sparsity of the chemical mechanism. Flexible direct-decoupled and adjoint sensitivity code implementations are achieved with minimal user intervention. In a companion paper, we present an extensive set of numerical experiments that validate the KPP software tools for several direct/adjoint sensitivity applications, and demonstrate the efficiency of KPP-generated sensitivity code implementations. (C) ", Elsevier Ltd. All rights reserved.,"Sandu, A|Daescu, DN|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,chemical kinetics|sensitivity analysis|direct-decoupled method|adjoint model,10.1016/j.atmosenv.2003.08.019
163,WOS:000323349500001,2013,Optimal Design of Signal Controlled Road Networks Using Differential Evolution Optimization Algorithm,EQUILIBRIUM TRANSPORTATION NETWORKS|VARIATIONAL INEQUALITY CONSTRAINTS|SIMULATED ANNEALING APPROACH|AREA TRAFFIC CONTROL|GENETIC ALGORITHM|SENSITIVITY ANALYSIS|ASSIGNMENT|TIMINGS|FLOW,"This study proposes a traffic congestion minimization model in which the traffic signal setting optimization is performed through a combined simulation-optimization model. In this model, the TRANSYT traffic simulation software is combined with Differential Evolution (DE) optimization algorithm, which is based on the natural selection paradigm. In this context, the EQuilibrium Network Design (EQND) problem is formulated as a bilevel programming problem in which the upper level is the minimization of the total network performance index. In the lower level, the traffic assignment problem, which represents the route choice behavior of the road users, is solved using the Path Flow Estimator (PFE) as a stochastic user equilibrium assessment. The solution of the bilevel EQND problem is carried out by the proposed Differential Evolution and TRANSYT with PFE, the so-called DETRANSPFE model, on a well-known signal controlled test network. Performance of the proposed model is compared to that of two previous works where the EQND problem has been solved by Genetic-Algorithms- (GAs-) and Harmony-Search- (HS-) based models. Results show that the DETRANSPFE model outperforms the GA- and HS-based models in terms of the network performance index and the computational time required.",,"Ceylan, Huseyin",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2013/696374
164,WOS:000288865100006,2011,Estimating Water Budgets and Vertical Leakages for Karst Lakes in North-Central Florida (United States) Via Hydrological Modeling,RAINFALL-RUNOFF MODELS|AUTOMATIC CALIBRATION|CONDUCTANCE,"Newnans, Lochloosa, and Orange Lakes are closely hydrologically connected karst lakes located in north-central Florida, United States. The complex karst hydrology in this region poses a great challenge to the hydrological modeling that is essential to the development of Total Maximum Daily Loads for these lakes. We used a Hydrological Simulation Program - Fortran model coupled with the parallel Parameter ESTimation model calibration and uncertainty analysis software to estimate effectively the hydrological interactions between the lakes and the underlying upper Floridan aquifer and the water budgets for these three lakes. The net results of the lake-groundwater interactions in Newnans and Orange Lakes are that both lakes recharge the underlying upper Floridan aquifer, with the recharge rate of the latter one magnitude greater than that of the former. However, for Lochloosa Lake, the net lake-groundwater interaction is that the lake gains water from groundwater in a significant amount, approximately % of its total terrestrial water input. The annual average vertical leakages estimated for Newnans, Lochloosa, and Orange Lakes are . x , -. x , and . x  m, respectively. The average vertical hydraulic conductance (K(v)/b) of the units between a lake bottom and the underlying upper Floridan aquifer in this region are also estimated to be from . x - to . x - day-.",,"Lin, Zhulu",JOURNAL OF THE AMERICAN WATER RESOURCES ASSOCIATION,hspf|karst hydrology|lake water budget|optimization|surface water|groundwater interaction,10.1111/j.1752-1688.2010.00513.x
165,WOS:000335783500001,2014,Verification of computational models of cardiac electro-physiology,BIDOMAIN EQUATIONS|EXCITABLE MEDIA|ELECTROPHYSIOLOGY|SIMULATIONS|CELLML|ADAPTIVITY|DYNAMICS|PROJECT|TISSUE|TOOLS,"For computational models of cardiac activity to be used in safety-critical clinical decision-making, thorough and rigorous testing of the accuracy of predictions is required. The field of 'verification, validation and uncertainty quantification' has been developed to evaluate the credibility of computational predictions. The first stage, verification, is the evaluation of how well computational software correctly solves the underlying mathematical equations. The aim of this paper is to introduce novel methods for verifying multi-cellular electro-physiological solvers, a crucial first stage for solvers to be used with confidence in clinical applications. We define D-D model problems with exact solutions for each of the monodomain, bidomain, and bidomain-with-perfusing-bath formulations of cardiac electro-physiology, which allow for the first time the testing of cardiac solvers against exact errors on fully coupled problems in all dimensions. These problems are carefully constructed so that they can be easily run using a general solver and can be used to greatly increase confidence that an implementation is correct, which we illustrate by testing one major solver, 'Chaste', on the problems. We then perform case studies on calculation verification (also known as solution verification) for two specific applications. We conclude by making several recommendations regarding verification in cardiac modelling."," Copyright (c) 2013 John Wiley & Sons, Ltd.","Pathmanathan, Pras|Gray, Richard A.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING,software|vvuq|cardiac modelling,10.1002/cnm.2615
166,WOS:000230023700002,2005,Applicability of neuro-fuzzy techniques in predicting ground-water vulnerability: a GIS-based sensitivity analysis,AQUIFER VULNERABILITY|HYDRAULIC-PROPERTIES|MODELS|WATER|CLASSIFICATION|NETWORKS|FLOW|MANAGEMENT|TRANSPORT|SELECTION,"Modeling groundwater vulnerability reliably and cost effectively for non-point source (NPS) pollution at a regional scale remains a major challenge. In recent years, Geographic Information Systems (GIS), neural networks and fuzzy logic techniques have been used in several hydrological studies. However, few of these research studies have undertaken an extensive sensitivity analysis. The overall objective of this research is to examine the sensitivity of neuro-fuzzy models used to predict groundwater vulnerability in a spatial context by integrating GIS and neuro-fuzzy techniques. The specific objectives are to assess the sensitivity of neuro-fuzzy models in a spatial domain using GIS by varying (i) shape of the fuzzy sets, (ii) number of fuzzy sets, and (iii) learning and validation parameters (including rule weights). The neuro-fuzzy models were developed using NEFCLASS-J software on a JAVA platform and were loosely integrated with a GIS. Four plausible parameters which are critical in transporting contaminants through the soil profile to the groundwater, included soil hydrologic group, depth of the soil profile, soil structure (pedality points) of the A horizon, and landuse. In order to validate the model predictions, coincidence reports were generated among model inputs, model predictions, and well/spring contamination data for NO-N. A total of  neuro-fuzzy models were developed for selected sub-basins of Illinois River Watershed, AR. The sensitivity analysis showed that neuro-fuzzy models were sensitive to the shape of the fuzzy sets, number of fuzzy sets, nature of the rule weights, and validation techniques used during the learning processes. Compared to bell-shaped and triangular-shaped membership functions, the neuro-fuzzy models with a trapezoidal membership function were the least sensitive to the various permutations and combinations of the learning and validation parameters. Over all, Models  and  showed relatively higher coincidence with well contamination data than other models. The strength of this method is that it offers a means of dealing with imprecise data, therefore, is a viable option for regional and continental scale environmental modeling where imprecise data prevail. The neuro-fuzzy models, however, should only be used as a tool within a broader framework of GIS, remote sensing and solute transport modeling to assess groundwater vulnerability along with functional, mechanistic and stochastic models.", (c) 2004 Elsevier B.V. All rights reserved.,"Dixon, B",JOURNAL OF HYDROLOGY,gis|gps|no3-n|modeling|fuzzy logic|neural networks,10.1016/j.jhydrol.2004.11.010
167,WOS:000394653000006,2017,Inverse heat transfer analysis of radiator central heating systems inside residential buildings using sensitivity analysis,THERMAL DESIGN|OPTIMIZATION|ENERGY|PARAMETERS|COEFFICIENTS|TEMPERATURE|SIMULATION|ALGORITHM,"In the present study, a novel inverse heat transfer methodology is presented for thermal energy analysis of residential buildings equipped with radiator Heating, Ventilation, Air Conditioning systems using a combination of numerical simulation and sensitivity analysis. With the use of numerical simulation, thermal analysis of building components is simulated in order to calculate temperature field in the computational domain. With inverse analysis technique, we perform the estimation of some thermal parameters with the use of Levenberg-Marquardt and conjugate gradient methods and reported the results with the use of sensitivity analysis inside the building. To confirm the accuracy of numerical simulation, the problem is also simulated with commercial software such as Carrier and Energy-Plus. Then, thermal parameter estimation results of inverse simulation are presented. The present study illustrates the sufficiency of inverse analysis in order to calculate building thermal design parameters with excellent accuracy and precision with the least possible financial design cost.",,"Moftakhari, Ardeshir|Aghanajafi, Cyrus|Ghazvin, Ardalan Moftakhari Chaei",INVERSE PROBLEMS IN SCIENCE AND ENGINEERING,inverse analysis|levenberg-marquardt method|conjugate gradient method|sensitivity analysis|building thermal design,10.1080/17415977.2016.1178258
168,WOS:000346630600011,2014,An Efficient and Very Accurate Method for Calculating Steady-State Sensitivities in Metabolic Reaction Systems,TRICARBOXYLIC-ACID CYCLE|POWER-LAW APPROXIMATION|DICTYOSTELIUM-DISCOIDEUM|MODEL,"Stability and sensitivity analyses of biological systems require the ad hoc writing of computer code, which is highly dependent on the particular model and burdensome for large systems. We propose a very accurate strategy to overcome this challenge. Its core concept is the conversion of the model into the format of biochemical systems theory (BST), which greatly facilitates the computation of sensitivities. First, the steady state of interest is determined by integrating the model equations toward the steady state and then using a Newton-Raphson method to fine-tune the result. The second step of conversion into the BST format requires several instances of numerical differentiation. The accuracy of this task is ensured by the use of a complex-variable Taylor scheme for all differentiation steps. The proposed strategy is implemented in a new software program, COSMOS, which automates the stability and sensitivity analysis of essentially arbitrary ODE models in a quick, yet highly accurate manner. The methods underlying the process are theoretically analyzed and illustrated with four representative examples: a simple metabolic reaction model; a model of aspartate-derived amino acid biosynthesis; a TCA-cycle model; and a modified TCA-cycle model. COSMOS has been deposited to https://github.com/BioprocessdesignLab/COSMOS.",,"Shiraishi, Fumihide|Yoshida, Erika|Voit, Eberhard O.",IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS,roots of nonlinear equations|algorithm design and analysis|biology and genetics|systems theory,10.1109/TCBB.2014.2338311
169,WOS:000251764200014,2007,Conservation planning in forest landscapes of Fennoscandia and an approach to the challenge of Countdown 2010,MODELING DEAD WOOD|BOREAL FOREST|BIODIVERSITY CONSERVATION|SUSTAINABLE FORESTRY|UNCERTAINTY ANALYSIS|RESERVE SELECTION|SOUTHERN FINLAND|NORWAY SPRUCE|MANAGEMENT|EUROPE,"Effective management of biodiversity in production landscapes requires a conservation approach that acknowledges the complexity of ecological and cultural systems in time and space. Fennoscandia has experienced major loss of forest biodiversity caused by intensive forestry. Therefore, the Countdown  initiative to halt the loss of biodiversity in Europe is highly relevant to forest management in this part of the continent. As a contribution to meeting the challenge posed by Countdown , we developed a spatially explicit conservation-planning exercise that used regional knowledge on forest biodiversity to provide support for managers attempting to halt further loss of biological diversity in the region. We used current data on the distribution of  species (including  red-listed species) representing different forest habitats and ecologies along with forest data within the frame of modern conservation software to devise a map of priority areas for conservation. The top % of priority areas contained over % of red-listed species locations and % of existing protected forest areas, but only % of these top priorities overlapped with core areas identified previously in a regional strategy that used more qualitative methods. We argue for aggregating present and future habitat value of single management units to landscape and regional scales to identify potential bottlenecks in habitat availability linked to landscape dynamics. To address the challenge of Countdown , a general framework for forest conservation planning in Fennoscandia needs to cover different conservation issues, tools, and data needs.",,"Mikusinski, G.|Pressey, R. L.|Edenius, L.|Kujala, H.|Moilanen, A.|Niemelae, J.|Ranius, T.",CONSERVATION BIOLOGY,boreal forests|reserve selection|spatial conservation planning|zonation software,10.1111/j.1523-1739.2007.00833.x
170,WOS:000333204200018,2014,Sensitivity coefficient-based uncertainty analysis for multi-functionality in LCA,LIFE-CYCLE ASSESSMENT|ALLOCATION CHOICES|CO-PRODUCT|SYSTEMS|SIMULATION|INVENTORY|SCENARIOS|CONSEQUENCES|ASSESSMENTS|EXAMPLES,"Purpose In LCA, a multi-functionality problem exists whenever the environmental impacts of a multi-functional process have to be allocated between its multiple functions. Methods for fixing this multi-functionality problem are controversially discussed because the methods include ambiguous choices. To study the influence of these choices, the ISO standard requires a sensitivity analysis. This work presents an analytical method for analyzing sensitivities and uncertainties of LCA results with respect to the choices made when a multi-functionality problem is fixed. Methods The existing matrix algebra for LCA is expanded by explicit equations for methods that fix multi-functionality problems: allocation and avoided burden. For allocation, choices exist between alternative allocation factors. The expanded equations allow calculating LCA results as a function of allocation factors. For avoided burden, choices exist in selecting an avoided burden process from multiple candidates. This choice is represented by so-called aggregation factors. For avoided burden, the expanded equations calculate LCA results as a function of aggregation factors. The expanded equations are used to derive sensitivity coefficients for LCA results with respect to allocation factors and aggregation factors. Based on the sensitivity coefficients, uncertainties due to fixing a multi-functionality problem by allocation or avoided burden are analytically propagated. The method is illustrated using a virtual numerical example. Results and discussion The presented approach rigorously quantifies sensitivities of LCA results with respect to the choices made when multi-functionality problems are fixed with allocation and avoided burden. The uncertainties due to fixing multi-functionality problems are analytically propagated to uncertainties in LCA results using a first-order approximation. For uncertainties in allocation factors, the first-order approximation is exact if no loops of the allocated functional flows exist. The contribution of uncertainties due to fixing multi-functionality problems can be directly compared to the uncertainty contributions induced by uncertain process data or characterization factors. The presented method allows the computationally efficient study of uncertainties due to fixing multi-functionality problems and could be automated in software tools. Conclusions This work provides a systematic method for the sensitivity analysis required by the ISO standard in case choices between alternative allocation procedures exist. The resulting analytical approach includes contributions of uncertainties in process data, characterization factors, and-in extension to existing methods-uncertainties due to fixing multi-functionality problems in a unifying rigorous framework. Based on the uncertainty contributions, LCA practitioners can select fields for data refinement to decrease the overall uncertainty in LCA results.",,"Jung, Johannes|von der Assen, Niklas|Bardow, Andre",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,"analytical uncertainty propagation|multi-functionality, allocation|sensitivity analysis|uncertainty analysis",10.1007/s11367-013-0655-4
171,WOS:000254595200018,2008,Waste management modeling with PC-based model - EASEWASTE,SYSTEMS,"As lite-cycle-thinking becomes more integrated into waste management, quantitative tools are needed for assessing waste management systems and technologies. This article presents a decision support model to deal with integrated solid waste management planning problems at a regional or national level. The model is called EASEWASTE (environmental assessment of solid waste systems and technologies). The model consists of a number of modules (submodels), each describing a process in a real waste management system, and these modules may combine to represent a complete waste management system in a scenario. EASEWASTE generates data on emissions (inventory), which are translated and aggregated into different environmental impact categories, e.g. the global warming, acidification, and toxicity. To facilitate a ""first level"" screening evaluation, default values for process parameters have been provided, wherever possible. The EASEWASTE model for life-cycle-assessment of waste management is described and applied to a case study for illustrative purposes. The case study involving hypothetical but realistic data demonstrates the functionality, usability, and flexibilities of the model. The design and implementation of the software successfully address the substantial challenges in integrating process modeling, life-cycle inventory (M), and impact assessment (LCIA) modeling, and optimization into an interactive decision support platform."," (c) 2008 American Institute of Chemical Engineers Environ Prog, 27: 133-142, 2008.","Bhander, Gurbakhash S.|Hauschild, Michael Z.|Christensen, Thomas H.",ENVIRONMENTAL PROGRESS,life cycle assessment|waste management|modeling|easewaste model|systems analysis|waste planning|environmental assessment|waste technologies|sensitivity analysis,10.1002/ep.10250
172,WOS:000308971400030,2012,An integrated assessment tool to define effective air quality policies at regional scale,POLLUTION|MODEL|PM10|STRATEGIES|VALIDATION|LONDON|EUROPE,"In this paper, the Integrated Assessment of air quality is dealt with at regional scale. First the paper describes the main challenges to tackle current air pollution control, including economic aspects. Then it proposes a novel approach to manage the problem, presenting its mathematical formalization and describing its practical implementation into the Regional Integrated Assessment Tool (RIAT). The main features of the software system are described and some preliminary results on a domain in Northern Italy are illustrated. The novel features in RIAT are then compared to the state-of-the-art in integrated assessment of air quality, for example the ability to handle nonlinearities (instead of the usual linear approach) and the multi-objective framework (alternative to cost-effectiveness and scenario analysis). Then the lessons learned during the RIAT implementation are discussed, focusing on the locality, flexibility and openness of the tool. Finally the areas for further development of air quality integrated assessment are highlighted, with a focus on sensitivity analysis, structural and non technical measures, and the application of parallel computing concepts. (C) ", Elsevier Ltd. All rights reserved.,"Carnevale, Claudio|Finzi, Giovanna|Pisoni, Enrico|Volta, Marialuisa|Guariso, Giorgio|Gianfreda, Roberta|Maffeis, Giuseppe|Thunis, Philippe|White, Les|Triacchini, Giuseppe",ENVIRONMENTAL MODELLING & SOFTWARE,integrated assessment modeling|model reduction|air quality modeling|multi-objective optimization|decision support,10.1016/j.envsoft.2012.07.004
173,WOS:000238297500003,2006,"Optimal crop planning and water resources allocation in a coastal groundwater basin, Orissa, India",LINEAR DECISION RULES|OPTIMIZATION MODEL|RIVER-BASIN|IRRIGATION|MANAGEMENT|OPERATION|DESIGN|PROJECT,"Intensive rice cultivation in a coastal groundwater basin in Balasore district of Orissa province (eastern India) during monsoon and winter seasons has resulted in extensive pumping of groundwater by a network of shallow, mini-deep and deep tubewells. Particularly, shallow tubewell owners using centrifugal pumps are unable to lift groundwater during winter seasons due to rapid drawdown of groundwater table below suction lift caused by mini-deep and deep tubewell owners. The seawater intrusion front is also progressing inland in an alarming rate. To develop a long-term sustainable land and water management strategies for the aforementioned issues in humid regions, the district administration realized the need for crop planning and water resources management policies in deterministic and stochastic regimes. As non-structural measure, the Deterministic linear programming (DLP) and chance-constrained linear programming (CCLP) models were developed to allocate available land and water resources optimally on seasonal basis so as to maximize the net annual return from the study area, considering net irrigation water requirement of crops as stochastic variable. These models were solved using the quantitative systems for business (QSB) software package. Sensitivity analysis of the models has been carried out by varying three ranges of cropping scenarios (,  and % deviation from the existing cropping pattern) and combinations of surface water and groundwater at various risk levels (, ,  and %). The total groundwater available and withdrawal in the region are . x () and . x () km(), respectively. The study reveals that % deviation of the existing cropping pattern is the optimal that satisfies the minimum food requirement and maintain geo-hydrological balance of the basin. The sensitivity analysis of conjunctive use of surface water and groundwater shows that % surface water and % groundwater availability as the optimum water allocation level. The proposed cropping and water resources allocation policies of the developed models were found to be socio-economically acceptable that maintained the balance of the entire system, considering all the constraints and restrictions imposed.", (c) 2005 Elsevier B.V. All rights reserved.,"Sethi, LN|Panda, SN|Nayak, MK",AGRICULTURAL WATER MANAGEMENT,coastal groundwater basin|conjunctive use|optimal cropping|sensitivity analysis|uncertainty|water resources allocation,10.1016/j.agwat.2005.11.009
174,WOS:000087885500003,2000,Buckling design optimization of complex built-up structures with shape and size variables,SENSITIVITY ANALYSIS,"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper.",,"Gu, YX|Zhao, GZ|Zhang, HW|Kang, Z|Grandhi, RV",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,shape optimization|size optimization|buckling|natural frequency|dynamic response|built-up structures,10.1007/s001580050101
175,WOS:000346541600018,2014,Synthesis of 4-Chloro-3-nitrobenzotrifluoride: Industrial thermal runaway simulation due to cooling system failure,LIQUID-LIQUID REACTIONS|SENSITIVITY-ANALYSIS|DYNAMIC SIMULATION|SEMIBATCH REACTOR|CHEMICAL REACTORS|SAFE OPERATION|POLYMERIZATION|CRITERIA|ACID|NITRATION,"In pharmaceutical and fine chemical industries, fast and strongly exothermic reactions are often carried out in semibatch reactors (SBRs) to better control the heat evolution by the feeding rate. In fact, for such processes, a thermal runaway event may be triggered whenever the rate of heat removal becomes lower than the rate of heat production. Such a dangerous phenomenon consists in an uncontrolled reactor temperature increase that, occurring in practically adiabatic conditions, can trigger secondary undesired exothermic reactions or worse, decompositions of the whole reacting mixture with consequent reactor pressurization due to uncontrollable gases formation. In this work, dedicated software has been developed and used to simulate a cooling system breakdown in an industrial SBR where the nitration of -Chlorobenzotrifluoride is carried out. The mathematical model is able to simulate both reactor temperature and pressure vs. time profiles thanks to a complete description of both the desired reaction and the unwanted reacting mixture decomposition kinetics. Different accidental scenarios have been simulated, showing both the wide different consequences that can arise from the same initiating event and, therefore, the usefulness of a complete simulation of the hypothesized accidental scenario in the frame of a Quantitative Risk Analysis. (C)  The Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Copelli, Sabrina|Derudi, Marco|Cattaneo, Carlo Sala|Nano, Giuseppe|Raboni, Massimo|Torretta, Vincenzo|Rota, Renato",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,thermal runaway|cooling system failure|aromatic nitrations|4-chlorobenzotrifluoride|semibatch reactor|decomposition kinetics,10.1016/j.psep.2013.11.006
176,WOS:000355760000014,2015,Optimization of the motion control mechanism of the hatch door of airliner,FLEXIBLE MULTIBODY SYSTEMS|DESIGN SENSITIVITY-ANALYSIS|FINITE SEGMENT APPROACH|DYNAMIC-SYSTEMS|RECURSIVE FORMULATION|PARALLEL MANIPULATORS|LOOP SYSTEMS|EQUATIONS|METHODOLOGY|ALGORITHM,"This paper deals with the problem of parameter optimization of the motion control mechanism of the hatch door of ARJ-, a regional airliner of China. Motion improvement of the hatch door is implemented by two kinds of passive designs. Firstly, a single-layer optimization model for trajectory modification is developed to find the optimum size of the key parts of the control mechanism. Secondly, a novel nested bi-level optimization model is presented for the design of the size tolerance limits of the selected parts. The design objective is minimization of the total extremum deviation of the motion trajectory of the objective point of the hatch door, where the extremum deviation is obtained by solution of the inner-level size optimization problem for the fixed size tolerance limits. The optimization models for motion control of the hatch door mechanism are solved using the response surface method. Numerical examples show that the precision of the real running trajectory of the objective point of the hatch door mechanism may be improved effectively by using the methods presented. A home-made multi-body dynamics solver (THUSOLVER) and the corresponding optimization software have been developed to implement the above tasks.",,"Du, Jianbin|Huang, Zhenting|Yang, Ruizhen",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,bi-level optimization model|motion control mechanism|tolerance design|response surface method|size optimization,10.1007/s00158-014-1191-y
177,WOS:000352040700012,2015,A comparative life cycle assessment of conventional hand dryer and roll paper towel as hand drying methods,UNCERTAINTY|HYGIENE,"A comparative life cycle assessment, under a cradle to gate scope, was carried out between two hand drying methods namely conventional hand dryer use and dispenser issued roll paper towel use. The inventory analysis for this study was aided by the deconstruction of a hand dryer and dispenser unit besides additional data provided by the Physical Resources department, from the product system manufacturers and information from literature. The LCA software SimaPro, supported by the ecoinvent and US-EI databases, was used towards establishing the environmental impacts associated with the lifecycle stages of both the compared product systems. The Impact + method was used for classification and characterization of these environmental impacts. An uncertainty analysis addressing key input data and assumptions made, a sensitivity analysis covering the use intensity of the product systems and a scenario analysis looking at a US based use phase for the hand dryer were also conducted. Per functional unit, which is to achieve a pair of dried hands, the dispenser product system has a greater life cycle impact than the dryer product system across three of four endpoint impact categories. The use group of lifecycle stages for the dispenser product system, which represents the cradle to gate lifecycle stages associated with the paper towels, constitutes the major portion of this impact. For the dryer product system, the use group of lifecycle stages, which essentially covers the electricity consumption during dryer operation, constitutes the major stake in the impact categories. It is evident from the results of this study that per dry, for a use phase supplied by Ontario's grid ( grid mix scenario) and a United States based manufacturing scenario, the use of a conventional hand dryer (rated at  W and under a  s use intensity) has a lesser environmental impact than with using two paper towels (% recycled content, unbleached and weighing  g) issued from a roll dispenser.", (C) 2015 Elsevier B.V. All rights reserved.,"Joseph, Tijo|Baah, Kelly|Jahanfar, Ali|Dubey, Brajesh",SCIENCE OF THE TOTAL ENVIRONMENT,hand dryer|paper towel|life cycle assessment|impact 2002+|uncertainty analysis,10.1016/j.scitotenv.2015.01.112
178,WOS:000378451500006,2016,Stiffness design of heterogeneous periodic beam by topology optimization with integration of commercial software,HOMOGENIZATION METHOD|CROSS-SECTIONS|PERMEABILITY,"A topology optimization method is developed for microstructure design of heterogeneous periodic beam structure aiming at its extreme or specified effective stiffness. The effective stiffness is calculated using a FEM formulation of asymptotic homogenization method for heterogeneous periodic beam. Sensitivity of stiffness to the density design variable is derived analytically based on the solution of unit cell problems under corresponding generalized strain fields. Implementation of optimization procedure is generalized to take full advantage of commercial FEM software capabilities, with several examples presented to demonstrate its effectiveness. It is shown here the proposed method is extendible to periodic truss beam design. (C) ", Elsevier Ltd. All rights reserved.,"Yi, Sinan|Cheng, Gengdong|Xu, Liang",COMPUTERS & STRUCTURES,periodic beam|topology optimization|sensitivity analysis|homogenization method|commercial software,10.1016/j.compstruc.2016.05.012
179,WOS:000327903400013,2013,Addressing ten questions about conceptual rainfall-runoff models with global sensitivity analyses in R,MULTIOBJECTIVE CALIBRATION|ENVIRONMENTAL-MODEL|CATCHMENT MODEL|CLIMATE-CHANGE|UNCERTAINTY|PERFORMANCE|INDEXES|PARAMETERS|AUSTRALIA|HYDROLOGY,"Sensitivity analysis (SA) is generally recognized as a worthwhile step to diagnose and remedy difficulties in identifying model parameters, and indeed in discriminating between model structures. An analysis of papers in three journals indicates that SA is a standard omission in hydrological modeling exercises. We provide some answers to ten reasonably generic questions using the Morris and Sobol SA methods, including to what extent sensitivities are dependent on parameter ranges selected, length of data period, catchment response type, model structures assumed and climatic forcing. Results presented demonstrate the sensitivity of four target functions to parameter variations of four rainfall runoff models of varying complexity (- parameters). Daily rainfall, streamflow and pan evaporation data are used from four -year data sets and from five catchments in the Australian Capital Territory (ACT) region. Similar results are obtained using the Morris and Sobol methods. It is shown how modelers can easily identify parameters that are insensitive, and how they might improve identifiability. Using a more complex objective function, however, may not result in all parameters becoming sensitive. Crucially, the results of the SA can be influenced by the parameter ranges selected. The length of data period required to characterize the sensitivities assuredly is a minimum of five years. The results confirm that only the simpler models have well-identified parameters, but parameter sensitivities vary between catchments. Answering these ten questions in other case studies is relatively easy using freely available software with the Hydromad and Sensitivity packages in R.", (C) 2013 Elsevier B.V. All rights reserved.,"Shin, Mun-Ju|Guillaume, Joseph H. A.|Croke, Barry F. W.|Jakeman, Anthony J.",JOURNAL OF HYDROLOGY,sensitivity analysis|rainfall-runoff model|identifiability,10.1016/j.jhydrol.2013.08.047
180,WOS:000323644600031,2013,Nitrous Oxide Emissions from Cropland: a Procedure for Calibrating the DayCent Biogeochemical Model Using Inverse Modelling,CARBON-DIOXIDE|SOIL|N2O|DENITRIFICATION|SIMULATIONS|COLORADO|SYSTEMS|DNDC,"DayCent is a biogeochemical model of intermediate complexity widely used to simulate greenhouse gases (GHG), soil organic carbon and nutrients in crop, grassland, forest and savannah ecosystems. Although this model has been applied to a wide range of ecosystems, it is still typically parameterized through a traditional ""trial and error"" approach and has not been calibrated using statistical inverse modelling (i.e. algorithmic parameter estimation). The aim of this study is to establish and demonstrate a procedure for calibration of DayCent to improve estimation of GHG emissions. We coupled DayCent with the parameter estimation (PEST) software for inverse modelling. The PEST software can be used for calibration through regularized inversion as well as model sensitivity and uncertainty analysis. The DayCent model was analysed and calibrated using NO flux data collected over  years at the Iowa State University Agronomy and Agricultural Engineering Research Farms, Boone, IA. Crop year  data were used for model calibration and  data were used for validation. The optimization of DayCent model parameters using PEST significantly reduced model residuals relative to the default DayCent parameter values. Parameter estimation improved the model performance by reducing the sum of weighted squared residual difference between measured and modelled outputs by up to  %. For the calibration period, simulation with the default model parameter values underestimated mean daily NO flux by  %. After parameter estimation, the model underestimated the mean daily fluxes by  %. During the validation period, the calibrated model reduced sum of weighted squared residuals by  % relative to the default simulation. Sensitivity analysis performed provides important insights into the model structure providing guidance for model improvement.",,"Rafique, Rashad|Fienen, Michael N.|Parkin, Timothy B.|Anex, Robert P.",WATER AIR AND SOIL POLLUTION,daycent model|inverse modelling|parameter estimation (pest)|nitrous oxide|sensitivity analysis|automatic calibration|validation,10.1007/s11270-013-1677-z
181,WOS:000395602600042,2017,Greenhouse gas emissions from municipal solid waste with a high organic fraction under different management scenarios,LIFE-CYCLE ASSESSMENT|CHINA|INCINERATION|IMPACTS|BALANCE|SYSTEMS|TECHNOLOGY|EFFICIENCY|RECOVERY|INDUSTRY,"Waste management is a major source of greenhouse gas (GHG) emissions and provides an opportunity to reduce carbon emissions that has yet to be fully exploited. The total GHG emissions in the waste management sector in China is quite different from that in developed countries, mainly due to the high biodegradable fraction (almost -%) of municipal solid waste (MSW). Based on the above characteristics of MSW and its current and future management strategies, five typical scenarios were modeled by the EaseTech software to compare GHG emissions under different scenarios. These strategies were evaluated by life cycle assessment (LCA) and sensitivity analysis. The two scenarios of landfilling, either with LFG flaring or with LFG to energy, would cause very high GHG emissions (. kgCO()-Eq/t and . kgCO()-Eqit, respectively) due to the rapid degradation and low efficiency of landfill gas (LFG) collection. Incineration with energy recovery would lower total GHG emissions (-. kgCO()-Eq/t) substantially. However, the auxiliary fuel needed would offset its environmental benefit. Two scenarios, anaerobic digestion (AD) of the source-separated organic fractions and residue landfilling, and AD of the source-separated organic fractions and incineration of the fractions with high calorific value, followed by residue landfilling, would result in significant carbon sinks (-. kgCO()-Eqit and . kgCO()-Eqit, respectively). From the perspective of GHG emissions reduction, the optimum technical route of MSW in China would be AD of the source-separated organic fractions, incineration of the fractions with high calorific value, followed by residue landfilling. (C) ", Elsevier Ltd. All rights reserved.,"Liu, Yili|Ni, Zhe|Kong, Xin|Liu, Jianguo",JOURNAL OF CLEANER PRODUCTION,greenhouse gas (ghg)|municipal solid waste (msw)|life cycle assessment (lca)|sensitivity analysis,10.1016/j.jclepro.2017.01.135
182,WOS:000209099900002,2011,ASSESSMENT OF COLLOCATION AND GALERKIN APPROACHES TO LINEAR DIFFUSION EQUATIONS WITH RANDOM DATA,,"We compare the performance of two methods, the stochastic Galerkin method and the stochastic collocation method, for solving partial differential equations (PDEs) with random data. The stochastic Galerkin method requires the solution of a single linear system that is several orders larger than linear systems associated with deterministic PDEs. The stochastic collocation method requires many solves of deterministic PDEs, which allows the use of existing software. However, the total number of degrees of freedom in the stochastic collocation method can be considerably larger than the number of degrees of freedom in the stochastic Galerkin system. We implement both methods using the Trilinos software package and we assess their cost and performance. The implementations in Trilinos are known to be efficient, which allows for a realistic assessment of the computational complexity of the methods. We also develop a cost model for both methods which allows us to examine asymptotic behavior.",,"Elman, Howard C.|Miller, Christopher W.|Phipps, Eric T.|Tuminaro, Raymond S.",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,uncertainty quantification|stochastic partial differential equations|polynomial chaos|stochastic galerkin method|stochastic sparse grid collocation|karhunen-loeve expansion,10.1615/Int.J.UncertaintyQuantification.v1.i1.20
183,WOS:000306623200008,2012,Sensitivity analysis of some critical factors affecting simulated intrusion volumes during a low pressure transient event in a full-scale water distribution system,HYDRAULIC TRANSIENTS|LEAKAGE|PIPES,"Intrusion events caused by transient low pressures may result in the contamination of a water distribution system (DS). This work aims at estimating the range of potential intrusion volumes that could result from a real downsurge event caused by a momentary pump shutdown. A model calibrated with transient low pressure recordings was used to simulate total intrusion volumes through leakage orifices and submerged air vacuum valves (AVVs). Four critical factors influencing intrusion volumes were varied: the external head of (untreated) water on leakage orifices, the external head of (untreated) water on submerged air vacuum valves, the leakage rate, and the diameter of AVVs' outlet orifice (represented by a multiplicative factor). Leakage orifices' head and AVVs' orifice head levels were assessed through fieldwork. Two sets of runs were generated as part of two statistically designed experiments. A first set of  runs was based on a complete factorial design in which each factor was varied over  levels. A second set of  runs was based on a latin hypercube design, better suited for experimental runs on a computer model. The simulations were conducted using commercially available transient analysis software. Responses, measured by total intrusion volumes, ranged from  to  L. A second degree polynomial was used to analyze the total intrusion volumes. Sensitivity analyses of both designs revealed that the relationship between the total intrusion volume and the four contributing factors is not monotonic, with the AVVs' orifice head being the most influential factor. When intrusion through both pathways occurs concurrently, interactions between the intrusion flows through leakage orifices and submerged AVVs influence intrusion volumes. When only intrusion through leakage orifices is considered, the total intrusion volume is more largely influenced by the leakage rate than by the leakage orifices' head. The latter mainly impacts the extent of the area affected by intrusion. (C) ", Elsevier Ltd. All rights reserved.,"Ebacher, G.|Besner, M. C.|Clement, B.|Prevost, M.",WATER RESEARCH,water distribution system|intrusion volumes|sensitivity analysis|transient analysis|downsurge event,10.1016/j.watres.2012.05.006
184,WOS:000380760400073,2016,Life Cycle Assessment and Life Cycle Cost Analysis of Magnesia Spinel Brick Production,CEMENT|INDUSTRY|ENERGY|LCA,"Sustainable use of natural resources in the production of construction materials has become a necessity both in Europe and Turkey. Construction products in Europe should have European Conformity (CE) and Environmental Product Declaration (EPD), an independently verified and registered document in line with the European standard EN . An EPD certificate can be created by performing a Life Cycle Assessment (LCA) study. In this particular work, an LCA study was carried out for a refractory brick production for environmental assessment. In addition to the LCA, the Life Cycle Cost (LCC) analysis was also applied for economic assessment. Firstly, a cradle-to-gate LCA was performed for one ton of magnesia spinel refractory brick. The CML IA method included in the licensed SimaPro .. software was chosen to calculate impact categories (namely, abiotic depletion, global warming potential, acidification potential, eutrophication potential, human toxicity, ecotoxicity, ozone depletion potential, and photochemical oxidation potential). The LCC analysis was performed by developing a cost model for internal and external cost categories within the software. The results were supported by a sensitivity analysis. According to the results, the production of raw materials and the firing process in the magnesia spinel brick production were found to have several negative effects on the environment and were costly.",,"Ozkan, Aysun|Gunkaya, Zerrin|Tok, Gulden|Karacasulu, Levent|Metesoy, Melike|Banar, Mufide|Kara, Alpagut",SUSTAINABILITY,cml method|firing process|global warming potential|life cycle assessment (lca)|life cycle cost (lcc)|magnesia spinel brick|refractory production,10.3390/su8070662
185,WOS:000330079500009,2014,Exploring incomplete information in maintenance materials inventory optimization,SPARE PARTS|INTERMITTENT DEMAND|STOCK CONTROL|CONTROL PERFORMANCE|REORDER POINT|PARAMETERS|SYSTEM|MODEL,"Purpose Ensuring the sufficient service level is essential for critical materials in industrial maintenance. This study aims to evaluate the use of statistically imperfect data in a stochastic simulation-based inventory optimization where items' failure characteristics are derived from historical consumption data, which represents a real-life situation in the implementation of such an optimization model. Design/methodology/approach - The risks of undesired shortages were evaluated through a service-level sensitivity analysis. The service levels were simulated within the error of margin of the key input variables by using StockOptim optimization software and real data from a Finnish steel mill. A random sample of  inventory items was selected. Findings - Service-level sensitivity is item specific, but, for many items, statistical imprecision in the input data causes significant uncertainty in the service level. On the other hand, some items seem to be more resistant to variations in the input data than others. Research limitations/implications - The case approach, with one simulation model, limits the generalization of the results. The possibility that the simulation model is not totally realistic exists, due to the model's normality assumptions. Practical implications - Margin of error in input data estimation causes a significant risk of not achieving the required service level. It is proposed that managers work to improve the preciseness of the data, while the sensitivity analysis against statistical uncertainty, and a correction mechanism if necessary, should be integrated into optimization models. Originality/value - The output limitations in the optimization, i.e. service level, are typically stated precisely, but the capabilities of the input data have not been addressed adequately. This study provides valuable insights into ensuring the availability of-critical materials.",,"Puurunen, Antti|Majava, Jukka|Kess, Pekka",INDUSTRIAL MANAGEMENT & DATA SYSTEMS,inventory optimization|maintenance materials|service-level sensitivity|spare parts|stockoptim,10.1108/IMDS-01-2013-0025
186,WOS:000278195100021,2010,The IAEA DEEP desalination economic model: A critical review,COSTS|POWER,"The IAEA DEEP software has been used worldwide for the economic evaluation of desalination plants (thermal or electrical) coupled with various energy sources (nuclear, fossil fueled or renewable). Throughout the years, the software was updated constantly. Such updates included the user interface and model structure but not the economic models. Previous continuous development was culminated in the development of the DEEP . version which has been recently released in . This paper presents a step forwards in the continuous effort to maintain high standards and reliability of DEEP. It also scrutinizes methods used, assumptions made, and constants or default values originally used. The validity of calculations as well as the identification of the most important parameters is presented. Sensitivity analysis is used to identify the most important parameters in the DEEP model. Overall, the review proves that both the DEEP economic model and software implementation are solid for economic evaluation of dual purpose plants. Based on results presented and recommendations made, a new version of DEEP is expected to be released in  which will address minor issues and improvements.", (C) 2010 Elsevier B.V. All rights reserved.,"Kavvadias, K. C.|Khamis, I.",DESALINATION,nuclear seawater desalination|cogeneration|desalination economic evaluation program (deep)|sensitivity analysis,10.1016/j.desal.2010.02.032
187,WOS:000353491500001,2015,Integrated Location-Production-Distribution Planning in a Multiproducts Supply Chain Network Design Model,GENETIC ALGORITHM APPROACH|SENSITIVITY-ANALYSIS|FACILITY LOCATION|PLANT LOCATION|OPTIMIZATION|MANAGEMENT|SYSTEM,"This paper proposes integrated location, production, and distribution planning for the supply chain network design which focuses on selecting the appropriate locations to build a new plant and distribution center while deciding the production and distribution of the product. We examine a multiechelon supply chain that includes suppliers, plants, and distribution centers and develop a mathematical model that aims at minimizing the total cost of the supply chain. In particular, the mathematical model considers the decision of how many plants and distribution centers to open and where to open them, as well as the allocation in each echelon. The LINGO software is used to solve the model for some problem cases. The study conducts various numerical experiments to illustrate the applicability of the developed model. Results show that, in small and medium size of problem, the optimal solution can be found using this solver. Sensitivity analysis is also conducted and shows that customer demand parameter has the greatest impact on the optimal solution.",,"Yu, Vincent F.|Normasari, Nur Mayke Eka|Huynh Trung Luong, ",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2015/473172
188,WOS:000253333700007,2008,Hardware and software efficacy in assessment of fine root diameter distributions,LENGTH|MORPHOLOGY|IMAGES,"Fine roots constitute the majority of root system surface area and thus most of the nutrient and water absorption surface. Fine roots are, however, the least understood of all plant roots. A sensitivity analysis of several software programs capable of providing root diameter distribution analyses was undertaken to determine if this software was capable of discriminating % changes in diameters of roots in the .-. mm diameter range. Digital images produced by drawing discrete lines, by scanning wires of various diameters, and by scanning roots from several legume species were analyzed and compared. None of the three packages were able to adequately analyze these images. Each introduced artifacts into the data that were severe enough to confound interpretation of the resulting diameter class length histograms at resolutions from  to  pixels (px) mm(-), and root diameters from . to .  mm or larger. One package was, however, clearly superior to the other two for routine digital analysis. All three packages require additional development before they are suitable for routine analysis of fine roots. Due to the  px mm- resolution ceiling with currently available scanners, the smallest roots for which this level of discrimination is possible is .mm diameter. For many agricultural and forest species, up to % of their total root length is less than . mm in diameter. It is concluded that both hardware and software constraints currently inhibit the sensitivity of investigations into fine root diameter shifts in response to environmental conditions.", Published by Elsevier B.V.,"Zobel, Richard W.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,fine roots|high resolution|scanner|digital image analysis|diameter distribution|root length,10.1016/j.compag.2007.08.002
189,WOS:000339531800200,2014,The sensitivity analysis of CO2 recovery & liquefaction flowsheet in the beer fermentation process by Aspen Plus software,,"Aspen Plus software is adopted to simulate the CO recovery & liquefaction flowsheet in the beer fermentation process. According to the simulation results, the parameters of scrubber, CO compressor and CO condenser are analyzed. The sensitivity analysis is also made for the scrubber water consumption, CO compressor pressure and flash temperature of CO condenser.",,"Yu, Kezhi|Yu, Yuan|Zhao, Haipeng|Xu, Q|Li, Y|Yang, ","ENERGY DEVELOPMENT, PTS 1-4",aspen plus|co2 recovery|flowsheet simulation|sensitivity analysis,10.4028/www.scientific.net/AMR.860-863.1049
190,WOS:000299955600008,2012,Preliminary flood risk assessment: the case of Athens,MULTICRITERIA EVALUATION|DECISION-MAKING|URBAN AREAS|MANAGEMENT|SCENARIOS|SYSTEMS|GIS,"Flood mapping, especially in urban areas, is a demanding task requiring substantial (and usually unavailable) data. However, with the recent introduction of the EU Floods Directive (//EC), the need for reliable, but cost effective, risk mapping at the regional scale is rising in the policy agenda. Methods are therefore required to allow for efficiently undertaking what the Directive terms ""preliminary flood risk assessment,"" in other words a screening of areas that could potentially be at risk of flooding and that consequently merit more detailed attention and analysis. Such methods cannot rely on modeling, as this would require more data and effort that is reasonable for this high-level, screening phase. This is especially true in urban areas, where modeling requires knowledge of the detailed urban terrain, the drainage networks, and their interactions. A GIS-based multicriteria flood risk assessment methodology was therefore developed and applied for the mapping of flood risk in urban areas. This approach quantifies the spatial distribution of flood risk and is able to deal with uncertainties in criteria values and to examine their influence on the overall flood risk assessment. It can further assess the spatially variable reliability of the resulting maps on the basis of the choice of method used to develop the maps. The approach is applied to the Greater Athens area and validated for its central and most urban part. A GIS database of economic, social, and environmental criteria contributing to flood risk was created. Three different multicriteria decision rules (Analytical Hierarchy Process, Weighted Linear Combination and Ordered Weighting Averaging) were applied, to produce the overall flood risk map of the area. To implement this methodology, the IDRISI Andes GIS software was customized and used. It is concluded that the results of the analysis are a reasonable representation of actual flood risk, on the basis of their comparison with historical flood events.",,"Kandilioti, Georgia|Makropoulos, Christos",NATURAL HAZARDS,floods|gis|multicriteria evaluation (mce)|sensitivity analysis|uncertainty|vulnerability,10.1007/s11069-011-9930-5
191,WOS:000325218500007,2013,Uncertainty in floodplain delineation: expression of flood hazard and risk in a Gulf Coast watershed,,"This paper investigates the development of flood hazard and flood risk delineations that account for uncertainty as improvements to standard floodplain maps for coastal watersheds. Current regulatory floodplain maps for the Gulf Coastal United States present % flood hazards as polygon features developed using deterministic, steady-state models that do not consider data uncertainty or natural variability of input parameters. Using the techniques presented here, a standard binary deterministic floodplain delineation is replaced with a flood inundation map showing the underlying flood hazard structure. Additionally, the hazard uncertainty is further transformed to show flood risk as a spatially distributed probable flood depth using concepts familiar to practicing engineers and software tools accepted and understood by regulators. A case study of the proposed hazard and risk assessment methodology is presented for a Gulf Coast watershed, which suggests that storm duration and stage boundary conditions are important variable parameters, whereas rainfall distribution, storm movement, and roughness coefficients contribute less variability. The floodplain with uncertainty for this coastal watershed showed the highest variability in the tidally influenced reaches and showed little variability in the inland riverine reaches. Additionally, comparison of flood hazard maps to flood risk maps shows that they are not directly correlated, as areas of high hazard do not always represent high risk."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Christian, Jason|Duenas-Osorio, Leonardo|Teague, Aarin|Fang, Zheng|Bedient, Philip",HYDROLOGICAL PROCESSES,flood hazard|flood risk|floodplain mapping|uncertainty analysis|coastal watersheds|storm surge,10.1002/hyp.9360
192,WOS:000402819500015,2017,Investigation on the effect of geometrical and geotechnical parameters on elongated offshore piles using fuzzy inference systems,PLATFORMS|BEHAVIOR,"Among numerous offshore structures used in oil extraction, jacket platforms are still the most favorable ones in shallow waters. In such structures, log piles are used to pin the substructure of the platform to the seabed. The pile's geometrical and geotechnical properties are considered as the main parameters in designing these structures. In this study, ANSYS was used as the FE modeling software to study the geometrical and geotechnical properties of the offshore piles and their effects on supporting jacket platforms. For this purpose, the FE analysis has been done to provide the preliminary data for the fuzzy-logic post-process. The resulting data were implemented to create Fuzzy Inference System (FIS) classifications. The resultant data of the sensitivity analysis suggested that the orientation degree is the main factor in the pile's geometrical behavior because piles which had the optimal operational degree of about A degrees are more sustained. Finally, the results showed that the related fuzzified data supported the FE model and provided an insight for extended offshore pile designs.",,"Aminfar, Ali|Mojtahedi, Alireza|Ahmadi, Hamid|Aminfar, Mohammad Hossain",CHINA OCEAN ENGINEERING,pile|soil|fem|offshore jacket platform|pile-soil interaction|fuzzy-logic|fuzzification,10.1007/s13344-017-0044-z
193,WOS:000274331300009,2010,Generalized Wishart distribution for probabilistic structural dynamics,REDUCED BASIS METHODS|MODAL-ANALYSIS|EXPERIMENTAL VALIDATION|COMPUTATIONAL MODELS|RANDOM UNCERTAINTIES|PARAMETERED BEAMS|SYSTEMS|MECHANICS,"An accurate and efficient uncertainty quantification of the dynamic response of complex structural systems is crucial for their design and analysis. Among the many approaches proposed, the random matrix approach has received significant attention over the past decade. In this paper two new random matrix models, namely () generalized scalar Wishart distribution and () generalized diagonal Wishart distribution have been proposed. The central aims behind the proposition of the new models are to () improve the accuracy of the statistical predictions, () simplify the analytical formulations and () improve computational efficiency. Identification of the parameters of the newly proposed random matrix models has been discussed. Closed-form expressions have been derived using rigorous analytical approaches. It is considered that the dynamical system is proportionally damped and the mass and stiffness properties of the system are random. The newly proposed approaches are compared with the existing Wishart random matrix model using numerical case studies. Results from the random matrix approaches have been validated using an experiment on a vibrating plate with randomly attached spring-mass oscillators. One hundred nominally identical samples have been created and separately tested within a laboratory framework. Relative merits and demerits of different random matrix formulations are discussed and based on the numerical and experimental studies the recommendation for the best model has been given. A simple step-by-step method for implementing the new computational approach in conjunction with general purpose finite element software has been outlined.",,"Adhikari, Sondipon",COMPUTATIONAL MECHANICS,unified uncertainty quantification|random matrix theory|wishart distribution|model validation|parameter identification,10.1007/s00466-010-0467-3
194,WOS:000266261300006,2009,Traffic Parameters Estimation to Predict Road Side Pollutant Concentrations using Neural Networks,NO2 CONCENTRATIONS|MODELS|STREET|AIR,"The analysis aims to evaluate which is the most important among traffic parameters (flows, queues length, occupancy degree, and travel time) to forecast CO and CH concentrations. The study area was identified by Notarbartolo Road and bounded by LibertA Street and Sciuti Street in the urban area of Palermo in Southern Italy. In this area, various loop detectors and one pollution-monitoring site were located. Traffic data related to the pollution-monitoring site immediately near the road link were estimated by Simulation of Urban MObility (SUMO) traffic microsimulator software using as input the flows measured by loop detectors on other links of road network. Traffic and weather data were used as input variables to predict pollutant concentrations by using neural networks. Finally, after a sensitivity analysis, it was showed that queues length were the mostly correlated traffic parameters to pollutant concentrations.",,"Galatioto, Fabio|Zito, Pietro",ENVIRONMENTAL MODELING & ASSESSMENT,microsimulator|traffic parameters|neural networks|pollutant concentrations,10.1007/s10666-007-9129-z
195,WOS:000309373900367,2012,The Sensitivity Analysis of Stability Factors of Rocky Slope with Ecological Restoration,,"Based on orthogonal experiment, the thickness of protection habitat base material (PHBM), the cohesion of PHBM, the internal friction angle of PHBM, the slope height, the slope angle and the density of PHBM are selected as influencing factors of stability of rocky slope with ecological restoration (RSER) and each factor is considered three levels. Then, using strength reduction FEM by the software ANSYS, the safety factor of RSER in various combinations is analyzed. Finally, range analysis and comprehensive assessment are applied to the assessment of results. It is showed that the cohesion of PHBM is the dominant sensitivity factor which affects the stability of RSER, flowing by the slope height, the slope angle, the thickness of PHBM, the internal friction angle of PHBM, the density of PHBM.",,"Cai, Xianyang|Xu, Wennian|Xia, Zhenyao|Zhou, Zhengjun|Xu, QJ|Ge, HH|Zhang, JX","NATURAL RESOURCES AND SUSTAINABLE DEVELOPMENT, PTS 1-3",orthogonal experiment|rocky slope with ecological restoration|strength reduction fem|range analysis,10.4028/www.scientific.net/AMR.361-363.2003
196,WOS:000283244900004,2010,Comparison of different treatment options for palm oil production waste on a life cycle basis,BIOFUELS|MALAYSIA|ENERGY,"Globally,  million metric tonnes of palm oil has been produced in . The production of  t crude palm oil requires  t of fresh fruit bunches (FFB). On average, processing of  t FFB in palm oil mills generates  kg empty fruit bunches (EFB) and  kg palm oil mill effluent (POME) as residues. These residues cause considerable environmental burdens, particularly greenhouse gas emissions. In order to reduce those emissions, four waste management options are compared in the present study using , kg of FFB as functional unit. A detailed life cycle model has been used to calculate the environmental impacts of POME and EFB treatment. The options under investigation are: () dumping EFB and storing POME and ponds, () returning EFB to the plantation and POME as before, () using EFB and POME for co-composting and returning the produced compost to the plantation, () generating biogas from POME and thereafter as in (). The CML  method included in the GABI . software package has been used for the impact calculations. Sensitivity analysis has been carried out in order to estimate the influence of good and poor management practice on the environmental performance. The main contributor to the GWP is methane from POME and EFB dumping. The GWP of palm oil mill waste treatment can be reduced from  kg CO(eq) per ton FFB to up to  kg CO(eq) per ton FFB due to reduced methane emissions and nutrient recycling. Co-composting of POME and EFB leads to considerable nutrient recovery, in addition to GWP reduction. Thus, the composting process reduces not only environmental burdens; it also leads to net environmental benefit regarding most environmental impact categories, e.g., acidification potential, eutrophication potential, ozone layer depletion potential, etc. due to the avoided emissions from inorganic fertilizer production. The recovery of nutrients in EFB can be achieved by solely returning it to the plantation, but only the combined treatment of EFB and POME allows nutrient recovery from POME while methane emissions from pond systems are avoided simultaneously. The fermentation of POME to produce biogas reduces environmental burdens when operating under best practice conditions. However, fugitive biogas emissions of more than % reverse that beneficial effect. A life-cycle-based comparison of conventional and advanced treatment systems for EFB and POME can support decision makers regarding waste treatment options and provide information on technology risks involved. The results of this study may be used as basic calculation data for clean development mechanism for palm oil mills. LCA is shown to be a powerful tool to estimate and compare environmental impacts of different options. Unfortunately, it is rarely used in the palm oil industry in order to improve or optimize palm oil production systems. This study has shown that nutrient recovery from POME and EFB offers considerable environmental and economic benefits to palm oil production systems. However, using EFB for energy production, as it is discussed and realized by some palm oil mills, prohibits environmental beneficial POME utilization. Best waste management practice reduces emissions at palm oil mills and consequently the carbon footprint of palm oil products. Co-composting of EFB and POME, with or without prefermentation of POME in a biogas plant, is a profitable way to use the nutrients from both POME and EFB.",,"Stichnothe, Heinz|Schuchardt, Frank",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,compost|empty fruit bunches (efb)|greenhouse gas (ghg)|life cycle assessment (lca)|methane|nutrient recycling|palm oil mill effluent (pome),10.1007/s11367-010-0223-0
197,WOS:000406135200014,2017,Evaluating the SWMM LID Editor rain barrel option for the estimation of retention potential of rainwater harvesting systems,WATER|RUNOFF,"The low-impact development (LID) Editor rain barrel option of release . of the EPA storm water management model (SWMM) software does not allow the consideration of demand-driven behaviour of domestic rain water harvesting (RWH) systems to evaluate their runoff retention potential. This paper compares the results of the LID Editor with those obtained by a detailed demand-driven tank model scheme - used as a benchmark - and developed using basic functions of SWMM. The comparison showed the LID Editor-based model to generally overestimate the benchmark model in the evaluation of both volumetric and peak retention efficiency. The high variability of the results of the comparison suggests the use of the LID Editor rain barrel option for long-term simulation but not for single event analysis. A sensitivity analysis revealed that the overestimation provided by the rain barrel option is significant for tanks smaller than m(), tank sizes of major diffusion for domestic RWH.",,"Campisano, Alberto|Catania, Francesco Valerio|Modica, Carlo",URBAN WATER JOURNAL,low-impact development|rain water harvesting|swmm|lid editor|retention efficiency,10.1080/1573062X.2016.1254259
198,WOS:000301013200010,2012,"Automating calibration, sensitivity and uncertainty analysis of complex models using the R package Flexible Modeling Environment (FME): SWAT as an example",WATER ASSESSMENT-TOOL|RAINFALL-RUNOFF MODELS|HYDROLOGIC-MODELS|PARAMETER UNCERTAINTY|GLOBAL OPTIMIZATION|BAYESIAN-APPROACH|CATCHMENT MODELS|RIVER-BASIN|SOIL|PREDICTION,"Parameter optimization and uncertainty issues are a great challenge for the application of large environmental models like the Soil and Water Assessment Tool (SWAT), which is a physically-based hydrological model for simulating water and nutrient cycles at the watershed scale. In this study, we present a comprehensive modeling environment for SWAT, including automated calibration, and sensitivity and uncertainty analysis capabilities through integration with the R package Flexible Modeling Environment (FME). To address challenges (e.g., calling the model in R and transferring variables between Fortran and R) in developing such a two-language coupling framework, ) we converted the Fortran-based SWAT model to an R function (R-SWAT) using the RFortran platform, and alternatively ) we compiled SWAT as a Dynamic Link Library (DLL). We then wrapped SWAT (via R-SWAT) with FME to perform complex applications including parameter identifiability, inverse modeling, and sensitivity and uncertainty analysis in the R environment. The final R-SWAT-FME framework has the following key functionalities: automatic initialization of R, running Fortran-based SWAT and R commands in parallel, transferring parameters and model output between SWAT and R, and inverse modeling with visualization. To examine this framework and demonstrate how it works, a case study simulating streamflow in the Cedar River Basin in Iowa in the United Sates was used, and we compared it with the built-in auto-calibration tool of SWAT in parameter optimization. Results indicate that both methods performed well and similarly in searching a set of optimal parameters. Nonetheless, the R-SWAT-FME is more attractive due to its instant visualization, and potential to take advantage of other R packages (e.g., inverse modeling and statistical graphics). The methods presented in the paper are readily adaptable to other model applications that require capability for automated calibration, and sensitivity and uncertainty analysis.", Published by Elsevier Ltd.,"Wu, Yiping|Liu, Shuguang",ENVIRONMENTAL MODELLING & SOFTWARE,calibration|fme|monte carlo|r|sensitivity and uncertainty analysis|swat,10.1016/j.envsoft.2011.11.013
199,WOS:000275621700002,2010,"Controlling setup cost in (Q, r, L) inventory model with defective items",LEAD TIME|REDUCTION,"This study discusses a mixture inventory model with back orders and lost sales in which the order quantity, reorder point, lead time and setup cost are decision variables. It is assumed that an arrival order lot may contain some defective items and the number of defective items is a random variable. There are two inventory models proposed in this paper, one with normally distributed demand and another with distribution free demand. Finally we develop two computational algorithms to obtain the optimal ordering policy, A computer code using the software Matlab is developed to derive the optimal solution and present numerical examples to illustrate the models. Additionally, sensitivity analysis is conducted with respect to the various system parameters.", (C) 2009 Elsevier Inc. All rights reserved.,"Annadurai, K.|Uthayakumar, R.",APPLIED MATHEMATICAL MODELLING,setup cost|lead time|defective items|minimax distribution-free procedure|computational algorithm|optimization,10.1016/j.apm.2009.04.010
200,WOS:000414802100012,2017,Life cycle assessment of the environmental influence of wooden and concrete utility poles based on service lifetime,NATURAL DURABILITY|BRAZIL|DETERIORATION|BUILDINGS|IMPACT|TESTS,"Purpose Many applications of life cycle assessment do not consider the variability of the service lifetime of different structures, and this may be a relevant factor in an environmental impact assessment. This paper aims to determine the influence of the service lifetime on the potential environmental impacts of wooden and concrete poles in the electricity distribution system. Methods The estimation of service lifetime was conducted using the factorial method. The life cycle assessment was applied using SimaPro software and considered the entire life cycle of utility poles, from the extraction of raw materials to the final disposal. Then, an evaluation of the environmental impacts using the CML IA baseline method was performed. The study included the analysis of uncertainty using the Monte Carlo method. Results and discussion In general, the wooden poles had a lower potential environmental impact compared to the concrete poles. The result of the sensitivity analysis considering the variability of the chromated copper arsenate wood preservative retention rate suggests that the frequency of maintenance affects the service lifetime. Often, the comparison of products in the LCA perspective is carried out by considering similar useful lifetime services for the different alternatives, and this study shows that the environmental performance of products or services is directly proportional to the lifetime. It is a crucial parameter that has to be clarified in order to reduce uncertainty in the results. Conclusions Thus, some factors such as material quality, design adjustments and routine maintenance extend the service lifetime of a product or process and are shown to be effective ways to reduce environmental impacts. Therefore, the service lifetime has a significant influence on the development of the life cycle assessment. Comparative LCA studies are often sensitive to parameters that may even change the ranking of selected impact categories. All in all, from the sensitivity analysis highlighted in this study, the variability of lifetime service has proven to be one of the most prominent factors influencing comparative LCA results.",,"de Simone Souza, Hugo Henrique|Ferreira Lima, Angela Maria|Esquerre, Karla Oliveira|Kiperstok, Asher",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,factorial method|life cycle assessment|life span|maintenance|wooden pole,10.1007/s11367-017-1293-z
201,WOS:000382746100002,2016,Pressure drop across wire mesh demister in desalination plants using Eulerian-Eulerian modeling and computational fluid dynamics simulation,CURVED VANE DEMISTERS|SEPARATION EFFICIENCY|MIST ELIMINATOR|FLASH DESALINATION|HEAT-TRANSFER|FLOW|PERFORMANCE|CFD|CONDENSERS|DESIGN,"This study focuses on the development of design correlation for pressure drop in wire mesh demisters, used in the multistage flash desalination process (MSF) as well as similar evaporation and flashing units found in other industrial processes. Development of the correlation is based on numerical simulation of the demister using steady-state and two-dimensional model for the flow of vapor and brine droplets through the demister. An Eulerian model was used to model the system and the resulting model equations were solved using a commercial computational fluid dynamics software (FLUENT). The system model was formed of three zones, which include the vapor space above and below the demister and the demister. In addition, the demister was approximated as a porous media. A sensitivity analysis of the model revealed that vapor velocity, demister packing density and height, and the inlet flashed-off vapor composition are the main parameters that affect demister performance. Consequently, numerical data were used to correlate pressure drop across the demister as a function of operating and design parameters. The developed correlation was validated using data from real MSF plants. Analysis indicated that the correlation predictions and experimental data were consistent and showed good agreement with an error less than %.",,"Al-Rabiah, Danah|Al-Fulaij, Hala|Ettouney, Hisham",DESALINATION AND WATER TREATMENT,desalination|eulerian modeling|multistage flashing|cfd|demister,10.1080/19443994.2015.1102774
202,WOS:000263595500028,2009,The effect of field conditions on low Reynolds number flow in a wetland,LONGITUDINAL DISPERSION|EMERGENT VEGETATION|RESISTANCE|DIFFUSION|STREAMS|MODEL,"Stormwater runoff has been an environmental concern since the s. Green infrastructure, such as constructed stormwater wetlands (CSWs), is a tool in stormwater management, however, little is known about the hydraulic diffusion processes that impact water quality in low flow, laminar (i.e. baseflow) conditions. Diffusion provides the mechanisms that distribute and mix water through a CSW and therefore how pollutants will be spread through the CSW impacting the water quality. Laboratory experiments were performed by Nepf, H.M., Sullivan, J.A., Zavistoski, R.A. [. A model for diffusion within emergent vegetation. Limnology and Oceanography, (), S-], and Serra, T., Fernando, H.J.S., Rodriquez, R.V. [. Effects of emergent vegetation on lateral diffusion in wetland. Water Research, (), - to examine the effect of plant density on diffusion in laminar flow conditions. Nepf, H.M. [. Drag, turbulence, and diffusion in flow through emergent vegetation. Water Resources Research, (), - proposed a model predicting the diffusion coefficient based upon the plant density for both laminar and turbulent flow conditions. The present study examines the effect of field conditions on diffusion in a laminar flow field and verifies the diffusion model created by Nepf, H.M. [. Drag, turbulence, and diffusion in flow through emergent vegetation. Water Resources Research, (), -. The results from the present study show that the laminar flow model, based solely on mechanical diffusion, is not sufficient for field conditions and the total diffusion model must be used. The variability in flow conditions and stem diameter found in the field produce pockets of turbulence and dead zones that must be considered to predict the diffusion coefficients in low flow CSWs. A sensitivity analysis of the dead zone term shows that the laboratory, field and diffusion models lie within an acceptable theoretical range for the observed or predicted diffusion coefficient. in addition, a model was created using the Danish Hydraulic Institutes Mike  software. Model results indicate that non-uniform velocities significantly affect the diffusion coefficient and a range of diffusion coefficients should be considered when designing CSWs. (C) ", Elsevier Ltd. All rights reserved.,"Burke, Erin N.|Wadzuk, Bridget M.",WATER RESEARCH,wetlands|diffusion|laminar flow|hydrodynamic model,10.1016/j.watres.2008.10.027
203,WOS:000309958100005,2012,A Simulation Study to Assess a Variable Selection Method for Selecting Single Nucleotide Polymorphisms Associated with Disease,GENOME-WIDE ASSOCIATION|ORACLE PROPERTIES|REGRESSION|LASSO|LIKELIHOOD,"In genome-wide association studies, where hundreds of thousands of single nucleotide polymorphisms (SNPs) are genotyped, the potential for false positives is high and methods for selecting models with only a few SNPs are required. Methods for variable selection giving sets of SNPs associated with disease have been developed, but are still less common than evaluation of individual SNPs one at a time. To assess the potential improvement available from multi-SNP approaches, we examined the performance of the software GeneRaVE as a variable selection method when applied to SNP data in case-control studies. The method was assessed via simulations, in which a haplotype identified by three SNPs was taken to be associated with the disease. Simulated data sets reflecting different levels and patterns of genetic association with the disease were generated. In order to have a baseline level of performance to assess the method against, we used a generalized linear model using only the three disease susceptibility SNPs to provide an upper bound on the possible performance of the selection methods. To investigate the advantage of using variable selection method as a multivariate method over a single SNP approach, we used chi-squared tests for each of the disease susceptibility (DS) SNPs with correction for multiple testing. Simulation results showed that GeneRaVE performed well and outperformed single SNP analysis using the chi-squared method in identifying disease-related SNPs. In application to a large dataset, it identified SNPs known to be associated with disease that were not identified by single SNP methods.",,"Rabie, Huwaida S.|Saunders, Ian W.",JOURNAL OF COMPUTATIONAL BIOLOGY,bayesian|generalized linear models (glm)|lasso|logistic regression|risk ratio,10.1089/cmb.2011.0105
204,WOS:000414817700108,2017,Optimization of freeze-drying using a Life Cycle Assessment approach: Strawberries' case study,ENVIRONMENTAL-IMPACT|FOOD-PRODUCTS|SENSITIVITY-ANALYSIS|OSMOTIC DEHYDRATION|ASSESSMENT LCA,"Drying of foods is a preservation method that aims to prolong the product shelf life and simplify its transport and storage. However, this process requires a large amount of energy, which results in high emissions of contaminants in the environment.& para;& para;In this work, Life Cycle Assessment (LCA) analysis was applied to the drying of strawberries. In particular, for the first time, traditional freeze drying and the combination of osmotic dehydration + freeze drying were analysed and compared to verify if the application of the pre-treatment was effective in reducing the environmental impact, obtaining a more sustainable process.& para;& para;The chosen functional unit was one  g dried strawberries' package. Strawberries were gathered in , from May to September. Primary data related to the drying process were used to perform mass and energy balances and compile the life cycle inventory. The LCA analysis was accomplished using SimaPro .. software, in accordance with ISO -. The calculations on the traditional freeze drying process were made considering actual operation factory data, whereas the calculations on the osmotic + freeze drying process were desk calculations. The comparison of freeze drying and osmotic dehydration + freeze drying on industrial scale revealed that the traditional process generated higher emissions in terms of all the studied environmental categories. A scenario analysis was, then, carried out to evaluate the potential emissions' reduction due to the variation of some process times.& para;& para;An improved scenario based on the use of osmotic dehydration + freeze drying with optimised process times was, therefore, proposed; using the improved process, a reduction of the emissions equal to % with respect to the traditional freeze drying process could be obtained.& para;& para;Sensitivity analysis showed that variations in fertilisers' amount and in condenser and vacuum pump power did not cause significant changes to the results. The results also showed moderate sensitivity to strawberries' transport distance. (C) ", Elsevier Ltd. All rights reserved.,"Prosapio, Valentina|Norton, Ian|De Marco, Iolanda",JOURNAL OF CLEANER PRODUCTION,lca|emissions|food drying|freeze drying|osmotic dehydration,10.1016/j.jclepro.2017.09.125
205,WOS:000388119400013,2016,ASSESSMENT OF INDUSTRIAL SOLID WASTE USING THE INTELLIGENT DECISION SYSTEM (IDS) METHOD,EVIDENTIAL REASONING APPROACH|MANAGEMENT|UNCERTAINTY|SAFETY,"About  tons of daily solid waste disposal is one of the consequences of the speedy industrial expansion in the province of Khuzestan in the south west of Iran. There are more often than not diverse criteria for assessing the resulted pollution loads from solid waste disposal. In this paper, a new application for the Intelligent Decision System (IDS) is demonstrated for industrial solid waste assessment. IDS software is a Windows-based package for handling Multiple Criteria Decision Making (MCDM) problems considering both qualitative and quantitative criteria under uncertainties. The basis of IDS is a recently developed theory named the Evidential Reasoning ( ER) approach. The major features, superiority and excellence of IDS will be clarified through its application to the ranking of the industrial units located in the Khuzestan Province. Moreover, as a complimentary assessment, a sensitivity analysis is carried out in which the effect of decision maker's attitude toward risk on the total utility which each industry would gain is investigated. The results show that Ahwaz, Abadan, and Khoramshahr are respectively the three most polluting cities in the Khuzestan province. It is concluded that IDS can be utilized not only to handle problems which traditional methods can work out, but also to arrange and evaluate more difficult decision problems that traditional methods are not sufficiently expert of handling.",,"Abed-Elmdoust, Armaghan|Kerachian, Reza",ENVIRONMENTAL ENGINEERING AND MANAGEMENT JOURNAL,evidential reasoning approach|industrial solid waste assessment|intelligent decision system|khuzestan province|multiple-criteria-decision-making (mcdm),10.30638/eemj.2016.191
206,WOS:000250314200004,2007,"Efficient algorithms for life cycle assessment, input-output analysis, and Monte-Carlo analysis",UNCERTAINTY|INVENTORIES|OPTIONS|SYSTEM|MODEL,"Goal, Scope, and Background. As Life Cycle Assessment (LCA) and Input-Output Analysis (IOA) systems increase in size, computation times and memory usage can increase rapidly. The use of efficient methods of solution allows the use of a wide range of analysis techniques. Some techniques, such as Monte-Carlo Analysis, may be limited if computational times are too slow. Discussion of Methods. In this article, I describe algorithms that substantially reduce computation times and memory usage for solving LCA and IOA systems and performing Monte-Carlo analysis. The algorithms are based on well-established iterative methods of solving linear systems and exploit the power series expansion of the Leontief inverse. The algorithms are further enhanced by using sparse matrix algebra. Results and Discussion. The algorithms presented in this article reduce computational time and memory usage by orders of magnitude, while still retaining a high degree of accuracy. For a  x  LCA system, the algorithm reduced computation time from s to .s while retaining an accuracy of (-)%. Storage was reduced from  megabytes to . megabytes. The algorithm was used to perform a Monte-Carlo analysis on the same system with , samples in s. I also discuss various issues of power series convergence for general LCA and IOA systems and show that convergence will generally hold due to the mathematical structure of LCA and IOA systems. Conclusions. By exploiting the mathematical structure of LCA and IOA iterative techniques substantially reduced the computational times required for solving LCA and IOA systems and for performing Monte-Carlo simulations. This allows more widespread implementation analysis techniques, such as Monte-Carlo analysis, in LCA and IOA. Recommendations and Perspectives. It is suggested that algorithms, such as the ones described in this article, should be implemented in LCA packages. Various checks can be used to verify that computational errors are kept to a minimum.",,"Peters, Glen P.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,algorithms|input-output analysis (ioa)|matrix inverse|monte-carlo|numerical approaches|power series expansion|sensitivity analysis,10.1065/lca2006.06.254
207,WOS:000328724500002,2014,SGEMS-UQ: An uncertainty quantification toolkit for SGEMS,DISTANCES|RESERVOIR|MODELS,"While algorithms and methodologies to study uncertainty in the Earth Sciences are constantly evolving, there is currently no free integrated software that allows the general practitioners access to these developments. This paper presents SGEMS-UQ a plugin for the SGEMS platform, that is used to perform distance-based uncertainty analysis on geostatistical simulations, and the resulting forward transfer function responses used in subsurface modeling and engineering. A versatile XML-derived dialect is defined for communicating with external programs that reduces the need for ad-hoc linking of codes, and a relational database system is implemented to automate many of the steps in data mining the spatial and forward model parameters. Through a graphical user interface, one can map a set of realizations and forward transfer function responses into a multidimensional scaling (MDS) space where visualization utilities, and clustering techniques are available. Once mapped in the MDS space, the user can explore linkage between simulation parameters and forward transfer function responses using a module based on a SQL database. Consideration is given to the use of software engineering paradigms and design patterns to produce a code-base that is manageable, efficient, and extensible for future applications, while being scalable to work with large datasets. Finally, we illustrate the versatility of the code-base on an application of modeling uncertainty in reservoir forecasts for an oil reservoir in the West Coast of Africa. (C) ", Elsevier Ltd. All rights reserved.,"Li, Lewis|Boucher, Alexandre|Caers, Jef",COMPUTERS & GEOSCIENCES,uncertainty quantification|software design|xml|sql database,10.1016/j.cageo.2013.09.009
208,WOS:000256243200015,2008,Sensitivity of population viability to spatial and nonspatial parameters using grip,METAPOPULATION DYNAMICS|PREDICTIVE ABILITY|PVA MODELS|MATRIX|REINTRODUCTION,"Metapopulation dynamics are influenced by spatial parameters including the amount and arrangement of suitable habitat, yet these parameters may be uncertain when deciding how to manage species or their habitats. Sensitivity analyses of population viability analysis (PVA) models can help measure relative parameter influences on predictions, identify research priorities for reducing uncertainty, and evaluate management strategies. Few spatial PVAs, however, include sensitivity analyses of both spatial and nonspatial parameters, perhaps because computationally efficient tools for such analyses are lacking or inaccessible. We developed GRIP, a program to facilitate sensitivity analysis of spatial and nonspatial input parameters for PVAs created in RAMAS Metapop, a widely applied software program. GRIP creates random sets of input files by varying parameters specified in the PVA model including vital rates and their correlations among populations, the number and configuration of populations, dispersal rates, dispersal survival, initial population abundances, carrying capacities, and the probability, intensity, and spatial extent of catastrophes, while drawing on specified parameter distributions. We evaluated GRIP's performance as a tool for sensitivity analysis of spatial PVAs and explored the consequences of varying spatial input parameters for predictions of a published PVA model of the sand lizard (Lacerta agilis). We used GRIP output to generate standardized regression coefficients (SRCs) and nonparametric correlation coefficients as indices of the relative sensitivity of predicted conservation status to input parameters. GRIP performed well; with a single analysis we were able to rank the relative influence of input parameters identified as influential by the PVA's original author, S. A. Berglind, who used three separate forms of sensitivity analysis. Our analysis, however, also underscored the value of exploring the relative influence of spatial parameters on PVA predictions; both SRCs and correlation coefficients indicated that the most influential parameters in the sand lizard model were spatial in nature. We provide annotated code so that GRIP may be modified to reflect particular species biology, customized for more complex spatial PVA models, upgraded to incorporate features added in newer versions of RAMAS Metapop, used as a template to develop similar programs, or used as it is for computationally efficient sensitivity analyses in support of conservation planning.",,"Curtis, J. M. R.|Naujokaitis-Lewis, I.",ECOLOGICAL APPLICATIONS,decision-support tool|lacerta agilis|recovery planning|sand lizard|sensitivity analysis|spatial population viability analysis|uncertainty,10.1890/07-1306.1
209,WOS:000331688500003,2014,Parallel flow routing in SWMM 5,URBAN DRAINAGE SYSTEMS|SENSITIVITY-ANALYSIS|MODEL|PERFORMANCE|IMPACT,"The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multicore computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis. (C) ", Elsevier Ltd. All rights reserved.,"Burger, G.|Sitzenfrei, R.|Kleidorfer, M.|Rauch, W.",ENVIRONMENTAL MODELLING & SOFTWARE,multi-core|openmp|parallel computing|storm water management model|urban drainage modeling,10.1016/j.envsoft.2013.11.002
210,WOS:000224375900007,2004,A tool for risk-based management of surface water quality,SENSITIVITY-ANALYSIS|UNCERTAINTY|MODELS|EUTROPHICATION|PREDICTION|PHOSPHORUS|SYSTEMS|FUTURE,"Water quality Risk Analysis Tool (WaterRAT) is software for supporting decision-making in surface water quality management. The philosophy behind the software is that uncertainty in water quality model predictions is inevitably high due to model equation error, parameter error, and limited definition of boundary conditions and management objectives. Using sensitivity and uncertainty analyses based on Monte Carlo simulation and first order methods, WaterRAT allows the modeller to identify the significant uncertainties, and evaluate the degree to which they control decision-making risk. WaterRAT has a library of river and lake water quality models of varying complexity, and these can be applied at a wide range of temporal and spatial scales, allowing the model design to be responsive to both the modelling task and the data constraints. (C) ", Elsevier Ltd. All rights reserved.,"McIntyre, NR|Wheater, HS",ENVIRONMENTAL MODELLING & SOFTWARE,water quality|uncertainty|risk|decision-making,10.1016/j.envsoft.2003.12.003
211,WOS:000292418000035,2011,Quantification of model-form and predictive uncertainty for multi-physics simulation,,"Traditional uncertainty quantification in multi-physics design problems involves the propagation of parametric uncertainties in input variables such as structural or aerodynamic properties through a single, or series of models constructed to represent the given physical scenario. These models are inherently imprecise, and thus introduce additional sources of error to the design problem. In addition, there often exists multiple models to represent the given situation, and complete confidence in selecting the most accurate model among the model set considered is beyond the capability of the user. Thus, quantification of the errors introduced by this modeling process is a necessary step in the complete quantification of the uncertainties in multi-physics design problems. In this work, a modeling uncertainty quantification framework was developed to quantify to quantify both the model-form and predictive uncertainty in a design problem through the use of existing methods as well as newly developed modifications to existing methods in the literature. The applicability of this framework to a problem involving full-scale simulation was then demonstrated using the AGARD . Weakened Wing and three different aeroelastic simulation packages to quantify the flutter conditions of the wing. (C) ", Elsevier Ltd. All rights reserved.,"Riley, Matthew E.|Grandhi, Ramana V.",COMPUTERS & STRUCTURES,uncertainty quantification|model-form uncertainty|predictive uncertainty|multi-physics simulation,10.1016/j.compstruc.2010.10.004
212,WOS:000315267500001,2013,Determination of the overlap factor and its enhancement for medium-size tropospheric lidar systems: a ray-tracing approach,GEOMETRICAL FORM-FACTOR|FIBEROPTIC OUTPUT|RECEIVER|SIGNALS,"The problem of overlap factor (OVF) computation and its near-range sensitivity for medium-size aperture (f/, f/) bi-axial tropospheric lidar systems using ray-tracing simulation software is presented. The method revisits both detector and fiber optics coupling alternatives at the telescope focal-plane along with the insertion of a field lens. A sensitivity analysis is carried out as a function of laser divergence, field lens, and detector/fiber positions, detector size, and the fiber's core diameter and numerical aperture. The ray-tracing approach presented here is straightforward and a comparatively much simpler solution than analytical-based methods. Parametric simulations are carried out to show that both approaches are coincident. Insertion of a field lens proves to be an elegant and low sensitivity solution for OVF enhancement, particularly, in the near-range of the lidar.", (C) 2013 Society of Photo-Optical Instrumentation Engineers (SPIE) [DOI: 10.1117/1.JRS.7.073591],"Kumar, Dhiraj|Rocadenbosch, Francesc",JOURNAL OF APPLIED REMOTE SENSING,overlap factor|cross-over laser-telescope function|fiber-coupled lidar systems,10.1117/1.JRS.7.073591
213,WOS:000318057900003,2013,A model-independent Particle Swarm Optimisation software for model calibration,RAINFALL-RUNOFF MODELS|SHUFFLED COMPLEX EVOLUTION|PAMPA DEL TAMARUGAL|GLOBAL OPTIMIZATION|DIFFERENTIAL EVOLUTION|SENSITIVITY-ANALYSIS|UNCERTAINTY ASSESSMENT|REGIONAL AQUIFER|PARAMETERS|ALGORITHM,"This work presents and illustrates the application of hydroPSO, a novel multi-OS and model-independent R package used for model calibration. hydroPSO allows the modeller to perform a standard modelling work flow including, sensitivity analysis, parameter calibration, and assessment of the calibration results, using a single piece of software. hydroPSO implements several state-of-the-art enhancements and fine-tuning options to the Particle Swarm Optimisation (PSO) algorithm to meet specific user needs. hydroPSO easily interfaces the calibration engine to different model codes through simple ASCII files and/or R wrapper functions for exchanging information on the calibration parameters. Then, optimises a user-defined goodness-of-fit measure until a maximum number of iterations or a convergence criterion are met. Finally, advanced plotting functionalities facilitate the interpretation and assessment of the calibration results. The current hydroPSO version allows easy parallelization and works with single-objective functions, with multi-objective functionalities being the subject of ongoing development. We compare hydroPSO against standard algorithms (SCE_UA, DE, DREAM, SPSO-, and GML) using a series of benchmark functions. We further illustrate the application of hydroPSO in two real-world case studies: we calibrate, first, a hydrological model for the Ega River Basin (Spain) and, second, a groundwater flow model for the Pampa del Tamarugal Aquifer (Chile). Results from the comparison exercise indicate that hydroPSO is: i) effective and efficient compared to commonly used optimisation algorithms, ii) ""scalable"", i.e. maintains a high performance for increased problem dimensionality, and iii) versatile to adapt to different response surfaces of the objective function. Case study results highlight the functionality and ease of use of hydroPSO to handle several issues that are commonly faced by the modelling community such as: working on different operating systems, single or batch model execution, transient- or steady-state modelling conditions, and the use of alternative goodness-of-fit measures to drive parameter optimisation. Although we limit the application of hydroPSO to hydrological models, flexibility of the package suggests it can be implemented in a wider range of models requiring some form of parameter optimisation. (C) ", Elsevier Ltd. All rights reserved.,"Zambrano-Bigiarini, Mauricio|Rojas, Rodrigo",ENVIRONMENTAL MODELLING & SOFTWARE,global optimisation|evolutionary algorithm|surface water modelling|groundwater modelling|swat-2005|modflow-2005|r,10.1016/j.envsoft.2013.01.004
214,WOS:000222719700006,2004,Interactive software for material parameter characterization of advanced engineering constitutive models,STATE,"The development of an overall strategy to estimate the material parameters for a class of viscoplastic material models is presented. The procedure is automated through the integrated software COMPARE (Constitutive Material PARameter Estimator) that enables the determination of an 'optimum' set of material parameters by minimizing the errors between the experimental test data and the model's predicted response. The core ingredients of COMPARE are (i) primal analysis, which utilizes a finite element-based solution scheme, (ii) sensitivity analysis utilizing a direct-differentiation approach for the material response sensitivities, and (iii) a gradient-based optimization technique of an error/cost function. Now that the COMPARE core code has reached a level of maturity, a graphical user interface (GUI) was deemed necessary. Without such an interface, use of COMPARE was previously restricted to very experienced users with the additional cumbersome, and sometimes tedious, task of preparing the required input files manually. The complexity of the input containing massive amounts of data has previously placed severe limitations on the use of such optimization procedures by the general engineering community. By using C+ + and the Microsoft Foundation Classes to develop a GUI, it is believed that an advanced code such as COMPARE can now make the transition to general usability in an engineering environment. (C) ", Elsevier Ltd. All rights reserved.,"Saleeb, AF|Marks, |Wilt, TE|Arnold, SM",ADVANCES IN ENGINEERING SOFTWARE,c plus|graphical user interface|optimization|material characterization|viscoplasticity,10.1016/j.advengsoft.2004.03.010
215,WOS:000306034100006,2012,Sensitivity analysis for volcanic source modeling quality assessment and model selection,UNCERTAINTY IMPORTANCE MEASURE|AKAIKE INFORMATION CRITERION|COUPLED REACTION SYSTEMS|SURFACE DEFORMATION|RATE COEFFICIENTS|F-TEST|INDEXES|MOTION,"The increasing knowledge and understanding of volcanic sources has led to the development and implementation of sophisticated and complex mathematical models with the main goal of describing field and experimental data. Quantification of the model's ability in describing the data becomes fundamental for a realistic estimate of the model parameters. The analysis of sensitivity can help us in identifying the parameters that significantly affect the model's output and in assessing its quality factor. In this paper, we describe the Global Sensitivity Analysis (GSA) methods based both on Fourier Amplitude Sensitivity Test and on the Sobol' approach and discuss their implementation in a Mat lab software tool (GSAT). We also introduce a new criterion for model selection based on sensitivity analysis. The proposed approach is tested and applied to quantify the fitting ability of an analytic volcanic source model on a synthetic deformation data. Results show the validity of the method, against the traditional approaches, in supporting the volcanic model selection and the flexibility of the GSAT software tool in analyzing the model sensitivity. (C) ", Elsevier Ltd. All rights reserved.,"Cannavo, Flavio",COMPUTERS & GEOSCIENCES,sensitivity analysis|inverse problem|volcanic source|modeling|model selection,10.1016/j.cageo.2012.03.008
216,WOS:000358627500004,2015,"Individual-based modeling of soil organic matter in NetLogo: Transparent, user-friendly, and open",SENSITIVITY-ANALYSIS|NITROGEN DYNAMICS|CARBON|MICROBIOLOGY|MINERALIZATION|SIMULATION|COMPLEX|PARAMETERIZATION|POPULATIONS|COMPONENTS,"Soil organic matter dynamics are essential for terrestrial ecosystem functions as they affect biogeochemical cycles and, thus, the provision of plant nutrients or the release of greenhouse gases to the atmosphere. Most of the involved processes are driven by microorganisms. To investigate and understand these processes, individual-based models allow analyzing complex microbial systems' behavior based on rules and conditions for individual entities within these systems, taking into account local interactions and individual variations. Here, we present a streamlined, user-friendly and open version of the individual-based model INDISIM-SOM, which describes the mineralization of soil carbon and nitrogen. It was implemented in NetLogo, a widely used and easily accessible software platform especially designed for individual-based simulation models. Including powerful means to observe the model behavior and a standardized documentation, this increases INDISIM-SOM's range of potential uses and users, and facilitates the exchange among soil scientists as well as between different modeling approaches. (C) ", Elsevier Ltd. All rights reserved.,"Banitz, Thomas|Gras, Anna|Ginovart, Marta",ENVIRONMENTAL MODELLING & SOFTWARE,individual-based model|soil organic matter|soil microorganisms|mineralization|nitrification|netlogo,10.1016/j.envsoft.2015.05.007
217,WOS:000298644400010,2012,Preliminary assessment for global warming potential of leading contributory gases from a 40-in. LCD flat-screen television,,"As liquid crystal display (LCD) flat-screen televisions increase in popularity, their potential contribution to global warming has received wide attention. This study presents global warming impacts resulting from the life cycle assessment (LCA) of LCD flat-screen televisions for key global warming contributors from the ""cradle-to-gate"" and use stages of the product's life cycle. The emissions from nitrogen trifluoride (NF()), a greenhouse gas with a global warming potential (GWP) , times more potent than carbon dioxide (CO()), are not monitored in the Kyoto Protocol. Emissions in the cradle-to-gate and use stages were modeled in this study according to their GWP (kg CO() equivalent), focusing and analyzing the most significant source of NF() emissions. NF() is used during the manufacturing process of LCDs to clean the vacuum chambers. In this study, a system diagram of the cradle-to-gate stage and use stage of a -in. LCD television was proposed using the software package GabiA (R), particularly investigating NF() to determine its possible effects on global warming based on a typical LCA. The energy inputs in the use stage of the LCD flat-screen television resulted in major global warming impacts, while the contribution of GWP resulting from NF() was trivial. However, as energy efficiency continuously improves over time, the GWP resulting from NF() may become significant. Findings in this study allow industry to focus on those critical stages of the production life cycle that most directly affect global warming while permitting government agencies to enact proper regulations to help decrease CO() equivalent emissions. The preliminary assessment of our LCA also offers manufacturers the ability to determine the largest sources of greenhouse gases and their connection in the life cycle analysis of a product. This extension may help guide legislation and industrial management in the future. For further decision making, an in-depth sensitivity analysis may be needed to strengthen the results.",,"Thomas, Nicholas J.|Chang, Ni-Bin|Qi, Cheng",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,cradle-to-gate|global warming potential (gwp)|impact assessment|life cycle assessment|life cycle inventory|liquid crystal display (lcd)|nitrogen trifluoride (nf(3))|product life cycle|use stage,10.1007/s11367-011-0341-3
218,WOS:000413389300005,2017,Detailed analysis of reverse osmosis systems in hot climate conditions,WATER DESALINATION|MULTIOBJECTIVE OPTIMIZATION|MEDITERRANEAN SEA|PLANT-OPERATION|ENERGY|TEMPERATURE|EFFICIENCY|DESIGN|IMPACT,"Hot climate countries require large amounts of desalinated water. The Reverse osmosis (RO) technique is currently considered the most reliable technique for brackish water and seawater desalination. However, its power consumption is considerably higher than all other techniques. Therefore, the present study investigates the performance of reverse osmosis plants in hot climate conditions. A typical Reverse osmosis system was designed, constructed and investigated. The ROSA software was also used for the analysis of seven different membrane elements. The experimental data were utilized in order to validate the simulation results of the ROSA software. A variance-based sensitivity analysis was performed in order to define the most effective design and operating parameters. The present investigation shows that the tap and brackish water membrane elements are more sensitive to the feed water temperature rather than the feed water pressure and concentration. Meanwhile, seawater membrane elements are more affected by the feed concentration. The detailed investigation of the different membrane elements shows that wastewater reclamation using reverse osmosis technology could be a significant source of low-cost fresh water for hot climate countries.",,"Shaaban, S.|Yahya, H.",DESALINATION,desalination|reverse osmosis|membrane|sensitivity analysis|optimization,10.1016/j.desal.2017.09.002
219,WOS:000262275400013,2008,Evaluating the effect of scale in flood inundation modelling in urban environments,SCANNING LASER ALTIMETRY|DIFFUSION-WAVE TREATMENT|RASTER-BASED MODEL|RESOLUTION|SIMULATION|VEGETATION|1D,"Cellular-based approaches for flood inundation modelling have been extensively calibrated and evaluated for this prediction of flood flows on rural river reaches. However, there has only been limited application of these approaches to urban environments, where the need for flood management is greatest. Practical application of two-dimensional (D) flood inundation models is often limited by computation time and processing power on standard desktop PCs when attempting to resolve flows on the high-resolution grids necessary to replicate urban features. Consequently, it is necessary to evaluate the effectiveness of coarse grids to represent flood flows through urban environments. To examine these effects, LOSFLOOD-FP, a D storage cell model, is applied to hypothetical flooding scenarios in Greenfields, Glasgow, Grid resampling techniques in GIS software packages are evaluated and a bilinear griddling technique appears to provide the most accurate and physically intuitive results. A gridding method maintaining sharp elevation changes at building interfaces and neighbouring land is presented and estimates of the discretization noise associated with the coarse resolution grids suggest little improvement over current gridding methods. The variation in model results from the friction sensitivity analysis suggests a non-stationary response to Manning's n with changing model resolution. Model results suggests that a coarse resolution model for urban applications is limited by the representation or urban media in coarse model grids. Furthermore, critical length scales related to building dimensions and building separation distances exist in urban areas that determine maximum possible grid resolutions for hydraulic models of urban flooding."," Copyright (C) 2008 John Wiley & Sons, Ltd.","Fewtrell, T. J.|Bates, P. D.|Horritt, M.|Hunter, N. M.",HYDROLOGICAL PROCESSES,friction sensitivity|hydraulic modelling|scale|urban flooding,10.1002/hyp.7148
220,WOS:000256840200114,2007,Estimating uncertainty on internal dose assessments,RADON PROGENY|UNIT EXPOSURE|DOSIMETRY,"The estimation of uncertainty on doses broadly falls into three categories. () Estimating the uncertainty on prospective doses. Here, the intake is known and the uncertainties in individual parameter values must be propagated through the calculated dose. () Estimating the error or uncertainty on dose assessments made from single measurements. Here, intake, model parameter and measurement uncertainties are propagated into the measurement, but default ICRP parameter values are used to estimate the intake and dose from the measurement. ()Estimating the probability distribution of an individual's dose from a set of monitoring data. Here, Bayesian inference methods must be used to estimate the uncertainty on the estimated dose. A computer code is being developed that performs all three types of uncertainty analysis using Monte Carlo simulation. The software samples biokinetic parameters from probability density functions and then calculates doses from these parameters by calling the dosimetry code IMBA Professional Plus. A description of the methodology, together with an example application of the software, is included in this paper.",,"Puncher, M.|Birchall, A.",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm361
221,WOS:000349077200002,2015,Second law and sensitivity analysis of large ME-TVC desalination units,MULTIPLE-EFFECT EVAPORATION|THERMAL-ANALYSIS|EXERGY ANALYSIS|SYSTEMS|PERFORMANCE|ENERGY,"Large number of low temperature multi-effect thermal vapor compression (ME-TVC) desalination units have been installed recently in most of the GCC countries. The new trend of combining ME-TVC with conventional multi-effect led to tremendous increase in the unit size more than eight times during a very short period. The unit size capacity of this technology is currently available with  million imperial gallons per day (MIGD), and research studies are expected to increase this unit capacity up to  MIGD in the near future. Hence, this technology becomes highly attractive and competitive against multi stage flash desalination system. A mathematical model of ME-TVC desalination system is developed in this paper, using Engineering Equation Solver Software. This model is used to evaluate and improve the system performance of some new commercial ME-TVC units with capacities of ., ., and . MIGD using energy and exergy analysis. A sensitivity analysis is also presented in this paper to investigate the system performance of Al-Jubail ME-TVC unit in KSA, which is considered as the largest ME-TVC desalination plant in the world. Results showed that the first effect was found to be responsible for about % of the total effects exergy destruction in Al-Jubail, compared to % in ALBA and % in Umm Al-Nar. Results also showed that the specific exergy destruction is reduced significantly by increasing the number of effects as well as working at lower top brine temperatures.",,"Binamer, Anwar",DESALINATION AND WATER TREATMENT,thermal vapor|multi-effect|exergy|desalination,10.1080/19443994.2013.852481
222,WOS:000377792100014,2016,"Application of run-off model as a contribution to the torrential flood risk management in Topiderska Reka watershed, Serbia",CATCHMENTS,"This paper deals with developing of hydrological flow model for the torrential watershed of the Topiderska River, located in the Belgrade macro-region, in which torrential flood events have had destructive human and material consequences. The aim of the paper was to show that model, developed by hydrological software SHETRAN, is useful tool enabling the simulation of physical properties and hydrological processes in watersheds, having a significant place in decision support system within the torrential flood risk management. Results achievement was enabled through three phases: () Sensitivity analysis showed that the closest hydrograph to registered hydrograph is modelled with averaged values of parameters; () model calibration was done by calibration of four main parameters with significant influence on flow component (hydraulic conductivity of soil and rock, Strickler roughness coefficient for overland and river network flow) in case of the torrential flood event from th July ; and () model verification was done by input of calibrated values in case of two another torrential flood events (, ). Statistical analysis of correspondence of modelled and registered discharges showed that due to correlation coefficient (.-.) and determination coefficient (.-.) as well as probability of error according to F test, good results are achieved. Considering obtained results, developed model should be a part of torrential flood risk management in watershed of Topiderska River.",,"Petrovic, Ana M.|Kovacevic-Majkic, Jelena|Milosevic, Marko V.",NATURAL HAZARDS,hydrological model|run-off|discharge|torrential flood|topciderska river,10.1007/s11069-016-2269-1
223,WOS:000302910100010,2012,Robust design in aerodynamics using third-order sensitivity analysis based on discrete adjoint. Application to quasi-1D flows,SHAPE OPTIMIZATION|DATA ASSIMILATION|1ST,"In this paper, the second-order second moment approach, coupled with an adjoint-based steepest descent algorithm, for the solution of the so-called robust design problem in aerodynamics is proposed. Because the objective function for the robust design problem comprises first-order and second-order sensitivity derivatives with respect to the environmental parameters, the application of a gradient-based method , which requires the sensitivities of this function with respect to the design variables, calls for the computation of third-order mixed derivatives. To compute these derivatives with the minimum CPU cost, a combination of the direct differentiation and the discrete adjoint variable method is proposed. This is presented for the first time in the relevant literature and is the most efficient among other possible schemes on condition that the design variables are much more than the environmental ones; this is definitely true in most engineering design problems. The proposed approach was used for the robust design of a duct, assuming a quasi-D flow model; the coordinates of the Bezier control points parameterizing the duct shape are used as design variables, whereas the outlet Mach number and the DarcyWeisbach friction coefficient are used as environmental ones. The extension to D and D flow problems, after developing the corresponding direct differentiation and adjoint variable methods and software, is straightforward."," Copyright (C) 2011 John Wiley & Sons, Ltd.","Papoutsis-Kiachagias, E. M.|Papadimitriou, D. I.|Giannakoglou, K. C.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,robust aerodynamic shape optimization|discreteadjoint method|third-order sensitivity derivatives|method of moments,10.1002/fld.2604
224,WOS:000236011600030,2006,A standard interface between simulation programs and systems analysis software,TRANSPORT PARAMETERS|AQUATIC SYSTEMS|UNCERTAINTY|MODELS|MINIMIZATION|FLOW,"A simple interface between simulation programs and systems analytical software is proposed. This interface is designed to facilitate linkage of environmental simulation programs with systems analytical software and thus can contribute to remedying the deficiency in applying systems analytical techniques to environmental modelling studies. The proposed concept, consisting of a text file interface combined with a batch mode simulation program call, is independent of model structure, operating system and programming language. It is open for implementation by academic and commercial simulation and systems analytical software developers and is very simple to implement. Its practicability is demonstrated by implementations for three environmental simulation packages (AQUASIM, SWAT and LEACHM) and two systems analytical program packages (UNCSIM, SUR). The properties listed above and the demonstration of the ease of implementation of the approach are prerequisites for the stimulation of a widespread implementation of the proposed interface that would be beneficial for the dissemination of systems analytical techniques in the environmental and engineering sciences. Furthermore, such a development could stimulate the transfer of systems analytical techniques between different fields of application.",,"Reichert, P",WATER SCIENCE AND TECHNOLOGY,systems analytical techniques|environmental simulation programs|statistical inference|sensitivity analysis|identifiability analysis|uncertainty analysis,10.2166/wst.2006.029
225,WOS:000245389900003,2007,A non-intrusive stochastic Galerkin approach for modeling uncertainty propagation in deformation processes,PARTIAL-DIFFERENTIAL-EQUATIONS|CONTINUUM SENSITIVITY METHOD|METAL-FORMING PROCESS|POLYNOMIAL CHAOS|DESIGN,"Large deformation processes are inherently complex considering the non-linear phenomena that need to be accounted for. Stochastic analysis of these processes is a formidable task due to the numerous sources of uncertainty and the various random input parameters. As a result, uncertainty propagation using intrusive techniques requires tortuous analysis and overhaul of the internal structure of existing deterministic analysis codes. In this paper, we present an approach called non-intrusive stochastic Galerkin (NISG) method, which can be directly applied to presently available deterministic legacy software for modeling deformation processes with minimal effort for computing the complete probability distribution of the underlying stochastic processes. The method involves finite element discretization of the random support space and piecewise continuous interpolation of the probability distribution function over the support space with deterministic function evaluations at the element integration points. For the hyperelastic-viscoplastic large deformation problems considered here with varying levels of randomness in the input and boundary conditions, the NISG method provides highly accurate estimates of the statistical quantities of interest within a fraction of the time required using existing Monte Carlo methods. (c) ", Elsevier Ltd. All rights reserved.,"Acharjee, Swagato|Zabaras, Nicholas",COMPUTERS & STRUCTURES,uncertainty|deformation processes|stochastic galerkin methods|stochastic modeling,10.1016/j.compstruc.2006.10.004
226,WOS:000311245300006,2012,Combining explanatory crop models with geospatial data for regional analyses of crop yield using field-scale modeling units,CLIMATE-CHANGE SCENARIOS|SIMULATION-MODELS|PRECISION AGRICULTURE|SPATIAL-RESOLUTION|COMPUTER-PROGRAM|DECISION-SUPPORT|RICE PRODUCTION|WINTER-WHEAT|GIS|IMPACTS,"Crop models are used to predict yield and resource requirements as well as to evaluate different climate or management scenarios at a specific site. However, problems involving land use or global climate change encompass larger, more diverse, spatial scales and would benefit from simulating over broader areas using high-resolution, spatially-distributed data. A geospatial interface was developed to combine the explanatory potato crop model SPUDSIM with the geographic information system (GIS) software ArcGIS using the scripting language Python. Multiple geospatial input data layers were incorporated, including weather, soil, management, and land use. Modeling units (MUs) were defined as homogeneous field-scale areas created by the intersection of the input layers. Crop production was simulated for each unique combination of climate, soil, and management for MUs classified as cropland. The outputs (crop yield, water use, and nitrogen uptake) were mapped to show the spatial distribution within each county and aggregated to the county-level over the region of interest. An example was provided for potato production in Maine and illustrates how potential crop yield varies spatially over the state. The geospatial crop model showed evidence of both spatial and temporal variability of crop yield at the county level. The interface was designed to be flexible and easy to apply to applications such as evaluating crop production capacity and response under different scenarios.", Published by Elsevier B.V.,"Resop, Jonathan P.|Fleisher, David H.|Wang, Qingguo|Timlin, Dennis J.|Reddy, Vangimalla R.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,crop modeling|geographic information systems|spatial aggregation|regional analysis|sensitivity analysis|land use change,10.1016/j.compag.2012.08.001
227,WOS:000252516900010,2008,Self-adjoint sensitivity analysis of lossy dielectric structures with electromagnetic time-domain simulators,MICROWAVE IMAGE-RECONSTRUCTION|OPTIMAL-DESIGN METHOD|GRIDS,"We present an efficient self-adjoint approach for the computation of response derivatives in lossy inhomogencous structures with time-domain electromagnetic solvers. Our approach yields the responses and their derivatives with only one system analysis regardless of the number of optimizable parameters. The only requirement is to access the field solution at the perturbation grid points. The computation is performed as an independent post-process outside the solver. This makes our approach easy to implement as stand-alone software, which aids microwave design based on commercial computer-aided design packages. We show that our sensitivity analysis approach yields Jacobians of second-order accuracy for lossy dielectric structures. The approach is verified through -D, -D and -D examples using the time-domain field solutions obtained with solvers based on the finite-difference time-domain (FDTD) and transmission line modeling (TLM) methods."," Copyright (c) 2007 John Wiley & Sons, Ltd.","Song, Yunpeng|Li, Ying|Nikolova, Natalia K.|Bakr, Mohamed H.",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,time-domain analysis|sensitivity analysis|adjoint-variable methods|tlm method|fdtd method,10.1002/jnm.659
228,WOS:000246956100013,2007,Evaluation of urban stormwater quality models,,"The use of urban stormwater quality models necessitates the estimation of their outputs uncertainty level. The results of the application of a Monte Carlo Markov Chain method based on the Bayesian theory for the calibration and uncertainty analysis of a storm water quality model commonly used in available software are presented in this paper. The tested model estimates the accumulation, erosion and transport of pollutants on surfaces and in sewers using a hydrologic/hydrodynamic scheme. The model was calibrated for  different initial conditions of in-sewer deposits. Calibration results showed a large variability in the model outputs in function of the initial conditions and demonstrated that the tested model predictive capacity is very low.",,"Kanso, Assem|Tassin, Bruno|Chebbo, Ghassan",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,,10.1051/lhb:2007025
229,WOS:000332292700001,2014,Multiobjective Combinatorial Auctions in Transportation Procurement,OF-THE-ART,"This paper presents a multiobjective winner determination combinatorial auction mechanism for transportation carriers to present multiple transport lanes and bundle the lanes as packet bids to the shippers for the purposes of ocean freight. This then allows the carriers to maximize their network of resources and pass some of the cost savings onto the shipper. Specifically, we formulate three multi-objective optimization models (weighted objective model, preemptive goal programming, and compromise programming) under three criteria of cost, marketplace fairness, and the marketplace confidence in determining the winning packages. We develop solutions on the three models and perform a sensitivity analysis to show the options the shipper can use depending on the existing conditions at the point of awarding the transport lanes.",,"Ignatius, Joshua|Hosseini-Motlagh, Seyyed-Mahdi|Goh, Mark|Sepehri, Mohammad Mehdi|Mustafa, Adli|Rahman, Amirah",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/951783
230,WOS:000321690600011,2013,Data-driven sensitivity analysis to detect missing data mechanism with applications to structural equation modelling,GOODNESS-OF-FIT|IMPROPER SOLUTIONS|STANDARD ERRORS|VALUES,"Missing data are a common problem in almost all areas of empirical research. Ignoring the missing data mechanism, especially when data are missing not at random (MNAR), can result in biased and/or inefficient inference. Because MNAR mechanism is not verifiable based on the observed data, sensitivity analysis is often used to assess it. Current sensitivity analysis methods primarily assume a model for the response mechanism in conjunction with a measurement model and examine sensitivity to missing data mechanism via the parameters of the response model. Recently, Jamshidian and Mata (Post-modelling sensitivity analysis to detect the effect of missing data mechanism, Multivariate Behav. Res.  (), pp. -) introduced a new method of sensitivity analysis that does not require the difficult task of modelling the missing data mechanism. In this method, a single measurement model is fitted to all of the data and to a sub-sample of the data. Discrepancy in the parameter estimates obtained from the the two data sets is used as a measure of sensitivity to missing data mechanism. Jamshidian and Mata describe their method mainly in the context of detecting data that are missing completely at random (MCAR). They used a bootstrap type method, that relies on heuristic input from the researcher, to test for the discrepancy of the parameter estimates. Instead of using bootstrap, the current article obtains confidence interval for parameter differences on two samples based on an asymptotic approximation. Because it does not use bootstrap, the developed procedure avoids likely convergence problems with the bootstrap methods. It does not require heuristic input from the researcher and can be readily implemented in statistical software. The article also discusses methods of obtaining sub-samples that may be used to test missing at random in addition to MCAR. An application of the developed procedure to a real data set, from the first wave of an ongoing longitudinal study on aging, is presented. Simulation studies are performed as well, using two methods of missing data generation, which show promise for the proposed sensitivity method. One method of missing data generation is also new and interesting in its own right.",,"Jamshidian, Mortaza|Yuan, Ke-Hai",JOURNAL OF STATISTICAL COMPUTATION AND SIMULATION,factor analysis|generating missing data|incomplete data|missing at random|missing not at random|sensitivity analysis|simulation|sub-sample,10.1080/00949655.2012.660486
231,WOS:000249622700001,2007,An integrated framework for multipollutant air quality management and its application in georgia,SOUTHEASTERN UNITED-STATES|SOURCE APPORTIONMENT|TIME-SERIES|POLLUTION|OZONE|MODEL|EMISSIONS|MORTALITY|HEALTH|PM2.5,"Air protection agencies in the United States increasingly confront non-attainment of air quality standards for multiple pollutants sharing interrelated emission origins. Traditional approaches to attainment planning face important limitations that are magnified in the multipollutant context. Recognizing those limitations, the Georgia Environmental Protection Division has adopted an integrated framework to address ozone, fine particulate matter, and regional haze in the state. Rather than applying atmospheric modeling merely as a final check of an overall strategy, photochemical sensitivity analysis is conducted upfront to compare the effectiveness of controlling various precursor emission species and source regions. Emerging software enables the modeling of health benefits and associated economic valuations resulting from air pollution control. Photochemical sensitivity and health benefits analyses, applied together with traditional cost and feasibility assessments, provide a more comprehensive characterization of the implications of various control options. The fuller characterization both informs the selection of control options and facilitates the communication of impacts to affected stakeholders and the public. Although the integrated framework represents a clear improvement over previous attainment-planning efforts, key remaining shortcomings are also discussed.",,"Cohan, Daniel S.|Boylan, James W.|Marmur, Amit|Khan, Maudood N.",ENVIRONMENTAL MANAGEMENT,air pollution control|cost-benefit analysis|ozone|fine particulate matter|state implementation plans|attainment,10.1007/s00267-006-0228-4
232,WOS:000302212800003,2012,Effect of Temporal and Spatial Rainfall Resolution on HSPF Predictive Performance and Parameter Estimation,HYDROLOGICAL SIMULATION PROGRAM|AUTOMATIC CALIBRATION|MODEL PARAMETERS|VARIABILITY|RUNOFF|PRECIPITATION|RADAR|UNCERTAINTY|IMPACT|FLOW,"Watershed-scale rainfall-runoff models are used for environmental management and regulatory modeling applications, but their effectiveness is limited by predictive uncertainties associated with model input data. This study evaluated the effect of temporal and spatial rainfall resolution on the predictive performance of Hydrological Simulation Program-Fortran (HSPF) using manual and automatic calibration procedures. Furthermore, the effect of automatic parameter estimation on the physical significance of calibrated parameter values was evaluated. Temporal resolutions examined included  min,  min,  h, and  h, and spatial resolution effects evaluated included the effect of a spatially averaged network of four rain gauges and Next-Generation Radar (NEXRAD) for selected rain events. Model efficiencies ranged from . to . when individual rain gauges (RG, RG, RG, and RG) were used one at a time. Model efficiency improved and ranged from . to . when a spatially averaged network of four rain gauges was used. The effect of temporal resolution on model performance varied with rain gauge location in the watershed and with use of a single gauge or spatially averaged rain gauges for model calibration. Rainfall resolution has a strong influence on parameter estimation because, to achieve high model performance, parameter values must shift whenever the resolution of the rainfall data is changed. Despite a shift in parameter values as a result of changes in rainfall resolution, the results showed that Parameter Estimation Software (PEST)-calibrated values remained within their parameter bounds. In summary, results obtained from a medium-sized Piedmont watershed in Georgia, USA, revealed that model performance was more sensitive to spatial resolution than temporal resolution. DOI: ./(ASCE)HE.-..", (C) 2012 American Society of Civil Engineers.,"Mohamoud, Yusuf M.|Prieto, Lourdes M.",JOURNAL OF HYDROLOGIC ENGINEERING,hspf|spatial resolution|temporal resolution|parameter estimation|model performance|watershed modeling,10.1061/(ASCE)HE.1943-5584.0000457
233,WOS:000300438500006,2012,GIS Water-Balance Approach to Support Surface Water Flood-Risk Management,RESOLUTION TOPOGRAPHIC DATA|DIGITAL ELEVATION MODELS|DISTRIBUTED MODEL|OVERLAND-FLOW|URBAN AREAS|INUNDATION|SIMULATION|DRAINAGE|RUNOFF|SYSTEM,"Controversy has arisen as to whether the lack of appropriate consideration to surface water flood risk in urban spatial planning is reducing the capacity to manage urban flood risk. A screening tool is required which would allow spatial planners to identify potential surface water flood risks and explore their management opportunities. An urban water balance approach is presented. The hypothesis is that key hydrological characteristics, storage volume and location, flow paths, and surface water generation, capture the key processes responsible for surface water flooding. The model is assembled and run by using ESRI ArcGIS software. Surface sinks and their catchment areas are identified by using a Lidar DEM. Excess surface water is calculated by using a runoff coefficient that is applied to rainfall volumes, and no other losses are considered. A surface water accumulation module sums the excess surface water from the catchment area of each sink. A sensitivity analysis of model assumptions demonstrates that these are valid for a screening tool. An informal validation of the model with local authority data revealed that most of the known flood risk locations were highlighted by the model. The model is applied to Keighley and sample results illustrate how knowledge of sink storage can be interpreted to explore opportunities for flood risk management. The model is a useful tool for quickly assessing potential flood risk locations and basic management options. DOI: ./(ASCE)HE.-..", (C) 2012 American Society of Civil Engineers.,"Diaz-Nieto, J.|Lerner, D. N.|Saul, A. J.|Blanksby, J.",JOURNAL OF HYDROLOGIC ENGINEERING,pluvial flooding|surface water|gis|water balance|flood risk|keighley|uk|lidar|urban drainage,10.1061/(ASCE)HE.1943-5584.0000416
234,WOS:000317749400002,2013,Estimation of uncertainty sources in the projections of Lithuanian river runoff,CLIMATE-CHANGE IMPACTS|FLOOD RISK-ASSESSMENT|PARAMETER|MODELS,"Particular attention is given to the reliability of hydrological modelling results. The accuracy of river runoff projection depends on the selected set of hydrological model parameters, emission scenario and global climate model. The aim of this article is to estimate the uncertainty of hydrological model parameters, to perform sensitivity analysis of the runoff projections, as well as the contribution analysis of uncertainty sources (model parameters, emission scenarios and global climate models) in forecasting Lithuanian river runoff. The impact of model parameters on the runoff modelling results was estimated using a sensitivity analysis for the selected hydrological periods (spring flood, winter and autumn flash floods, and low water). During spring flood the results of runoff modelling depended on the calibration parameters that describe snowmelt and soil moisture storage, while during the low water period-the parameter that determines river underground feeding was the most important. The estimation of climate change impact on hydrological processes in the Merkys and Neris river basins was accomplished through the combination of results from AB, A and B emission scenarios and global climate models (ECHAM and HadCM). The runoff projections of the thirty-year periods (-, -, -) were conducted applying the HBV software. The uncertainties introduced by hydrological model parameters, emission scenarios and global climate models were presented according to the magnitude of the expected changes in Lithuanian rivers runoff. The emission scenarios had much greater influence on the runoff projection than the global climate models. The hydrological model parameters had less impact on the reliability of the modelling results.",,"Kriauciuniene, Jurate|Jakimavicius, Darius|Sarauskiene, Diana|Kaliatka, Tadas",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,lithuanian rivers|climate change|hbv|model calibration|sensitivity and uncertainty analysis|susa,10.1007/s00477-012-0608-7
235,WOS:000386560600005,2016,SOFTWARE RELIABILITY GROWTH MODEL WITH TEMPORAL CORRELATION IN A NETWORK ENVIRONMENT,CHANGE-POINT|NOISE,"Increasingly software systems are developed to provide great flexibility to customers but also introduce great uncertainty for system development. The uncertain behavior of fault-detection rate has irregular fluctuation and is described as a Markovian stochastic processes (white noise). However, in many cases the white noise idealization is insufficient, and real fluctuations are always correlated and correlated fluctuations (or colored noise) are non-Markovian stochastic processes. We develop a new model to quantify the uncertainties within non-homogeneous Poisson process (NHPP) in the noisy environment. Based on a stochastic model of the software fault detection process, the environmental uncertainties collectively are treated as a noise of arbitrary distribution and correlation structure. Based on the stochastic model, the analytical solution can be derived. To validate our model, we consider five particular scenarios with distinct environmental uncertainty. Experimental comparisons with existing methods demonstrate that the new framework shows a closer fitting to actual data and exhibits a more accurately predictive power.",,"Xu, Jiajun|Yao, Shuzhen|Yang, Shunkun|Wang, Peng",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,uncertainty quantification|reliability|nhpp|noise|correlation,10.1615/Int.J.UncertaintyQuantification.2016016194
236,WOS:000185985700004,2003,Structural design optimization on thermally induced vibration,TRANSIENT HEAT-CONDUCTION|PRECISE TIME INTEGRATION|SENSITIVITY ANALYSIS|DYNAMIC LOADS|DERIVATIVES,"The numerical method of design optimization for structural thermally induced vibration is originally studied in this paper and implemented in, the software JIFEX The direct and adjoint methods of sensitivity analysis for thermal-induced vibration coupled with both linear and non-linear transient heat conduction is firstly proposed. Based on the finite element method, the linear structural dynamics is treated simultaneously with linear and non-linear transient heat conduction. In the heat conduction, the non-linear factors include the radiation and temperature-dependent materials. The sensitivity analysis of transient linear and non-linear heat conduction is performed with the precise time integration method; and then, the sensitivity analysis of structural transient responses is performed by the Newmark method. Both the direct method and the adjoint method are employed to derive the sensitivity equations of thermal vibration. In the adjoint method, two adjoint vectors of structure and of heat conduction are used to derive the adjoint equations. The coupling effect of heat conduction on thermal vibration in the sensitivity analysis is particularly investigated. With the coupling sensitivity analysis, the optimization model is constructed and solved by the sequential linear programming or sequential quadratic programming algorithm. Numerical examples are given to validate the proposed methods and to demonstrate the importance of the coupled design optimization."," Copyright (C) 2003 John Wiley Sons, Ltd.","Chen, BS|Gu, YX|Zhang, HW|Zhao, GZ",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,heat conduction|thermal vibration|sensitivity analysis|design optimization|precise time integration|thermal coupled structural system,10.1002/nme.814
237,WOS:000311188300025,2012,"Integrating environmental gap analysis with spatial conservation prioritization: A case study from Victoria, Australia",DIGITAL ELEVATION MODELS|RESERVE SELECTION|PROTECTED AREAS|SOUTH-AFRICA|BIODIVERSITY CONSERVATION|VEGETATION CONDITION|UNCERTAINTY ANALYSIS|QUANTITATIVE METHOD|PLANNING TOOLS|CLIMATE-CHANGE,"Gap analysis is used to analyse reserve networks and their coverage of biodiversity, thus identifying gaps in biodiversity representation that may be filled by additional conservation measures. Gap analysis has been used to identify priorities for species and habitat types. When it is applied to identify gaps in the coverage of environmental variables, it embodies the assumption that combinations of environmental variables are effective surrogates for biodiversity attributes. The question remains of how to fill gaps in conservation systems efficiently. Conservation prioritization software can identify those areas outside existing conservation areas that contribute to the efficient covering of gaps in biodiversity features. We show how environmental gap analysis can be implemented using high-resolution information about environmental variables and ecosystem condition with the publicly available conservation prioritization software, Zonation. Our method is based on the conversion of combinations of environmental variables into biodiversity features. We also replicated the analysis by using Species Distribution Models (SDMs) as biodiversity features to evaluate the robustness and utility of our environment-based analysis. We apply the technique to a planning case study of the state of Victoria. Australia. (C) ", Elsevier Ltd. All rights reserved.,"Sharafi, Seyedeh Mahdieh|Moilanen, Atte|White, Matt|Burgman, Mark",JOURNAL OF ENVIRONMENTAL MANAGEMENT,reserve selection|zonation software|conservation prioritization|spatial optimization|environmental variables|habitat condition,10.1016/j.jenvman.2012.07.010
238,WOS:000240365500002,2006,Environmental and ecological hydroinformatics to support the implementation of the European Water Framework Directive for river basin management,DECISION-SUPPORT|MODELING APPROACH|SWAT MODEL|SPECIES RICHNESS|QUALITY MODELS|LANDSCAPE|INTEGRATION|DESIGN|SYSTEM|UNCERTAINTY,"Research and development in hydroinformatics can play an important role in environmental impact assessment by integrating physically-based models, data-driven models and other information and Communication Tools (ICT). An illustration is given in this paper describing the developments around the Soil and Water Assessment Tool (SWAT) to support the implementation of the EU water Framework Directive. SWAT operates on the river basin scale and includes processes for the assessment of complex diffuse pollution; it is open-source software, which allows for site-specific modifications to the source and easy linkage to other hydroinformatics tools. A crucial step in the world-wide applicability of SWAT was the integration of the model into a GIS environment, allowing for a quick model set-up using digital information on terrain elevation, land use and management, soil properties and weather conditions. Model analysis tools can be integrated with SWAT to assist in the tedious tasks of model calibration, parameter optimisation, sensitivity and uncertainty analysis and allows better understanding of the model before addressing scientific and societal questions. Finally, further linkage of SWAT to ecological assessment tools, Land Use prediction tools and tools for optimal Experimental Design shows that SWAT can play an important role in multi-disciplinary eco-environmental impact assessment studies.",,"van Griensven, A.|Breuer, L.|Di Luzio, M.|Vandenberghe, V.|Goethals, P.|Meixner, T.|Arnold, J.|Srinivasan, R.",JOURNAL OF HYDROINFORMATICS,catchment modelling|eco-hydrology|environmental hydroinformatics|eu water framework directive|model integration|swat,10.2166/hydro.2006.010
239,WOS:000330675000001,2014,Intelligent Platform for Model Updating in a Structural Health Monitoring System,PARAMETER SELECTION|DYNAMICS,"The main aim of this study is to develop an automated smart software platform to improve the time-consuming and laborious process of model updating. We investigate the key techniques of model updating based on intelligent optimization algorithms, that is, accuracy judgment methods for basic finite element model, parameter choice theory based on sensitivity analysis, commonly used objective functions and their construction methods, particle swarm optimization, and other intelligent optimization algorithms. An intelligent model updating prototype software framework is developed using the commercial software systems ANSYS and MATLAB. A parameterized finite element modeling technique is proposed to suit different bridge types and different model updating requirements. An objective function library is built to fit different updating targets. Finally, two case studies are conducted to verify the feasibility of the techniques used by the proposed software platform.",,"Dan, Danhui|Yang, Tong|Gong, Jiongxin",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/628619
240,WOS:000312884600010,2013,"Development of a Long-term, Ecologically Oriented Dam Release Plan for the Lake Baiyangdian Sub-basin, Northern China",RESERVOIR OPERATION|RIVER-BASIN|ALLOCATION,"Using China's Lake Baiyangdian sub-basin for a case study, we developed an ecologically oriented dam release plan that can be used to define an optimal dam operation scheme that provides both the environmental flows required by bodies of water and wetlands downstream from the Xidayang Reservoir dam and enough water for agricultural, and industrial water users. In addition, we evaluated the benefits that might be provided by modifying releases of water from the reservoir. To attain ecological sustainability in the sub-basin, we used the supply for each water user as a decision variable based on three objectives: () to achieve sustainable socioeconomic development; () to keep the water volume as close as possible to the ideal environmental flows in the urban rivers of Baoding City; and () to keep the water amount as close as possible to Lake Baiyangdian's ideal environmental water requirements. We used the ideal-point method to provide dimensionless values for the first objective, and then used a weighting method to integrate the three objectives into a single holistic goal. We then used the GAMS/CONOPT software to solve the nonlinear model and predict the optimal results. We discuss the optimal water allocation and ecologically oriented dam release plans for the three scenarios. To determine the limitations of the method, we performed a sensitivity analysis, and discuss the optimal results for different weightings of objectives provided by decision-makers. The results of the optimization analysis provide a set of effective compromises among the target objectives that can guide future management of water releases from the reservoir.",,"Yang, Wei|Yang, Zhifeng",WATER RESOURCES MANAGEMENT,ecologically oriented dam release plans|multi-objective optimization|environmental water requirements|lake baiyangdian,10.1007/s11269-012-0198-7
241,WOS:000088879600005,2000,Scales and similarities in runoff processes with respect to geomorphometry,SPATIAL VARIABILITY|CATCHMENT|SOIL,"Numerous investigations using various techniques have been carried out towards a more detailed understanding of relationships and interactions between catchment morphometry and rainfall-runoff processes. Recently, this research question has become more relevant through the need for accurate, yet simple, computer models simulating the water balance of large areas. Moreover, advances in the analysis of landform morphometry through the availability of high-resolution digital elevation models (DEMs) and powerful geographical information systems (GIS) have enhanced research efforts with this aim. In this study several computer techniques and models were applied to investigate the effects of geomorphometry on rainfall-runoff processes at different scales. The sensitivity of dynamic hydrological processes to comparatively static boundary conditions requires different methods for modelling, analysis and visualization of different kinds of data appropriate to different scales. Therefore an approach integrating several geocomputational concepts, including spatial analysis of different types of geodata, static modelling of spatial structures, dynamic four-dimensional modelling of hydrological processes and statistical techniques was chosen. Geomorphometric analysis of the study sites was carried out with GIS packages (including ARC/INFO and GRASS), special purpose software and self-developed tools. Soil-morphometry relationships were modelled within a GIS environment. Hydrological models (SAKE and TOPMODEL) were then used to simulate rainfall-runoff processes, and finally statistical tools and sensitivity analysis were applied to gain an insight into the hydrological significance of the various geomorphometric properties. The results demonstrate the importance of small subregions of the catchment, particularly those having low slope angles, low flow lengths and concavities. The spatial distribution of soil types significantly influences modelled runoff. Spatial distributions of soil types are partly related to morphometry and can be captured using soil-morphometry models. Further results show that catchments which differ significantly in morphometry show different runoff responses and different hydrological sensitivity to changes in boundary conditions. A crude derivation of geomorphometric-hydrological landform types could be reached. Therefore, geomorphometric classifications of catchment types could form a basis for representative hydrological modelling at the large scale. Models describing soil distribution in relation to geomorphometry could assist regionalization of spatial heterogeneity and structure of soil parameters relevant in hydrological modelling. Moreover, quantification of geomorphometric catchment structure, e.g. in terms of contributing areas, is needed to describe significant geomorphometric catchment characteristics."," Copyright (C) 2000 John Wiley & Sons, Ltd.","Schmidt, J|Hennrich, K|Dikau, R",HYDROLOGICAL PROCESSES,hydrological modelling|geomorphometry|gis|dem|soil-morphometry relationship,10.1002/1099-1085(20000815/30)14:11/12<1963::AID-HYP48>3.0.CO;2-M
242,WOS:000333743900027,2014,Comparative Life Cycle Assessment (LCA) of Accelerated Carbonation Processes Using Steelmaking Slag for CO2 Fixation,ROTATING PACKED-BED|OXYGEN FURNACE SLAG|METALWORKING WASTE-WATER|CALCIUM-CARBONATE|MINERAL CARBONATION|STEEL SLAG|CAPTURE|STORAGE|DIOXIDE|SEQUESTRATION,"Carbon capture, utilization, and storage (CCUS) is one of the most prominent emerging technologies for mitigating global climate change. In this study, a comparative evaluation for CO fixation by carbonation of steelmaking slag was performed by life cycle assessment (LCA) using Umberto .. software, with the Swiss Eco-invent . database. Six scenarios of carbonation for basic oxygen furnace slag (BOFS), steel converted slag (SCS), and blended hydraulic slag cement (BHC) if different types of reactors and/or method were established. The environmental impacts for each scenario are quantified using the valuation system of ReCiPe, where global warming potential (GWP), ecosystem quality potential (EQP), an human health potential (HHP) were evaluated. In addition, sensitivity analysis was carried out to evaluate the relevant uncertainties of heating efficiency on the GHG emissions in direct carbonation processes. According to the results of LCA and sensitivity analysis, the direct carbonation of steelmaking slag in a slurry reactor was found to be the most attractive method, since the GWP was the lowest among the selected scenarios. Furthermore, the best available technology (BAT) for CO capture by carbonation processes of alkaline wastes was proposed according to the key performance indicators (KPIs) with respect to engineering considerations and environmental impacts. It was concluded that the accelerated carbonation of steelmaking slag should be performed by combining the slurry reactor with a rotating packed bed (RPB) to maximize carbonation conversion and minimize environmental impacts and additional CO emissions.",,"Xiao, Li-Shan|Wang, Run|Chiang, Pen-Chi|Pan, Shu-Yuan|Guo, Qing-Hai|Chang, E. E.",AEROSOL AND AIR QUALITY RESEARCH,technology assessment|umberto|environmental impacts|recipe|sensitivity analysis|rotating packed bed,10.4209/aaqr.2013.04.0121
243,WOS:000281237800007,2010,Sensitivity Analysis of Land Unit Suitability for Conservation Using a Knowledge-Based System,FOREST ECOSYSTEM SUSTAINABILITY|COLUMBIA RIVER BASIN|RESERVE SELECTION|NETWORKS|MANAGEMENT|ECOLOGY|SCALES|AREAS|MODEL,"The availability of spatially continuous data layers can have a strong impact on selection of land units for conservation purposes. The suitability of ecological conditions for sustaining the targets of conservation is an important consideration in evaluating candidate conservation sites. We constructed two fuzzy logic-based knowledge bases to determine the conservation suitability of land units in the interior Columbia River basin using NetWeaver software in the Ecosystem Management Decision Support application framework. Our objective was to assess the sensitivity of suitability ratings, derived from evaluating the knowledge bases, to fuzzy logic function parameters and to the removal of data layers (land use condition, road density, disturbance regime change index, vegetation change index, land unit size, cover type size, and cover type change index). The amount and geographic distribution of suitable land polygons was most strongly altered by the removal of land use condition, road density, and land polygon size. Removal of land use condition changed suitability primarily on private or intensively-used public land. Removal of either road density or land polygon size most strongly affected suitability on higher-elevation US Forest Service land containing small-area biophysical environments. Data layers with the greatest influence differed in rank between the two knowledge bases. Our results reinforce the importance of including both biophysical and socio-economic attributes to determine the suitability of land units for conservation. The sensitivity tests provided information about knowledge base structuring and parameterization as well as prioritization for future data needs.",,"Humphries, Hope C.|Bourgeron, Patrick S.|Reynolds, Keith M.",ENVIRONMENTAL MANAGEMENT,sensitivity analysis|map removal|knowledge base|conservation suitability|land suitability|fuzzy logic|ecosystem management decision support,10.1007/s00267-010-9520-4
244,WOS:000285234300013,2010,Life cycle inventory analysis of biological hydrogen production by thermophilic and photo fermentation of potato steam peels (PSP),,"The environmental impact of hydrogen production in a -step fermentation process from potato steam peels was identified. Based on the ISO , ISO , ecoinvent data base and SimaPro . software, a life cycle inventory analysis was performed. Reflecting the current state of process development, the LCA shows an impact of . points (pts) which is at least . times higher than the selected reference technologies regarded as state-of-the-art. Over half (.%) of the environmental impact is generated by the use of phosphate in the fermentation processes. A sensitivity analysis shows a potential impact reduction of .% due to recirculation of sewage or reduction of buffer concentration. The analysis also demonstrates that the production of the process ingredients cause .% of the environmental impact. The impact of the process itself is . pts which is up to  times lower than the reference technologies. (C) ", Elsevier Ltd. All rights reserved.,"Ochs, Dominik|Wukovits, Walter|Ahrer, Werner",JOURNAL OF CLEANER PRODUCTION,life cycle analysis|hydrogen production|environmental impact|hyvolution|biomass|fermentation,10.1016/j.jclepro.2010.05.018
245,WOS:000419231500134,2017,Passive Optimization Design Based on Particle Swarm Optimization in Rural Buildings of the Hot Summer and Warm Winter Zone of China,ENERGY PERFORMANCE|RESIDENTIAL BUILDINGS|SENSITIVITY-ANALYSIS|MULTIOBJECTIVE OPTIMIZATION|HIGHRISE BUILDINGS|THERMAL COMFORT|SIMULATION|CONSUMPTION|ENVELOPE|SYSTEM,"The development of green building is an important way to solve the environmental problems of China's construction industry. Energy conservation and energy utilization are important for the green building evaluation criteria (GBEC). The objective of this study is to evaluate the quantitative relationship between building shape parameter, envelope parameters, shading system, courtyard and the energy consumption (EC) as well as the impact on indoor thermal comfort of rural residential buildings in the hot summer and warm winter zone (HWWZ). Taking Quanzhou (Fujian Province of China) as an example, based on the field investigation, EnergyPlus is used to build the building performance model. In addition, the classical particle swarm optimization algorithm in GenOpt software is used to optimize the various factors affecting the EC. Single-objective optimization has provided guidance to the multi-dimensional optimization and regression analysis is used to find the effects of a single input variable on an output variable. Results shows that the energy saving rate of an optimized rural residence is about -% corresponding to the existing rural residence. Moreover, the payback period is about  years. A simple case study is used to demonstrate the accuracy of the proposed optimization analysis. The optimization can be used to guide the design of new rural construction in the area and the energy saving transformation of the existing rural houses, which can help to achieve the purpose of energy saving and comfort.",,"Lu, Shilei|Wang, Ran|Zheng, Shaoqun",SUSTAINABILITY,rural residence|green building|energy consumption|multidimensional optimization|particle swarm optimization|regression analysis,10.3390/su9122288
246,WOS:000319162300013,2013,"An integrated ""process modelling-life cycle assessment"" tool for the assessment and design of water treatment processes",IMPACT ASSESSMENT|POTABLE WATER|METHODOLOGY|OPTIMIZATION,"The application of Life Cycle Assessment (LCA) to the design of water treatment plants is hampered by: () a large diversity of unit processes, () the high variability of the operation conditions in relation with the water quality input, and () the range of possible technical solutions to fulfil the treatment needs. For a consistent prospective assessment, the LCA should be based on the simulated functioning of the unit processes rather than on average data, as it is most often the case when no real data are available. Here, a novel, integrated and flexible process modelling-life cycle assessment (PM-LCA) tool for design and LCA of water treatment technologies is presented. The tool (EVALEAU) was developed in UmbertoA (R) (v.) using the Python language for code scripting. A library of unit process (UP) modules was built. Each module is a detailed and highly parameterized model of a specific water treatment process, which is further linked with the software PHREEQCA (R) for water chemistry calculation. Input data are: water composition, design, operation parameters, including literature or user-defined values. The modules are linked to Ecoinvent datasets (v.) for background processes. By combining the modules, water treatment chains can be designed and evaluated in UmbertoA (R) with a high level of detail and specifications. A sensitivity analysis toolbox (Morris method) was included for the identification of the process parameters mainly affecting the impact results. The tool was successfully applied to the test bed case of an existing drinking water plant located in the Paris region. The conventional LCA results, based on average recorded data, were compared with the results obtained using the PM-LCA tool. Modelling results for technical parameters were also compared with data collected on site. An overall good agreement between simulations and real data was obtained, proving the relevance of the developed tool. Sensitivity analysis indicated that ozone production and transfer into water are the main technological parameters influencing climate change (taken as example since it is of high interest for stakeholders), which have therefore to be fine-tuned. The EVALEAU tool successfully solves the challenge of linking LCA results to the related engineering design choices, from the assessment and eco-design perspectives. The concepts and methodologies embedded within the tool provide the user with complementary views of the designed system, in terms of potable water quality, design and operation parameters and environmental impacts generated over its life cycle.",,"Mery, Yoann|Tiruta-Barna, Ligia|Benetto, Enrico|Baudin, Isabelle",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,drinking water|eco-design|life cycle assessment|life cycle inventory|process modelling|sensitivity analysis|water treatment,10.1007/s11367-012-0541-5
247,WOS:000392165500023,2017,Shale gas flowback water desalination: Single vs multiple-effect evaporation with vapor recompression cycle and thermal integration,PERFORMANCE EVALUATION|MEMBRANE DISTILLATION|COMPRESSION|SYSTEM|MANAGEMENT|OPTIMIZATION|DESIGN|UNCERTAINTY|RESOURCES|SELECTION,"This paper introduces a new optimization model for the single and multiple-effect evaporation (SEE/MEE) systems design, including vapor recompression cycle and thermal integration. The SEE/MEE model is specially developed for shale gas flowback water desalination. A superstructure is proposed to solve the problem, comprising several evaporation effects coupled with intermediate flashing tanks that are used to enhance thermal integration by recovering condensate vapor. Multistage equipment with intercooling is used to compress the vapor formed by flashing and evaporation. The compression cycle is driven by electricity to operate on the vapor originating from the SEE/MEE system, providing all the energy needed in the process. The mathematical model is formulated as a nonlinear programming (NLP) problem optimized under GAMS software by minimizing the total annualized cost. The SEE/MEE system application for zero liquid discharge (ZLD) is investigated by allowing brine salinity discharge near to salt saturation conditions. Additionally, sensitivity analysis is carried out to evaluate the optimal process configuration and performance under distinct feed water salinity conditions. The results highlight the potential of the proposed model to cost-effectively optimize SEE/MEE systems by producing fresh water and reducing brine discharges and associated environmental impacts. (C)  The Authors.", Published by Elsevier B.V.,"Onishi, Viviani C.|Carrero-Parreno, Alba|Reyes-Labarta, Juan A.|Ruiz-Femenia, Ruben|Salcedo-Diaz, Raquel|Fraga, Eric S.|Caballero, Jose A.",DESALINATION,shale gas|single-effect evaporation (see)|multiple-effect evaporation (mee)|mechanical vapor recompression (mvr)|thermal integration|zero liquid discharge (zld),10.1016/j.desal.2016.11.003
248,WOS:000323457200002,2013,Comparative study on simulation performances of CORSIM and VISSIM for urban street network,ACTUATED SIGNAL SYSTEM|MICROSCOPIC SIMULATION|MODEL CALIBRATION|TRAFFIC FLOW|VALIDATION|BEHAVIOR,"With the progress of simulation technologies, many transportation simulation packages were developed. However, little information is available to the users in applying these models to the most appropriate situations, or even seldom with the simulation accuracy of the individual model. This study conducts a comparative analysis of two popular simulation models (VISSIM and CORSIM), based on their simulation performances on an urban transportation network. Road network and field traffic data from North Bund, Hongkou District, Shanghai, China were used as the simulation background and input. Sensitivity analysis was carried out to compare the performance of both models based on four key indices, namely software usability, average control delay, average queuing length, and cross-sectional traffic volume. Advantages of each simulator were identified based on comparison analyses of simulations with different levels of congestion and intersection geospatial scales. The main performance difference was found lying in the default parameter configuration within the models, including driver behavior settings, traffic environment settings, and vehicle types, etc. Consequently, it was recommended that analysts should choose their appropriate tools based on intersection type and level of saturation within the simulation case.", (c) 2013 Elsevier B.V. All rights reserved.,"Sun, Daniel (Jian)|Zhang, Lihui|Chen, Fangxi",SIMULATION MODELLING PRACTICE AND THEORY,micro-simulation model|urban transportation network|comparative study|signal intersections|sensitivity analysis,10.1016/j.simpat.2013.05.007
249,WOS:000413669300001,2017,Numerical Study of the Performance and Emission of a Diesel-Syngas Dual Fuel Engine,METHANOL STEAM REFORMER|MECHANISM REDUCTION|PARTIAL OXIDATION|N-HEPTANE|COMBUSTION|GASIFICATION|SIMULATION|OPERATION|MIXTURES|GAS,"Based on the theory of direct relation graph (DRG) and the sensitivity analysis, a reduced mechanism for the diesel-syngas dual fuel was constructed. Three small thresholds were applied in the process of the detailed mechanism simplification by DRG, and a skeletal mechanism with  elements and the  elementary reactions was obtained. According to the framework of the skeletal mechanism, the time-consuming approach of sensitivity analysis was employed for further simplification, and the skeletal mechanism was further reduced to the size of  elements and  reactions. The Chemkin software with the detailed mechanism was utilized to calculate the effect of syngas addition on the combustion characteristics of diesel combustion. The findings showed that the addition of syngas could reduce the ignition delay time and increase the laminar flame speed. Based on the reduced mechanism and engine parameters, a D model of the engine was constructed with the Forte code. The D model was adopted to study the effect of syngas addition on the performance and exhaust emissions of the engine and the relevant data of the experiment was used in the calibration of the D model.",,"Feng, Shiquan",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2017/6825079
250,WOS:000365335000031,2016,A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes,MULTIOBJECTIVE OPTIMIZATION|EVOLUTIONARY ALGORITHMS|FLOWSHEET OPTIMIZATION|PROCESS SIMULATORS|GENETIC ALGORITHM|DESIGN|METHODOLOGY|PLANTS|PERSPECTIVES|INTEGRATION,"Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab (TM) is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented. (C) ", Elsevier Ltd. All rights reserved.,"Otte, Daniel|Lorenz, Hilke-Marie|Repke, Jens-Uwe",COMPUTERS & CHEMICAL ENGINEERING,optimization|molecular-inspired parallel tempering|multicomponent separation process|retrofit|stochastic algorithm|chemcad (tm),10.1016/j.compchemeng.2015.08.023
251,WOS:000378854600008,2016,"Towards the integration of process design, control and scheduling: Are we getting closer?",MODEL-PREDICTIVE CONTROL|INTEGER DYNAMIC OPTIMIZATION|CONSTRAINED LINEAR-SYSTEMS|OPTIMAL GRADE TRANSITION|CHEMICAL-PROCESSES|BATCH PROCESSES|UNCERTAINTY|FRAMEWORK|FLEXIBILITY|PARAMETERS,"The integration of design and control, control and scheduling and design, control and scheduling, all have been core PSE challenges. While significant progress has been achieved over the years, it is fair to say that at the moment there is not a generally accepted methodology and/or ""protocol"" for such an integration - it is also interesting to note that currently, there is not a commercially available software [or even in a prototype form] system to fully support such an activity. Here, we present the foundations for such an integrated framework and especially a software platform that enables such integration based on research developments over the last  years. In particular, we describe PAROC, a prototype software system which allows for the representation, modeling and solution of integrated design, scheduling and control problems. Its main features include: (i) a high-fidelity dynamic model representation, also involving global sensitivity analysis, parameter estimation and mixed integer dynamic optimization capabilities; (ii) a suite/toolbox of model approximation methods; (iii) a host of multi-parametric programming solvers for mixed continuous/integer problems; (iv) a state-space modeling representation capability for scheduling and control problems; and (v) an advanced toolkit for multi-parametric/explicit Model Predictive Control and moving horizon reactive scheduling problems. Algorithms that enable the integration capabilities of the systems for design, scheduling and control are presented on a case of a series of cogeneration units. (C) ", Elsevier Ltd. All rights reserved.,"Pistikopoulos, Efstratios N.|Diangelakis, Nikolaos A.",COMPUTERS & CHEMICAL ENGINEERING,multi-parametric receding horizon policies|control|design optimization|optimal scheduling|integration,10.1016/j.compchemeng.2015.11.002
252,WOS:000262884900019,2009,Kinetic modelling of methane decomposition in a tubular solar reactor,AEROSOL FLOW REACTOR|HYDROGEN-PRODUCTION|THERMAL-DECOMPOSITION|NATURAL-GAS|RAPID DECOMPOSITION|CHEMICAL REACTOR|PYROLYSIS|CARBON|DISSOCIATION|DECARBONIZATION,"Solar methane cracking is a promising pathway to produce hydrogen and carbon black with the bonus of zero CO() emission. A kinetic simulation of the methane decomposition in a tubular solar chemical reactor prototype is presented. This reactor is composed of four independent tubular reaction zones inserted in a graphite cavity receiver. Chemical reaction modelling is carried out thanks to the Dsmoke software, using a detailed kinetic scheme for the wide range modelling of alkane transformation. First. a kinetic analysis of the chemical system is presented to determine the sequence of methane cracking and a sensitivity analysis of the results on temperature (in the range - K) and on natural gas composition is performed. Then, a kinetic simulation of the solar reactor is proposed and implemented, in which each tubular reaction zone is modelled by three plug-flow reactors in series representing the pre-heating, isothermal, and cooling zones of the reactor. It predicts the evolution of gas species concentrations as a function of residence time. Comparisons with experimental results between  K and  K show good agreement for CH() conversion, and CH() and H() off-gas compositions.", (c) 2008 Elsevier B.V. All rights reserved.,"Rodat, Sylvain|Abanades, Stephane|Coulie, Julien|Flamant, Gilles",CHEMICAL ENGINEERING JOURNAL,methane cracking|kinetic model|plug-flow reactor|hydrogen production|solar reactor,10.1016/j.cej.2008.09.008
253,WOS:000283561900012,2010,"A model for the development of a power production system in Greece, Part I: Where RES do not meet EU targets",RENEWABLE ELECTRICITY POLICIES|ENERGY|SECTOR|PLANTS|CONSUMPTION|CHALLENGES|STRATEGIES|EMISSIONS|CAPACITY|THAILAND,"This paper studies the electricity production system of the Greek Interconnected Electric Production System using a model created with the software package WASP-IV. The period of study is from  to . It consists of three scenarios using three different criteria: energy, environmental and economic. The three scenarios are the business as usual, the lignite and the natural gas. Subsequently, a sensitivity analysis is carried out for the annual growth rate of electricity consumption and load demand. The paper examines how the three criteria change, when there are no other energy sources beyond those already in use (lignite, oil, natural gas, biomass, solar, wind and hydropower) with no CO capture policies and with the electricity production from Renewable Energy Sources not to reach the targets of the European Union for . In a second paper, three other scenarios examine production with the Renewable Energy Sources to reach the targets of the European Union for . (C) ", Elsevier Ltd. All rights reserved.,"Kalampalikas, Nikolaos G.|Pilavachi, Petros A.",ENERGY POLICY,electricity production system|greek interconnected electric system|co2 emissions,10.1016/j.enpol.2010.05.038
254,WOS:000228401800048,2005,Investigating the dynamic behavior of biochemical networks using model families,SYSTEMS BIOLOGY|SIMULATION|CELL,"Motivation: Supporting the evolutionary modeling process of dynamic biochemical networks based on sampled in vivo data requires more than just simulation. In the course of the modeling process, the modeler is typically concerned not only with a single model but also with sequences, alternatives and structural variants of models. Powerful automatic methods are then required to assist the modeler in the organization and the evaluation of alternative models. Moreover, the structure and peculiarities of the data require dedicated tool support. Summary: To support all stages of an evolutionary modeling process, a new general formalism for the combinatorial specification of large model families is introduced. It allows for automatic navigation in the space of models and excludes biologically meaningless models on the basis of elementary flux mode analysis. An incremental usage of the measured data is supported by using splined data instead of state variables. With MMT, a versatile tool has been developed as a computational engine intended to be built into a tool chain. Using automatic code generation, automatic differentiation for sensitivity analysis and grid computing technology, a high performance computing environment is achieved. MMT supplies XML model specification and several software interfaces. The performance of MMT is illustrated by several examples from ongoing research projects.",,"Haunschild, MD|Freisleben, B|Takors, R|Wiechert, W",BIOINFORMATICS,,10.1093/bioinformatics/bti225
255,WOS:000393021400022,2017,ODM: an analytical solution-based tool for reacting oxygen diffusion modelling in mine spoils,COAL STRIP MINES|PYRITE OXIDATION|THIOBACILLUS-FERROOXIDANS|DRAINAGE REMEDIATION|WASTE PILE|ACID|TAILINGS|TRANSPORT|IRAN|SIMULATIONS,"A simple one-dimensional analytical solution is presented to model oxygen diffusion through the pore space of mine spoils containing pyrite. The model incorporates volumetric oxygen consumption terms due to pyrite oxidation, oxidation of Fe+ to Fe+ and bacterial activity. Based on this analytical solution, a graphical user interface (GUI) tool is programmed and designed in MATLAB software. This tool can be used to model transport of oxygen through the mine spoils either with or without a cap. Results of several simulation scenarios of sensitivity analysis showed a significant change in oxygen concentration with varying effective diffusion coefficient of oxygen transport model and simulation time. Efficiency and flexibility of the tool developed here is verified by modelling oxygen transport through the pore space of a coal waste pile (case A) and a copper mine tailings (case B). Maximum depth of oxygen diffusion is obtained approximately equal to  and . m through the cases A and B, respectively.",,"Bahrami, Saeed|Ardejani, Faramarz Doulati",ENVIRONMENTAL EARTH SCIENCES,acid mine drainage|mine tailings|mine waste|pyrite oxidation|bacterial activity|matlab gui,10.1007/s12665-017-6389-z
256,WOS:000232093200009,2006,Sensitivity analysis of the strain criterion for multidimensional scaling,SPECTRAL FUNCTIONS|OPTIMIZATION,"Multidimensional scaling (MDS) is a collection of data analytic techniques for constructing configurations of points from dissimilarity information about interpoint distances. Classsical MDS assumes a fixed matrix of dissimilarities. However, in some applications, e.g., the problem of inferring -dimensional molecular structure from bounds on interatomic distances, the dissimilarities are free to vary, resulting in optimization problems with a spectral objective function. A perturbation analysis is used to compute first- and second-order directional derivatives of this function. The gradient and Hessian are then inferred as representers of the derivatives. This coordinate-free approach reveals the matrix structure of the objective and facilitates writing customized optimization software. Also analyzed is the spectrum of the Hessian of the objective.", (c) 2004 Elsevier B.V. All rights reserved.,"Lewis, RM|Trosset, MW",COMPUTATIONAL STATISTICS & DATA ANALYSIS,classical multidimensional scaling|principal coordinate analysis|distance matrices|distance geometry|spectral decomposition|perturbation analysis,10.1016/j.csda.2004.07.011
257,WOS:000255415000009,2008,Estimation of process parameter variations in a pre-defined process window using a Latin hypercube method,,"The aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. To exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. A sampling plan based on the Latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. Moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. The conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. Consequently, it gives significant insights into the usefulness of individual design parameters. It has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. Furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. Finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",,"Moshfegh, R.|Nilsson, L.|Larsson, M.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,stochastical analysis|sensitivity indicator|admissible process parameter variation|finite element method|sheet-metal forming,10.1007/s00158-007-0136-0
258,WOS:000345470700014,2014,Oxygen blast furnace with CO2 capture and storage at an integrated steel mill-Part I: Technical concept analysis,,"In this study application of OBF with and without CCS to an integrated steel mill is investigated. The study is based on the real, Ruukki Metals Ltd.'s existing steel mill, located in the city of Raahe, Finland. Implications of application of OBF to energy and mass balances at the site are studied. Based on the technical evaluation, costs and feasibility for carbon capture are estimated. The energy and mass balance basis is presented in this first part of the series of two papers. Costs, feasibility and sensitivity analysis are assessed in the second part of the series (Tsupari et al. . Int. J. Greenhouse Gas Control). The impact of applying OBF at an integrated steel mill is evaluated based on a consequential assessment following the methodology of Arasto et al. (). Int. J. Greenhouse Gas Control  (August) pp. - concentrating only on the parts of the steelmaking processes affected by the deployment of OBF and CO capture. The technical processes, CO capture and the steelmaking processes affected were modelled using Aspen Plus process modelling software and the results were used to estimate the CO emission reduction potential with OBF technology at an integrated steel mill. The results show that the CO emission from an iron and steel mill can be significantly reduced by application of an oxygen blast furnace and CCS. By applying only the blast furnace process, the emissions can already be reduced by . Mt/a without storing the separated CO. If captured CO is also purified and stored permanently, the emission can be further reduced by an additional . Mt/a. This is a significant reduction considering that the production of the mill stays the same as in the reference case. In addition to carbon footprint of the production, application of oxygen blast furnace also has significant impact on coke consumption and energy balance on the site. (C) ", Elsevier Ltd. All rights reserved.,"Arasto, Antti|Tsupari, Eemeli|Karki, Janne|Lilja, Jarmo|Sihvonen, Miika",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,ccs|iron and steel industry|oxygen blast furnace|obf|concept evaluation|vacuum pressure swing adsorption|aspen plus,10.1016/j.ijggc.2014.09.004
259,WOS:000243742700004,2007,The Data Uncertainty Engine (DUE): A software tool for assessing and simulating uncertain environmental variables,MODELING ERROR|REJECTION|BETA|PROPAGATION|PREDICTION|POISSON|GAMMA|GIS,"This paper describes a software tool for: () assessing uncertainties in environmental data; and () generating realisations of uncertain data for use in uncertainty propagation analyses: the ""Data Uncertainty Engine (DUE)"". Data may be imported into DUE from file or from a database, and are represented in DUE as objects whose positions and attribute values may be uncertain. Objects supported by DUE include spatial vectors, spatial rasters, time-series of spatial data, simple time-series and objects that are constant in space and time. Attributes supported by DUE include continuous numerical variables (e.g. rainfall), discrete numerical variables (e.g. bird counts) and categorical variables (e.g. land-cover). Once data are imported, an uncertainty model can be developed for the positional and attribute uncertainties of environmental objects. This is currently limited to probability models, but confidence intervals and scenarios will be provided in the future. Using DUE, the spatial and temporal patterns of uncertainty (autocorrelation), as well as cross-correlations between related inputs, can be incorporated into an uncertainty analysis. Alongside expert judgement, sample data may be used to help estimate uncertainties, and to reduce the uncertainty of the simulated output by ensuring each realisation reproduces the sample data. Most importantly, DUE provides a conceptual framework for structuring an uncertainty analysis, allowing users without direct experience of uncertainty methods to develop realistic uncertainty models for their data. (c) ", Elsevier Ltd. All rights reserved.,"Brown, James D.|Heuvelink, Gerard B. M.",COMPUTERS & GEOSCIENCES,uncertainty analysis|monte carlo|uncertainty propagation|java,10.1016/j.cageo.2006.06.015
260,WOS:000260992800007,2009,The OECD software tool for screening chemicals for persistence and long-range transport potential,ORGANIC-CHEMICALS|MULTIMEDIA MODELS|SPATIAL RANGE|UNCERTAINTY|BEHAVIOR|FATE,"We present the software implementation of The OECD P-OV & LRTP Screening Tool (The Tool) that is used to assess the environmental hazard of organic chemicals using metrics of overall persistence (P-OV) and long-range transport potential (LRTP). The Tool is designed to support decision making for chemical management and includes features that are recommended by the Organization for Economic Cooperation and Development (OECD) expert group on multimedia modeling. The Tool is useful for screening the environmental hazard potential of non-ionizing organic chemicals whose environmental partitioning can be described by absorptive capacities of environmental media estimated from partitioning between air, water and octanol in the laboratory. The software includes data storage functionality, and a user interface that is designed to facilitate simple data input and straightforward interpretation of the model results. The effect of uncertainties in input properties describing chemicals can be assessed with a Monte Carol analysis. The software is evaluated and illustrated by comparing results from The Tool with those from other models and by evaluating four substances that are candidates for regulation or ban under the Stockholm convention on Persistent Organic Pollutants. (C) ", Elsevier Ltd. All rights reserved.,"Wegmann, Fabio|Cavin, Laurent|MacLeod, Matthew|Scheringer, Martin|Hungerbuehler, Konrad",ENVIRONMENTAL MODELLING & SOFTWARE,chemicals|persistent organic pollutants|multimedia modeling|hazard assessment|uncertainty analysis|decision support,10.1016/j.envsoft.2008.06.014
261,WOS:000384855300018,2016,On ISSM and leveraging the Cloud towards faster quantification of the uncertainty in ice-sheet mass balance projections,NORTHEAST GREENLAND|MODEL|FLOW|SENSITIVITY|CREEP,"With the Amazon EC Cloud becoming available as a viable platform for parallel computing, Earth System Models are increasingly interested in leveraging its capabilities towards improving climate projections. In particular, faced with long wait periods on high-end clusters, the elasticity of the Cloud presents a unique opportunity of potentially ""infinite"" availability of small-sized clusters running on high-performance instances. Among specific applications of this new paradigm, we show here how uncertainty quantification in climate projections of polar ice sheets (Antarctica and Greenland) can be significantly accelerated using the Cloud. Indeed, small-sized clusters are very efficient at delivering sensitivity and sampling analysis, core tools of uncertainty quantification. We demonstrate how this approach was used to carry out an extensive analysis of ice-flow projections on one of the largest basins in Greenland, the North-East Greenland Glacier, using the Ice Sheet System Model, the public-domain NASA-funded ice-flow modeling software. We show how errors in the projections were accurately quantified using Monte-Carlo sampling analysis on the EC Cloud, and how a judicious mix of high-end parallel computing and Cloud use can best leverage existing infrastructures, and significantly accelerate delivery of potentially ground-breaking climate projections, and in particular, enable uncertainty quantification that were previously impossible to achieve. (C) ", Elsevier Ltd. All rights reserved.,"Larour, E.|Schlegel, N.",COMPUTERS & GEOSCIENCES,polar|ice sheet|modeling|cloud|uncertainty quantification,10.1016/j.cageo.2016.08.007
262,WOS:000399806500014,2016,Virtual exploration of early stage atherosclerosis,LOW-DENSITY-LIPOPROTEIN|MONOCYTE-DERIVED MACROPHAGES|LIPID-LOWERING AGENTS|STATIN THERAPY|LDL|CELL|OXIDATION|ATHEROGENESIS|AUTOANTIBODIES|INFLAMMATION,"Motivation: Biological mechanisms contributing to atherogenesis are multiple and complex. The early stage of atherosclerosis (AS) is characterized by the accumulation of low-density lipoprotein (LDL) droplets, leading to the creation of foam cells (FC). To address the difficulty to explore the dynamics of interactions that controls this process, this study aimed to develop a model of agents and infer on the most influential cell-and molecule-related parameters. Results: FC started to accumulate after six to eight months of simulated hypercholesterolemia. A sensitivity analysis revealed the strong influence of LDL oxidation rate on the risk of FC creation, which was exploited to model the antioxidant effect of statins. Combined with an empirical simulation of the drug ability to decrease the level of LDL, the virtual statins treatment led to reductions of oxidized LDL levels similar to reductions measured in vivo. Availability and Implementation: An Open source software was used to develop the agent-based model of early AS. Two different concentrations of LDL agents were imposed in the intima layer to simulate healthy and hypercholesterolemia groups of 'virtual patients'. The interactions programmed between molecules and cells were based on experiments and models reported in the literature. A factorial sensitivity analysis explored the respective effects of the less documented model parameters as (i) agent migration speed, (ii) LDL oxidation rate and (iii) concentration of autoantibody agents. Finally, the response of the model to known perturbations was assessed by introducing statins agents, able to reduce the oxidation rate of LDL agents and the LDL boundary concentrations. Contact: jerome.noailly@upf.edu Supplementary information: Supplementary data are available at Bioinformatics online.",,"Olivares, Andy L.|Gonzalez Ballester, Miguel A.|Noailly, Jerome",BIOINFORMATICS,,10.1093/bioinformatics/btw551
263,WOS:000260920100004,2008,ON THE SENSITIVITY OF DESIRABILITY FUNCTIONS FOR MULTIRESPONSE OPTIMIZATION,,"Desirability functions have been one of the most important multiresponse optimization technique since the early eighties. Main reasons for this popularity might be counted as the convenience of the implementation of the method and it's availability in many experimental design software packages. Technique itself involves somehow subjective parameters such as the importance coefficients between response characteristics that are used to calculate overall desirability, weights used in determining the shape of each individual response and the size of the specification band of the response. However, the impact of these sensitive parameters on the solution set is mostly uninvestigated. This paper proposes a procedure to analyze the sensitivity of the important characteristic parameters of desirability functions and their impact on pareto-optimal solution set. The proposed procedure uses the experimental design tools on the solution space and estimates a prediction equation on the overall desirability to identify the sensitive parameters. For illustration, a classical desirability example is selected from the literature and results are given along with the discussion.",,"Aksezer, Caglar S.",JOURNAL OF INDUSTRIAL AND MANAGEMENT OPTIMIZATION,desirability functions|parametric sensitivity analysis|multiresponse optimization,10.3934/jimo.2008.4.685
264,WOS:000260806000010,2008,Evaluation of CO2 fluxes from an agricultural field using a process-based numerical model,LAND-USE CHANGE|SOIL CARBON|RESPIRATION|TEMPERATURE|SENSITIVITY|DIOXIDE|SEQUESTRATION|VARIABILITY|EMISSIONS|TRANSPORT,"During , soil CO fluxes, and meteorological and soil variables were measured at multiple locations in a -ha agricultural field in the Sacramento Valley, California, to evaluate the effects of different tillage practices on CO emissions at the field scale. Field scale CO fluxes were then evaluated using the one-dimensional process-based SOILCO module of the HYDRUS-D software package. This model simulates dynamic interactions between soil water contents, temperature, and soil respiration by numerically solving partial-differential water flow (Richards) and heat and CO transport (convection-dispersion) equations using the finite element method. The model assumes that the overall CO production in the soil profile is the sum of soil and plant respiration, whose optimal values are affected by time, depth, water content, temperature, and CO concentration in the soil profile. The effect of each variable is introduced using various reduction functions that multiply the optimal Soil CO production. Our results show that the numerical model could predict CO fluxes across the soil surface reasonably well using soil hydraulic parameters determined from textural characteristics and the HYDRUS-D software default values for heat transport, CO transport and production parameters without any additional calibration. An uncertainty analysis was performed to quantify the effects of input parameters and soil heterogeneity on predicted soil water contents and CO fluxes. Both simulated volumetric water contents and surface CO fluxes show a significant dependency on soil hydraulic properties.", (C) 2008 Elsevier B.V. All rights reserved.,"Buchner, Jens S.|Simunek, Jiri|Lee, Juhwan|Rolston, Dennis E.|Hopmans, Jan W.|King, Amy P.|Six, Johan",JOURNAL OF HYDROLOGY,numerical model|carbon dioxide|uncertainty analysis|soil respiration|soilco2|hydrus-1d,10.1016/j.jhydrol.2008.07.035
265,WOS:000265341800001,2009,GUI-HDMR - A software tool for global sensitivity analysis of complex models,STREET CANYON MODEL|ENVIRONMENTAL-MODELS|RS-HDMR|REPRESENTATIONS|UNCERTAINTY|PARAMETERS|INDEXES|OUTPUT,"The high dimensional model representation (HDMR) method is a set of tools which can be used to construct a fully functional metamodel and to calculate variance based sensitivity indices very efficiently. Extensions to the existing set of random sampling (RS)-HDMR tools have been developed in order to make the method more applicable for complex models with a large number of input parameters as often appear in environmental modelling. The HDMR software described here combines the RS-HDMR tools and its extensions in one Matlab package equipped with a graphical user interface (GUI). This makes the HDMR method easily available for all interested users. The performance of the GUI-HDMR software has been tested in this paper using two analytical test models, the Ishigami function and the Sobol' g-function. In both cases the model is highly non-linear, non-monotonic and has significant parameter interactions. The developed GUI-HDMR software copes very well with the test cases and sensitivity indices of first and second order could be calculated accurately with only low computational effort. The efficiency of the software has also been compared against other recently developed approaches and is shown to be competitive. GUI-HDMR can be applied to a wide range of applications in all fields, because in principle only one random or quasi-random set of input and output values is required to estimate all sensitivity indices up to second order. The size of the set of samples is however dependent on the problem and can be successively increased if additional accuracy is required. A brief description of its application within a range of modelling environments is given. (C) ", Elsevier Ltd. All rights reserved.,"Ziehn, T.|Tomlin, A. S.",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis|high dimensional model representation|random sampling|matlab software|graphical user interface,10.1016/j.envsoft.2008.12.002
266,WOS:000419225500038,2017,Quantifying Roughness Coefficient Uncertainty in Urban Flooding Simulations through a Simplified Methodology,POLYNOMIAL CHAOS|SWMM MODEL|INUNDATION|FLOW|CALIBRATION|PARAMETERS|CATCHMENT|SYSTEMS,"A methodology is presented which can be used in the evaluation of parametric uncertainty in urban flooding simulation. Due to the fact that such simulations are time consuming, the following methodology is proposed: (a) simplification of the description of the physical process; (b) derivation of a training data set; (c) development of a data-driven surrogate model; (d) use of a forward uncertainty propagation scheme. The simplification comprises the following steps: (a) unit hydrograph derivation using a D hydrodynamic model; (b) calculation of the losses in order to determine the effective rainfall depth; (c) flood event simulation using the principle of the proportionality and superposition. The above methodology was implemented in an urban catchment located in the city of Athens, Greece. The model used for the first step of the simplification was FLOW-RD, whereas the well-known SWMM software (US Environmental Protection Agency, Washington, DC, USA) was used for the second step of the simplification. For the training data set derivation, an ensemble of  Unit Hydrographs was derived with the FLOW-RD model. The parameters which were modified in order to produce this ensemble were the Manning coefficients in the two friction zones (residential and urban open space areas). The surrogate model used to replicate the unit hydrograph derivation, using the Manning coefficients as an input, was based on the Polynomial Chaos Expansion technique. It was found that, although the uncertainties in the derived results have to be taken into account, the proposed methodology can be a fast and efficient way to cope with dynamic flood simulation in an urban catchment.",,"Bellos, Vasilis|Kourtis, Ioannis M.|Moreno-Rodenas, Antonio|Tsihrintzis, Vassilios A.",WATER,urban flooding|swmm|flow-r2d|uncertainty|surrogate models|polynomial chaos expansion,10.3390/w9120944
267,WOS:000280656300004,2010,Simulation model for extended double-ended queueing,QUEUES|IMPATIENCE|CUSTOMERS,"The purpose of this paper is to extend traditional double-ended queuing models using a simulation approach. Traditional double-ended queuing models assume that one supply queue should satisfy one demand queue through instantaneous pairing. Inter-arrival time is assumed to follow an exponential distribution, with arrivals to the system assumed to occur just one at a time. However, this assumption is frequently violated in many real-world situations. The pairing or batch size can either be multiple or a random variable, and the pairing processing time can be greater than . Inter-arrival time may follow distributions other than exponential. In some cases bulk arrivals may come at the same time, and pairing is not always guaranteed. Because the analytical approach has enormous difficulties obtaining performance measures under these relaxed situations, a simulation approach for extended double-ended queueing processes is presented. This includes an algorithm to find state probabilities and a newly developed simulation procedure. Using this new procedure, sensitivity analyses of performance measures were performed using various input conditions implemented using ProModel and SimRumnner simulation software. A business case is studied to demonstrate the versatility of the proposed approaches. (c) ", Elsevier Ltd. All rights reserved.,"Kim, Won Kyung|Yoon, K. Paul|Mendoza, Gaston|Sedaghat, Mohammad",COMPUTERS & INDUSTRIAL ENGINEERING,double-ended queue|simulation|state probability|optimization|job placement agency,10.1016/j.cie.2010.04.002
268,WOS:000412253100005,2017,NHPP software reliability model considering the uncertainty of operating environments with imperfect debugging and testing coverage,RANDOM-FIELD ENVIRONMENTS|FAULT-DETECTION RATE|GROWTH-MODELS|COST MODELS|DEPENDENCY,"In this paper, we propose a testing-coverage software reliability model that considers not only the imperfect debugging (ID) but also the uncertainty of operating environments based on a non-homogeneous Poisson process (NHPP). Software is usually tested in a given control environment, but it may be used in different operating environments by different users, which are unknown to the developers. Many NHPP software reliability growth models (SRGMs) have been developed to estimate the software reliability measures, but most of the underlying common assumptions of these models are that the operating environment is the same as the developing environment. But in fact, due to the unpredictability of the uncertainty in the operating environments for the software, environments may considerably influence the reliability and software's performance in an unpredictable way. So when a software system works in a field environment, its reliability is usually different from the theory reliability, and also from all its similar applications in other fields. In this paper, a new model is proposed with the consideration of the fault detection rate based on the testing coverage and examined to cover ID subject to the uncertainty of operating environments. We compare the performance of the proposed model with several existing NHPP SRGMs using three sets of real software failure data based on seven criteria. Improved normalized criteria distance (NCD) method is also used to rank and select the best model in the context of a set of goodness-of-fit criteria taken all together. All results demonstrate that the new model can give a significant improved goodness-of-fit and predictive performance. Finally, the optimal software release time based on cost and reliability requirement and its sensitivity analysis are discussed.", (C) 2017 Elsevier Inc. All rights reserved.,"Li, Qiuying|Pham, Hoang",APPLIED MATHEMATICAL MODELLING,testing coverage|uncertainty|operating environment|imperfect debugging (id)|software reliability growth models (srgms)|non-homogeneous poisson process (nhpp),10.1016/j.apm.2017.06.034
269,WOS:000406818500009,2017,Estimates of plant density of wheat crops at emergence from very low altitude UAV imagery,AERIAL VEHICLE UAV|WINTER-WHEAT|PRECISION AGRICULTURE|SPACING MEASUREMENT|VEGETATION INDEXES|ROW|VISION|RECONSTRUCTION|POPULATION|SYSTEMS,"Plant density is useful variable that determines the fate of the wheat crop. The most commonly used method for plant density quantification is based on visual counting from ground level. The objective of this study is to develop and evaluate a method for estimating wheat plant density at the emergence stage based on high resolution imagery taken from UAV at very low altitude with application to high throughput phenotyping in field conditions. A Sony ILCE alpha L RGB camera with  Mpixels and equipped with a  mm focal length lens was flying aboard an hexacopter at  to  m altitude at about  m/s speed. This allows getting ground resolution between . mm to . mm, while providing -% overlap between images. The camera was looking with  degrees zenith angle in a compass direction perpendicular to the row direction to maximize the cross section viewed of the plants and minimize the effect of the wind created by the rotors. Agisoft photoscan software was then used to derive the position of the cameras for each image. Images were then projected on the ground surface to finally extract subsamples used to estimate the plant density. The extracted images were first classified to separate the green pixels from the background and the rows were then identified and extracted. Finally, image object (group of connected green pixels) was identified on each row and the number of plants they contain was estimated using a Support Vector Machine whose training was optimized using a Particle Swarm Optimization. Three experiments were conducted in Greoux, Avignon and Clermont sites with some variability in the sowing dates, densities, genotypes, flight altitude, and growth stage at the time of the image acquisition. The application of the method on the  samples available over the three sites provides a RMSE and relative RMSE on estimates of . plants/m() and .% with a bias of . plants/m(). However, differences in performances were observed between the three sites, mostly related to the growth stage at the time of the flight. Plants should have between one to two leaves when images are taken. Further, a specific sensitivity analysis shows that the ground resolution of the images should be better than . mm. Finally, the repeatability of the method is good especially when images are taken from similar observational geometries. The current limits and possible improvements of the method proposed are finally discussed.", (C) 2017 Elsevier Inc. All rights reserved.,"Jin, Xiuliang|Liu, Shouyang|Baret, Frederic|Hemerle, Matthieu|Comar, Alexis",REMOTE SENSING OF ENVIRONMENT,plant density|unmanned aerial vehicle|computer vision algorithm|particle swarm optimization (pso)-support vector machine (svm)|winter wheat,10.1016/j.rse.2017.06.007
270,WOS:000356196000011,2015,PUQ; A code for non-intrusive uncertainty propagation in computer simulations,STRENGTH|MAXIMUM|SCIENCE|CHAOS,"We present a software package for the non-intrusive propagation of uncertainties in input parameters through computer simulation codes or mathematical models and associated analysis; we demonstrate its use to drive micromechanical simulations using a phase field approach to dislocation dynamics. The PRISM uncertainty quantification framework (PUQ) offers several methods to sample the distribution of input variables and to obtain surrogate models (or response functions) that relate the uncertain inputs with the quantities of interest (QoIs); the surrogate models are ultimately used to propagate uncertainties. PUQ requires minimal changes in the simulation code, just those required to annotate the QoI(s) for its analysis. Collocation methods include Monte Carlo, Latin Hypercube and Smolyak sparse grids and surrogate models can be obtained in terms of radial basis functions and via generalized polynomial chaos. PUQ uses the method of elementary effects for sensitivity analysis in Smolyak runs. The code is available for download and also available for cloud computing in nanoHUB. PUQ orchestrates runs of the nanoPLASTICITY tool at nanoHUB where users can propagate uncertainties in dislocation dynamics simulations using simply a web browser, without downloading or installing any software. Program summary Program title: PUQ Catalogue identifier: AEWP_v_ Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWP_v_.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.:  No. of bytes in distributed program, including test data, etc.:  Distribution format: tar.gz Programming language: Python, C. Computer: Workstations. Operating system: Linux, Mac OSX. Classification: ., ., ., External routines: SciPy, Matplotlib, hpy Nature of problem: Uncertainty propagation and creation of response surfaces. Solution method: Generalized Polynomial Chaos (gPC) using Smolyak sparse grids. Running time: PUQ performs uncertainty quantification and sensitivity analysis by running a simulation multiple times using different values for input parameters. Its run time will be the product of the run time of the chosen simulation code and the number of runs required to achieve the desired accuracy.", (C) 2015 Elsevier B.V. All rights reserved.,"Hunt, Martin|Haley, Benjamin|McLennan, Michael|Koslowski, Marisol|Murthy, Jayathi|Strachan, Alejandro",COMPUTER PHYSICS COMMUNICATIONS,uncertainty quantification|surrogate model|sensitivity analysis,10.1016/j.cpc.2015.04.011
271,WOS:000237701400007,2006,Kinetic model for the degradation of MTBE by Fenton's oxidation,TERT-BUTYL ETHER|HABER-WEISS REACTION|HYDROGEN-PEROXIDE|AQUEOUS-SOLUTION|HYDROXYL RADICALS|RATE CONSTANTS|ULTRASONIC IRRADIATION|SONOLYTIC DESTRUCTION|ORGANIC-COMPOUNDS|PULSE RADIOLYSIS,"A kinetic model for the degradation of methyl tert-butyl ether (MTBE) in batch reactors with Fenton's oxidation (Fe+/HO) in aqueous solutions was developed. This kinetic model consists of equations accounting for () hydrogen peroxide chemistry in aqueous solution, () iron speciation, and () MTBE oxidation. The mechanisms of MTBE degradation, and the resultant pathways for the formation and degradation of the byproducts, were proposed on the basis of previous studies. A set of stiff nonlinear ordinary differential equations that describe the rate of formation of each species in this batch system was solved using Matlab (R) software. The kinetic model was validated with published experimental data. The degradation of MTBE by Fenton's oxidation is predicted well by the model, as are the formation and degradation of byproducts, especially methyl acetate (MA) and tert-butyl alcohol (TBA). Finally, a sensitivity analysis based on calculating the sum of the squares of the residuals (SSR) after making a perturbation of one rate constant at a time was applied to discern the effect of each reaction on MTBE disappearance.",,"Al Ananzeh, N|Bergendahl, JA|Thompson, RW",ENVIRONMENTAL CHEMISTRY,oxidation|fenton's reagent|kinetic modeling|mtbe|water,10.1071/EN05044
272,WOS:000329561100024,2014,A comprehensive evaluation of various sensitivity analysis methods: A case study with a hydrological model,RAINFALL-RUNOFF MODELS|IMPROVED CALIBRATION|COMPUTER EXPERIMENTS|GLOBAL OPTIMIZATION|REGRESSION|INDEXES|SYSTEMS|OUTPUT,"Sensitivity analysis (SA) is a commonly used approach for identifying important parameters that dominate model behaviors. We use a newly developed software package, a Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), to evaluate the effectiveness and efficiency of ten widely used SA methods, including seven qualitative and three quantitative ones. All SA methods are tested using a variety of sampling techniques to screen out the most sensitive (i.e., important) parameters from the insensitive ones. The Sacramento Soil Moisture Accounting (SAC-SMA) model, which has thirteen tunable parameters, is used for illustration. The South Branch Potomac River basin near Springfield, West Virginia in the U.S. is chosen as the study area. The key findings from this study are: () For qualitative SA methods, Correlation Analysis (CA), Regression Analysis (RA), and Gaussian Process (GP) screening methods are shown to be not effective in this example. Morris One-At-a-Time (MOAT) screening is the most efficient, needing only  samples to identify the most important parameters, but it is the least robust method. Multivariate Adaptive Regression Splines (MARS), Delta Test (DT) and Sum-Of-Trees (SOT) screening methods need about - samples for the same purpose. Monte Carlo (MC), Orthogonal Array (OA) and Orthogonal Array based Latin Hypercube (OALH) are appropriate sampling techniques for them; () For quantitative SA methods, at least  samples are needed for Fourier Amplitude Sensitivity Test (FAST) to identity parameter main effect. McKay method needs about  samples to evaluate the main effect, more than  samples to assess the two-way interaction effect. OALH and LP tau (LPTAU) sampling techniques are more appropriate for McKay method. For the Sobol' method, the minimum samples needed are  to compute the first-order and total sensitivity indices correctly. These comparisons show that qualitative SA methods are more efficient but less accurate and robust than quantitative ones. (C)  The Authors. Published by", Elsevier Ltd. All rights reserved.,"Gan, Yanjun|Duan, Qingyun|Gong, Wei|Tong, Charles|Sun, Yunwei|Chu, Wei|Ye, Aizhong|Miao, Chiyuan|Di, Zhenhua",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification|sensitivity analysis|parameter screening|space-filling sampling|psuade,10.1016/j.envsoft.2013.09.031
273,WOS:000361481300001,2015,Multisite Assessment of Hydrologic Processes in Snow-Dominated Mountainous River Basins in Colorado Using a Watershed Model,CONTERMINOUS UNITED-STATES|LAND-COVER DATABASE|CLIMATE-CHANGE|SWAT MODEL|AUTOMATIC CALIBRATION|SENSITIVITY-ANALYSIS|GLOBAL OPTIMIZATION|VALIDATION|SIMULATIONS|COMPLETION,"Hydrologic fluxes in mountainous watersheds are particularly important as these areas often provide a significant source of freshwater for more arid surrounding lowlands. The state of Colorado in the United States comprises a principal snow catchment area, with all major headwater river basins in Colorado providing substantial water flows to surrounding western and midwestern states. The ability to represent and quantify hydrologic processes controlling the generation and movement of water in headwater basins of Colorado therefore has significant implications for effective management of water resources in the western United States under varying climatic and land-use conditions. In the research reported in this paper, hydrologic modeling was applied to four snow-dominated, mountainous basins of Colorado [i.e.,the river basins of ()Cache la Poudre, ()Gunnison, ()San Juan, and ()Yampa] to evaluate the relevance of specific hydrologic components (i.e.,evapotranspiration, snow processes, groundwater processes, surface runoff, and so on) in the complex, high-elevation watersheds. The soil and water assessment tool (SWAT) model was calibrated and tested for multiple river locations within each basin using monthly naturalized flows over the - period. The model was able to adequately simulate streamflows at all locations within the four basins. Monthly patterns of precipitation, snowfall, evapotranspiration (ET), and total water yield were similar for all the basins, while subsurface lateral flow was the dominant hydrologic pathway, contributing between  and % to gross basin water yields on an average annual basis. Overall, results indicated the strong influence of snowmelt and groundwater processes on amounts and timing of streamflows in the study basins. Hence, enhanced representation of these processes may be essential to improve hydrological estimation using computer software in snowmelt-driven mountainous basins. In particular, examination of monthly streamflow residuals indicated that the normality and independence of model residuals, which are often assumed in parameter estimation and uncertainty analysis, were not always satisfied.",,"Foy, Caleb|Arabi, Mazdak|Yen, Haw|Gironas, Jorge|Bailey, Ryan T.",JOURNAL OF HYDROLOGIC ENGINEERING,watershed modeling|hydrological processes|mountainous watersheds|snow processes|soil and water assessment tool (swat),10.1061/(ASCE)HE.1943-5584.0001130
274,WOS:000262497400004,2008,Fidelity of Network Simulation and Emulation: A Case Study of TCP-Targeted Denial of Service Attacks,,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",,"Chertov, Roman|Fahmy, Sonia|Shroff, Ness B.",ACM TRANSACTIONS ON MODELING AND COMPUTER SIMULATION,simulation|emulation|testbeds|tcp|congestion control|denial of service attacks|low-rate tcp-targeted attacks,10.1145/1456645.1456649
275,WOS:000417388800006,2017,Direct effect of atmospheric turbulence on plume rise in a neutral atmosphere,LARGE-EDDY SIMULATION|DIRECT NUMERICAL-SIMULATION|POLLUTANT DISPERSION|CROSS-FLOW|BOUNDARY-LAYERS|FINITE-ELEMENT|CFD SIMULATION|ENVIRONMENT|TERRAINS|MODELS,"The direct effect of atmospheric turbulence on plume rise in the current research work is studied through examining the turbulence intensity parameter. A hybrid unsteady Reynolds averaged Navier Stokes (RANS) and large eddy simulation (LES) numerical approach is applied with a new mixed scale sub-grid parameterization technique in the commercial ANSYS Fluent software in order to simulate the buoyant plume behavior in a turbulent crossflow. The accuracy of the simulation method is crosschecked against the wind tunnel data available in the literature. The numerical simulation results in various operating conditions are used to derive a new plume rise formula in which the direct effect of atmospheric turbulence intensity at stack height (I-Air) is explicitly introduced in the plume rise formula. Furthermore, the buoyancy parameter of the flue gas is determined at some distances upstream of the stack top surface to include the whole effects of source buoyancy on the plume rise. The value of I-Air at stack height is obtained by measuring the standard deviation of wind velocity at stack height. The sensitivity analysis showed that by increasing the atmospheric turbulence intensity, the final plume rise decreases because of the updraft and downdraft motions of turbulence and it has been found that there is a linear dependency between the plume rise and ( I-Air)(-.). The quantile-quantile plots show that the new model can predict the simulated plume rise with a deviation factor of . whereas the conventional models overestimate the final plume rise at least by a factor of ..", (C) 2017 Turkish National Committee for Air Pollution Research and Control. Production and hosting by Elsevier B.V. All rights reserved.,"Ashrafi, Khosro|Orkomi, Ali Ahmadi|Motlagh, Majid Shafipour",ATMOSPHERIC POLLUTION RESEARCH,neutral|numerical model|plume rise|rans-les method|turbulence,10.1016/j.apr.2017.01.001
276,WOS:000330087600012,2013,COMPARATIVE ANALYSIS OF MUNICIPAL SOLID WASTE SYSTEMS: CRACOW CASE STUDY,MANAGEMENT|TOOL,"The evaluation method to compare municipal solid waste management (MSWM) systems has been presented. The results of the integrated waste management model (IWM-), were used as the input data for the analysis. The results were integrated into life cycle analysis (LCA) impact categories. The authors present possible to calculate categories, and calculate them for the two MSWM scenarios. Next, the system performance was compared using a multicriteria method, called analytic hierarchy process (AHP). The hierarchical preference analysis on the World Wide Web software (Web-HIPRE) was applied to conduct the analysis. The criteria ratios for the AHP were assumed arbitrarily based on the best knowledge of the authors. Finally, the presented sensitivity analysis showed the confidence of the obtained results and pointed out the most important assumptions of the whole analysis. The two Cracow MSWM systems were used as a case study.",,"Stypka, Tomasz|Flaga-Maryanczyk, Agnieszka",ENVIRONMENT PROTECTION ENGINEERING,,10.5277/epe130412
277,WOS:000287575500001,2011,Laboratory and numerical modeling of water balance in a layered sloped soil cover with channel flow pathway over mine waste rock,WETTING FRONT INSTABILITY|ENGINEERED TEST COVERS|CHARACTERISTIC CURVE|UNSATURATED SOILS|OXYGEN BARRIERS|WHISTLE MINE|TAILINGS|INFILTRATION|EVAPORATION|ONTARIO,"Macropores developed in barrier layers in soil covers overlying acid-generating waste rock may produce preferential flow through the barrier layers and compromise cover performance. However, little has been published on the effects of preferential flow on water balance in soil covers. In the current study, an inclined, layered soil cover with a -cm-wide sand-filled channel pathway in a silty clay barrier layer was built over reactive waste rock in the laboratory. The channel or preferential flow pathway represented the aggregate of cracks or fissures that may occur in the barrier during compaction and/or climate-induced deterioration. Precipitation, runoff, interflow, percolation, and water content were recorded during the test. A commercial software VADOSE/W was used to simulate the measured water balance and to conduct further sensitivity analysis on the effects of the location of the channel and the saturated hydraulic conductivity of the channel material on water balance. The maximum percolation, .% of the total precipitation, was obtained when the distance between the mid-point of the channel pathway and the highest point on the slope accounted for % of the total horizontal length of the soil cover. The modeled percolation increased steadily with an increase in the hydraulic conductivity of the channel material. Percolation was found to be sensitive to the location of the channel and the saturated hydraulic conductivity of the channel material, confirming that proper cover design and construction should aim at minimizing the development of vertical preferential flow in barrier layers. The sum of percolation and interflow was relatively constant when the location of the channel changed along the slope, which may be helpful in locating preferential flow pathways and repairing the barrier.",,"Song, Qing|Yanful, Ernest K.",ENVIRONMENTAL EARTH SCIENCES,acid rock drainage|channel flow|laboratory test|soil cover|vadose/w|water balance,10.1007/s12665-010-0488-4
278,WOS:000405292800013,2017,Do green wooden composites using lignin-based binder have environmentally benign alternatives? A preliminary LCA case study in China,LIFE-CYCLE ASSESSMENT|MEDIUM DENSITY FIBERBOARD|KRAFT LIGNIN|POLYETHYLENIMINE|ADHESIVES|LIGNOSULFONATE|POLLUTANTS|INVENTORY|IMPACTS,"Purpose Nowadays, formaldehyde emissions from petroleum-based adhesives contribute considerably to environmental problems and are a constraint to the development of forest-based industries. Although many efforts are being made to develop new lignin-based adhesives for panels, very few studies were carried out via life cycle assessment (LCA). This study aims to assess the life cycle of green wooden composites by using hybrid-modified ammonium lignosulfonate (HMAL) as the binder and investigate the possibility of lignin-based binder to be a good alternative. Methods This study is a step further of the previous work conducted on HMAL as an alternative binder for medium-density fiberboard (MDF) or, in other words, the wooden composite made from HMAL and wood fiber (WF). LCA was carried out to assess the environmental impacts during the life cycle of the new manufacturing process of HMAL/WF production using ReCiPe . Endpoint and IPCC global warming potential (GWP) method built into the GaBi version . software. The production system involved two subsystems: raw material supply and board manufacture. Meanwhile, a comparative LCA of conventional MDF, with three main damage categories and GWP, was also carried out. Results and discussion The hydrogen peroxide (HO) production, electricity, and the HMAL/WF manufacturing stages had the greatest environmental impact. The comparative results pointed out that HMAL/WF production is environmentally superior to conventional MDF in general. Due to the environmental impacts associated with the HMAL binder, a sensitivity analysis was carried out. Suggestions were made for a cleaner production, in which the HO dose was reduced to  wt%. Conclusions HO use, energy, and electricity consumption are main contributors to most impact categories, which help us to find the potential improvements of sustainability, choose the appropriate HMAL technology, and optimize the HMAL/WF system. Feasible production processes and life cycle costs are factors that still need to be studied.",,"Yuan, Yuan|Guo, Minghui",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,ammonium lignosulfonate (al)|hmal/wf production|hydrogen peroxide (h2o2)|life cycle assessment|medium-density fiberboard (mdf),10.1007/s11367-016-1235-1
279,WOS:000295029800004,2011,A simulation of the fate of nitrogen in an on-site wastewater treatment system,DYNAMIC-MODEL|NITRIFICATION|VALIDATION,"An experiment to study and to build a mathematical model to reproduce the behavior of nitrogen in a residential wastewater treatment process was performed. A pilot unit receiving grey and black water was used. The pilot consisted in a septic tank followed by a fixed-film, partly aerated bioreactor with effluent recirculation operated under two different scenarios: normal operating conditions and increased influent flow. Modeling was performed with the GPS-X (TM) software. Following a sensitivity analysis, the model was calibrated by comparing results from the pilot experiment and those of the simulation predictions. Obtained results show that the studied pilot unit is able to eliminate most of the ammonia contained in the influent, except for days with exceptionally high influent concentrations. Chemical oxygen demand (COD) and total suspended solids (TSS) reduction is also near complete and a partial denitrification is present. The calibrated model shows a good agreement with results obtained in the pilot unit, although work remains to be done for nitrates. Modeling of residential wastewater treatment appears to be a useful tool for understanding, optimizing and predicting the expected performances of such a system.",,"Bernier, Jean|Lessard, Paul|Girard, Martin",WATER QUALITY RESEARCH JOURNAL OF CANADA,biological treatment|modeling|nitrogen removal|on-site treatment,10.2166/wqrjc.2011.028
280,WOS:000270369500007,2009,Probabilistic Assessment Study of Channels Downstream Slopes Erosion in the Maritime Environment,,"This research deals with the probabilistic simulation and assessment of erosion in the downstream maritime slopes in hop ports (ports with deep approach channels to be able to accommodate the recent vessels generations) with natural side slopes. The study concentrated on the liquefaction effect in the erosion factor, which is the main controllable parameter for this phenomena. The probability of failure for the limit state function represents the erosion factor, which has a liable representation by a normal distribution with parameters mu = . and sigma = ., as a representative limit state function. This research deals with a maritime channel with certain dimensions as an example. The probabilistic simulations for downstream slope erosion were carried out using the Monte Carlo technique by using a probabilistic model. The generated probabilistic histograms of the erosion factor based on one run and different numbers of simulated random samples were determined. Based on these reliability simulation results, the erosion volumes per unit width of the channel were evaluated. Validation and sensitivity analyses were also carried out to ensure more reliability for this research. The study produced a group of guiding regression models for estimates and the determined conclusions related to the evaluated erosion volumes we carefully examined by considering calculation conditions based on a % confidence level with different assumptions. Then preliminary estimates for the eroded volumes (m()/m) in the downstream slope of the channel were evaluated and so used to determine the relevant regression models. These distributions were determined based on a group of assumed realistic conditions, which include variable berm depths and constant downstream slope angles in one simulated group with erosion volumes against downstream slopes depths heights variation and constant berm depths and variable slope angles in another with erosion volumes against downstream slopes angles variation. The limit state functions representing the erosion volumes variation behavior under the different conditions were also determined by using reliable statistical goodness-of-fit software. The research results are presented in a graphical form for the purpose of improving the current application capabilities in the subject and providing practical usage for the unprotected maritime navigation channel, trenches, and maritime downstream slopes.",,"Khalifa, M. A.|El Ganainy, M. A.|Nasr, R. I.",JOURNAL OF COASTAL RESEARCH,maritime downstream slopes|hop ports approach channels|downstream erosion factor|downstream slopes liquefaction|probabilistic simulations|monte carlo simulation|probabilistic slope failure|sensitivity analysis|downstream erosion volumes|limit state functions,10.2112/08-1074.1
281,WOS:000344387000014,2014,Non-stationary extreme value analysis in a changing climate,DIFFERENTIAL EVOLUTION|MODEL PROJECTIONS|RETURN LEVELS|SIMULATIONS|EVENTS|TEMPERATURE|VARIABILITY|ENSEMBLE|IMPACTS|RECORDS,"This paper introduces a framework for estimating stationary and non-stationary return levels, return periods, and risks of climatic extremes using Bayesian inference. This framework is implemented in the Non-stationary Extreme Value Analysis (NEVA) software package, explicitly designed to facilitate analysis of extremes in the geosciences. In a Bayesian approach, NEVA estimates the extreme value parameters with a Differential Evolution Markov Chain (DE-MC) approach for global optimization over the parameter space. NEVA includes posterior probability intervals (uncertainty bounds) of estimated return levels through Bayesian inference, with its inherent advantages in uncertainty quantification. The software presents the results of non-stationary extreme value analysis using various exceedance probability methods. We evaluate both stationary and non-stationary components of the package for a case study consisting of annual temperature maxima for a gridded global temperature dataset. The results show that NEVA can reliably describe extremes and their return levels.",,"Cheng, Linyin|AghaKouchak, Amir|Gilleland, Eric|Katz, Richard W.",CLIMATIC CHANGE,,10.1007/s10584-014-1254-5
282,WOS:000254878500014,2008,In silico Biochemical Reaction Network Analysis (IBRENA): a package for simulation and analysis of reaction networks,SIGNAL-TRANSDUCTION|SOFTWARE,"We present In silico Biochemical Reaction Network Analysis (IBRENA), a software package which facilitates multiple functions including cellular reaction network simulation and sensitivity analysis (both forward and adjoint methods), coupled with principal component analysis, singular-value decomposition and model reduction. The software features a graphical user interface that aids simulation and plotting of in silico results. While the primary focus is to aid formulation, testing and reduction of theoretical biochemical reaction networks, the program can also be used for analysis of high-throughput genomic and proteomic data.",,"Liu, Gang|Neelamegham, Sriram",BIOINFORMATICS,,10.1093/bioinformatics/btn061
283,WOS:000338551800010,2014,Risk and uncertainty analysis of gas pipeline failure and gas combustion consequence,MODELS|DETONATION|METHANE,"Taking into account a general concept of risk parameters and knowing that natural gas provides very significant portion of energy, firstly, it is important to insure that the infrastructure remains as robust and reliable as possible. For this purpose, authors present available statistical information and probabilistic analysis related to failures of natural gas pipelines. Presented historical failure data is used to model age-dependent reliability of pipelines in terms of Bayesian methods, which have advantages of being capable to manage scarcity and rareness of data and of being easily interpretable for engineers. The performed probabilistic analysis enables to investigate uncertainty and failure rates of pipelines when age-dependence is significant and when it is not relevant. The results of age-dependent modeling and analysis of gas pipeline reliability and uncertainty are applied to estimate frequency of combustions due to natural gas release when pipeline failure occurs. Estimated age-dependent combustion frequency is compared and proposed to be used instead of conservative and age-independent estimate. The rupture of a high-pressure natural gas pipeline can lead to consequences that can pose a significant threat to people and property in the close vicinity to the pipeline fault location. The dominant hazard is combustion and thermal radiation from a sustained fire. The second purpose of the paper is to present the combustion consequence assessment and application of probabilistic uncertainty analysis for modeling of gas pipeline combustion effects. The related work includes performance of the following tasks: to study gas pipeline combustion model, to identify uncertainty of model inputs noting their variation range, and to apply uncertainty and sensitivity analysis for results of this model. The performed uncertainty analysis is the part of safety assessment that focuses on the combustion consequence analysis. Important components of such uncertainty analysis are qualitative and quantitative analysis that identifies the most uncertain parameters of combustion model, assessment of uncertainty, analysis of the impact of uncertain parameters on the modeling results, and communication of the results' uncertainty. As outcome of uncertainty analysis the tolerance limits and distribution function of thermal radiation intensity are given. The measures of uncertainty and sensitivity analysis were estimated and outcomes presented applying software system for uncertainty and sensitivity analysis. Conclusions on the importance of the parameters and sensitivity of the results are obtained using a linear approximation of the model under analysis. The outcome of sensitivity analysis confirms that distance from the fire center has the greatest influence on the heat flux caused by gas pipeline combustion.",,"Alzbutas, Robertas|Iesmantas, Tomas|Povilaitis, Mantas|Vitkute, Jurate",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,risk parameters|natural gas pipelines|age-dependent reliability|bayesian inference|gas combustion|uncertainty and sensitivity analysis,10.1007/s00477-013-0845-4
284,WOS:000239334600007,2006,"Computer simulation model for determining reclamation liability costs of the Ekati Diamond Mine (TM) in the Northwest Territories, Canada",,"Estimation of closure liabilities for financial reporting and security deposits in the mining industry is often based on past estimates or deterministic models. These methods are limited in their ability to account for the variability of contributing costing factors. In , Komex International Ltd. developed a Monte Carlo simulation (MCS) model to provide a best estimate for the reclamation liabilities of the Ekati Diamond Mine (TM) (BHP Billiton Diamonds Inc.). The model was built upon Decisioneering's((R)) Crystal Ball risk analysis and forecasting software to simulate the various facets of the mine's closure plans and reclamation activities. Reclamation activities included decommissioning, demolition, site remediation, reclamation and post closure monitoring for seven mine pits and related infrastructures over four time periods. The model predicted the median, th and th percentile costs and other statistical measures. A sensitivity analysis was also conducted to identify the most significant cost contributors. This type of model offers many advantages for companies in determining probable costs of future environmental activities. Through the identification of cost contributors, work scenarios, computer code and modeling software, a MCS model can simulate a wide range of probable costs and scenarios. Probabilistic models are more effective at incorporating risk and uncertainty in liability estimates than deterministic estimation tools that rely heavily on global contingencies to account for risk.", (c) 2005 Published by Elsevier Ltd.,"McKay, Sharon|Funk, Wes|Rimbey, Shawn|Butler, Helen",JOURNAL OF CLEANER PRODUCTION,liability|monte carlo|probabilistic|model|simulation|risk analysis,10.1016/j.jclepro.2004.10.006
285,WOS:000288120800003,2011,Mixed municipal waste management in the Czech Republic from the point of view of the LCA method,LIFE-CYCLE ASSESSMENT|SOLID-WASTE|SYSTEMS|SCENARIOS|SPAIN|MODEL,"Purpose This paper presents the results of a life-cycle assessment (LCA) study for integrated systems (IS) of mixed municipal waste (MMW) management in the Czech Republic. The seven IS categories assessed were: (a) incineration with slag recovery, (b) incineration without slag recovery, (c) landfills with incineration of the landfill gas by flaring, (d) landfills with recovery of the landfill gas, (e) mechanical-biological treatment (MBT) with aerobic treatment, (f) MBT biodrying with co-incineration of refuse-derived fuel, and (g) MBT biodrying with incineration of refuse-derived fuel from a monosource. Methods The environmental impacts were evaluated using the CML  methodological approach. The methodology from EDIP  was used for performing the sensitivity analysis on the selection of the methodologies for characterization. The treatment of  t of MMW was the functional unit selected. Data was collected from both within the Czech Republic (for incineration plants and landfills), as well as from abroad (for the MBTs). The IS assessed were modelled on the basis of available data and using the best processes and data available from the LCA software. Results and discussion We established that the integrated system of mixed municipal waste management (IS) of landfills without energy recovery of the landfill gas, as well as the aerobic MBT have the highest environmental impacts. On the other hand, the lowest environmental impacts were found for the MBT biodrying IS. An overall assessment of this IS, both with and without the toxicity and ecotoxicity impact category pollutants and emissions indicators, were compared. Conclusions A comparison of the environmental impacts of IS landfills to the other IS categories should be made, using both a detailed and long-term inventory. Further, this should also include the closures of the landfill sites, as well as all of the future environmental impacts. It would also be appropriate to include several additional aspects (such as social, technical, and economic factors) for a fully objective assessment and in making the optimal choice of an IS.",,"Koci, Vladimir|Trecakova, Tatiana",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,incineration|landfilling|landfills|life-cycle assessment|mechanical-biological treatment|mixed municipal waste,10.1007/s11367-011-0251-4
286,WOS:000325733400001,2013,APPLICATION OF SENSITIVITY ANALYSIS FOR ASSESSMENT OF ENERGY AND ENVIRONMENTAL ALTERNATIVES IN THE MANUFACTURE BY USING ANALYTIC HIERARCHY PROCESS,CLEANER PRODUCTION|PAPER-MILL|INDUSTRY|IRAN,"Multi-criteria decision making (MCDM) was used to make comparative analysis of projects or heterogeneous measures for prioritization criteria and subcriteria simultaneously in a complex situation. The aim of this paper is to determine the alternatives and the sensitivity of main factors affecting water and energy consumption as well as environmental impact in a recycled paper manufacturing by using analytic hierarchy process (AHP). The AHP enables one to consider all the elements of decision process in a model, and to compare criteria and subcriteria of the model to find the best alternative. The AHP technique is applied through specific software package with user-friendly interfaces called Expert Choice. The results indicated that reduction of water consumption is the most important alternative for sustainable development in a recycled paper mill in Iran. Also, good housekeeping is the most sensitive criterion affecting the alternatives. The paper illustrates how the AHP method can help industrial management to overcome the energy usage and environmental impact in the manufacture.",,"Ghorbannezhad, Payam|Azizi, Majid|Ray, Charles|Yoo, Changkyoo|Ramazani, Omid",ENVIRONMENT PROTECTION ENGINEERING,,10.5277/epe130301
287,WOS:000276253100012,2010,Numerical analysis of the unsteady flow in the near-tongue region in a volute-type centrifugal pump for different operating points,PRESSURE-FLUCTUATIONS|IMPELLER|PULSATIONS|FREQUENCY,"An investigation is presented on the unsteady flow behaviour near the tongue region of a single-suction volute-type centrifugal pump with a specific speed of .. For this study, the flow through the test pump, which was available at laboratory, was simulated by means of a commercial CFD software that solved the Navier-Stokes equations for three-dimensional unsteady flow (D-URANS). A sensitivity analysis of the numerical model was performed in order to impose appropriate parameters regarding grid size, time step size and turbulence model. The predictions of the numerical model were contrasted with experimental results of both global (flow-head curve and static pressure distribution at volute front side) and unsteady variables (unsteady pressure distribution at the volute front side filtered at the blade-passing frequency). Once validated, the model was used to study the flow pulsations associated to the interaction between the impeller blades and the volute tongue as a function of the flow rate, for several flow rates ranging from % to % of the nominal flow rate. The study allowed relating the blade passage with the pulsations of pressure and tangential and radial velocity at a number of reference locations in the near-tongue region. The numerical model was also used to evaluate the evolution of the leakage flow between the impeller-tongue gap and of the flow exiting the impeller through some specific angular intervals, during one single-blade passage. (C) ", Elsevier Ltd. All rights reserved.,"Barrio, Raul|Parrondo, Jorge|Blanco, Eduardo",COMPUTERS & FLUIDS,centrifugal pump|numerical simulation|unsteady flow|rotor-stator interaction|blade-passing frequency,10.1016/j.compfluid.2010.01.001
288,WOS:000259895700004,2008,Design of sustainable chemical processes: Systematic retrofit analysis generation and evaluation of alternatives,EFFICIENCY|NETWORKS,"The objective of this paper is to present a generic and systematic methodology for identifying the feasible retrofit design alternatives of any chemical process. The methodology determines a set of mass and energy indicators from steady-state process data, establishes the operational and design targets, and through a sensitivity-based analysis, identifies the design alternatives that can match a set of design targets. The significance of this indicator-based method is that it is able to identify alternatives, where one or more performance criteria (factors) move in the same direction thereby eliminating the need to identify tradeoff-based solutions. These indicators are also able to reduce (where feasible) a set of safety indicators. An indicator sensitivity analysis algorithm has been added to the methodology to define design targets and to generate sustainable process alternatives. A computer-aided tool has been developed to facilitate the calculations needed for the application of the methodology. The application of the indicator-based methodology and the developed software are highlighted through a process flowsheet for the production of vinyl chlorine monomer (VCM). (C)  The Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Carvalho, Ana|Gani, Rafiqul|Matos, Henrique",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,process design|sustainability metrics|mass and energy indicators|safety index|indicator sensitivity algorithm,10.1016/j.psep.2007.11.003
289,WOS:000249895700008,2007,Automatic concept model generation for optimisation and robust design of passenger cars,,"A fully automated method of structural optimisation for the body in white structure is presented. The body in white is a technical term for the car body without windows and closures. The iterations in the optimisation loop comprise the following steps: fully parameterised design creation, automated meshing and model assembly, parallel computation and evaluation. For this purpose several free and commercially available software applications were combined, including: SFE concept, Hypermesh, Perl, Matlab, and Radioss. The optimisation was conducted using Genetic Algorithms (GA), which are ideally suited to solve problems with solution spaces that are too large to be exhaustively searched. The viability of the method is demonstrated for a vehicle component model of a front bumper system utilizing both material and geometry related properties as design variables. (c) ", Elsevier Ltd. and Civil-Comp Ltd. All rights reserved.,"Hilmann, J.|Paas, M.|Haenschke, A.|Vietor, T.",ADVANCES IN ENGINEERING SOFTWARE,vehicle engineering|structural optimisation|sfe concept|genetic algorithms|finite element method|parametric modelling|sensitivity analysis,10.1016/j.advengsoft.2006.08.031
290,WOS:000254090100016,2008,Numerical simulation of 3D bubbles rising in viscous liquids using a front tracking method,FREE-BOUNDARY PROBLEMS|GAS BUBBLE|MULTIPHASE FLOW|REYNOLDS-NUMBER|SURFACE-TENSION|FLUID|VOLUME|RISE|DYNAMICS|DEFORMATION,"The rise of bubbles in viscous liquids is not only a very common process in many industrial applications, but also an important fundamental problem in fluid physics. An improved numerical algorithm based on the front tracking method, originally proposed by Tryggvason and his co-workers, has been validated against experiments over a wide range of intermediate Reynolds and Bond numbers using an axisymmetric model [J. Hua, J. Lou, Numerical simulation of bubble rising in viscous liquid, J. Comput. Phys.  () -]. In the current paper, this numerical algorithm is further extended to simulate D bubbles rising in viscous liquids with high Reynolds and Bond numbers and with large density and viscosity ratios representative of the common air-water two-phase flow system. To facilitate the D front tracking simulation, mesh adaptation is implemented for both the front mesh on the bubble surface and the background mesh. On the latter mesh, the governing Navier-Stokes equations for incompressible, Newtonian flow are solved in a moving reference frame attached to the rising bubble. Specifically, the equations are solved using a finite volume scheme based on the Semi-Implicit Method for Pressure-Linked Equations (SIMPLE) algorithm, and it appears to be robust even for high Reynolds numbers and high density and viscosity ratios. The D bubble surface is tracked explicitly using an adaptive, unstructured triangular mesh. The numerical model is integrated with the software package PARAMESH, a block-based adaptive mesh refinement (AMR) tool developed for parallel computing. PARAMESH allows background mesh adaptation as well as the solution of the governing equations in parallel on a supercomputer. Further, Peskin distribution function is applied to interpolate the variable values between the front and the background meshes. Detailed sensitivity analysis about the numerical modeling algorithm has been performed. The current model has also been applied to simulate a number of cases of D gas bubbles rising in viscous liquids, e.g. air bubbles rising in water. Simulation results are compared with experimental observations both in aspect of terminal bubble shapes and terminal bubble velocities. In addition, we applied this model to simulate the interaction between two bubbles rising in a liquid, which illustrated the model's capability in predicting the interaction dynamics of rising bubbles.", (C) 2007 Elsevier Inc. All rights reserved.,"Hua, Jinsong|Stene, Jan F.|Lin, Ping",JOURNAL OF COMPUTATIONAL PHYSICS,computational fluid dynamics|incompressible flow|multiphase flow|bubble rising|simple algorithm|front tracking method|adaptive mesh refinement|moving reference frame,10.1016/j.jcp.2007.12.002
291,WOS:000344530400002,2014,An SMT Based Method for Optimizing Arithmetic Computations in Embedded Software Code,PROGRAMS|EXAMPLES,"We present a new method for optimizing the source code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of the satisfiability modulo theory (SMT) solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on a new inductive synthesis procedure, which is guaranteed to find a valid solution as long as such solution exists. Furthermore, we propose an incremental optimization procedure, which applies the synthesis procedure only to small code regions-one at a time-as opposed to the entire program, which is crucial for scaling the method up to programs of realistic size and complexity. We have implemented our new method in a software tool based on the Clang/LLVM compiler frontend and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and digital signal processing applications, show that the method is both effective and efficient in optimizing arithmetic computations in embedded software code.",,"Eldib, Hassan|Wang, Chao",IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS,fixed point arithmetic|inductive program synthesis|satisfiability modulo theory (smt) solver|superoptimization,10.1109/TCAD.2014.2341931
292,WOS:000295765200012,2011,Modelling nitrogen transformations in waters receiving mine effluents,STABILIZATION PONDS|WASTE-WATER|POTENTIAL DENITRIFICATION|BIOLOGICAL TREATMENT|N-TRANSFORMATION|ORGANIC-MATTER|LAKE ECOSYSTEM|SHALLOW LAKES|DUCKWEED POND|RIVER-BASINS,"This paper presents a biogeochemical model developed for a clarification pond receiving ammonium nitrogen rich discharge water from the Boliden concentration plant located in northern Sweden. Present knowledge about nitrogen (N) transformations in lakes is compiled in a dynamic model that calculates concentrations of the six N species (state variables) ammonium-N (N(am)), nitrate-N (N(ox)), dissolved organic N in water (N(org)), N in phytoplankton (N(pp)), in macrophytes (N(mp)) and in sediment (N(sed)). It also simulates the rate of  N transformation processes occurring in the water column and sediment as well as water-sediment and water-atmosphere interactions. The model was programmed in the software Powersim using  data, whilst validation was performed using data from  to . The sensitivity analysis showed that the state variables are most sensitive to changes in the coefficients related to the temperature dependence of the transformation processes. A six-year simulation of N(am) showed stable behaviour over time. The calibrated model rendered coefficients of determination (R()) of ., . and . for N(am), N(ox) and N(org) respectively. Performance measures quantitatively expressing the deviation between modelled and measured data resulted in values close to zero, indicating a stable model structure. The simulated denitrification rate was on average five times higher than the ammonia volatilisation rate and about three times higher than the permanent burial of N(sed) and, hence, the most important process for the permanent removal of N. The model can be used to simulate possible measures to reduce the nitrogen load and, after some modification and recalibration, it can be applied at other mine sites affected by N rich effluents.", (C) 2011 Elsevier B.V. All rights reserved.,"Chlot, Sara|Widerlund, Anders|Siergieiev, Dmytro|Ecke, Frauke|Husson, Eva|Ohlander, Bjorn",SCIENCE OF THE TOTAL ENVIRONMENT,process water|ammonium|natural removal|biogeochemical modelling|northern sweden|boliden,10.1016/j.scitotenv.2011.07.024
293,WOS:000241005500006,2006,Distance-based and stochastic uncertainty analysis for multi-criteria decision analysis in Excel using Visual Basic for Applications,SENSITIVITY-ANALYSIS|GENETIC ALGORITHMS|WATER-RESOURCES|MANAGEMENT|SOFTWARE|SUPPORT,"A program has been developed in Excel and written in Visual Basic for Applications, which enables a decision maker to examine the robustness of a solution obtained when using multi-criteria decision analysis (MCDA). The distance-based and stochastic uncertainty analysis approaches contained in the program allow a decision to be made with confidence that the alternative chosen is the best performing alternative under the range of probable circumstances. The uncertainty analysis methodology overcomes the limitations of existing sensitivity analysis techniques for MCDA by enabling all of the input parameters to be varied simultaneously within their expected ranges. The Weighted Sum Method (WSM) and PROMETHEE are the MCDA techniques available for the user to select in the program. The program is illustrated by applying it to a sustainable water resource development problem in the Northern Adelaide Plains, South Australia. (c) ", Elsevier Ltd. All fights reserved.,"Hyde, K. M.|Maier, H. R.",ENVIRONMENTAL MODELLING & SOFTWARE,multi-criteria decision analysis|visual basic|uncertainty analysis|excel|water resources,10.1016/j.envsoft.2005.08.004
294,WOS:000355870800008,2015,Stability analysis of rotary power flow controller,FACTS DEVICES|SYSTEMS,"This paper deals with the stability analysis of a rotary power flow controller (RPFC). RPFC is a newly presented flexible alternating current transmission system device based on the rotary phase shifting transformer (RPST) configuration. This device comprises a shunt and a series transformer and two RPSTs. In this paper, a linear model is developed for RPFC in the dq-reference frame. The model includes two RPSTs, shunt, and series transformers. Stability of the device is studied using eigenvalues of the state matrix. Effects of system parameters such as resistance, reactance, and phase difference of the shunt and series transformers and RPSTs on stability are investigated using eigenvalue sensitivity analysis. Simulation results using PSCAD/EMTDC software and analytical results based on eigenvalue analysis are presented to evaluate effects of different parameters on stability of RPFC."," Copyright (c) 2014 John Wiley & Sons, Ltd.","Khayami, Mohammad Tolue|Shayanfar, Heidarali|Kazemi, Ahad",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,dynamic stability|eigenvalue analysis|rotary power flow controller (rpfc)|stability analysis|small-signal stability,10.1002/jnm.2025
295,WOS:000174593500001,2002,Use of the most likely failure point method for risk estimation and risk uncertainty analysis,,"The most likely failure point (MLFP) method, developed within the field of structural reliability analysis (where it is known as the FORM/SORM method) is a technique for estimating the risk (probability) that a calculated quantity Q exceeds a set limit Q(lim) when some or all of the inputs to the calculation are uncertain. It can be used as an efficient stand-alone method for this type of risk calculation. However, for application within the field of toxic hazards, it is proposed as a means for performing sensitivity analyses, possibly in parallel with a risk calculation carried out by conventional methods. The basis of the method is outlined and its use is demonstrated by means of an example calculation of the risk arising froth an installation containing chlorine. The calculation uses, as a consequence model, commercial software for the prediction of dense gas transport. The risk estimate is shown to be acceptably close to that obtained by the Monte Carlo method. The use of a proposed screening procedure utilising the sensitivity formulas that the method provides, in order to identify the most significant uncertainties, is demonstrated. The identification of a single set of input values containing sufficient information to summarise (at least approximately) the entire risk analysis is considered to be an important feature of the method and is proposed as the basis of a means for assessing the validity of the consequence model.", (C) 2002 Elsevier Science B.V All rights reserved.,"Mitchell, B",JOURNAL OF HAZARDOUS MATERIALS,risk|toxic|hazard|uncertainty|sensitivity,10.1016/S0304-3894(01)00378-8
296,WOS:000183089700001,2003,Direct treatment of uncertainty: I - Applications in aquatic invertebrate risk assessment and soil metabolism for chlorpyrifos,RESPONSE VARIABILITY|DYNAMIC-RESPONSE|ACUTE TOXICITY|SYSTEMS|MODELS,"Environmental processes are wrought with uncertainty. Therefore, an efficient means to propagate uncertainty is advantageous, especially if regulatory decisions are based upon any research or data analysis where uncertainty is present. The Deterministic Equivalent Modeling Method (DEMM) propagates parametric uncertainties in model input parameters to output predictions. DEMM is used to calculate uncertainty in output parameters based upon the direct effect of every uncertain input parameter. Rather than sampling input distributions and running hundreds or thousands of model calculations as in Monte Carlo or Latin Hypercube Sampling, DEMM carries a representation of each distribution throughout the calculation of the dependent variable. An overview of DEMM is provided. Once DEMM algorithms are established using symbolic mathematical software program(s), and the dependent variable expansion hypothesized, then the additional overhead required to set up and solve algebraic or differential systems is small. Examples of DEMM using literature values for chlorpyrifos (a widely used insecticide) effects and fate illustrate DEMM's capability for uncertainty propagation. Determination of chlorpyrifos risk quotients for invertebrates (algebraic system) and chlorpyrifos metabolic fate in soil (differential equation system) are presented. These examples illustrate DEMM methodology on problems of interest in environmental fate and risk assessment. Multiple data sets and field/laboratory observations for chlorpyrifos were assembled and utilized with DEMM to propagate uncertainty in output predictions. Chlorpyrifos environmental fate (environmental degradation and metabolite formation/degradation) and risk for aquatic invertebrates, with uncertainty characterized using DEMM are discussed.",,"Cryer, SA|Applequist, GE",ENVIRONMENTAL ENGINEERING SCIENCE,deterministic equivalent modeling method|demm|uncertainty|chlorpyrifos|soil metabolism|risk quotient,10.1089/109287503321671375
297,WOS:000341218800002,2014,An environmental assessment system for environmental technologies,LIFE-CYCLE ASSESSMENT|WASTE MANAGEMENT-SYSTEMS|SOLID-WASTE|LCA|INCINERATION|COLLECTION|EASEWASTE|MODEL,"A new model for the environmental assessment of environmental technologies, EASETECH, has been developed. The primary aim of EASETECH is to perform life-cycle assessment (LCA) of complex systems handling heterogeneous material flows. The objectives of this paper are to describe the EASETECH framework and the calculation structure. The main novelties compared to other LCA software are as follows. First, the focus is put on material flow modelling, as each flow is characterised as a mix of material fractions with different properties and flow compositions are computed as a basis for the LCA calculations. Second, the tool has been designed to allow for the easy set-up of scenarios by using a toolbox, the processes within which can handle heterogeneous material flows in different ways and have different emission calculations. Finally, tools for uncertainty analysis are provided, enabling the user to parameterise systems fully and propagate probability distributions through Monte Carlo analysis. (C) ", Elsevier Ltd. All rights reserved.,"Clavreul, Julie|Baumeister, Hubert|Christensen, Thomas H.|Damgaard, Anders",ENVIRONMENTAL MODELLING & SOFTWARE,easetech|life cycle assessment|waste|lca model|uncertainty|flow modelling,10.1016/j.envsoft.2014.06.007
298,WOS:000231963300008,2005,Parameterization based shape optimization: theory and practical implementation aspects,DESIGN SENSITIVITY-ANALYSIS|TOPOLOGY OPTIMIZATION|MECHANICAL SYSTEMS|EFFICIENT|HOMOGENIZATION|TOOL,"Purpose - To present an approach to parameterization based shape optimization of statically loaded structures and to propose its practical implementation. Design/methodology/approach - In order to establish a convenient shape parameterization, the design element technique is employed. A rational Bezier body is used to serve as the design element. The design element is used to retrieve the nodal geometrical data of finite elements (FEs). Their field geometrical data are obtained using the FE own internal functions. For practical implementation it is proposed to establish the optimization cycle by two separately running processes. The data exchange is established by using self-descriptive and platform-independent XML conforming data files. Findings - The proposed approach offers an unified approach to shape optimization of skeletal, as well as continuous structures. Structural shape may be varied smoothly with a relative small set of design variables. The employment of a gradient-based optimization algorithm assures computational efficiency. Research limitations/implications - The aspects of FE mesh deterioration are not considered in this work. This would be necessary if for the actual problem at hand major and excessively non-uniform shape changes of the FE mesh are expected. Practical implications - A useful source of information for someone who is planning to develop a general or special-purpose integrated structural analysis and shape optimization software. Originality/value - The paper offers a rather simple, but quite powerful approach to structural shape optimization together with practical hints for its computational implementation.",,"Kegl, M",ENGINEERING COMPUTATIONS,optimization techniques|structures|numerical analysis,10.1108/02644400510603041
299,WOS:000278842000032,2010,Sensitivity analysis of release time of software reliability models incorporating testing effort with multiple change-points,RESOURCE-ALLOCATION|GROWTH-MODEL|PREDICTION|SYSTEMS|UNCERTAINTY|INDEXES|COST,"To accurately model software failure process with software reliability growth models, incorporating testing effort has shown to be important. In fact, testing effort allocation is also a difficult issue, and it directly affects the software release time when a reliability criteria has to be met. However, with an increasing number of parameters involved in these models, the uncertainty of parameters estimated from the failure data could greatly affect the decision. Hence, it is of importance to study the impact of these model parameters. In this paper, sensitivity of the software release time is investigated through various methods, including one-factor-at-a-time approach, design of experiments and global sensitivity analysis. It is shown that the results from the first two methods may not be accurate enough for the case of complex nonlinear model. Global sensitivity analysis performs better due to the consideration of the global parameter space. The limitations of different approaches are also discussed. Finally, to avoid further excessive adjustment of software release time, interval estimation is recommended for use and it can be obtained based on the results from global sensitivity analysis.", (c) 2010 Elsevier Inc. All rights reserved.,"Li, Xiang|Xie, Min|Ng, Szu Hui",APPLIED MATHEMATICAL MODELLING,software reliability|release time|sensitivity analysis|testing effort|multiple change-points,10.1016/j.apm.2010.03.006
300,WOS:000229363800003,2005,Modeling distributed software defect removal effectiveness in the presence of code churn,,"Two types of discrete defect removal models that consider the dynamics of code churn behavior during software testing phases under distributed software development environment are proposed. The first model is based on a sequential debugging process, while the second model is based on an iterative debugging process during each testing phase. The mathematical relationship between the number of defects detected during a testing phase and the total estimated remaining defects at the end of the same testing phase for both models are elaborated in detail. The defect detection ratio is identified to have the greatest contribution to the variance of the estimated number of remaining defects based on the sensitivity analysis using Monte-Carlo simulation. Using the proposed models, we quantitatively show how to estimate the number of defects by varying the defect detection ratio, defect correction ratio, the percentage of added code, and the percentage of deleted code. (c) ", Elsevier Ltd. All rights reserved.,"Tian, L|Noore, A",MATHEMATICAL AND COMPUTER MODELLING,defect removal|distributed environment|imperfect debugging,10.1016/j.mcm.2004.10.021
301,WOS:000333204200003,2014,Fusion of conflicting information for improving representativeness of data used in LCAs,LIFE-CYCLE ASSESSMENT|INVENTORY ANALYSIS|FUZZY-SETS|UNCERTAINTY|ENERGY|PROBABILITY,"Purpose Employing representative data is necessary for producing a credible LCA informing decision making process. When the data is available from multiple sources, and in incompatible formats such as point estimates, intervals, approximations, and may even be conflicting in nature, it is important to synthesize it with minimal loss of information to enhance the credibility of LCA. This article introduces a framework for information fusion that can serve this purpose within the current operational procedure of LCA. Methods The character of information gathered from multiple sources is inherently different than that exhibited by the information generated by a single random source. The framework of possibility theory can be used to merge such heterogeneous information as demonstrated by its application in the diverse fields such as engineering, finance, and social sciences. This article introduces this methodology for LCAs by first introducing the theory behind data modeling and data fusion with possibility theory. Then, this framework is applied to the disparate data from literature on the manufacturing energy requirements for semiconductor device fabrication, and also to a hypothetical example of linguistic inputs from experts in order to demonstrate the operationalization of the theory. A flowchart is provided to recap the framework and for easy navigation through the steps of merging procedure. Results and discussion The framework for fusion of information applied the numerical and linguistic heterogeneous data in the LCA context illustrates that this methodology can be implemented relatively easily to increase the data quality and credibility of LCA. This can be done without making any changes in the usual preferred way of conducting an LCA. Information fusion may be performed either after the sensitivity analysis identifies the most impactful categories that need further investigation, or it can be performed upfront to the select input categories of interest. Conclusions The article introduces a well-established framework of information fusion to the field of LCA where disparate data may need to be fused to perform the assessment under certain conditions. This framework can be easily implemented, and will enhance data quality and LCA credibility. We also hope that data entry software such as ecoEditor make provision for the data entry mechanism necessary to enter fused data.",,"Gavankar, Sheetal|Suh, Sangwon",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,data fusion|data quality|data representativeness|epistemic uncertainty|life cycle assessment|possibility theory,10.1007/s11367-013-0673-2
302,WOS:000398425700001,2017,Life cycle assessment of Quayside Crane: A case study in China,ENVIRONMENTAL-IMPACT|ENERGY,"China is an important Quayside Crane producer and exporter in the world. The Quayside Crane has been used as crucial equipment in container terminals and also in supporting towards the global trade momentum. However, it simultaneously impacts the environment negatively on air quality and in addition contributes adversely towards energy consumption. To reduce the deleterious environmental effects of the Quayside Crane so as to improve the sustainability of this industry, evaluation of its environmental impacts through life cycles is acute necessary. This research undertook a cradle-to-grave life cycle assessment for a Quayside Crane made in China. Its purpose was to identify the key processes that effected the environment, as well as, sought out opportunities for improving the environmental profiles of Quayside Cranes. The inventory data was obtained from on-site investigations of Quayside Crane producers in Shanghai, China. The environmental impacts were evaluated using the ReCiPe method at midpoint level and operating the GaBi . assessment software. The results showed that nearly all environmental impact categories and life cycle costs were relatively high in the utilization stage. Steel production stage was the most important driver to the categories that impact environmental degradation, such as freshwater ecotoxicity, metal depletion, freshwater eutrophication and ozone depletion. Accordingly recovery of materials, such as steel reuse after refurbishment in the end-of-life stage could notably diminish most of the environmental impacts. In addition, sensitivity analysis and economic analysis were employed to promote the reliability of the life cycle assessment results. Based on these findings, several improvements could be put forward, for instance, optimized design for motor efficiency of the Quayside Crane was recognized as the most effective way to reduce the environmental impacts which should be urgently amended. The life cycle assessment could help decision-makers to know the key points that should be streamlined to make the Quayside Crane more reliable, energy-efficient, technologically and environmentally sustainable. (C) ", Elsevier Ltd. All rights reserved.,"Wen, Bo|Jin, Qiang|Huang, Hong|Tandon, Puja|Zhu, Yuanhang",JOURNAL OF CLEANER PRODUCTION,life cycle assessment|quayside crane|china|environmental impact assessment|life cycle costing,10.1016/j.jclepro.2017.01.146
303,WOS:000221832000028,2004,"Robust satellite techniques for seismically active areas monitoring: a sensitivity analysis on September 7, 1999 Athens's earthquake",IMPENDING EARTHQUAKES|PRECURSOR|GREECE|TURKEY|AUGUST|CHINA|IZMIT,"Space-time TIR anomalies, observed from months to weeks before the occurrence of earthquakes, have been suggested, by several authors, as pre-seismic signals. A robust approach (RAT) has recently been proposed (and successfully applied in the field of monitoring major natural and environmental risks) which permits a statistically based definition of TIR anomaly even in the presence of highly variable contributions from atmospheric (e.g. transmittance), surface (e.g. emissivity and morphology) and observational (time/season, but also solar and satellite zenithal angles) conditions. In this paper the actual potential of satellite TIR surveys is evaluated on the basis of several years of NOAA/AVHRR and METEOSAT observations over Europe. TIR anomalies, possibly associated to the Athens's earthquake which occurred on September , , have been particularly considered in order to evaluate the capability of the proposed approach to filter-out noisy contributions to the measured TIR signal due to variable, observational and meteorological, conditions. This study demonstrated the capability of the proposed method to isolate (if any) possible pre-seismic anomalous TIR patterns from the most important noisy contributions to the measured signal. The advantages offered by the use of geo-stationary (quite doubling the achievable signal-to-noise ratio) instead of polar satellite packages result also quite evident after the tests performed in the case of Athens's earthquake. Even if it was not the aim of this paper to confirm or confute the existence of pre-seismic TIR anomalies (an extended number of test-cases should be analyzed before), results here achieved surely encourage the continuation of the studies in this direction permitting, moreover, to devise suitable strategies in order to obtain more firm answers to this fascinating hypothesis. (C) ", Elsevier Ltd. All rights reserved.,"Filizzola, C|Pergola, N|Pietrapertosa, C|Tramutoli, ",PHYSICS AND CHEMISTRY OF THE EARTH,earthquake|satellite thermal infrared|avhrr|meteosat|msg,10.1016/j.pce.2003.11.019
304,WOS:000278842000024,2010,"Reducing lost-sales rate in (T,R,L) inventory model with controllable lead time",BACKORDERS|MIXTURE|REDUCTION|DISCOUNT,"In order to establish a good image and to enhance customer's loyalty, many efforts such as upgrading the servicing facilities, maintaining a high quality of products and increasing expenditure on advertisement could be made by a selling shop. Naturally, an extra-added cost must be spent for these efforts and it is expected to have a result to reduce the shortage cost of lost-sales and the total expected annual cost. This paper explores a probabilistic inventory model with optimal lost-sales caused by investment due to two different types of cost functions. We consider that the lead time can be shortened at an extra crashing cost, which depends on the length of the lead time. Moreover, we assume that the lost-sales rate can also be reduced by capital investment. The purpose of this paper is to establish a (T,R,L) inventory model with controllable lead time and to analyze the effects of increasing two different types of investments to reduce the lost-sales rate, in which the review period, lead time and lost-sales rate are treated as decision variables. We first formulate the basic periodic review model mathematically with the capital investment to reduce lost-sales rate. Then two models are discussed, one with normally distributed protection interval demand and another with distribution-free case. For each model, two investment cost functional forms, logarithmic and power, are employed for lost-sales rate reduction. Two computational algorithms with the help of the software Matlab are furnished to determine the optimal solution. In addition, six numerical examples and sensitivity analysis are presented to illustrate the theoretical results and obtain some managerial insights. Finally, the effect of lost-sales rate reduction is investigated. By framing this new model, we observe that a significant amount of savings can be easily achieved to increase the competitive edge in business. The results in the numerical examples indicate that the savings of expected annual total cost are realized through lost-sales reduction.", (C) 2010 Elsevier Inc. All rights reserved.,"Annadurai, K.|Uthayakumar, R.",APPLIED MATHEMATICAL MODELLING,inventory|lost sales rate|variable lead time|minimax distribution-free procedure|optimization,10.1016/j.apm.2010.02.035
305,WOS:000388092400006,2016,"ALBANY: USING COMPONENT-BASED DESIGN TO DEVELOP A FLEXIBLE, GENERIC MULTIPHYSICS ANALYSIS CODE",EMBEDDED ANALYSIS CAPABILITIES|MANAGING SOFTWARE COMPLEXITY|FINITE-ELEMENT|PARALLEL|SIMULATION|FRAMEWORK|EQUATIONS|SYSTEMS|LIBRARY,"Albany is a multiphysics code constructed by assembling a set of reusable, general components. It is an implicit, unstructured grid finite element code that hosts a set of advanced features that are readily combined within a single analysis run. Albany uses template-based generic programming methods to provide extensibility and flexibility; it employs a generic residual evaluation interface to support the easy addition and modification of physics. This interface is coupled to powerful automatic differentiation utilities that are used to implement efficient nonlinear solvers and preconditioners, and also to enable sensitivity analysis and embedded uncertainty quantification capabilities as part of the forward solve. The flexible application programming interfaces in Albany couple to two different adaptive mesh libraries; it internally employs generic integration machinery that supports tetrahedral, hexahedral, and hybrid meshes of user specified order. We present the overall design of Albany, and focus on the specifics of the integration of many of its advanced features. As Albany and the components that form it are openly available on the internet, it is our goal that the reader might find some of the design concepts useful in their own work. Albany results in a code that enables the rapid development of parallel, numerically efficient multiphysics software tools. In discussing the features and details of the integration of many of the components involved, we show the reader the wide variety of solution components that are available and what is possible when they are combined within a simulation capability.",,"Salinger, Andrew G.|Bartlett, Roscoe A.|Bradley, Andrew M.|Chen, Qiushi|Demeshko, Irina P.|Gao, Xujiao|Hansen, Glen A.|Mota, Alejandro|Muller, Richard P.|Nielsen, Erik|Ostien, Jakob T.|Pawlowski, Roger P.|Perego, Mauro|Phipps, Eric T.|Sun, WaiChing|Tezaur, Irina K.",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,partial differential equations|finite element analysis|template-based generic programming,10.1615/IntJMultCompEng.2016017040
306,WOS:000291785400002,2011,Concurrent Decisions on Design Concept and Material Using Analytical Hierarchy Process at the Conceptual Design Stage,MATERIALS SELECTION|SYSTEM,"There is an increased study for considering the precise decisions on the design concept (DC) and material concurrently at the early stage of development of product. Inappropriate decisions on DC and material always lead to huge cost involvement and ultimately drive toward premature component or product failure. To overcome this problem, concurrent engineering (CE) is an approach which allows designers to consider early decision making (EDM) need to be implemented. To illustrate the use of CE principle at the early stage of design process, a concept selection framework called concurrent DC selection and materials selection (CDCSMS) was proposed. In order to demonstrate the proposed CDCSMS framework, eight DC s and six different types of composite materials of automotive bumper beam have been considered. Both of these decisions were then verified by performing various scenarios of sensitivity analysis by using analytical hierarchy process through utilizing Expert Choice software.",,"Hambali, A.|Sapuan, S. M.|Rahim, A. S.|Ismail, N.|Nukman, Y.",CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS,concurrent engineering|analytical hierarchy process|early decision making,10.1177/1063293X11408138
307,WOS:000257041800013,2008,Selection of infectious medical waste disposal firms by using the analytic hierarchy process and sensitivity analysis,DELPHI,"While Taiwanese hospitals dispose of large amounts of medical waste to ensure sanitation and personal hygiene, doing so inefficiently creates potential environmental hazards and increases operational expenses. However, hospitals lack objective criteria to select the most appropriate waste disposal firm and evaluate its performance, instead relying on their own subjective judgment and previous experiences. Therefore, this work presents an analytic hierarchy process (AHP) method to objectively select medical waste disposal firms based on the results of interviews with experts in the field, thus reducing overhead costs and enhancing medical waste management. An appropriate weight criterion based on AHP is derived to assess the effectiveness of medical waste disposal firms. The proposed AHP-based method offers a more efficient and precise means of selecting medical waste firms than subjective assessment methods do, thus reducing the potential risks for hospitals. Analysis results indicate that the medical sector selects the most appropriate infectious medical waste disposal firm based on the following rank: matching degree, contractor's qualifications, contractor's service capability, contractor's equipment and economic factors, By providing hospitals with an effective means of evaluating medical waste disposal firms, the proposed AHP method can reduce overhead costs and enable medical waste management to understand the market demand in the health sector. Moreover, performed through use of Expert Choice software, sensitivity analysis can survey the criterion weight of the degree of influence with an alternative hierarchy. (C) ", Elsevier Ltd. All rights reserved.,"Hsu, Pi-Fang|Wu, Cheng-Ru|Li, Ya-Ting",WASTE MANAGEMENT,,10.1016/j.wasman.2007.05.016
308,WOS:000397072800012,2017,On the variational data assimilation problem solving and sensitivity analysis,LAPLACE TRANSFORM INVERSION|COVARIANCE MATRICES|CONDITION NUMBER|REGULARIZATION|IMPLEMENTATION,"We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets.", (C) 2017 Elsevier Inc. All rights reserved.,"Arcucci, Rossella|D'Amore, Luisa|Pistoia, Jenny|Toumi, Ralf|Murli, Almerico",JOURNAL OF COMPUTATIONAL PHYSICS,data assimilation|sensitivity analysis|inverse problem,10.1016/j.jcp.2017.01.034
309,WOS:000186661200005,2003,Development and application of computer simulation tools for ecological risk assessment,FOOD WEBS|EXPOSURE|MODEL|WATER|CONTAMINANTS|CHEMICALS|TOXICITY|ROUTES|SOIL,"Based on a review of available models for ecological risk estimation, most are site-specific and their applications are limited. However, general models, which can be easily adapted to other sites, remain few, in addition, they are simple and associated with significant uncertainties. In this paper, an approach is introduced for an ecological risk assessment ( ERA) model that can be modified for site-specific conditions. Using computer simulation as a screening tool for ecological risk assessment can assist environmental managers and policy decision-makers in the planning and implementation of potentially highly focused assessments and remediation, should the ERA dictate the need. The model was integrated with a Windows-based interface and interactive database management system (DBMS) as a user-friendly software package. In addition, based on trophic sources, a food web has been integrated into the framework of the DBMS. In an effort to evaluate the model, a case study was implemented to characterize the effects on an ecosystem of replacing electroplated chromium coatings with sputtered tantalum at U. S. Army Yuma and Aberdeen Proving Grounds. Potential exposure pathways included ingestion, inhalation, and dermal absorption for terrestrial animals; root and foliar uptake for plants; and direct absorption for aquatic species. Overall, results showed that the most significant exposure resulted from molybdenum and hexavalent chromium, which posed higher risks to select aquatic and terrestrial species at both sites. On the other hand, tantalum ( with vanadium as the surrogate) resulted in the least risk to all receptors within the studied areas. A sensitivity analysis demonstrated that soil-water distribution coefficients have a significant impact on the results. Based on the results, neither molybdenum nor chromium are recommended as a coating in gun barrels, and further study would be essential to address any affected firing range area. Tantalum is recommended for use, although for those species receiving a slight adverse risk, field investigations that include receptor sampling maybe necessary once soil/sediment and water sampling validates projected concentrations.",,"Lu, HY|Axe, L|Tyson, TA",ENVIRONMENTAL MODELING & ASSESSMENT,ecological risk assessment|exposure model|heavy metals|food web,10.1023/B:ENMO.0000004585.85305.3d
310,WOS:000343083600028,2014,MCMC_CLIB-an advanced MCMC sampling package for ODE models,,"A Summary: We present a new C implementation of an advanced Markov chain Monte Carlo (MCMC) method for the sampling of ordinary differential equation (ODE) model parameters. The software MCMC_CLIB uses the simplified manifold Metropolis-adjusted Langevin algorithm (SMMALA), which is locally adaptive; it uses the parameter manifold's geometry (the Fisher information) to make efficient moves. This adaptation does not diminish with MC length, which is highly advantageous compared with adaptive Metropolis techniques when the parameters have large correlations and/or posteriors substantially differ from multivariate Gaussians. The software is standalone (not a toolbox), though dependencies include the GNU scientific library and sundials libraries for ODE integration and sensitivity analysis.",,"Kramer, Andrei|Stathopoulos, Vassilios|Girolami, Mark|Radde, Nicole",BIOINFORMATICS,,10.1093/bioinformatics/btu429
311,WOS:000264918900016,2009,Modelling of anaerobic treatment of evaporator condensate (EC) from a sulphite pulp mill using the IWA anaerobic digestion model no. 1 (ADM1),WASTE-WATER|MOLASSES|REACTOR,"This paper presents the application of the ADM model to simulate the dynamic behaviour of an anaerobic reactor treating the condensate effluent (EC) generated in a sulphite pulp mill. The model was implemented in the simulation software AQUASIM (R) .d and its predictions were compared to experimental data obtained in lab-scale semi-continuous assays treating the industrial effluent. Sensitivity analysis revealed high influence of kinetic parameters on the process behaviour, which were further estimated: maximum specific uptake rate (k(m) = . d(-)) and half-saturation constant (K-s = . kg COD m(-)). The accuracy of the optimised parameters was assessed against experimental data from a second lab-scale reactor treating EC effluent with an additional carbon source (molasses). It was concluded that the model predicted reasonably the dynamic behaviour of the anaerobic reactor under different loading rates. In addition, simulations successfully predicted a better stability and performance of the process (lower VIA accumulation and higher COD removal and methane production) for the EC treatment when an external carbon source is added to the reactor, specifically at high organic loads ( kg COD m(-) d(-) or higher). The model was not able to describe adequately the reactor behaviour at high organic loads when molasses was not added, thus application of the model for the anaerobic treatment of EC effluent needs to be further evaluated.", (C) 2008 Elsevier B.V. All rights reserved.,"Silva, F.|Nadais, H.|Prates, A.|Arroja, L.|Capela, I.",CHEMICAL ENGINEERING JOURNAL,dynamic simulation|adm1|anaerobic reactor|acetic acid|sulphite pulping process|semi-continuous assay,10.1016/j.cej.2008.09.002
312,WOS:000276075900006,2010,Object-oriented design of process line simulation and optimization-A case study in papermaking,MULTIDISCIPLINARY DESIGN|PAPER|FRAMEWORK|SYSTEM|FLOW,"Simulation-based optimization for industrial process lines is discussed in this paper. Our approach combines multidisciplinary modeling, modern sensitivity analysis methodology as well as multiobjective optimization by means of object-oriented software design principles. As a result, a simulation and optimization approach that can be extended and modified due to users' needs can be developed. Our approach is illustrated by a real-world example from papermaking industry.",,"Madetoja, Elina|Tarvainen, Pasi",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,process line optimization|multiobjective optimization|multidisciplinary modeling|object-oriented programming,10.1007/s00158-009-0451-8
313,WOS:000265341800006,2009,Estimating storm discharge and water quality data uncertainty: A software tool for monitoring and modeling applications,AGRICULTURAL WATERSHEDS|SAMPLE PRESERVATION|REACTIVE PHOSPHORUS|INORGANIC-PHOSPHATE|RISK-ASSESSMENT|RIVER WATER|NITROGEN|STRATEGIES|SEDIMENT|SORPTION,"Uncertainty estimates corresponding to measured hydrologic and water quality data can contribute to improved monitoring design, decision-making, model application, and regulatory formulation. With these benefits in mind, the Data Uncertainty Estimation Tool for Hydrology and Water Quality (DUET-H/WQ) was developed from an existing uncertainty estimation framework for small watershed discharge, sediment, and N and P data. Both the software and its framework-basis utilize the root mean square error propagation methodology to provide uncertainty estimates instead of more rigorous approaches requiring detailed statistical information, which is rarely available. DUET-H/WQ lists published uncertainty information for data collection procedures to assist the user in assigning appropriate data-specific uncertainty estimates and then calculates the uncertainty for individual discharge, concentration, and load values. Results of DUET-H/WQ application in several studies indicated that substantial uncertainty can be contributed by each procedural category (discharge measurement, sample collection, sample preservation/storage, laboratory analysis, and data processing and management). For storm loads, the uncertainty was typically least for discharge (+/- -%), greater for sediment (+/- -%) and dissolved N and P (+/- -%) loads, and greater yet for total N and P (+/- -%). When these uncertainty estimates for individual values were aggregated within study periods (i.e. total discharge, average concentration, and total load), uncertainties followed the same pattern (Q < TSS < dissolved N and P < total N and P). This rigorous demonstration of uncertainty in discharge and water quality data illustrates the importance of uncertainty analysis and the need for appropriate tools. It is our hope that DUET-H/WQ contributes to making uncertainty estimation a routine data collection and reporting procedure and thus enhances environmental monitoring, modeling, and decision-making. Hydrologic and water quality data are too important for scientists to continue to ignore the inherent uncertainty.", Published by Elsevier Ltd.,"Harmel, R. D.|Smith, D. R.|King, K. W.|Slade, R. M.",ENVIRONMENTAL MODELLING & SOFTWARE,error propagation|data collection|hydrology|nutrients|watershed models,10.1016/j.envsoft.2008.12.006
314,WOS:000381039000006,2016,Sensitivity of Flood-Depth Frequency to Watershed-Runoff Change and Sea-Level Rise Using a One-Dimensional Hydraulic Model,IMPACT,"Climate change and sea-level rise are expected to alter the likelihood of extreme events, such as floods, within the design lifetime of infrastructure components. Critical civil infrastructure facilities, including wastewater treatment, transportation, and energy, need site-specific flood contingency plans that reflect the effects of changing climate. This study developed a sensitivity analysis method to assess future flood risk by estimating flood frequency under conditions of higher sea level and streamflow response to increased precipitation intensity. The method was applied to an ungauged location on a tidal estuary in the Mid-Atlantic region as a case study. One-dimensional (D) unsteady flow analysis using a hydraulic analysis software developed by the U.S. Army Corps of Engineers was used to predict discharge and water surface elevation along the estuary reach, subject to prescribed boundary conditions of upstream discharge and downstream water surface elevation. A current-climate flood-depth frequency curve was estimated for the study site based on simulations of high-flow events in the years for which simultaneous upstream and downstream records were available. The simulations were repeated, applying additive water surface elevation (WSEL) perturbations at the downstream boundary (to represent anticipated sea-level rise) and multiplicative event discharge perturbations at the upstream boundary (to represent anticipated change in watershed hydrology). The perturbations were applied separately and together. Revised flood-depth frequency curves were calculated for each set of perturbations. For this location, the % annual exceedance (-year) WSEL is .m (.ft) higher, and the .% annual exceedance (-year) WSEL is m (.ft) higher, than current climate in the worst-case scenario, .m (ft) of sea-level rise and a % increase in event discharge. For that scenario, the current % exceedance (-year) WSEL has a % probability of exceedance (.year). The results indicate that the effects of the upstream and downstream changes are not additive. This research will help infrastructure stakeholders be aware of the flood risk and vulnerability while environmental changes are underway.",,"Feng, Yilu|Brubaker, Kaye L.",JOURNAL OF HYDROLOGIC ENGINEERING,,10.1061/(ASCE)HE.1943-5584.0001378
315,WOS:000249177200007,2007,Determination of skin and aquifer parameters for a slug test with wellbore-skin effect,OPTIMAL GROUNDWATER-MANAGEMENT|PARTIALLY PENETRATING WELLS|FINITE-THICKNESS SKIN|HYDRAULIC CONDUCTIVITY,"Slug test is considered to reflect the hydraulic parameters in the vicinity of the test well. The aquifer parameters are usually identified by fitting an appropriate mathematical solution or graphical type curves with slug test data. In this paper, we developed an approach by combining [Moench, A.F., Hsieh, P.A., . Analysis of slug test data in a well with finite-thickness skin. In: Memoirs of the th International Congress on the Hydrogeology of Rocks of Low Permeability, U.S.A. Members of the International Association of Hydrologists, Tucson, AZ, vol. , pp. -] and simulated annealing (SA) approach to estimate five parameters, i.e., three skin parameters and two aquifer parameters. The three skin parameters are hydraulic conductivity, specific storage, and thickness of the wellbore-skin zone, white the two aquifer parameters are hydraulic conductivity and specific storage of the formation zone. It is worthy to note although the thickness of the wellbore-skin zone is usually taken as an input data for the data-analyzed software, it is actually an unknown parameter that cannot be measured directly. This paper proposes a methodology for estimating the thickness of the wellbore-skin zone with other hydraulic parameters at the same time. Eight sets of well water-level. (WWL) data of aquifers with both positive and negative skins are generated by Moench and Hsieh [Moench and Hsieh, ] and four sets of standard normally distributed noise are then added to each set of WWL data. The results indicate that the negative-skin cases generally give a better estimated result than that of the positive-skin cases. Sensitivity analysis is also employed to demonstrate the physical behavior when slug test was performed under positive-skin effect. For the case of an aquifer with a positive-skin, the use of a longer series of WWL data for analysis is strongly recommended for better estimation of aquifer hydraulic conductivity. Analyzing the WWL data of the test and observation wells simultaneously could significantly improve the estimations on specific storages. Impetuously presuming an arbitrary value for the thickness of the wellbore-skin zone may lead to poor estimation for the other four parameters.", (C) 2007 Elsevier B.V. All rights reserved.,"Yeh, Hund-Der|Chen, Yen-Ju",JOURNAL OF HYDROLOGY,groundwater|parameter estimation|sensitivity analysis|simulated annealing|skin thickness|slug test,10.1016/j.jhydrol.2007.05.029
316,WOS:000399586700038,2017,Accelerating Monte Carlo estimation with derivatives of high-level finite element models,SENSITIVITY DERIVATIVES|CHAOS,In this paper we demonstrate the ability of a derivative-driven Monte Carlo estimator to accelerate the propagation of uncertainty through two high-level non-linear finite element models. The use of derivative information amounts to a correction to the standard Monte Carlo estimation procedure that reduces the variance under certain conditions. We express the finite element models in variational form using the high-level Unified Form Language (UFL). We derive the tangent linear model automatically from this high-level description and use it to efficiently calculate the required derivative information. To study the effectiveness of the derivative-driven method we consider two stochastic PDEs; a one-dimensional Burgers equation with stochastic viscosity and a three-dimensional geometrically non-linear Mooney-Rivlin hyperelastic equation with stochastic density and volumetric material parameter. Our results show that for these problems the first-order derivative-driven Monte Carlo method is around one order of magnitude faster than the standard Monte Carlo method and at the cost of only one extra tangent linear solution per estimation problem. We find similar trends when comparing with a modern non-intrusive multi-level polynomial chaos expansion method. We parallelise the task of the repeated forward model evaluations across a cluster using the ipyparallel and mpipy software tools. A complete working example showing the solution of the stochastic viscous Burgers equation is included as supplementary material., (C) 2017 Published by Elsevier B.V.,"Hauseux, Paul|Hale, Jack S.|Bordas, Stephane P. A.",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,monte carlo methods|uncertainty propagation|tangent linear models|partially intrusive methods|polynomial chaos expansion|parallel computing,10.1016/j.cma.2017.01.041
317,WOS:000363966900091,2015,Energy analyses and greenhouse gas emissions assessment for saffron production cycle,CROCUS-SATIVUS L.|ARTIFICIAL NEURAL-NETWORKS|CROP PRODUCTION SYSTEMS|INPUT-OUTPUT-ANALYSIS|SENSITIVITY-ANALYSIS|ECONOMICAL ANALYSIS|CARBON SEQUESTRATION|ACTIVE CONSTITUENTS|POTATO PRODUCTION|WHEAT PRODUCTION,"Population growth and world climate changes are putting high pressure on agri-food production systems. Exacerbating use of energy sources and expanding the environmental damaging symptoms are the results of these difficult situations. This study was conducted to determine the energy balance for saffron production cycle and investigate the corresponding greenhouse gas (GHG) emissions in Iran. Saffron (Crocus sativus L.) is one of the main spice that historically cultivated in Iran. Data were obtained from  randomly selected saffron growers using a face to face questionnaire technique. The results revealed that in  years of saffron production cycle, the overall input and output energy use were to be ,. and ,. MJ ha(-), respectively. The highest-level of energy consumption belongs to seeds (. %) followed by chemical fertilizers (. %). Energy use efficiency, specific energy, net energy, and energy productivity of saffron production were ., . MJ kg(-), ,.MJ ha(-), and . kgMJ(-), respectively. The result shows that the cultivation of saffron emits . kg CO()eq. ha(-) greenhouse gas, in which around . % belonged to electricity followed by chemical fertilizers. In addition the Cobb-Douglas production function was applied into EViews  software to define the functional relationship. The results of econometric model estimation showed that the impact of human labor, electricity, and water for irrigation on stigma, human labor, electricity, and seed on corm and also human labor and farmyard manure (FYM) on flower and leaf yield were found to be statistically significant. Sensitivity analysis results of the energy inputs demonstrated that the marginal physical productivity (MPP) worth of electricity energy was the highest for saffron stigma and corm, although saffron flower and leaf had more sensitivity on chemicals energy inputs. Moreover, MPP values of renewable and indirect energies were higher than non-renewable and direct energies, respectively.",,"Bakhtiari, Amir Abbas|Hematian, Amir|Sharifi, Azin",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,energy input|efficiency|environment|econometric model|ghgemissions|cobb-douglas function,10.1007/s11356-015-4843-6
318,WOS:000362007900004,2015,Estimation of evapotranspiration from ground-based meteorological data and global land data assimilation system (GLDAS),ALFALFA-REFERENCE EVAPOTRANSPIRATION|REFERENCE CROP EVAPOTRANSPIRATION|KEY CLIMATIC VARIABLES|PENMAN-MONTEITH|POTENTIAL EVAPOTRANSPIRATION|RIVER-BASIN|SENSITIVITY ANALYSES|EVAPO-TRANSPIRATION|UNITED-STATES|CHINA,"Evapotranspiration (ET) is one of the most significant factors in understanding global hydrological budgets, and its accurate estimation is crucial for understanding water balance and developing efficient water resource management plans. For calculation of reference ET (ETref), the meteorological data from weather stations have been widely used for estimation at the point scale; however, meteorological data from the global land data assimilation system (GLDAS) at the regional scale are rarely used for the estimation of ET. In this study,  different equations provided in the Reference Evapotranspiration Calculator Software (REF-ET) were utilized for estimating ETref with GLDAS and point scale data collected at  observation sites in the Korean Peninsula during . Using ETref calculated from observation and GLDAS,  equations were evaluated by estimating the overall rank number, as determined by the correlation coefficient, normalized standard deviation, bias, and root mean square error (RMSE). Results showed that the Penman (Proc R Soc Lond Ser A Math Phys Sci :-, ) FAO- Penman-Monteith,  Kpen equation (combination equations), the  Makkink, Priestley-Taylor equation (radiation based equation), and the  Hargreaves equation had a good overall rank. Using the six selected equations, seasonal analysis was conducted and evaluated using the bias and RMSE. Comparison of the ETref gathered from observation and GLDAS revealed that both of them showed similar seasonal variation, although ETref calculated from GLDAS were underestimated. Sensitivity analysis conducted by changing three main climatic variables (i.e., temperature, wind speed, and sunshine hours) by +/- , +/- , +/- , +/- , and +/-  % with one variable fixed also revealed that ETref was more affected by air temperature than sunshine hours and wind speed throughout the  selected stations.",,"Park, Jongmin|Choi, Minha",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,evapotranspiration|reference evapotranspiration|gldas|sensitivity analysis,10.1007/s00477-014-1004-2
319,WOS:000230708100004,2005,"Determination of the optimal installation capacity of small hydro-power plants through the use of technical, economic and reliability indices",,"One of the most important issues in planning Small Hydro-Power Plants (SHPPs) of the ""run-off river"" type is to determine the optimal installation capacity of the SHPP and estimate its optimal annual energy value. In this paper, a method to calculate the annual energy is presented, as is the program developed using Excel software. This program analyzes and estimates the most important economic indices of an SHPP using the sensitivity analysis method. Another program, developed by Matlab software, calculates the reliability indices for a number of units of an SHPP with a specified load duration curve using the Monte Carlo method. Ultimately, comparing the technical, economic and reliability indices will determine the optimal installation capacity of an SHPP. By applying the above-mentioned algorithm to a sample SHPP named ""Nari"" (located in the northern part of Iran), the optimal capacity of . MW is obtained. (c) ", Elsevier Ltd. All rights reserved.,"Hosseini, SMH|Forouzbakhsh, F|Rahimpoor, M",ENERGY POLICY,installation capacity|small hydro-power plant (shpp)|economic analysis|monte carlo method,10.1016/j.enpol.2004.03.007
320,WOS:000374602000009,2016,Assessing minimum environmental flows in nonpermanent rivers: The choice of thresholds,INSTANTANEOUS PEAK FLOW|MEAN DAILY FLOW|WATERSHED SCALE|SPAIN|MODEL|BASIN|REQUIREMENTS|VARIABILITY|SOFTWARE|SURFACES,"The criteria used in the computation of the minimum environmental flow regime and flow cessation periods in nonpermanent rivers are often left to open criteria. This study proposes a stochastic approach for evaluating the choice of local thresholds in the characterization of minimum environmental flows through both the Monte Carlo technique and local hydrological relationships. This approach is applied to four regimes obtained by hydrologic and hydraulic habitat modeling in a Mediterranean watershed. The operationality, defined as the probability of the calculated environmental regime being satisfied by the natural regime over  years, was assessed for eight different scenarios. Two monthly minimum environmental flow regimes were then generated, with  and % operationality levels. This analysis allows the generation of minimum flow regime prescriptions from a strictly hydrologic point of view. The methodology proposed constitutes a useful tool for the implementation of uncertainty analysis of environmental flows in water resource management. (C) ", Elsevier Ltd. All rights reserved.,"Aguilar, Cristina|Jose Polo, Maria",ENVIRONMENTAL MODELLING & SOFTWARE,operationality|minimum environmental flows|minimum environmental flow regime|flow cessation periods|semiarid systems|wimmed,10.1016/j.envsoft.2016.02.003
321,WOS:000344297900001,2014,A Novel Closed-Form Solution for Circular Openings in Generalized Hoek-Brown Media,BRITTLE-PLASTIC ROCK|GROUND RESPONSE|DISPLACEMENTS|STRENGTH|TUNNEL,"A novel closed-formsolution is presented in this paper for the estimation of displacements around circular openings in a brittle rock mass subject to a hydrostatic stress field. The rock mass is assumed to be elastic-brittle-plastic media governed by the generalized Hoek-Brown yield criterion. The present closed-form solution was validated by employing the existing analytical solutions. Results of several example cases are analyzed to show that, with the simplified assumption, a novel closed-form solution is derived and found to be in an excellent agreement with those obtained by using the exact integration method with mathematical software. Parametric sensitivity analysis is carried out and the parameter a(r), tends to be the sensitive factor. As a closed-form solution that does not require transformation technique and the use of any numerical method, this work can provide a better choice in the preliminary design for circular opening.",,"Meng, Qing Xiang|Wang, Wei",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/870835
322,WOS:000238960100008,2006,Variance-based sensitivity analysis of the probability of hydrologically induced slope instability,STABILITY MODEL|UNSATURATED SOILS|UNCERTAINTY|DESIGN,"Analysis of the sensitivity of predictions of slope instability to input data and model uncertainties provides a rationale for targeted site investigation and iterative refinement of geotechnical models. However, sensitivity methods based on local derivatives do not reflect model behaviour over the whole range of input variables. whereas methods based on standardised regression or correlation coefficients cannot detect non-linear and non-monotonic relationships between model input and output. Variance-based sensitivity analysis (VBSA) provides a global, model-independent sensitivity measure. The approach is demonstrated using the Combined Hydrology and Stability Model (CHASM) and is applicable to a wide variety of computer models. The method of Sobol', assuming independence between input variables, was used to identify interactions between model input variables, whilst replicated Latin Hypercube Sampling (LHS) is used to investigate the effects of statistical dependence between the input variables. The SIMLAB software was used, both to generate the input sample and to calculate the sensitivity indices. The analysis provided quantified evidence of well-known sensitivities as well demonstrating how uncertainty in slope failure during rainfall is, for the examples tested here. more attributable to uncertainty in the soil strength than to uncertainty in the rainfall. (c) ", Elsevier Ltd. All rights reserved.,"Hamm, N. A. S.|Hall, J. W.|Anderson, M. G.",COMPUTERS & GEOSCIENCES,site investigation|slope stability analysis|statistical analysis|sensitivity analysis|uncertainty analysis,10.1016/j.cageo.2005.10.007
323,WOS:000309267300007,2012,Comparison of empirical and numerical methods in tunnel stability analysis,,"The stability of a tunnel can be evaluated using mathematical solutions, empirical methods, or numerical modelling. Mathematical solutions are precise methods; however the need to conduct mathematical calculations usually decreases the user's desire to use this method. Empirical methods are based on the experience gathered by researchers in various parts of the world whereas numerical modelling utilises computing power and, using various modelling techniques, can be a precise way of solving very complex problems. In this method the environment and the geometry can be set by the user. This method allows the user to conduct sensitivity analysis. In this article, empirical methods and numerical modelling using UDEC software were used to conduct a stability analysis of the access tunnel at the Shahriar dam crest, which was one of the most important tunnels of this project. In addition, numerical modelling was used to predict the stresses and deformations around the perimeter of the tunnel, and select the most suitable ground support system. The results obtained from both methods were compared for selection of the best suited support system. The results indicated that the empirical methods presented similar results to the results of numerical modelling at the first stages of tunnel design in jointed rocks. Therefore, in the absence of sufficient information for numerical analysis, the results of the empirical method can be used for this project.",,"Rahmani, Niousha|Nikbakhtan, Babak|Ahangari, Kaveh|Apel, Derek",INTERNATIONAL JOURNAL OF MINING RECLAMATION AND ENVIRONMENT,mathematical analyses|empirical methods|numerical modelling|tunnel|udec,10.1080/17480930.2011.611615
324,WOS:000371989100016,2016,Model development and process simulation of postcombustion carbon capture technology with aqueous AMP/PZ solvent,CO2 CAPTURE|DIOXIDE CAPTURE|MASS-TRANSFER|PILOT-SCALE|FLUE-GAS|STRUCTURED PACKINGS|TERNARY VLE|PIPERAZINE|2-AMINO-2-METHYL-1-PROPANOL|KINETICS,"This study presents the development, application, and uncertainty analysis of a process simulation model for postcombustion CO capture with an AMP/PZ solvent blend based on state of the art knowledge on AMP/PZ solvent technology. The development includes the improvement of the physical property models of a software package designed for simulation of acid gas treatment and CO capture technologies. The improvement particularly consisted of regression of AMP-PZ binary interaction parameters. The model was applied to a case study of postcombustion CO capture from an Advanced Super Critical Pulverized Coal power plant. Uncertainly analysis was undertaken by validating the physical property models against laboratory measurements reported in literature; by comparing model results with pilot study results, and by evaluating the strength of the model with a novel method called pedigree analysis. The results show that AMP/PZ postcombustion technology performs better than MEA technology on most performance indicators, e.g., the Specific Reboiler Duty is reduced from . GJ/t CO for MEA, to . GJ/t CO for AMP/PZ, and the specific cooling water requirement is reduced from . to . GJ/t CO. Only amine slip to the atmosphere increases with AMP/PZ technology: from . g/t CO to . g/t CO, although this value is still within emission limits from existing regulatory frameworks. The coal power plant net efficiency with AMP/PZ capture amounts to a value of .%(LHV), compared to .%(LHV) for the case without CCS and .%(LHV) in case of CCS with MEA. The uncertainty analysis shows that the model is well capable of predicting experimental and pilot result. The remaining uncertainty is mostly in the reaction kinetics and in the flowsheet design. Validation could be further improved, by more elaborate comparison to independent measures of physical properties, and by comparison of the model outputs to results from large demonstration or commercial size capture plants. (C) ", Elsevier Ltd. All rights reserved.,"van der Spek, Mijndert|Arendsen, Richard|Ramirez, Andrea|Faaij, Andre",INTERNATIONAL JOURNAL OF GREENHOUSE GAS CONTROL,model development|process simulation|postcombustion co2 capture|amp/pz solvent|uncertainty analysis|pedigree analysis,10.1016/j.ijggc.2016.01.021
325,WOS:000372683200003,2016,Concurrent design and process optimization of forging,METAL-FORMING PROCESSES|DIE SHAPE DESIGN|SENSITIVITY-ANALYSIS|GENETIC ALGORITHMS|PREFORM DESIGN|PROCESS PARAMETERS|NEURAL-NETWORKS|SIMULATION|PRODUCT|UNCERTAINTIES,"In this study, a concurrent design optimization methodology is proposed to minimize the cost of a cold forged part using both product and process design parameters as optimization variables. The objective function combines the material, manufacturing, and post manufacturing costs of the product. The part to be optimized is a simply supported I-beam under a centric load. Various constraints are imposed related to the performance of the product in use and the effectiveness of manufacturing. Nelder-Mead is used as search algorithm and analyses are conducted using commercial finite element software, ANSYS. Results show considerable improvement in the cost. (C) ", Elsevier Ltd. All rights reserved.,"Ozturk, Murat|Kocaoglan, Sinem|Sonmez, Fazil O.",COMPUTERS & STRUCTURES,concurrent engineering|metal|cold forging|optimization|global optimum|fem,10.1016/j.compstruc.2016.01.016
326,WOS:000170761700007,2001,Integration of topology and shape optimization for design of structural components,,"This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach.",,"Tang, PS|Chang, KH",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,cad|design sensitivity analysis|fea|shape optimization|topology optimization,10.1007/PL00013282
327,WOS:000306043900002,2012,MVC3: A MATLAB graphical interface toolbox for third-order multivariate calibration,PARALLEL FACTOR-ANALYSIS|TRILINEAR LEAST-SQUARES|RESIDUAL TRILINEARIZATION|CURVE RESOLUTION|FOLIC-ACID|4-WAY CALIBRATION|MASS-SPECTROMETRY|2ND-ORDER|METHOTREXATE|SAMPLES,"A new MATIAB graphical interface toolbox for implementing third-order multivariate calibration methodologies is discussed. Multivariate calibration  (MVC) is a sequel of the already described first-order (MVC) and second-order (MVC) toolboxes. MVC accepts a variety of ASCII data for input, depending on whether the third-order data are vectorized or matricized. If required, data for sample sets are arranged into four-way arrays for processing with several quadrilinear and non-quadrilinear algorithms. Quadrilinear decomposition techniques and latent structured models based on partial least-squares regression and residual trilinearization are included in the software. Appropriate working sensor regions in the three data dimensions can be selected. Model development and its subsequent application to unknown samples are straightforward from the interface. Prediction results are provided along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (C) 2012 Elsevier B.V. All rights reserved.,"Olivieri, Alejandro C.|Wu, Hai-Long|Yu, Ru-Qin",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,third-order multivariate calibration|matlab program|graphical interface|figures of merit,10.1016/j.chemolab.2012.03.018
328,WOS:000303384800006,2012,Life cycle assessment of densified wheat straw pellets in the Canadian Prairies,CROPPING SYSTEMS|BIOMASS|SIZE|GAS|LCA,"Densification, a process used to manufacture pellets in order to increase biomass bulk density, plays a crucial role in the economics of biomass utilization. The Canadian Prairies produce large quantities of agricultural residues each year, in particular wheat straw. This study performs life cycle assessment of wheat straw pellets by evaluating environmental effects of the entire pellet production system comprising feedstock production (on-farm wheat straw production), harvesting, baling, transportation, and the industrial processing involving drying, grinding, pelletizing, and packing in the densification plant. The effects of each process on the environmental performance of wheat straw pellets were investigated. This study was conducted using LCA software and incorporating the Ecoinvent database supplemented with literature data for the Canadian Prairies. Wheat straw pellets manufactured from the densification plant are evaluated with respect to their use of resources and energy consumption. Environmental emissions associated with the agricultural processing and manufacturing systems are quantified. Sensitivity analysis is conducted to compare allocation methods and investigate the environmental impact of pelletizing and drying processes. The functional unit is defined as  kg wheat straw pellet. The study quantified the environmental impact of producing wheat straw pellets in terms of global warming potential, acidification, eutrophication, ozone layer depletion, abiotic depletion, human toxicity, photochemical oxidation, fresh water aquatic ecotoxicity, and terrestrial ecotoxicity. Drying, pelletizing, and fertilizer are the main contributors to global warming, acidification, abiotic depletion, human toxicity, terrestrial ecotoxicity, photochemical oxidation, and most of the other environmental impacts. Wheat seed has more impact on eutrophication. Transportation has an impact on ozone layer depletion, while grinding has an effect on freshwater aquatic ecotoxicity. The environmental impact of materials and energy fluxes on producing wheat straw pellet in the Canadian Prairies is assessed. The effect of each processing step on the entire manufacturing process is described. Overall, drying and pelletizing processes contribute the most environmental burdens except eutrophication and terrestrial ecotoxicity which are dominated by agricultural fertilizer/seed utilization and harvesting. In order to mitigate the environmental impact of wheat straw pellet production, minimizing energy consumption and machinery burdens from the drying and pelletizing processes are the main intervention points for wheat straw densification. Fertilizer production and utilization are key variables in strategies to lower eutrophication and terrestrial ecotoxicity.",,"Li, Xue|Mupondwa, Edmund|Panigrahi, Satyanarayan|Tabil, Lope|Adapa, Phani",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,canadian prairies|densification system|energy consumption|life cycle assessment|pellet|wheat straw,10.1007/s11367-011-0374-7
329,WOS:000411574400095,2017,The Effect of Vitamin A on Fracture Risk: A Meta-Analysis of Cohort Studies,BONE-MINERAL DENSITY|HIP FRACTURE|RETINOIC ACID|BETA-CAROTENE|SERUM RETINOL|POSTMENOPAUSAL WOMEN|OSTEOCLAST FORMATION|HYPERVITAMINOSIS-A|CLINICAL-TRIALS|FOLLOW-UP,"This meta-analysis evaluated the influence of dietary intake and blood level of vitamin A (total vitamin A, retinol or beta-carotene) on total and hip fracture risk. Cohort studies published before July  were selected through English-language literature searches in several databases. Relative risk (RR) with corresponding % confidence interval (CI) was used to evaluate the risk. Heterogeneity was checked by Chi-square and I- test. Sensitivity analysis and publication bias were also performed. For the association between retinol intake and total fracture risk, we performed subgroup analysis by sex, region, case ascertainment, education level, age at menopause and vitamin D intake. R software was used to complete all statistical analyses. A total of , participants over the age of  years were included. Higher dietary intake of retinol and total vitamin A may slightly decrease total fracture risk (RR with % CI: . (., .) and . (., .), respectively), and increase hip fracture risk (RR with % CI: . (., .) and . (., .), respectively). Lower blood level of retinol may slightly increase total fracture risk (RR with % CI: . (., .)) and hip fracture risk (RR with % CI: . (., .)). In addition, higher beta-carotene intake was weakly associated with the increased risk of total fracture (RR with % CI: . (., .)). Our data suggest that vitamin A intake and level may differentially influence the risks of total and hip fractures. Clinical trials are warranted to confirm these results and assess the clinical applicability.",,"Zhang, Xinge|Zhang, Rui|Moore, Justin B.|Wang, Yuegiao|Yan, Hanyi|Wu, Yingru|Tan, Anran|Fu, Jialin|Shen, Zigiong|Qin, Guiyu|Li, Rui|Chen, Guoxun",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,vitamin a|retinol|beta-carotene|hip fracture|total fracture,10.3390/ijerph14091043
330,WOS:000174870000001,2002,Optimization of biofiltration for odor control: Model development and parameter sensitivity,HYDROGEN-SULFIDE|BIOFILTERS|VALIDATION|REMOVAL|DESIGN|BEHAVIOR|BIOMASS,"A dynamic model that describes the mass transport and attenuation of odor-causing air emissions (i.e., hydrogen sulfide and other reduced sulfur compounds) in a biofiltration unit was developed and incorporated into a software package called Biofilter(TM). Mechanisms included advective flow, mass transfer from the bulk phase to the biofilm, biofilm internal diffusion, and biological reaction in the biofilm. A dimensionless analysis revealed that the mass transport and attenuation of target compounds can be characterized by several dimensionless groups. Model equations were converted to ordinary differential equations using orthogonal collocation and the resulting ordinary differential equations were solved using the DGEAR algorithm. Numerical solutions were verified by comparing model simulations to analytical solutions. The model simulations showed that the existence of a water layer surrounding the biofilm in a biofiltration unit lowers the removal efficiency of hydrogen sulfide. A sensitivity analysis of model parameters (including the film transfer coefficient, biofilm diffusivity, biofilm thickness, maximum specific biomass growth rate, yield coefficient, half-saturation coefficient, and initial active biomass concentration) using data from two biofilters located at the Cedar Rapids (Iowa) Water Pollution Control Facilities, showed that biofilm. internal diffusion and biofilm. kinetics have a significant effect on hydrogen sulfide removal, while external mass transfer has little effect. Water Environ. Res., ,  ().",,"Li, HB|Crittenden, JC|Mihelcic, |Hautakangas, H",WATER ENVIRONMENT RESEARCH,biofiltration|biofilter|biotrickling filter|modeling|odors|hydrogen sulfide|volatile organic compounds|wastewater,10.2175/106143002X139703
331,WOS:000363949300025,2015,Feasibility analysis of a hybrid off-grid wind-DG-battery energy system for the eco-tourism remote areas,POWER PINCH ANALYSIS|MALAYSIA|PERFORMANCE|GENERATION|BANGLADESH|SIMULATION|STORAGE|DESIGN|PLANT,"The electrification process of the remote areas and decentralized areas is being a vital fact for the improvement of its eco-tourism issues such as the Cameron Highland of Malaysia. Renewable energy (RE) resources can be used extensively to support and fulfill the demand of the expected loads of these areas. This article presents an analysis of a complete off-grid wind-diesel-battery hybrid RE model. The main objective of the present analysis is to visualize the optimum volume of systems capable of fulfilling the requirements of  kWh/day primary load in coupled with . kW peak for  residential hotels of Cameron Highlands. The hybrid power system can be effective for the tourists of that area as it is a decentralized region of Malaysia. The main motto of this analysis is to minimize the electricity unit cost and ensure the most reliable and feasible system to fulfill the requirements of the desired or expected energy system using HOMER software. From the simulation result, it can be seen that  wind turbines ( kW),  diesel generator ( kW), and  battery (Hoppecke  OPzS) hybrid RE system is the most economically feasible and lowest cost of energy is nearing USD ./kWh and net present cost is USD , . The decrement of the CO emissions also can be identified from the simulation results using that most feasible RE system including the renewable fraction value which is about ., . % capacity shortage and . % electricity as storage as compared to the other energy system.",,"Shezan, S. K. A.|Saidur, R.|Ullah, K. R.|Hossain, A.|Chong, W. T.|Julai, S.",CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY,renewable energy|wind energy|wind turbines homer|diesel generator|sensitivity analysis|optimization|hybrid model,10.1007/s10098-015-0983-0
332,WOS:000246092700001,2007,Using the WISE database to parameterize soil inputs for crop simulation models,SATURATED HYDRAULIC CONDUCTIVITY|BULK-DENSITY|WATER|CARBON|SOFTWARE|NITROGEN|TEXTURE|SYSTEM|WORLD|APSIM,"During the s, a soils database was developed by the International Soil Reference and Information Centre in The Netherlands for the project ""World Inventory of Soil Emission Potentials"" (WISE). Using this database, we converted  soil profiles from around the world into a format that can be used as input data to some commonly used biophysical computer models, such as the crop simulation models within the Decision Support System for Agrotechnology Transfer (DSSAT). Soil data are often unavailable, particularly for many locations in the tropical and subtropical regions. If little or nothing is known about the soil profile for a particular location, a soil database can be used to estimate some of its parameters, based on a comparison with other soils from the same region. The WISE database is one of the most comprehensive soil databases, with samples well distributed in the World. The resulting soil profile can then be used as input parameters for a model to simulate growth, development and yield for one or more crops for this location. With multiple profiles available for many soil classes, it is possible to obtain an indication about the range of values for each soil parameter and then conduct an uncertainty analysis with respect to the model's response to this range. All soil profiles have been geo-referenced, and can thus be linked to the digital version of the FAO-UNESCO soil map of the world. We describe the methods used to convert the soil profile database, discuss the variability of key soil variables by soil class, illustrate how the database can be used, and conclude with recommendations for further work to improve the database for biophysical modeling applications.", (c) 2007 Elsevier B.V. All rights reserved.,"Gijsman, Arjan J.|Thornton, Philip K.|Hoogenboom, Gerrit",COMPUTERS AND ELECTRONICS IN AGRICULTURE,database|crop simulation model|decision support system|soil profile|crop production|resource management,10.1016/j.compag.2007.01.001
333,WOS:000290190800017,2011,Advances in concrete arch dams shape optimization,DESIGN,"This paper presents an efficient methodology to find the optimum shape of arch dams. In order to create the geometry of arch dams a new algorithm based on Hermit Splines is proposed. A finite element based shape sensitivity analysis for design-dependent loadings involving body force, hydrostatic pressure and earthquake loadings is implemented. The sensitivity analysis is performed using the concept of mesh design velocity. In order to consider the practical requirements in the optimization model such as construction stages, many geometrical and behavioral constrains are included in the model in comparison with previous researches. The optimization problem is solved via the sequential quadratic programming (SQP) method. The proposed methods are applied successfully to an Iranian arch dam, and good results are achieved. By using such methodology, efficient software for shape optimization of concrete arch dams for practical and reliable design now is available.", (C) 2011 Elsevier Inc. All rights reserved.,"Akbari, Jalal|Ahmadi, Mohammad Taghi|Moharrami, Hamid",APPLIED MATHEMATICAL MODELLING,arch dam|shape sensitivity analysis|finite element modeling|shape optimization,10.1016/j.apm.2011.01.020
334,WOS:000286284700004,2011,A methodology for the design and development of integrated models for policy support,RIVER-BASIN MANAGEMENT|10 ITERATIVE STEPS|LAND-USE PATTERNS|PLANNING-SUPPORT|OPTIMIZATION METHODOLOGY|ORGANIZATIONAL-CHANGE|ENVIRONMENTAL-MODELS|SENSITIVITY-ANALYSIS|CELLULAR-AUTOMATA|SCENARIO ANALYSIS,"The development of Decision Support Systems (DSS) to inform policy making has been increasing rapidly. This paper aims to provide insight into the design and development process of policy support systems that incorporate integrated models. It will provide a methodology for the development of such systems that attempts to synthesize knowledge and experience gained over the past - years from developing a suite of these DSSs for a number of users in different geographical contexts worldwide. The methodology focuses on the overall iterative development process that includes policy makers, scientists and IT-specialists. The paper highlights important tasks in model integration and system development and illustrates these with some practical examples from DSS that have dynamic, spatial and integrative attributes. Crucial integrative features of modelling systems that aim to provide support to policy processes, and to which we refer as integrated Decision Support Systems, are: Synthesis of relevant drivers, processes and characteristics of the real world system at relevant spatial and temporal scales. An integrated approach linking economic, environmental and social domains. Connection to the policy context, interest groups and end-users. Engagement with the policy process. Ability to provide added value to the current decision-making practice. With this paper we aim to provide a methodology for the design and development of these integrated Decision Support Systems that includes the 'hard' elements of model integration and software development as well as the 'softer' elements related to the user-developer interaction and social learning of all groups involved in the process. (C) ", Elsevier Ltd. All rights reserved.,"van Delden, H.|Seppelt, R.|White, R.|Jakeman, A. J.",ENVIRONMENTAL MODELLING & SOFTWARE,decision support system (dss)|model integration|design and development process|iterative process|social learning|policy support,10.1016/j.envsoft.2010.03.021
335,WOS:000236131200002,2006,Discrete element representation of manure products,WHEAT EN-MASSE|SIMULATION|MODELS|PARAMETERS|FLOW,"To simulate the machine-product interactions taking place in land application equipment, models of manure products must first be developed and validated. Several parameters must be defined to appropriately represent organic fertilizers in the discrete element method (DEM) framework. The work reported herein was aimed at determining the properties of the virtual product that would allow mimicking the behaviour of manure in the DEM software PFCD. A procedure was developed to generate an assembly of particles within the domain under investigation according to a user-defined particle size distribution, as would be measured by screening. The results generated by this procedure in terms of granulometry of the assembly of particles were very close to the user specifications with errors on the number of particles and on their size averaging .% and .%, respectively. A procedure was also developed to create clusters of particles randomly oriented and located within the modeled domain. The cluster-generation code was tested for clusters made of up to six particles, but could be expanded to include more particles. A calibration procedure based on a virtual direct shear test was developed to define the properties of the resulting virtual manure. A sensitivity analysis was performed to study the influence of parameters defining the linear and Hertz-Mindlin contact constitutive models. The simulations were based on experimental results obtained for pig manure at a total solids (TS) concentration of %. The results showed that numerous parameters have an influence on the behaviour of the virtual product in the direct shear test. Implementing the measured particle size distribution for pig manure at % TS, a friction coefficient of . and a Young's modulus value of . MPa allowed reaching an angle of internal friction of . degrees and an apparent cohesion value of . kPa that favourably compared to the . degrees and . kPa values measured experimentally.", (c) 2005 Elsevier B.V. All rights reserved.,"Landry, H|Lague, C|Roberge, M",COMPUTERS AND ELECTRONICS IN AGRICULTURE,discrete element method|dem|constitutive models|input parameters|numerical modeling|manure|organic fertilizers,10.1016/j.compag.2005.10.004
336,WOS:000266765700013,2009,Nitritation performance and biofilm development of co- and counter-diffusion biofilm reactors: Modeling and experimental comparison,MEMBRANE-AERATED BIOFILM|AUTOTROPHIC NITROGEN REMOVAL|WASTE-WATER TREATMENT|PARTIAL NITRIFICATION|NITRIFYING BIOFILM|DENITRIFICATION|BIOREACTOR|AMMONIA|STRATIFICATION|OXIDATION,"A comparative study was conducted on the start-up performance and biofilm development in two different biofilm reactors with aim of obtaining partial nitritation. The reactors were both operated under oxygen limited conditions, but differed in geometry. While substrates (O-, NH) co-diffused in one geometry, they counter-diffused in the other. Mathematical simulations of these two geometries were implemented in two -D multispecies biofilm models using the AQUASIM software. Sensitivity analysis results showed that the oxygen mass transfer coefficient (K-i) and maximum specific growth rate of ammonia-oxidizing (AOB) and nitrite-oxidizing bacteria (NOB) were the determinant parameters in nitrogen conversion simulations. The modeling simulations demonstrated that Ki had stronger effects on nitrogen conversion at lower (- m d(-)) than at the higher values (>  m d(-)). The experimental results showed that the counter-diffusion biofilms developed faster and attained a larger maximum biofilm thickness than the co-diffusion biofilms. Under oxygen limited condition (DO < . mg L-) and high pH (.-.), nitrite accumulation was triggered more significantly in co-diffusion than counter-diffusion biofilms by increasing the applied ammonia loading from . to . g NH+-N L- d(-). The co- and counter-diffusion biofilms displayed very different spatial structures and population distributions after  days of operation. AOB were dominant throughout the biofilm depth in co-diffusion biofilms, while the counter-diffusion biofilms presented a stratified structure with an abundance of AOB and NOB at the base and putative heterotrophs at the surface of the biofilm, respectively. (C) ", Elsevier Ltd. All rights reserved.,"Wang, Rongchang|Terada, Akihiko|Lackner, Susanne|Smets, Barth F.|Henze, Mogens|Xia, Siqing|Zhao, Jianfu",WATER RESEARCH,nitritation|co-diffusion|counter-diffusion|biofilm development|fluorescence in situ hybridization|membrane-aerated biofilm reactor,10.1016/j.watres.2009.03.017
337,WOS:000237378200026,2006,Simulation modeling of soil and plant nitrogen use in a potato cropping system in the humid and cool environment,FERTILIZER|GROWTH|DYNAMICS|YIELD|EFFICIENCY|RATES,"With rising environmental concerns for current practices of fertilizer N management in the humid and cool areas, we simulated soil N dynamics and plant N use in the potato (Solanum tuberosum L.) cropping system using the software Stella. The objectives were to predict in-season N requirements by the potato crop, tuber yield, N uptake, N partitioning within root, leaf, stem and tuber, and N loss in the plant-soil system, and to examine the accuracy of using model predictions for N management in potato. The first-order linear and S-shaped growth processes were used in the simulation. The model was unidimensional and used a daily time step. Sensitivity analysis indicated that N inflow in the system was the key trait affecting potato N uptake and tuber yield. The model was validated by comparisons of the predictions with field study datasets at four sites conducted across Quebec, Canada. The simulated daily N uptake by the potato followed a S-growth pattern from the early vegetative stage to full bloom, and a plateau of N uptake appeared at late tuberization. The predicted maximum daily N uptake rate (. kg ha(-) day(-)) occurred at early bloom whereas the predicted maximum N transfer from stems and leaves to tubers (. kg ha(-) day(-)) occurred  weeks after the peak of N uptake. Simultaneously, high daily N uptake occurred when N concentrations in the root zone ranged between  and  kg ha(-). The predicted N uptake and potato tuber yield values were correlated to N inflows in the model (R- = .). The model estimated loss of N was % of the field measurements. Using model balancing the amounts of N needed by crops would lead to optimize plant growth and N use efficiency and to minimize N lost to the environment.", (c) 2006 Elsevier B.V. All rights reserved.,"Li, H|Parent, LE|Karam, A",AGRICULTURE ECOSYSTEMS & ENVIRONMENT,model prediction|n balance|n partitioning|potato tuber yield|simulation modeling,10.1016/j.agee.2006.01.013
338,WOS:000261743900026,2008,Modeling of radionuclide transport through repository components using finite volume finite element and multidomain methods,,"This paper presents D saturated flow and transport calculations performed using numerical methods developed within the MELODIE software in the framework of assessing the performance of a radioactive waste geological repository. This type of computational modeling is a challenging task due to the presence of strong physical, chemical and hydrogeological heterogeneities, as well as the very different geometrical scales (from dm to km) to be handled simultaneously. First, the software is briefly described, with a particular focus on the specific features that help for dealing with such contrasted system, notably the finite volume finite element and the Schwarz domain decomposition methods. IRSN calculations based on the French ""spent fuel/iron canister/clay"" concept designed by Andra in the framework of its feasibility study of a deep geological repository are then presented. Through sensitivity analyses based on various evolution scenarios, these calculations aim at assessing the possible influence of design features, waste degradation mechanisms or radionuclide transfer properties of repository components on the behavior of the containment system as a whole. The results of the simulation have shown the transport regime in the drifts depends on the efficiency of the seals. When seals are efficient, velocities are low and transfer regime in the drifts is dominated by diffusion, whereas advection is dominant when seals are poorly sealed. However, whatever the dominant transport regime (diffusion or advection) in the drifts, the activity released out from the canisters was shown to be mainly transferred by diffusion to the host rock exits. This transfer through clayey components (bentonite and host rock) highlights the importance of chemical properties of the three types of long-lived radionuclides considered in the present study. The sensitivity analysis has shown that strongly sorbed and/or weakly soluble radionuclides are less influenced by activity release mechanisms and by seal efficiency. Concerning the strongly sorbed radionuclide ()Nb, the activity is hardly transferred beyond the vicinity of the disposal cells: the sorption properties allow limiting and delaying efficiently radionuclide migration. Concerning the weakly soluble radionuclide ()Se, a part of the activity is instantaneously precipitated in the bentonite surrounding the canisters and the activity release in the system is controlled by the dissolution of this precipitated activity. Such radionuclides are thus hardly concerned with canister dissolution mechanisms. On the contrary, the migration and release at the host rock exits of ()I, which is a non-sorbed and soluble radionuclide, is shown to be influenced by the radionuclide source term, i.e. dissolution mechanism and instantaneous released fraction (IRF). The IRF, which corresponds to % of the total activity contained in the canisters impacts significantly the activity released out of the host rock when UO() matrix dissolution controlled by uranium solubility lasts up to several million years, whereas its contribution is far less marked when matrix dissolves rapidly by alpha-radiolysis. (C) ", Elsevier Ltd. All rights reserved.,"Mathieu, G.|Dymitrowska, M.|Bourgeois, M.",PHYSICS AND CHEMISTRY OF THE EARTH,radionuclide transport|radioactive waste geological repository|finite volume finite element method|domain decomposition,10.1016/j.pce.2008.10.041
339,WOS:000404558100014,2017,"RIVICE-A Non-Proprietary, Open-Source, One-Dimensional River-Ice Model",SAINT JOHN RIVER|ATHABASCA RIVER|COVER FORMATION|DAUPHIN RIVER|CANADA|SASKATCHEWAN|MANITOBA|FLOODS|FLOWS|RISK,"Currently, no river ice models are available that are free and open source software (FOSS), which can be a hindrance to advancement in the field of modelling river ice processes. This paper introduces a non-proprietary (conditional), open-source option to the scientific and engineering community, the River Ice Model (RIVICE). RIVICE is a one-dimensional, fully-dynamic wave model that mimics key river ice processes such as ice generation, ice transport, ice cover progression (shoving, submergence and juxtapositioning) and ice jam formation, details of which are highlighted in the text. Three ice jam events at Fort McMurray, Alberta, along the Athabasca River, are used as case studies to illustrate the steps of model setup, model calibration and results interpretation. A local sensitivity analysis reveals the varying effects of parameter and boundary conditions on backwater flood levels as a function of the location of ice jam lodgment along the river reach and the location along the ice jam cover. Some limitations of the model and suggestions for future research and model development conclude the paper.",,"Lindenschmidt, Karl-Erich",WATER,athabasca river|fort mcmurray|ice jam flooding|river ice modelling|rivice,10.3390/w9050314
340,WOS:000398544700012,2017,Evaluation of TRMM-Precipitation with Rain-Gauge Observation Using Hydrological Model J2000,MEASURING MISSION TRMM|WATER-BALANCE|SENSITIVITY-ANALYSIS|SATELLITE RAINFALL|BIAS CORRECTION|CLIMATE-CHANGE|BASIN|VALIDATION|SIMULATION|AFRICA,"Spatial precipitation is a major input to distributed hydrological models, and the accuracy of runoff predictions greatly depends on its accuracy. Satellite-based precipitation products are expected to offer an alternative to ground-based rainfall estimates in the present and the foreseeable future. In the present study, the suitability of tropical rainfall measuring mission (TRMM) multisatellite precipitation analysis (TMPA) rainfall in driving a distributed hydrological model for runoff prediction was evaluated. For this purpose, a hydrological model from the literature was calibrated and validated using raingauge data on daily time step for simulation of runoff in Kopili River basin (=,km) during -. The calibrated model was then used for simulation of runoff employing specialized software and compared with the observed discharge at Kherunighat gauging site. Evaluation criteria, i.e.,coefficient of correlation (CC), Nash-Sutcliffe coefficient (NSE), percent bias (PBIAS) and root mean square error (RMSE)-observations standard deviation ratio (RSR) were adopted to judge the performance of the model under different rainfall datasets. Simulation using gauge precipitation, the values of CC, NSE, PBIAS, and RSR were found to be ., ., ., and . respectively during calibration and , ., -., and ., respectively, during validation indicating overall good model performance. Furthermore, using the raw TMPA precipitation, the values of CC, NSE, PBIAS, and RSR were found to be ., -., ., and ., respectively during the period  to  and ., -., ., and ., respectively during simulation time period from  to . The moderate value of RSR indicates that the raw TMPA precipitation-based simulation represents the low flow, including rising and recession limbs fairly well, however, in case of the high-flow periods, overpredictions for all years were observed. The evaluation result concluded that the raw TRMM precipitation data are heavily biased from observed precipitation and are incompatible for daily runoff simulation in the study area and bias correction is essential for TRMM precipitation correction. However, after employing adequate bias-correction techniques, the TRMM precipitation performed well with higher degree of accuracy and can be used as an alternative to measured rainfall data due to its high spatial resolution where data are insufficient for runoff prediction, i.e.,for ungauged basins. Moreover, performance of the TRMM precipitation improved when simulated in combination with the gauge precipitation. Therefore, along with the efforts to improve satellite-based precipitation-estimation techniques, it is also important to develop more-effective near-real-time precipitation bias-adjustment techniques for hydrological applications.", (C) 2015 American Society of Civil Engineers.,"Kumar, Dheeraj|Pandey, Ashish|Sharma, Nayan|Fluegel, Wolfgang-Albert",JOURNAL OF HYDROLOGIC ENGINEERING,hydrological modeling|runoff|tmpa3b42 v7|bias-correction|j2000,10.1061/(ASCE)HE.1943-5584.0001317
341,WOS:000277447500011,2010,partDSA: deletion/substitution/addition algorithm for partitioning the covariate space in prediction,REGRESSION,"Motivation: Until now, much of the focus in cancer has been on biomarker discovery and generating lists of univariately significant genes, as well as epidemiological and clinical measures. These approaches, although significant on their own, are not effective for elucidating the synergistic qualities of the numerous components in complex diseases. These components do not act one at a time, but rather in concert with numerous others. A compelling need exists to develop analytically sound and computationally advanced methods that elucidate a more biologically meaningful understanding of the mechanisms of cancer initiation and progression by taking these interactions into account. Results: We propose a novel algorithm, partDSA, for prediction when several variables jointly affect the outcome. In such settings, piecewise constant estimation provides an intuitive approach by elucidating interactions and correlation patterns in addition to main effects. As well as generating 'and' statements similar to previously described methods, partDSA explores and chooses the best among all possible 'or' statements. The immediate benefit of partDSA is the ability to build a parsimonious model with 'and' and 'or' conjunctions that account for the observed biological phenomena. Importantly, partDSA is capable of handling categorical and continuous explanatory variables and outcomes. We evaluate the effectiveness of partDSA in comparison to several adaptive algorithms in simulations; additionally, we perform several data analyses with publicly available data and introduce the implementation of partDSA as an R package. Availability: http://cran.r-project.org/web/packages/partDSA/index.html Contact: annette.molinaro@yale.edu Supplementary information: Supplementary data are available at Bioinformatics online.",,"Molinaro, Annette M.|Lostritto, Karen|van der Laan, Mark",BIOINFORMATICS,,10.1093/bioinformatics/btq142
342,WOS:000299625100003,2012,A topological optimization approach for structural design of a high-speed low-load mechanism using the equivalent static loads method,DYNAMIC LOADS|SYSTEMS|ALGORITHM,"In high-speed low-load mechanisms, the principal loads are the inertial forces caused by the high accelerations and velocities. Hence, mechanical design should consider lightweight structures to minimize such loads. In this paper, a topological optimization method is presented on the basis of the equivalent static loads method. Finite element (FE) models of the mechanism in different positions are constructed, and the equivalent loads are obtained using flexible multibody dynamics simulation. Kinetic DOFs are used to simulate the motion joints, and a quasi-static analysis is performed to obtain the structural responses. The element sensitivity is calculated according to the static-load-equivalent equilibrium, in such a way that the influence on the inertial force is considered. A dimensionless component sensitivity factor (strain energy caused by unit load divided by kinetic energy from unit velocity) is used, which quantifies the significance of each element. Finally, the topological optimization approach is presented on the basis of the evolutionary structural optimization method, where the objective is to find the maximum ratio of strain energy to kinetic energy. In order to show the efficiency of the presented method, we presented two numerical cases. The results of these analyses show that the presented method is more efficient and can be easily implemented in commercial FE analysis software."," Copyright (C) 2011 John Wiley & Sons, Ltd.","Yang, Zhi-Jun|Chen, Xin|Kelly, Robert",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,topological optimization|high-speed low-load mechanism|equivalent static loads method|quasi-static analysis|sensitivity analysis,10.1002/nme.3253
343,WOS:000314802700001,2013,An educational model for ensemble streamflow simulation and uncertainty analysis,CLIMATE-CHANGE|SENSITIVITY|HYDROLOGY|FUTURE|TOOL,"This paper presents the hands-on modeling toolbox, HBV-Ensemble, designed as a complement to theoretical hydrology lectures, to teach hydrological processes and their uncertainties. The HBV-Ensemble can be used for in-class lab practices and homework assignments, and assessment of students' understanding of hydrological processes. Using this modeling toolbox, students can gain more insights into how hydrological processes (e.g., precipitation, snowmelt and snow accumulation, soil moisture, evapotranspiration and runoff generation) are interconnected. The educational toolbox includes a MATLAB Graphical User Interface (GUI) and an ensemble simulation scheme that can be used for teaching uncertainty analysis, parameter estimation, ensemble simulation and model sensitivity. HBV-Ensemble was administered in a class for both in-class instruction and a final project, and students submitted their feedback about the toolbox. The results indicate that this educational software had a positive impact on students understanding and knowledge of uncertainty in hydrological modeling.",,"AghaKouchak, A.|Nakhjiri, N.|Habib, E.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-445-2013
344,WOS:000367774700005,2015,Chaospy: An open source tool for designing methods of uncertainty quantification,,"The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables. (C)  The Authors.", Published by Elsevier B.V.,"Feinberg, Jonathan|Langtangen, Hans Petter",JOURNAL OF COMPUTATIONAL SCIENCE,uncertainty quantification|polynomial chaos expansions|monte carlo simulation|rosenblatt transformations|python package,10.1016/j.jocs.2015.08.008
345,WOS:000239980800018,2006,"Series of experiments for empirical validation of solar gain modeling in building energy simulation codes - Experimental setup, test cell characterization, specifications and uncertainty analysis",SPACE ANALYSIS TOOLS|PROGRAMS,"Empirical validation of building energy simulation codes is an important component in understanding the capacity and limitations of the software. Within the framework of Task /Annex  of the International Energy Agency (IEA), a series of experiments was performed in an outdoor test cell. The objective of these experiments was to provide a high-quality data set for code developers and modelers to validate their solar gain models for windows with and without shading devices. A description of the necessary specifications for modeling these experiments is provided in this paper, which includes information about the test site location, experimental setup, geometrical and thermophysical cell properties including estimated uncertainties. Computed overall thermal cell properties were confirmed by conducting a steady-state experiment without solar gains. A transient experiment, also without solar gains, and corresponding simulations from four different building energy simulation codes showed that the provided specifications result in accurate thermal cell modeling. A good foundation for the following experiments with solar gains was therefore accomplished. (c) ", Elsevier Ltd. All rights reserved.,"Manz, H.|Loutzenhiser, P.|Frank, T.|Strachan, P. A.|Bundi, R.|Maxwell, G.",BUILDING AND ENVIRONMENT,building energy simulation|empirical validation|test cell specification,10.1016/j.buildenv.2005.07.020
346,WOS:000418976100009,2017,Simulation of water distribution under surface dripper using artificial neural networks,HYDRAULIC CONDUCTIVITY|WETTING PATTERNS|IRRIGATION|DYNAMICS|HYDRUS-2D|MOVEMENT|MODELS|MICROIRRIGATION|SOFTWARE|EQUATION,"Predicting the wetting pattern of a dripper helps in the proper design of the drip irrigation system. An artificial neural network predictor model was developed based on the data from the well-tested model HYDRUS D/D. The simulation data grid from HYDRUS was converted to simpler -variables vectors of wetting ellipses. The output vectors contain the radii in x and z directions and the center's location in the z direction. The simulations were performed for several textural classes, infiltration times, emitter's discharges, hydraulic models, and other features. After training the neural network, the testing dataset showed a correlation of .-., and the tested patterns showed high similarity to the HYDRUS outputs. Additionally, the paper provided solutions for the problem of simulating larger flow emitters where the flux exceeds the soil's hydraulic conductivity and the problem of converting HYDRUS outputs to easy-to-use vectors of three parameters representing specific moisture content at a particular time. This work tried a set of  input variables' permutations suggesting the best set of top results. The best trained neural network is freely available for the benefit of researchers and for future development. The sensitivity analysis of the input variables showed that the wetting pattern is mostly affected by time of infiltration, emitter discharge, and the saturated hydraulic conductivity. Future developments of the model are promising by increasing the training data extremes and possibly by adding more features like emitter's depth for the subsurface drippers.",,"Elnesr, M. N.|Alazba, A. A.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,drip irrigation design|machine learning|neural network prediction|soil-water movement|hydrus model,10.1016/j.compag.2017.10.003
347,WOS:000371777100004,2015,Sensitivity of algorithm parameters and objective function scaling in multi-objective optimisation of water distribution systems,OF-THE-ART|DISTRIBUTION NETWORKS|GENETIC ALGORITHMS|EVOLUTIONARY ALGORITHMS|OPTIMAL OPERATION|NSGA-II|DECISION-MAKING|TOTAL-COST|DESIGN|QUALITY,"This paper presents an extensive analysis of the sensitivity of multi-objective algorithm parameters and objective function scaling tested on a large number of parameter setting combinations for a water distribution system optimisation problem. The optimisation model comprises two operational objectives minimised concurrently, the pump energy costs and deviations of constituent concentrations as a water quality measure. This optimisation model is applied to a regional non-drinking water distribution system, and solved using the optimisation software GANetXL incorporating the NSGA-II linked with the network analysis software EPANet. The sensitivity analysis employs a set of performance metrics, which were designed to capture the overall quality of the computed Pareto fronts. The performance and sensitivity of NSGA-II parameters using those metrics is evaluated. The results demonstrate that NSGA-II is sensitive to different parameter settings, and unlike in the single-objective problems, a range of parameter setting combinations appears to be required to reach a Pareto front of optimal solutions. Additionally, inadequately scaled objective functions cause the NSGA-II bias towards the second objective. Lastly, the methodology for performance and sensitivity analysis may be used for calibration of algorithm parameters.",,"Mala-Jetmarova, Helena|Barton, Andrew|Bagirov, Adil",JOURNAL OF HYDROINFORMATICS,algorithm parameters|multi-objective optimisation|performance metrics|scaling|sensitivity|water distribution systems,10.2166/hydro.2015.062
348,WOS:000302794000003,2012,Techno-Economics for Conversion of Lignocellulosic Biomass to Ethanol by Indirect Gasification and Mixed Alcohol Synthesis,,"This techno-economic study investigates the production of ethanol and a higher alcohols coproduct by conversion of lignocelluosic biomass to syngas via indirect gasification followed by gas-to-liquids synthesis over a precommercial heterogeneous catalyst. The design specifies a processing capacity of , dry U.S. tons (, dry metric tonnes) of woody biomass per day and incorporates  research targets from NREL and other sources for technologies that will facilitate the future commercial production of cost-competitive ethanol. Major processes include indirect steam gasification, syngas cleanup, and catalytic synthesis of mixed alcohols, and ancillary processes include feed handling and drying, alcohol separation, steam and power generation, cooling water, and other operations support utilities. The design and analysis is based on research at NREL, other national laboratories, and The Dow Chemical Company, and it incorporates commercial technologies, process modeling using Aspen Plus software, equipment cost estimation, and discounted cash flow analysis. The design considers the economics of ethanol production assuming successful achievement of internal research targets and n(th)-plant costs and financing. The design yields . gallons of ethanol and . gallons of higher-molecular-weight alcohols per U.S. ton of biomass feedstock. A rigorous sensitivity analysis captures uncertainties in costs and plant performance."," (C) 2012 American Institute of Chemical Engineers Environ Prog, 31: 182-190, 2012","Dutta, Abhijit|Talmadge, Michael|Hensley, Jesse|Worley, Matt|Dudgeon, Doug|Barton, David|Groenendijk, Peter|Ferrari, Daniela|Stears, Brien|Searcy, Erin|Wright, Christopher|Hess, J. Richard",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,biomass|thermochemical conversion|indirect gasification|tar reforming|mixed alcohols|process design,10.1002/ep.10625
349,WOS:000388621000001,2016,Predictive Simulation of Seawater Intrusion in a Tropical Coastal Aquifer,WATER-INTRUSION|CLIMATE-CHANGE|FLORIDA|ISLAND|MODEL|FLOW,"The solute transport in a tropical, coastal aquifer of southern India is numerically simulated considering the possible cases of aquifer recharge, freshwater draft, and seawater intrusion using numerical modeling software. The aquifer considered for the study is a shallow, unconfined aquifer with lateritic formations having good monsoon rains up to about ,mm during June to September and the rest of the months almost dry. The model is calibrated for a two-year period and validated against the available dataset, which gave satisfactory results. The groundwater flow pattern during the calibration period shows that for the month of May a depleted water table and during the monsoon month of August a saturated water table was predicted. The sensitivity analysis of model parameters reveals that the hydraulic conductivity and recharge rate are the most sensitive parameters. Based on seasonal investigation, the seawater intrusion is found to be more sensitive to pumping and recharge rates compared to the aquifer properties. The water balance study confirms that river seepage and rainfall recharge are the major input to the aquifer. The model is used to forecast the landward movement of seawater intrusion because of the anticipated increase in freshwater draft scenarios in combination with the decreased recharge rate over a longer period. The results of the predictive simulations indicate that seawater intrusion may still confine up to a distance of approximately -m landward for the scenarios considered and thus are sustainable.", (C) 2015 American Society of Civil Engineers.,"Lathashri, U. A.|Mahesha, A.",JOURNAL OF ENVIRONMENTAL ENGINEERING,coastal aquifer|freshwater|modflow|seawater intrusion|seawat|solute transport,10.1061/(ASCE)EE.1943-7870.0001037
350,WOS:000361906900013,2015,A bootstrap method for estimating uncertainty of water quality trends,LOAD ESTIMATION|REGRESSION|STREAM|VARIABLES|MODELS|TESTS,"Estimation of the direction and magnitude of trends in surface water quality remains a problem of great scientific and practical interest. The Weighted Regressions on Time, Discharge, and Season (WRTDS) method was recently introduced as an exploratory data analysis tool to provide flexible and robust estimates of water quality trends. This paper enhances the WRTDS method through the introduction of the WRTDS Bootstrap Test (WBT), an extension of WRTDS that quantifies the uncertainty in WRTDS-estimates of water quality trends and offers various ways to visualize and communicate these uncertainties. Monte Carlo experiments are applied to estimate the Type I error probabilities for this method. WBT is compared to other water-quality trend-testing methods appropriate for data sets of one to three decades in length with sampling frequencies of - observations per year. The software to conduct the test is in the EGRETci R-package.", Published by Elsevier Ltd.,"Hirsch, Robert M.|Archfield, Stacey A.|De Cicco, Laura A.",ENVIRONMENTAL MODELLING & SOFTWARE,water quality|bootstrap|trend|uncertainty analysis,10.1016/j.envsoft.2015.07.017
351,WOS:000175645300003,2002,Automatic versus manual model differentiation to compute sensitivities and solve non-linear inverse problems,,"Emerging tools for automatic differentiation (AD) of computer programs should be of great benefit for the implementation of many derivative-based numerical methods such as those used for inverse modeling. The Odyssee software, one such tool for Fortran  codes, has been tested on a sample model that solves a D non-linear diffusion-type equation. Odyssee offers both the forward and the reverse differentiation modes, that produce the tangent and the cotangent models, respectively. The two modes have been implemented on the sample application. A comparison is made with a manually-produced differentiated code for this model (MD), obtained by solving the adjoint equations associated with the model's discrete state equations. Following a presentation of the methods and tools and of their relative advantages and drawbacks, the performances of the codes produced by the manual and automatic methods are compared, in terms of accuracy and of computing efficiency (CPU and memory needs). The perturbation method (finite-difference approximation of derivatives) is also used as a reference. Based on the test of Taylor, the accuracy of the two AD modes proves to be excellent and as high as machine precision permits, a good indication of Odyssee's capability to produce error-free codes. In comparison, the manually-produced derivatives (MD) sometimes appear to be slightly biased, which is likely due to the fact that a theoretical model (state equations) and a practical model (computer program) do not exactly coincide, while the accuracy of the perturbation method is very uncertain. The MD code largely outperforms all other methods in computing efficiency, a subject of current research for the improvement of AD tools. Yet these tools can already be of considerable help for the computer implementation of many numerical methods, avoiding the tedious task of hand-coding the differentiation of complex algorithms.", (C) 2002 Published by Elsevier Science Ltd.,"Elizondo, D|Cappelaere, B|Faure, C",COMPUTERS & GEOSCIENCES,code differentiation|optimization|adjoint state|data assimilation|sensitivity analysis|odyssee,10.1016/S0098-3004(01)00048-6
352,WOS:000320010900007,2013,UNCERTAINTY QUANTIFICATION IN DAMAGE MODELING OF HETEROGENEOUS MATERIALS,POLYMER COMPOSITES|FAILURE ANALYSIS|MATRIX CRACKING|CARBON-FIBER|FATIGUE|CALIBRATION|BEHAVIOR,"This manuscript investigates the use of Bayesian statistical methods for calibration and uncertainty quantification in rate-dependent damage modeling of composite materials. The epistemic and aleatory uncertainties inherent in the model prediction due to model parameter uncertainty, model form error, solution approximations, and measurement errors are investigated. Gaussian process surrogate models are developed to replace expensive finite element models in the analysis. A viscous damage model is employed with a solution algorithm designed for implementation within a commercial finite element software package (Abaqus). Experimental results from a suite of monotonic load tests conducted on unidirectional glass fiber reinforced epoxy composite samples at multiple strain rates and strain orientations are used to quantify the uncertainty in the prediction of the composite response within a Bayesian framework.",,"Bogdanor, Michael J.|Mahadevan, Sankaran|Oskay, Caglar",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,composite materials|bayesian calibration|rate-dependent damage|gaussian process surrogate model,10.1615/IntJMultCompEng.2013005821
353,WOS:000382797200018,2016,Thermodynamic feasibility and life cycle assessment of hydrogen production via reforming of poultry fat,GREENHOUSE-GAS EMISSIONS|PACKED-BED REACTOR|WASTE ANIMAL FATS|BIODIESEL PRODUCTION|VEGETABLE-OILS|BIOMASS GASIFICATION|NATURAL-GAS|COOKING OIL|STEAM|FUEL,"This study aims at contributing to the area of bio-based hydrogen production system development. A hydrogen production system via autothermal reforming of poultry fat was comprehensively investigated by life cycle assessment, after identification of the optimal thermodynamic operating conditions obtained via a detailed analysis of the involved chemical reactions. In the life cycle assessment, the system boundaries include reforming and rendering along with the required transportation processes. The rendering data are adapted from a literature review, whereas the reforming inventories data are derived from the process design and simulation of the entire hydrogen production process in Aspen PIus (TM) software. The life cycle inventories data for the hydrogen system are computationally implemented into SimaPro .. Six relevant environmental impact categories are evaluated based on the CML baseline . An energy analysis is also carried out based on cumulative energy demand and cumulative exergy demand as additional impacts categories. The life cycle assessment results are subjected to a systematic sensitivity analysis and compared to those achieved by other routes used for hydrogen production. The results show that poultry fat is a promising option for renewable hydrogen production considering the high productivity achievable with poultry fat (. mol H-/kg of poultry fat); however, minimization of the heat requirement of the process is highly recommended to improve the system energetics and environmental performance. (C) ", Elsevier Ltd. All rights reserved.,"Hajjaji, Noureddine|Houas, Ammar|Pons, Marie-Noelle",JOURNAL OF CLEANER PRODUCTION,hydrogen|poultry fat|autothermal reforming|energy analysis|life cycle assessment,10.1016/j.jclepro.2015.12.018
354,WOS:000331776000033,2014,Characterisation factors for life cycle impact assessment of sound emissions,ROAD TRAFFIC NOISE|SENSITIVITY-ANALYSIS|LCA|FRAMEWORK,"Noise is a serious stressor affecting the health of millions of citizens. It has been suggested that disturbance by noise is responsible for a substantial part of the damage to human health. However, no recommended approach to address noise impacts was proposed by the handbook for life cycle assessment (LCA) of the European Commission, nor are characterisation factors (CFs) and appropriate inventory data available in commonly used databases. This contribution provides CFs to allow for the quantification of noise impacts on human health in the LCA framework. Noise propagation standards and international reports on acoustics and noise impacts were used to define the model parameters. Spatial data was used to calculate spatially-defined CFs in the form of -by--km maps. The results of this analysis were combined with data from the literature to select input data for representative archetypal situations of emission (e.g. urban day with a frequency of  Hz, rural night at  Hz, etc.). A total of  spatial and  archetypal CFs were produced to evaluate noise impacts at a European level (i.e. EU). The possibility of a user-defined characterisation factor was added to support the possibility of portraying the situation of full availability of information, as well as a highly-localised impact analysis. A Monte Carlo-based quantitative global sensitivity analysis method was applied to evaluate the importance of the input factors in determining the variance of the output. The factors produced are ready to be implemented in the available LCA databases and software. The spatial approach and archetypal approach may be combined and selected according to the amount of information available and the life cycle under study. The framework proposed and used for calculations is flexible enough to be expanded to account for impacts on target subjects other than humans and to continents other than Europe.", (C) 2013 Elsevier B.V. All rights reserved.,"Cucurachi, S.|Heijungs, R.",SCIENCE OF THE TOTAL ENVIRONMENT,noise|noise impacts|life cycle|lcia|lca|annoyance,10.1016/j.scitotenv.2013.07.080
355,WOS:000245766800002,2007,Methods and object-oriented software for FE reliability and sensitivity analysis with application to a bridge structure,OPTIMIZATION,"This paper addresses the growing demand for finite-element software with capabilities to incorporate uncertainty in the input parameters. Reliability and response sensitivity algorithms are implemented in the general-purpose finite-element software OpenSees, which employs an object-oriented programming approach to achieve a sustainable software with focus on maintainability and extensibility. The product is a comprehensive and freely available library of software tools for finite-element reliability and response sensitivity analysis. A numerical example involving a detailed model of a highway bridge with inelastic material behavior and  random variables is presented to demonstrate features of the methodology and the software. Importance vectors are employed to rank the input parameters according to their relative influence on the structural reliability. The required response sensitivities are obtained by an extensive implementation of the direct differentiation method.",,"Haukaas, Terje|Kiureghian, Armen Der",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2007)21:3(151)
356,WOS:000352651200005,2015,Global warming potential of hydrogen and methane production from renewable electricity via power-to-gas technology,LIFE-CYCLE ASSESSMENT|ENERGY|CO2|CAPTURE|INFRASTRUCTURE|ABSORPTION|SOLVENTS|STORAGE|SYSTEM|PLANTS,"Power-to-gas technology enables storage of surplus electricity from fluctuating renewable sources such as wind power or photovoltaics, by generating hydrogen (H-) via water electrolysis, with optional methane (CH) synthesis from carbon dioxide (CO) and H-; the advantage of the latter is that CH can be fed into existing gas infrastructure. This paper presents a life cycle assessment (LCA) of this technological concept, evaluating the main parameters influencing global warming potential (GWP) and primary energy demand. The conducted LCA of power-to-gas systems includes the production of H- or CH from cradle to gate. Product utilization was not evaluated but considered qualitatively during interpretation. Material and energy balances were modeled using the LCA software GaBi  (PE International). The assessed impacts of H- and CH from power-to-gas were compared to those of reference processes, such as steam reforming of natural gas and crude oil as well as natural gas extraction. Sensitivity analysis was used to evaluate the influence of the type of electricity source, the efficiency of the electrolyzer, and the type of CO source used for methanation. The ecological performance of both H- and CH produced via power-to-gas strongly depends on the electricity generation source. The assessed impacts of H- production are only improved if GWP of the utilized electricity does not exceed  g CO per kWh. Due to reduced efficiency, the assessed impacts of CH are higher than that of H-. Thus, the environmental break-even point for CH production is  g CO per kWh if utilized CO is treated as a waste product, and  g CO per kWh if the CO separation effort is included. Electricity mix of EU- countries is therefore not at all suitable as an input. Utilization of renewable H- and CH in the industry or the transport sector offers substantial reduction potential in GWP and primary energy demand. H- and CH production through power-to-gas with electricity from renewable sources, such as wind power or photovoltaics, offers substantial potential to reduce GWP and primary energy demand. However, the input of electricity predominately generated from fossil resources leads to a higher environmental impact of H- and CH compared to fossil reference processes and is not recommended. As previously bound CO is re-emitted when CH is utilized for instance in vehicles, the type of CO source and the allocation method have a significant influence on overall ecological performance.",,"Reiter, Gerda|Lindorfer, Johannes",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,alternative fuels|carbon dioxide utilization|energy storage|life cycle assessment (lca)|methane|power-to-gas|hydrogen,10.1007/s11367-015-0848-0
357,WOS:000374807600006,2016,Sensitivity of a third generation wave model to wind and boundary condition sources and model physics: A case study from the South Atlantic Ocean off Brazil coast,NONLINEAR ENERGY-TRANSFER|SHALLOW-WATER|DISSIPATION|SPECTRUM|PARAMETERIZATIONS|COMPUTATIONS|PERFORMANCE|SWAN,"Three different packages describing the white capping dissipation process, and the corresponding energy input from wind to wave were used to study the surface wave dynamics in South Atlantic Ocean, close to the Brazilian coast. A host of statistical parameters were computed to evaluate the performance of wave model in terms of simulated bulk wave parameters. Wave measurements from a buoy deployed off Santa Catarina Island, Southern Brazil and data along the tracks of Synthetic Aperture Radars were compared with simulated bulk wave parameters; especially significant wave height, for skill assessment of different packages. It has been shown that using a single parameter representing the performance of source and sink terms in the wave model, or relying on data from only one period of simulations for model validation and skill assessment would be misleading. The model sensitivity to input parameters such as time step and grid size were addressed using multiple datasets. The wind data used for the simulation were obtained from two different sources, and provided the opportunity to evaluate the importance of input data quality. The wind speed extracted from remote sensing satellites was compared to wind datasets used for wave modeling. The simulation results showed that the wind quality and its spatial resolution is highly correlated to the quality of model output. Two different sources of wave information along the open boundaries of the model domain were used for skill assessment of a high resolution wave model for the study area. It has been shown, based on the sensitivity analysis, that the effect of using different boundary conditions would decrease as the distance from the open boundary increases; however, the difference were still noticeable at the buoy location which was located - km away from the model boundaries; but restricted to the narrow band of the low frequency wave spectrum. (C) ", Elsevier Ltd. All rights reserved.,"Siadatmousavi, S. Mostafa|Jose, Felix|da Silva, Graziela Miot",COMPUTERS & GEOSCIENCES,white capping|wave spectrum|model uncertainties|santa catarina island,10.1016/j.cageo.2015.09.025
358,WOS:000276920300002,2010,The decision model of task allocation for constrained stochastic distributed systems,COMPUTING SYSTEMS|MAXIMIZING RELIABILITY|ALGORITHMS|OPTIMIZATION,"In distributed systems, an application program is divided into several software modules, which need to be allocated to processors connected by communication links. The distributed system reliability (DSR) could be defined as the probability of successfully completing the distributed program. Previous studies about optimal task allocation with respect to DSR focused on the effects of the inter-connectivity of processors, the failure rates of the processors, and the failure rates of the communication links. We are the first to study the effects of module software reliabilities and module execution frequencies on the optimal task allocation. By viewing each module as a state in the Markov process, we build a task allocation decision model to maximize DSR for distributed systems with % reliable network. In this model, the DSR is derived from the module software reliabilities, the processor hardware reliabilities, the transition probabilities between modules, and the task allocation matrix. Resource constraints of memory space limitation and computation load limitation on each processor are considered. The constraint of total system cost, including the execution cost, the communication cost, and the failure cost, is also considered. We solve the problem by Constraint Programming using the ILOG SOLVER library. We then apply the proposed model to a case extended from previous studies. Finally, a sensitivity analysis is performed to verify the effects of module software reliabilities and processor hardware reliabilities on the DSR and on the task allocation decision. (C) ", Elsevier Ltd. All rights reserved.,"Jou, Chichang",COMPUTERS & INDUSTRIAL ENGINEERING,task allocation|decision model|distributed system reliability|markov process|constraint programming,10.1016/j.cie.2009.04.004
359,WOS:000264171200019,2009,A Life Cycle Comparison of Alternative Cheese Packages,,"A comparative life cycle assessment (LCA) between three different cheese packages (P: completely polypropylene (PP), P: tin and polyethylene (PE), and P: carton and PE) has been carried out for the production, distribution and waste disposal (% landfill) phase. A package for  kg of cheese was selected as the functional unit. SimaPro software (PReConsultants, The Netherlands) was used for the LCA study. The EcoIndicator  method was selected for comparison of the packages. The comparisons show that the total environmental performance of the cheese package types in order from worst to best is P, P, and P. This conclusion was supported by a sensitivity analysis, which was conducted by using different impact assessment methods.",,"Banar, Muefide|Cokaygil, Zerrin",CLEAN-SOIL AIR WATER,ecoindicator 99|food technology|landfilling|life cycle assessment|packaging|simapro7,10.1002/clen.200700185
360,WOS:000355985700016,2015,Carbon dioxide utilisation for production of transport fuels: process and economic analysis,FISCHER-TROPSCH SYNTHESIS|TECHNOECONOMIC ASSESSMENT|ANAEROBIC-DIGESTION|DIMETHYL ETHER|CO2 CAPTURE|GASIFICATION|MONOETHANOLAMINE|TECHNOLOGIES|CHALLENGES|LIQUIDS,"Utilising CO as a feedstock for chemicals and fuels could help mitigate climate change and reduce dependence on fossil fuels. For this reason, there is an increasing world-wide interest in carbon capture and utilisation (CCU). As part of a broader project to identify key technical advances required for sustainable CCU, this work considers different process designs, each at a high level of technology readiness and suitable for large-scale conversion of CO into liquid hydrocarbon fuels, using biogas from sewage sludge as a source of CO. The main objective of the paper is to estimate fuel production yields and costs of different CCU process configurations in order to establish whether the production of hydrocarbon fuels from commercially proven technologies is economically viable. Four process concepts are examined, developed and modelled using the process simulation software Aspen Plus (R) to determine raw materials, energy and utility requirements. Three design cases are based on typical biogas applications: () biogas upgrading using a monoethanolamine (MEA) unit to remove CO, () combustion of raw biogas in a combined heat and power (CHP) plant and () combustion of upgraded biogas in a CHP plant which represents a combination of the first two options. The fourth case examines a post-combustion CO capture and utilisation system where the CO removal unit is placed right after the CHP plant to remove the excess air with the aim of improving the energy efficiency of the plant. All four concepts include conversion of CO to CO via a reverse water-gas-shift reaction process and subsequent conversion to diesel and gasoline via Fischer-Tropsch synthesis. The studied CCU options are compared in terms of liquid fuel yields, energy requirements, energy efficiencies, capital investment and production costs. The overall plant energy efficiency and production costs range from -% and .-. pound per litre of liquid fuels, respectively. A sensitivity analysis is also carried out to examine the effect of different economic and technical parameters on the production costs of liquid fuels. The results indicate that the production of liquid hydrocarbon fuels using the existing CCU technology is not economically feasible mainly because of the low CO separation and conversion efficiencies as well as the high energy requirements. Therefore, future research in this area should aim at developing novel CCU technologies which should primarily focus on optimising the CO conversion rate and minimising the energy consumption of the plant.",,"Dimitriou, Ioanna|Garcia-Gutierrez, Pelayo|Elder, Rachael H.|Cuellar-Franca, Rosa M.|Azapagic, Adisa|Allen, Ray W. K.",ENERGY & ENVIRONMENTAL SCIENCE,,10.1039/c4ee04117h
361,WOS:000243927200008,2007,Reliability-based multiobjective optimization for automotive crashworthiness and occupant safety,,"This paper presents a methodology for reliability-based multiobjective optimization of large-scale engineering systems. This methodology is applied to the vehicle crashworthiness design optimization for side impact, considering both structural crashworthiness and occupant safety, with structural weight and front door velocity under side impact as objectives. Uncertainty quantification is performed using two first order reliability method-based techniques: approximate moment approach and reliability index approach. Genetic algorithm-based multiobjective optimization software GDOT, developed in-house, is used to come up with an optimal pareto front in all cases. The technique employed in this study treats multiple objective functions separately without combining them in any form. It shows that the vehicle weight can be reduced significantly from the baseline design and at the same time reduce the door velocity. The obtained pareto front brings out useful inferences about optimal design regions. A decision-making criterion is subsequently invoked to select the ""best"" subset of solutions from the obtained nondominated pareto optimal solutions. The reliability, thus computed, is also checked with Monte Carlo simulations. The optimal solution indicated by knee point on the optimal pareto front is verified with LS-DYNA simulation results.",,"Sinha, Kaushik",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,reliability-based multiobjective optimization|uncertainty quantification|form|nondominated points|gdot|pareto optimal solution|knee point|automotive crashworthiness|occupant safety|side impact|monte carlo simulation,10.1007/s00158-006-0050-x
362,WOS:000389785300007,2017,An automated decision support system for aided assessment of variogram models,GLOBAL SENSITIVITY-ANALYSIS|ENVIRONMENTAL-MODELS|VALIDATION|FRAMEWORK|NETWORKS|ROBUST|SOIL,"In the present paper, an extensive cross-validation procedure, based on the analysis of numerical indices and graphical tools, is described and discussed. The procedure has been implemented in a software application designed to support practitioners in the variogram model assessment. It provides an extensive report, which summarizes a large post-processing stage and suggests how to interpret the performed analysis to rate the model to be validated. Besides classical accuracy indices, two new integrated tools based on the variogram of residuals are introduced, which take the spatial nature of the dataset into account. Finally, inspecting the summary report, the user can decide whether the considered model is satisfactory for his/her goals or it needs to be improved. Finally, a case study is presented related to the variogram assessment of groundwater level measured in a porous shallow aquifer of the Apulia Region (South -Italy). (C) ", Elsevier Ltd. All rights reserved.,"Barca, Emanuele|Porcu, Emilio|Bruno, Delia|Passarella, Giuseppe",ENVIRONMENTAL MODELLING & SOFTWARE,geostatistics|extensive cross-validation|rank coefficient of spatial correlation|decision-support system,10.1016/j.envsoft.2016.11.004
363,WOS:000415358000013,2017,Uncertain supply chain network design considering carbon footprint and social factors using two-stage approach,ANALYTIC HIERARCHY PROCESS|CONSTRAINED PROGRAMMING-MODEL|LIFE-CYCLE ASSESSMENT|FACILITY LOCATION|FUZZY ENVIRONMENT|REVERSE LOGISTICS|VENDOR SELECTION|GREEN LOGISTICS|DECISION-MAKING|SERVICE LEVEL,"Sustainable development has become one of the leading global issues over the period of time. Currently, implementation of sustainability in supply chain has been continuously in center of attention due to introducing stringent legislations regarding environmental pollution by various governments and increasing stakeholders' concerns toward social injustice. Unfortunately, literature is still scarce on studies considering all three dimensions (economical, environmental and social) of sustainability for the supply chain. An effective supply chain network design (SCND) is very important to implement sustainability in supply chain. This study proposes an uncertain SCND model that minimizes the total supply chain-oriented cost and determines the opening of plants, warehouses and flow of materials across the supply chain network by considering various carbon emissions and social factors. In this study, a new AHP and fuzzy TOPSIS-based methodology is proposed to transform qualitative social factors into quantitative social index, which is subsequently used in chance-constrained SCND model with an aim at reducing negative social impact. Further, the carbon emission of supply chain is estimated by considering a composite emission that consists of raw material, production, transportation and handling emissions. In the model, a carbon emission cap is imposed on total supply chain to reduce the carbon footprint of supply chain. To solve the proposed model, a code is developed in AMPL software using a nonlinear solver SNOPT. The applicability of the proposed model is illustrated with a numerical example. The sensitivity analysis examines the effects of reducing carbon footprint cap, negative social impacts and varying probability on the total cost of the supply chain. It is observed that a stricter carbon cap over supply chain network leads to opening of more plants across the supply chain. In addition, carbon footprint of supply chain is found to be decreased in certain extent with the reduction in negative social impacts from suppliers. The carbon footprint of the supply chain is found to be reduced with increasing certainty of material supply from the suppliers. The total supply chain cost is observed to be augmented with increasing probability.",,"Das, Rakhi|Shaw, Krishnendu",CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY,chance-constrained programming|supply chain network design|carbon footprint|sustainability|stochastic programming|social sustainability,10.1007/s10098-017-1446-6
364,WOS:000401878000001,2017,Minmax regret combinatorial optimization problems with investments,INTERVAL DATA|ALGORITHM,"A new minmax regret optimization model in a system with uncertain parameters is proposed. In this model it is allowed to make investments before a minmax regret solution is implemented in order to modify the source or the nature of the existing uncertainty. Therefore, it is allowed to spend resources in order to change the basic cost structure of the system and take advantage of the modified system to find a robust solution. Some properties of this model allow us to have proper Mathematical Programming formulations that can be solved by standard optimization packages. As a practical application we consider the shortest path problem in a network in which it is possible to modify the uncertainty intervals for the arc costs by investing in the system. We also give an approximate algorithm and generalize some existing results on constant factor approximations. (C) ", Elsevier Ltd. All rights reserved.,"Conde, Eduardo|Leal, Marina",COMPUTERS & OPERATIONS RESEARCH,minmax regret models|robustness and sensitivity analysis|shortest path problem,10.1016/j.cor.2017.03.007
365,WOS:000165465600002,2000,Generating probabilistic spatially-explicit individual and population exposure estimates for ecological risk assessments,LANDSCAPES|DYNAMICS|PREY,"Exposure to chemical contaminants in various media must be estimated when performing ecological risk assessments. Exposure estimates are often based on the th-percentile upper confidence limit on the mean concentration of all samples, calculated without regard to critical ecological and spatial information about the relative relationship of receptors, their habitats, and contaminants. This practice produces exposure estimates that are potentially unrepresentative of the ecology of the receptor. This article proposes a habitat area and quality conditioned exposure estimator, E[HQ], that requires consideration of these relationships. It describes a spatially explicit ecological exposure model to facilitate calculation of E[HQ]. The model provides () a flexible platform for investigating the effect of changes in habitat area, habitat quality, foraging area, and population size on exposure estimates, and () a tool for calculating E[HQ] for use in actual risk assessments. The inner loop of a Visual Basic(R) program randomly walks a receptor over a multicelled landscape-each cell of which contains values for cell area, habitat area, habitat quality, and concentration-accumulating an exposure estimate until the total area foraged is less than or equal to a given foraging area. An outer loop then steps through foraging areas of increasing size. This program is iterated by Monte Carlo software, with the number of iterations representing the population size. Results indicate that () any single estimator may over- or underestimate exposure, depending on foraging strategy and spatial relationships of habitat and contamination, and () changes in exposure estimates in response to changes in foraging and habitat area are not linear.",,"Hope, BK",RISK ANALYSIS,ecological risk|spatially explicit exposures|populations|uncertainty analysis,10.1111/0272-4332.205053
366,WOS:000246090900006,2007,Therapeutic drug monitoring of kidney transplant recipients using profiled support vector machines,NEURAL-NETWORKS|CYCLOSPORINE|TIME|PREDICTION|REGRESSION|PHARMACOKINETICS|DOSAGE|MODEL,"This paper proposes a twofold approach for therapeutic drug monitoring (TDM) of kidney recipients using support vector machines (SVMs), for both predicting and detecting Cyclosporine A (CyA) blood concentrations. The final goal is to build useful, robust, and ultimately understandable models for individualizing the dosage of CyA. We compare SVMs with several neural network models, such as the multilayer perceptron (MLP), the Elman recurrent network, finite/infinite impulse response networks, and neural network ARMAX approaches. In addition, we present a profile-dependent SVM (PD-SVM), which incorporates a priori knowledge in both tasks. Models are compared numerically, statistically, and in the presence of additive noise. Data from  renal allograft recipients were used to develop the models. Patients followed a standard triple therapy, and CyA trough concentration was the dependent variable. The best results for the CyA blood concentration prediction were obtained using the PD-SVM (mean error of . ng/mL and root-mean-square error of . ng/mL in the validation set) and appeared to be more robust in the presence of additive noise. The proposed PD-SVM improved results from the standard SVM and MLP, specially significant (both numerical and statistically) in the one-against-all scheme. Finally, some clinical conclusions were obtained from sensitivity rankings of the models and distribution of support vectors. We conclude that the PD-SVM approach produces more accurate and robust models than do neural networks. Finally, a software tool for aiding medical decision-making including the prediction models is presented.",,"Camps-Valls, Gustavo|Soria-Olivas, Emilio|Perez-Ruixo, Juan Jose|Perez-Cruz, Fernando|Artes-Rodriguez, Antonio|Jimenez-Torres, Nicolas Victor",IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART C-APPLICATIONS AND REVIEWS,cyclosporine|kidney transplantation|neural networks|sensitivity analysis|support vector machines (svms)|therapeutic drug monitoring (tdm),10.1109/TSMCC.2007.893279
367,WOS:000391079300008,2016,"Comparative life cycle assessment of ceramic brick, concrete brick and cast-in-place reinforced concrete exterior walls",INTERIOR WOOD DOORS|IMPACT ASSESSMENT|NONRESIDENTIAL BUILDINGS|ENVIRONMENTAL FOOTPRINT|ENERGY-CONSUMPTION|GLAZING SYSTEMS|GREEN ROOFS|CONSTRUCTION|ECOTOXICITY|PERFORMANCE,"The construction sector has a meaningful contribution to the global scarcity of natural resources, as well as to impacts on the natural environment Most life cycle assessments have focused on impacts associated with the energy efficiency of buildings, in particular during the operational phase. However, the construction phase of buildings accounts for a significant share of a building's embodied energy and is responsible for impacts related to resource depletion. With the aim to contribute to more all-embracing assessments in the construction sector, this study aims to compare three different wall types commonly used in Brazil, according to their environmental performances: ceramic brick, concrete brick and cast-in place reinforced concrete exterior walls. The results were analyzed with the software SimaPro . and with the life cycle impact assessment method IMPACT + (version Q.). Ceramic brick walls have less impact than the concrete brick and the cast-in-place reinforced concrete exterior walls on three different endpoint indicators (Climate Change, Resource Depletion and Water Withdrawal). The results were not significant regarding impacts on Human Health and Ecosystem, Quality. Different sensitivity analyses were carried out in order to test the final results, as well as uncertainty analysis, related to the variability of inventory data and the characterization of the life cycle inventory results into midpoints and/or endpoints. (C) ", Elsevier Ltd. All rights reserved.,"de Souza, Danielle Maia|Lafontaine, Mia|Charron-Doucet, Francois|Chappert, Benoit|Kicak, Karine|Duarte, Fernanda|Lima, Luis",JOURNAL OF CLEANER PRODUCTION,life cycle assessment|ceramic bricks|concrete blocks|brazil|simapro|impact 2002+,10.1016/j.jclepro.2016.07.069
368,WOS:000354547700014,2015,Identification of Critical Erosion Watersheds for Control Management in Data Scarce Condition Using the SWAT Model,LEAST-SQUARES REGRESSION|CRITICAL SUBWATERSHEDS|SENSITIVITY-ANALYSIS|SEDIMENT YIELD|SOIL-EROSION|PRONE AREAS|CLIMATE|CATCHMENT|INDIA|REQUIREMENTS,"Identification of critical watersheds prone to soil erosion has been performed by using a hydrological model in data scarce Damodar River catchment, located in Jharkhand state of India. Model is calibrated and validated for two watersheds, i.e.,()Nagwan, .km; and ()Banikdih, .km, nested within the catchment. The achieved R values of predicted monthly runoff and sediment yield varies, respectively, .-. and .-., for both the watersheds during calibration and validation period. Calibration and validation results revealed that model is predicting monthly runoff and sediment yield satisfactory for the two watersheds of the Damodar River catchment. The validated model parameters were then up-scaled to the whole catchment and model was run from - to identify the critical watersheds. Model was successfully used for prioritization of  watersheds delineated using the computer software model within the catchment. In delineation process, the boundaries of  watersheds matched exactly with the watersheds delineated manually by Damodar Valley Corporation. Out of these  watersheds, erosion classes of  exactly matched with the manually described erosion class. For remaining  watersheds, priority of  watersheds was either one class higher or lower, whereas eight watersheds showed complete mismatch. Overall results showed that the hydrological model used in this paper may be helpful in prioritization of management strategies to manage the resources where availability of data is a big concern. Such approach for managing resources is particularly needed in developing countries for better utilization of limited resources.",,"Kumar, Sanjeet|Mishra, Ashok|Raghuwanshi, Narendra Singh",JOURNAL OF HYDROLOGIC ENGINEERING,soil and water assessment tool (swat)|calibration|validation|watershed|critical erosion|identification|prioritization,10.1061/(ASCE)HE.1943-5584.0001093
369,WOS:000301688100020,2012,Generalized estimating equations and regression diagnostics for longitudinal controlled clinical trials: A case study,DELETION DIAGNOSTICS|INFLUENTIAL OBSERVATIONS|LINEAR-MODELS|GEE|SOFTWARE,"Generalized estimating equations (GEE) were proposed for the analysis of correlated data. They are popular because regression parameters can be consistently estimated even if only the mean structure is correctly specified. GEE have been extended in several ways, including regression diagnostics for outlier detection. However, GEE have rarely been used for analyzing controlled clinical trials. The SB-LOT trial, a double-blind placebo-controlled randomized multicenter trial in which the oedema-protective effect of a vasoactive drug was investigated in patients suffering from chronic insufficiency was re-analyzed using the GEE approach. It is demonstrated that the autoregressive working correlation structure is the most plausible working correlation structure in this study. The effect of the vasoactive drug is a difference in lower leg volume of . ml per week (p=., % confidence interval .-. ml per week), making a difference of  ml at the end of the study. Deletion diagnostics are used for identification of outliers and influential probands. After exclusion of the most influential patients from the analysis, the overall conclusion of the study is not altered. At the same time, the goodness of fit as assessed by half-normal plots increases substantially. In summary, the use of GEE in a longitudinal clinical trial is an alternative to the standard analysis which usually involves only the last follow-up. Both the GEE and the regression diagnostic techniques should accompany the GEE analysis to serve as sensitivity analysis.", (C) 2011 Elsevier B.V. All rights reserved.,"Vens, Maren|Ziegler, Andreas",COMPUTATIONAL STATISTICS & DATA ANALYSIS,chronic venous insufficiency|cook statistic|deletion diagnostics|independence estimating equations|half-normal plot,10.1016/j.csda.2011.04.010
370,WOS:000090149600007,2000,An object-oriented structural optimization program,SHAPE OPTIMIZATION,"In this paper, implementation concepts of a structural optimization software using object-oriented programming (OOP) in CS ++ is presented. A brief mathematical formulation of structural optimization and continuum-based sensitivity analysis is presented. The requirements of a computational optimization environment are derived from this formulation. The OOP characteristics are analysed and this paradigm is employed in the implementation of design variables, structural performance functionals, velocity fields, design model and mathematical programming algorithms using CI-Jr. Finally, the program obtained is applied to D linear elastic examples of sizing and shape optimization.",,"Silva, CAC|Bittencourt, ML",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization|sensitivity analysis|c plus|object-oriented programming|linear elasticity,10.1007/s001580050146
371,WOS:000178672100002,2002,The potential of latent semantic analysis for machine grading of clinical case summaries,COMPREHENSION|TEXT,"`Objective: This paper introduces latent semantic analysis (LSA), a machine learning method for representing the meaning of words, sentences, and texts. LSA induces a high-dimensional semantic space from reading a very large amount of texts. The meaning of words and texts can be represented as vectors in this space and hence can be compared automatically and objectively. Psychological theory: A generative theory of the mental lexicon based on LSA is described. The word vectors LSA constructs are context free, and each word, irrespective of how many meanings or senses it has, is represented by a single vector. However, when a word is used in different contexts, context appropriate word senses emerge. Current applications: Several applications of LSA to educational software are described, involving the ability of LSA to quickly compare the content of texts, such as an essay written by a student and a target essay. Potential medical applications: An LSA-based software tool is sketched for machine grading of clinical case summaries written by medical students.", (C) 2002 Elsevier Science (USA). All rights reserved.,"Kintsch, W",JOURNAL OF BIOMEDICAL INFORMATICS,latent semantic analysis|knowledge representation|cognitive science,10.1016/S1532-0464(02)00004-7
372,WOS:000283340800004,2010,A DECISION SUPPORT TOOL FOR IRRIGATION INFRASTRUCTURE INVESTMENTS,OPTIMAL ALLOCATION|WATER-RESOURCES|SYSTEM|MODEL|MANAGEMENT|IMPACTS|FARM,"Increasing water scarcity, climate change and pressure to provide water for environmental flows urge irrigators to be more efficient. In Australia, ongoing water reforms and most recently the National Water Security Plan offer incentives to irrigators to adjust their farming practices by adopting water-saving Irrigation infrastructures to match soil, crop and climatic conditions. Water Works is a decision support tool to facilitate irrigators to make long- and short-term irrigation infrastructure investment decisions at the farm level. It helps irrigators to improve the economic efficiency, water use efficiency and environmental performance of their farm businesses. Water Works has been tested, validated and accepted by the irrigation community and researchers in NSW, Australia. The interface of Water Works is user-friendly and flexible. The simulation and optimisation module in Water Works provides an opportunity to evaluate Infrastructure investment decisions to suit their seasonal or long-term water availability. The sensitivity analysis allows substantiation of the impact of major variables Net present value, internal rate of return, benefit cost ratio and payback period are used to analyse the costs and benefits of modern irrigation technology. Application of Water Works using a whole farm-level case study indicates its effectiveness in making long- and short-term investment decisions Water Works can be easily integrated into commercial software such as spreadsheets, GIS, real-time data acquisition and control systems to further enhance its usability. Water Works can also be used in regional development planning."," Copyright (C) 2009 John Wiley & Sons, Ltd.","Khan, Shahbaz|Mushtaq, Shahbaz|Chen, Charlie",IRRIGATION AND DRAINAGE,decision support tool|water management|seasonal and long-term investment|optimisation|simulation|benefit-cost analysis|whole farm|water trading|water saving,10.1002/ird.501
373,WOS:000337062400006,2014,Life cycle assessment of nanocomposites made of thermally conductive graphite nanoplatelets,GRAPHENE|COMPOSITES,"Polymers typically have intrinsic thermal conductivity much lower than other materials. Enhancement of this property may be obtained by the addition of conductive fillers. Nanofillers are preferred to traditional ones, due to their low percolation threshold resulting from their high aspect ratio. Beyond these considerations, it is imperative that the development of such new fillers takes place in a safe and sustainable manner. A conventional life cycle assessment (LCA) has been conducted on epoxy-based composites, filled with graphite nanoplatelets (GnP). In particular, this study focuses on energy requirements for the production of such composites, in order to stress environmental hot spots and primary energy of GnP production process (nano-wastes and nanoparticles emissions are not included). A cradle-to-grave approach has been employed for this assessment, in an attributional modeling perspective. The data for the LCA have been gathered from both laboratory data and bibliographic references. A technical LCA software package, SimaPro (SimaPro .), which contains Ecoinvent () life cycle inventory (LCI) database, has been used for the life cycle impact assessment (LCIA), studying  mid-point indicators. Sensitivity and uncertainty analyses have also been performed. One kilogram of GnP filler requires , MJ of primary energy while the preparation of  kg of epoxy composite loaded with . kg of GnP  MJ. Besides energy consumption in the filler preparation, it is shown that the thermoset matrix material has also a non-negligible impact on the life cycle despite the use of GnP: the primary energy required to make epoxy resin is  MJ, i.e.,  % of the total energy to make  kg of composite. Raw material extraction and filler and resin preparation phase exhibit the highest environmental impact while the composite production is negligible. Thermosetting resin remains the highest primary energy demand when used as matrix for GnP fillers. The result of the sensitivity analysis carried out on the electricity mix used during the GnP and the composite production processes does not affect the conclusions.",,"Pizza, Alfredo|Metz, Renaud|Hassanzadeh, Mehrdad|Bantignies, Jean-Louis",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,epoxy resin|graphite nanoplatelets|impact assessment|life cycle assessment|nanocomposites|sustainable nanoproducts,10.1007/s11367-014-0733-2
374,WOS:000250352400010,2007,Parallel computing techniques for sensitivity analysis in optimum structural design,FINITE-ELEMENT-ANALYSIS|OPTIMIZATION|SYSTEMS|ENVIRONMENT|LOADS,"Among different activities of the optimum structural design using the gradient-based optimization approaches, design sensitivity analysis is the most time-consuming computational process. By introducing parallel computing techniques for sensitivity computation, significant speedup has been obtained in optimum structural design. Computation of design sensitivities is characteristically uncoupled, thus opening the door to parallelization. In this paper, two types of approaches viz. single-level and multilevel parallelisms are pursued for design sensitivities. The design sensitivities are computed using analytical and finite-difference methods. Numerical studies show that the performance of the parallel algorithms for design sensitivities on message passing systems is very good. Good speedups have been achieved in parallel multilevel sensitivity calculation. The parallel algorithms for design sensitivity analysis have been implemented on message passing parallel systems within the software platform of Parallel Computer Adaptive Language.",,"Umesha, P. K.|Venuraju, M. T.|Hartmann, D.|Leimbach, K. R.",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1016/(ASCE)0887-3801(2007)21:6(463)
375,WOS:000267670900006,2009,Life cycle assessment of Australian automotive door skins,,"Policy initiatives, such as the EU End of Life Vehicle (ELV) Directive for only % landfilling by , are increasing the pressure for higher material recyclability rates. This is stimulating research into material alternatives and end-of-life strategies for automotive components. This study presents a Life Cycle Assessment (LCA) on an Australian automotive component, namely an exterior door skin. The functional unit for this study is one door skin set ( exterior skins). The material alternatives are steel, which is currently used by Australian manufacturers, aluminium and glass-fiber reinforced polypropylene composite. Only the inputs and outputs relative to the door skin production, use and end-of-life phases were considered within the system boundary. Landfill, energy recovery and mechanical recycling were the end-of-life phases considered. The aim of the study is to highlight the most environmentally attractive material and end-of-life option. The LCA was performed according to the ISO  standard series. All information considered in this study (use of fossil and non fossil based energy resources, water, chemicals etc.) were taken up in in-depth data. The data for the production, use and end-of-life phases of the door skin set was based upon softwares such as SimaPro and GEMIS which helped in the development of the inventory for the different end-of-life scenarios. In other cases, the inventory was developed using derivations obtained from published journals. Some data was obtained from GM-Holden and the Co-operative research Centre for Advanced Automotive Technology (AutoCRC), in Australia. In cases where data from the Australian economy was unavailable, such as the data relating to energy recovery methods, a generic data set based on European recycling companies was employed. The characterization factors used for normalization of data were taken from (Saling et. al. Int J Life Cycle Assess ():- ) which detailed the method of carrying out an LCA. The production phase results in maximum raw material consumption for all materials, and it is higher for metals than for the composite. Energy consumption is greatest in the use phase, with maximum consumption for steel. Aluminium consumes most energy in the production phase. Global Warming Potential (GWP) also follows a trend similar to that of energy consumption. Photo Oxidants Creation Potential (POCP) is the highest for the landfill scenario for the composite, followed by steel and aluminium. Acidification Potential (AP) is the highest for all the end-of-life scenarios of the composite. Ozone Depletion Potential (ODP) is the highest for the metals. The net water emissions are also higher for composite in comparison to metals despite high pollution in the production phases of metallic door skins. Solid wastes are higher for the metallic door skins. The composite door skin has the lowest energy consumption in the production phase, due to the low energy requirements during the manufacturing of E-glass and its fusion with polypropylene to form sheet molding compounds. In general, the air emissions during the use phase are strongly dependent on the mass of the skins, with higher emissions for the metals than for the composite. Material recovery through recycling is the highest in metals due to efficient separation techniques, while mechanical recycling is the most efficient for the composite. The heavy steel skins produce the maximum solid wastes primarily due to higher fuel consumption. Water pollution reduction benefit is highest in case of metals, again due to the high efficiency of magnetic separation technique in the case of steel and eddy current separation technique in the case of aluminium. Material recovery in these metals reduces the amount of water needed to produce a new door skin set (water employed mainly in the ingot casting stage). Moreover, the use of heavy metals, inorganic salts and other chemicals is minimized by efficient material recovery. The use of the studied type of steel for the door skins is a poor environmental option in every impact category. Aluminium and composite materials should be considered to develop a more sustainable and energy efficient automobile. In particular, this LCA study shows that glass-fiber composite skins with mechanical recycling or energy recovery method could be environmentally desirable, compared to aluminium and steel skins. However, the current limit on the efficiency of recycling is the prime barrier to increasing the sustainability of composite skins. The study is successful in developing a detailed LCA for the three different types of door skin materials and their respective recycling or end-of-life scenarios. The results obtained could be used for future work on an eco-efficiency portfolio for the entire car. However, there is a need for a detailed assessment of toxicity and risk potentials arising from each of the four different types of door skin sets. This will require greater communication between academia and the automotive industry to improve the quality of the LCA data. Sensitivity analysis needs to be performed such as the assessment of the impact of varying substitution factors on the life cycle of a door skin. Incorporation of door skin sets made of new biomaterials need to be accounted for as another functional unit in future LCA studies.",,"Puri, Prateek|Compston, Paul|Pantano, Victor",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,acidification potential (ap)|aluminium|automotive door skins|composite material|energy recovery|global warming potential (gwp)|landfill|life cycle assessment (lca)|mechanical recycling|ozone depletion potential (odp)|photo oxidants creation potential (pocp)|steel,10.1007/s11367-009-0103-7
376,WOS:000344676000001,2014,Kinetic Study of Nonequilibrium Plasma-Assisted Methane Steam Reforming,DIELECTRIC-BARRIER DISCHARGE|CONVERSION|TRANSPORT|MECHANISM|REACTOR,"To develop a detailed reaction mechanism for plasma-assisted methane steam reforming, a comprehensive numerical and experimental study of effect laws on methane conversion and products yield is performed at different steam to methane molar ratio (S/C), residence time s, and reaction temperatures. A CHEMKIN-PRO software with sensitivity analysis module and path flux analysis module was used for simulations. A set of comparisons show that the developed reaction mechanism can accurately predict methane conversion and the trend of products yield in different operating conditions. Using the developed reaction mechanism in plasma-assisted kinetic model, the reaction path flux analysis was carried out. The result shows that CH recombination is the limiting reaction for CO production and O is the critical species for CO production. Adding wt.% Ni/SiO in discharge region has significantly promoted the yield of H-, CO, or CO in dielectric packed bed (DPB) reactor. Plasma catalytic hybrid reforming experiment verifies the reaction path flux analysis tentatively.",,"Zheng, Hongtao|Liu, Qian",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/938618
377,WOS:000322354200016,2013,Comparison of bacon packaging on a life cycle basis: a case study,WASTE|IDENTIFICATION|MANAGEMENT|SYSTEM|FOOD,"This study was conducted to compare the environmental effect of weight reduction and different material composition of packages using life cycle analysis of a traditional bacon package (L-board bacon package) compared to a new light weighted bacon package (OLB bacon package). A sensitivity analysis of the main components for the L-board and OLB bacon package is included to confirm either the potential environmental benefits obtained by weight reduction of the traditional bacon package or changing packaging materials. The L-board bacon package is composed of polyethylene/wax coated paper/polyethylene with an overwrap pouch. The new lightweight OLB-board bacon package is composed of reverse printed oriented polypropylene/expanded polystyrene with adhesive, along with a lighter weight overwrap pouch. Environmental impacts were characterized by life cycle assessment with SimaPro software. The general principles, framework of this study, goal and scope definition of the problem, inventory analysis, and interpretation of the results were conducting according to the ISO  family of standards. The new light weighted OLB-board bacon package shows lower environmental burden than the traditional bacon package in most impact indicators, except the mineral extraction indicator. Based on a sensitivity analysis, it was found that changing the material of the original bacon package reduces more the environmental burden of the final bacon package than reducing the weight of the original bacon board material. (C) ", Elsevier Ltd. All rights reserved.,"Kang, DongHo|Sgriccia, Nikki|Selke, Susan|Auras, Rafael",JOURNAL OF CLEANER PRODUCTION,wax coated paper|polypropylene|polystyrene|light weighting|bacon|pouch,10.1016/j.jclepro.2013.05.008
378,WOS:000330491600049,2014,Improvement of the R-SWAT-FME framework to support multiple variables and multi-objective functions,MISSISSIPPI RIVER-BASIN|RAINFALL-RUNOFF MODELS|AUTOMATIC CALIBRATION|PARAMETER-ESTIMATION|UNCERTAINTY ANALYSIS|SENSITIVITY-ANALYSIS|BAYESIAN-APPROACH|CATCHMENT MODELS|LAND-USE|POLLUTION,"Application of numerical models is a common practice in the environmental field for investigation and prediction of natural and anthropogenic processes. However, process knowledge, parameter identifiability, sensitivity, and uncertainty analyses are still a challenge for large and complex mathematical models such as the hydrological/water quality model, Soil and Water Assessment Tool (SWAT). In this study, the previously developed R program language-SWAT-Flexible Modeling Environment (R-SWAT-FME) was improved to support multiple model variables and objectives at multiple time steps (i.e., daily, monthly, and annually). This expansion is significant because there is usually more than one variable (e.g., water, nutrients, and pesticides) of interest for environmental models like SWAT. To further facilitate its easy use, we also simplified its application requirements without compromising its merits, such as the user-friendly interface. To evaluate the performance of the improved framework, we used a case study focusing on both streamflow and nitrate nitrogen in the Upper Iowa River Basin (above Marengo) in the United States. Results indicated that the R-SWAT-FME performs well and is comparable to the built-in auto-calibration tool in multi-objective model calibration. Overall, the enhanced R-SWAT-FME can be useful for the SWAT community, and the methods we used can also be valuable for wrapping potential R packages with other environmental models.", Published by Elsevier B.V.,"Wu, Yiping|Liu, Shuguang",SCIENCE OF THE TOTAL ENVIRONMENT,calibration|fme|monte carlo|r|sensitivity and uncertainty analyses|swat,10.1016/j.scitotenv.2013.07.048
379,WOS:000308319300005,2012,A Computational Fluid Dynamic Model for Prediction of Organic Dyes Adsorption from Aqueous Solutions,LOW-COST ADSORBENT|WASTE-WATER|RED MUD|METHYLENE-BLUE|INDUSTRY WASTE|CONGO-RED|BASIC DYE|FLY-ASH|REMOVAL|EQUILIBRIUM,"Modelling of the removal of synthetic dyes from aqueous solutions by adsorbents is important to develop an appropriate treatment plan using adsorption process. This paper presents a computational fluid dynamic model incorporating the Langmuir isotherm scheme and second-order kinetic expression to describe the adsorption process. The governing equation of the model was numerically solved using PHOENICS software to simulate synthetic dyes adsorption from the aqueous system. The experimental results presented in this study and taken from the literature for the removal of synthetic dyes were compared with those results predicted by the numerical model. The predicted outputs of the model match the experimental measurements satisfactory. A sensitivity analysis of the major parameters that influence the percent of dye removal from solution phase has been carried out. Three of the main parameters taken into account were the kinetic rate constant, amount of dye adsorbed at equilibrium and the Langmuir isotherm constant. It was found that the model is most sensitive to the amount of dye adsorbed at equilibrium. This effect is most obvious at the early stages of the adsorption process when the rate of dye removal is very fast. Quantification of the reaction mechanism allows developing an appropriate remediation strategy based on the adsorption process.",,"Ardejani, F. Doulati|Badii, Kh|Farhadi, F.|Saberi, M. Aziz|Shokri, B. Jodeiri",ENVIRONMENTAL MODELING & ASSESSMENT,computational fluid dynamic|synthetic dyes|kinetics|adsorption|sensitivity analysis,10.1007/s10666-012-9310-x
380,WOS:000297595300005,2011,Shape optimisation of preform design for precision close-die forging,FINITE-ELEMENT METHOD|TOPOLOGY OPTIMIZATION|STRUCTURAL OPTIMIZATION|EVOLUTIONARY PROCEDURE|SENSITIVITY-ANALYSIS|HOMOGENIZATION|SIMULATION|ALGORITHM|BLADE,"Preform design is an essential stage in forging especially for parts with complex shapes. In this paper, based on the evolutionary structural optimisation (ESO) concept, a topological optimisation method is developed for preform design. In this method, a new criterion for element elimination and addition on the workpiece boundary surfaces is proposed to optimise material distribution. To improve the quality of the boundary after element elimination, a boundary smoothing technique is developed using B-spline curve approximation. The developed methods are programmed using C# code and integrated with DEFORM D software package. Two D case problems including forging of an aerofoil shape and forging of rail wheel are evaluated using the developed method. The results suggest that the developed topology optimisation method is an efficient approach for preform design optimisation.",,"Lu, Bin|Ou, Hengan|Cui, Z. S.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,preform design|topology|optimisation|forging,10.1007/s00158-011-0668-1
381,WOS:000346751800031,2014,Plant Modelling Framework: Software for building and running crop models on the APSIM platform,MEDICAGO-SATIVA L.|WATER EXTRACTION|PHENOLOGICAL DEVELOPMENT|SYSTEMS SIMULATION|TEMPERATE CLIMATE|SOIL-WATER|LUCERNE|GROWTH|WHEAT|ENVIRONMENT,"The Plant Modelling Framework (PMF) is a software framework for creating models that represent the plant components of farm system models in the agricultural production system simulator (APSIM). It is the next step in the evolution of generic crop templates for APSIM, building on software and science lessons from past versions and capitalising on new software approaches. The PMF contains a top-level Plant class that provides an interface with the APSIM model environment and controls the other classes in the plant model. Other classes include mid-level Organ, Phenology, Structure and Arbitrator classes that represent specific elements or processes of the crop and sub-classes that the mid-level classes use to represent repeated data structures. It also contains low-level Function classes which represent generic mathematical, logical, procedural or reference code and provide values to the processes carried out by mid-level classes. A plant configuration file specifies which mid-level and Function classes are to be included and how they are to be arranged and parameterised to represent a particular crop model. The PMF has an integrated design environment to allow plant models to be created visually. The aims of the PMF are to maximise code reuse and allow flexibility in the structure of models. Four examples are included to demonstrate the flexibility of application of the PMF; . Slurp, a simple model of the water use of a static crop, . Oat, an annual grain crop model with detailed growth, development and resource use processes, . Lucerne. perennial forage model with detailed growth, development and resource use processes, . Wheat, another detailed annual crop model constructed using an alternative set of organ and process classes. These examples show the PMF can be used to develop models of different complexities and allows flexibility in the approach for implementing crop physiology concepts into model set up. (C)  The Authors.", Published by Elsevier Ltd.,"Brown, Hamish E.|Huth, Neil I.|Holzworth, Dean P.|Teixeira, Edmar I.|Zyskowski, Rob F.|Hargreaves, John N. G.|Moot, Derrick J.",ENVIRONMENTAL MODELLING & SOFTWARE,canopy dynamics|biomass and nitrogen partitioning|integrated design environment|phenological and morphological development|reusable organ and function classes,10.1016/j.envsoft.2014.09.005
382,WOS:000345254600006,2014,Quantification of the uncertainties related to velocity-area streamgauging data,,"Estimating the contribution of the different error sources in a given streamgauging offers a practical tool to improve the measurement strategy. To address the limitations of the method proposed by the ISO  standard, a generalized approach is introduced for computing the uncertainty associated with velocity-area discharge measurements. Direct computation methods are suggested for estimating the uncertainty components related to the vertical integration of velocity and to the transversal integration of velocity and depth. Discharge extrapolations to the edges and in the top/bottom layers are explicitly taken into account, as well as the distribution of the verticals throughout the cross-section. The new uncertainty analysis method was applied to streamgauging data which are representative of varied site conditions and field procedures. The new method appears to be more versatile than the ISO  method, and better suited to the diversity of streamgauging procedures. It is possible to implement it in discharge computation software such as BAREME.",,"Le Coz, Jerome|Bechon, Pierre-Marie|Camenen, Benoit|Dramais, Guillaume",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,gauging|uncertainty|velocity area|procedure,10.1051/lhb/2014047
383,WOS:000310476400006,2012,"Parameterization of a numerical 2-D debris flow model with entrainment: a case study of the Faucon catchment, Southern French Alps",CLAY-SHALE BASIN|BARCELONNETTE BASIN|SIMULATION-MODELS|CHECK DAMS|RUNOUT|LANDSLIDES|AVALANCHES|EVENTS|FRANCE|PREDICTION,"The occurrence of debris flows has been recorded for more than a century in the European Alps, accounting for the risk to settlements and other human infrastructure that have led to death, building damage and traffic disruptions. One of the difficulties in the quantitative hazard assessment of debris flows is estimating the run-out behavior, which includes the run-out distance and the related hazard intensities like the height and velocity of a debris flow. In addition, as observed in the French Alps, the process of entrainment of material during the run-out can be - times in volume with respect to the initially mobilized mass triggered at the source area. The entrainment process is evidently an important factor that can further determine the magnitude and intensity of debris flows. Research on numerical modeling of debris flow entrainment is still ongoing and involves some difficulties. This is partly due to our lack of knowledge of the actual process of the uptake and incorporation of material and due the effect of entrainment on the final behavior of a debris flow. Therefore, it is important to model the effects of this key erosional process on the formation of run-outs and related intensities. In this study we analyzed a debris flow with high entrainment rates that occurred in  at the Faucon catchment in the Barcelonnette Basin (Southern French Alps). The historic event was back-analyzed using the Voellmy rheology and an entrainment model imbedded in the RAMMS -D numerical modeling software. A sensitivity analysis of the rheological and entrainment parameters was carried out and the effects of modeling with entrainment on the debris flow run-out, height and velocity were assessed.",,"Hussin, H. Y.|Luna, B. Quan|van Westen, C. J.|Christen, M.|Malet, J-P.|van Asch, Th W. J.",NATURAL HAZARDS AND EARTH SYSTEM SCIENCES,,10.5194/nhess-12-3075-2012
384,WOS:000401556800003,2017,Life cycle analysis of pistachio production in Greece,GREENHOUSE-GAS EMISSIONS|ENVIRONMENTAL BENEFITS|PHOSPHORUS RECOVERY|CROPPING SYSTEMS|BIOCHAR|SOIL|COMPOST|IRRIGATION|FRAMEWORK|ENERGY,"In the present paper, a life cycle assessment (LCA) study regarding pistachio (Pistacia vera L.) cultivation in Aegina island, Greece, was performed to evaluate the energy use footprint and the associated environmental impacts. In this context, a detailed life cycle inventory was created based on site-survey data and used for a holistic cradle-to-farm gate LCA analysis using the GaBi . software. The main impact categories assessed were acidification potential (AP), eutrophication potential (EP), global warming potential (GWP), ozone depletion potential (ODP), photochemical ozone creation potential (POCP) and cumulative energy demand (CED). In order to reveal the main environmental concerns pertinent to pistachio production and in turn propose measures for the reduction of environmental and energetic impacts, three scenarios were compared, namely the Baseline scenario (BS) that involves current cultivation practices, the Green Energy (GE) scenario that involves the use of biological fertilizers i.e. compost, and the Waste Utilization (WU) scenario that involves the production of biochar from pistachio and other agricultural wastes and its subsequent soil application to promote carbon sequestration and improve soil quality. Based on the results of this study, the use of compost for fertilization (GE scenario), which results in approximately % savings in terms of energy consumption and the five environmental impact categories studied compared to BS scenario, is considered a promising alternative cultivation strategy. Slightly higher savings (% on average) in terms of the five calculated environmental impact categories, compared to the BS scenario, were indicated when the WU scenario was considered. Regarding energy consumption, the WU scenario results in minor increase, %, compared to the BS scenario. Results of uncertainty analysis performed using the Monte Carlo technique and contribution analysis showed that GE and WU scenarios offer reliable and significant eco-profile improvements for pistachio production in the study area compared to the current situation.", (C) 2017 Elsevier B.V. All rights reserved.,"Bartzas, Georgios|Komnitsas, Kostas",SCIENCE OF THE TOTAL ENVIRONMENT,life cycle assessment (lca)|pistachios|aegina|waste management|compost|biochar,10.1016/j.scitotenv.2017.03.251
385,WOS:000326685400009,2013,Uncertainty analysis in urban drainage modelling: should we break our back for normally distributed residuals?,PARAMETER-ESTIMATION|CALIBRATION,"This study presents results on the assessment of the application of a Bayesian approach to evaluate the sensitivity and uncertainty associated with urban rainfall-runoff models. The software MICA was adopted, in which the prior information about the parameters is updated to generate the parameter posterior distribution. The likelihood function adopted in MICA assumes that the residuals between the measured and modelled values have a normal distribution. This is a trait of many uncertainty/sensitivity procedures. This study compares the results from three different scenarios: (i) when normality of the residuals was checked but if they were not normal then nothing was done (unverified); (ii) normality assumption was checked, verified (using data transformations) and a weighting strategy was used that gives more importance to high flows; and (iii) normality assumption was checked and verified, but no weights were applied. The modelling implications of such scenarios were analysed in terms of model efficiency, sensitivity and uncertainty assessment. The overall results indicated that verifying the normality assumption required the models to fit a wider portion of the hydrograph, allowing a more detailed inspection of parameters and processes simulated in both models. Such an outcome provided important information about the advantages and limitations of the models' structure.",,"Dotto, C. B. S.|Deletic, A.|McCarthy, D. T.",WATER SCIENCE AND TECHNOLOGY,bayesian approach|normality assumption|uncertainty analysis|urban drainage models,10.2166/wst.2013.360
386,WOS:000347582800052,2015,Development of the integrated fuzzy analytical hierarchy process with multidimensional scaling in selection of natural wastewater treatment alternatives,TREATMENT SYSTEMS|NETWORK PROCESS|OPTIMIZATION|MANAGEMENT|WETLANDS|AHP,"Multi-criteria decision-making in selection of wastewater treatment alternatives has been explored widely, while few past studies comprehensively addressed the integration of various aspects (e.g., environmental, economical, ecological and management, and technical factors), which is a priority for decision-makers. This paper develops the integrated fuzzy analytical hierarchy process (AHP) with multidimensional scaling (MDS) approach to improve current methods for determining the optimal alternative. The integrated method incorporates the weights computed by AHP into the fuzzy matter-element, and allows evaluators to understand the relative importance of each index or criterion at a high level. This is followed by the MDS method to determine the optimal alternative directly through the coordinates associated with each alternative in a two-dimensional configuration. The method was evaluated via specific programming language software packages, and was applied to select natural wastewater treatment alternatives in a case study. Results indicate the stabilization pond was the optimal alternative among five natural wastewater treatment systems. Sensitivity analysis was conducted and reflects the importance of weighing on alternative selection.", (C) 2014 Elsevier B.V. All rights reserved.,"Ouyang, Xiaoguag|Guo, Fen|Shan, Dan|Yu, Huanyun|Wang, Jian",ECOLOGICAL ENGINEERING,analytical hierarchy process|fuzzy matter-element|multidimensional scaling|wastewater treatment alternative|sensitivity analysis,10.1016/j.ecoleng.2014.11.006
387,WOS:000343322300006,2014,Eulerian-Eulerian modelling and computational fluid dynamics simulation of wire mesh demisters in MSF plants,PLATE MIST ELIMINATORS|SEPARATION EFFICIENCY|DRAINAGE CHANNELS|DROPLET MOTION|HEAT-TRANSFER|FLOW|DESIGN|ENTRAINMENT|PERFORMANCE|CONDENSERS,"Purpose - The purpose of this study is to focus on simulation of wire mesh demisters in multistage flash desalination (MSF) plants. The simulation is made by the use of computational fluid dynamics (CFD) software. Design/methodology/approach - A steady state and two-dimensional (D) model was developed to simulate the demister. The model employs an Eulerian-Eulerian approach to simulate the flow of water vapor and brine droplets in the demister. The computational domain included three zones, which are the vapor space above and below the demister and the demister. The demister zone was modeled as a tube bank arrange or as a porous media. Findings - Sensitivity analysis of the model showed the main parameters that affect demister performance are the vapor velocity and the demister permeability. On the other hand, the analysis showed that the vapor temperature has no effect on the pressure drop across the demister. Research limitations/implications - The developed model was validated against previous literature data as well as real plant data. The analysis shows good agreement between model prediction and data. Originality/value - This work is the first in the literature to simulate the MSF demister using CFD modeling. This work is part of a group effort to develop a comprehensive CFD simulation for the entire flashing stage of the MSF process, which would provide an extremely efficient and inexpensive design and simulation tool to the desalination community.",,"Al-Fulaij, Hala|Cipollina, Andrea|Micale, Giorgio|Ettouney, Hisham|Bogle, David",ENGINEERING COMPUTATIONS,cfd|demister|desalination|eulerian modeling|multistage flashing,10.1108/EC-03-2012-0063
388,WOS:000355262900012,2015,A meta-heuristic solution for automated refutation of complex software systems specified through graph transformations,ANT COLONY OPTIMIZATION|FLY MODEL CHECKING|SEARCH,"One of the best approaches for verifying software systems (especially safety critical systems) is the model checking in which all reachable states are generated from an initial state. All of these states are searched for errors or desirable patterns. However, the drawback for many real and complex systems is the state space explosion in which model checking cannot generate all the possible states. In this situation, designers can use refutation to check refusing a property rather than proving it. In refutation, it is very important to handle the state space for finding errors efficiently. In this paper, we propose an efficient solution to implement refutation in complex systems modeled by graph transformation. Since meta-heuristic algorithms are efficient solutions for searching in the problems with very large state spaces, we use them to find errors (e.g., deadlocks) in systems which cannot be verified through existing model checking approaches due to the state space explosion. To do so, we employ a Particle Swarm Optimization (PSO) algorithm to consider only a subset of states (called population) in each step of the algorithm. To increase the accuracy, we propose a hybrid algorithm using PSO and Gravitational Search Algorithm (GSA). The proposed approach is implemented in GROOVE, a toolset for designing and model checking graph transformation systems. The experiments show improved results in terms of accuracy, speed and memory usage in comparison with other existing approaches.", (C) 2015 Elsevier B.V. All rights reserved.,"Rafe, Vahid|Moradi, Maryam|Yousefian, Rosa|Nikanjam, Amin",APPLIED SOFT COMPUTING,model checking|refutation|pso|gsa|graph transformation system|state space explosion,10.1016/j.asoc.2015.04.032
389,WOS:000418314300026,2017,Optimal design of a multi-echelon supply chain in a system thinking framework: An integrated financial-operational approach,ARCHITECTURE MODULARITY IMPLICATIONS|NETWORK DESIGN|BENDERS DECOMPOSITION|STOCHASTIC OPTIMIZATION|DEMAND UNCERTAINTIES|INVENTORY SYSTEM|PROCESS INDUSTRY|TRADE CREDIT|MANAGEMENT|ROBUST,"The purpose of this study is to design a four-echelon supply chain network in which operational and financial dimensions have been considered by a holistic, comprehensive and rigorous viewpoint within tactical and. strategic decision-making levels. In this process, the aforementioned dimensions are extended and integrated in mathematical modeling framework. The research modeling is compared with traditional approaches, which merely rely on operational dimension to optimize profit and due to problems in profit, the research objectives changed to the multiple objectives of corporate value, change in equity and economic value added. Within this framework, a comparison is primarily made between profit and each of corporate value, change in equity and economic value added two by two (in the form of traditional approach (scenario A) and new approach (scenario B)). Then the objectives are simultaneously assessed and a compromise is gained among them through fuzzy goal programming. In this process, the effectiveness and efficiency of the scenario B is analyzed and assessed. Furthermore, a sensitivity analysis is performed on such factors as demand, return of equity rate, tax rate, production capacity and supplier capacity while the impacts of their changes on multi-objective function and satisfaction coefficient are simultaneously calculated. The mathematical model in the study is multi-product and multi-period while the values of parameters are determined under definite circumstances. For modeling and solving the mathematical model, GAMS  software and CPLEX solver are employed. To test the model, data of an Iranian petrochemical company are used. Besides highlighting the significance of the financial dimension and its integration with the operational dimension in gaining sustainable competitive advantage, the research results revealed that CVM- in contrast to change in equity and EVA- is much more favorable than the other objectives. Similarly, changes in demand had more favorable effects on multi-objective analysis.",,"Mohammadi, Ali|Abbasi, Abbas|Alimohammadlou, Moslem|Eghtesadifard, Mahmood|Khalifeh, Mojtaba",COMPUTERS & INDUSTRIAL ENGINEERING,multi-echelon supply chain system|mathematical modeling|integrated financial/operational approach|fuzzy goal programming,10.1016/j.cie.2017.10.019
390,WOS:000334183800009,2014,"Organizing preliminary storage sites of organic material, waste fuels and recyclables and their separating distance from populated areas",EMISSIONS|IGNITION,"European Union directives have urged member countries to enhance the recycling and separation of waste fractions, and this has increased the number of temporary storage sites of recyclables and waste fuels. Spontaneous fires at temporary storage sites have become common and pose social/health/environmental risks. Storage sites should be sited sufficiently far from populated regions, so that the concentration of released pollutants from open fires falls below the critical air quality index before the plume reach the downwind population. In this study, the open-burn/open-detonation model developed by the US Environmental Protection Agency was employed, and  simulations were performed for nine scenarios of open burning of household waste to estimate suitable sizes of storage heaps and adequate distances between storage sites and populated regions. Furthermore, sensitivity analysis was performed for , additional simulations to determine the effects of variations in the burn rate, storage dimension and volume of the waste heap on the model output. The resulting chart can be directly employed by waste operators/environmental agencies to organize storage sites to minimize externalities due to open fires. Furthermore, using ArcGIS software, first-cut information of the total Swedish population facing the risk of hazards due to spontaneous fires was provided.",,"Ibrahim, Muhammad Asim|Hogland, William",JOURNAL OF MATERIAL CYCLES AND WASTE MANAGEMENT,temporary storage|risk of fires|externalities of open fires|organic materials|solid|waste fuels,10.1007/s10163-013-0184-z
391,WOS:000343415200006,2013,UNCERTAINTY IN THE DEVELOPMENT AND USE OF EQUATION OF STATE MODELS,,"In this paper we present the results from a series of focus groups on the visualization of uncertainty in equation-of-state (EOS) models. The initial goal was to identify the most effective ways to present EOS uncertainty to analysts, code developers, and material modelers. Four prototype visualizations were developed to present EOS surfaces in a three-dimensional, thermodynamic space. Focus group participants, primarily from Sandia National Laboratories, evaluated particular features of the various techniques for different use cases and discussed their individual workflow processes, experiences with other visualization tools, and the impact of uncertainty on their work. Related to our prototypes, we found the D presentations to be helpful for seeing a large amount of information at once and for a big-picture view; however, participants also desired relatively simple, two-dimensional graphics for better quantitative understanding and because these plots are part of the existing visual language for material models. In addition to feedback on the prototypes, several themes and issues emerged that are as compelling as the original goal and will eventually serve as a starting point for further development of visualization and analysis tools. In particular, a distributed workflow centered around material models was identified. Material model stakeholders contribute and extract information at different points in this workflow depending on their role, but encounter various institutional and technical barriers which restrict the flow of information. An effective software tool for this community must be cognizant of this workflow and alleviate the bottlenecks and barriers within it. Uncertainty in EOS models is defined and interpreted differently at the various stages of the workflow. In this context, uncertainty propagation is difficult to reduce to the mathematical problem of estimating the uncertainty of an output from uncertain inputs.",,"Weirs, V. Gregory|Fabian, Nathan|Potter, Kristin|McNamara, Laura|Otahal, Thomas",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,materials|uncertainty quantification|representation of uncertainty|model validation and verification|continnum mechanics,10.1615/Int.J.UncertaintyQuantification.2012003960
392,WOS:000404559900023,2017,Simulating the Fate and Transport of Coal Seam Gas Chemicals in Variably-Saturated Soils Using HYDRUS,HYDRAULIC CONDUCTIVITY|VADOSE ZONE|REACTIVE TRANSPORT|TRANSFORMATION PRODUCTS|NITROSAMINE FORMATION|CONSTRUCTED WETLANDS|SENSITIVITY ANALYSIS|AGRICULTURAL SOILS|SOLUTE TRANSPORT|FIELD CONDITIONS,"The HYDRUS-D and HYDRUS (D/D) computer software packages are widely used finite element models for simulating the one-, and two- or three-dimensional movement of water, heat, and multiple solutes in variably-saturated media, respectively. While the standard HYDRUS models consider only the fate and transport of individual solutes or solutes subject to first-order degradation reactions, several specialized HYDRUS add-on modules can simulate far more complex biogeochemical processes. The objective of this paper is to provide a brief overview of the HYDRUS models and their add-on modules, and to demonstrate possible applications of the software to the subsurface fate and transport of chemicals involved in coal seam gas extraction and water management operations. One application uses the standard HYDRUS model to evaluate the natural soil attenuation potential of hydraulic fracturing chemicals and their transformation products in case of an accidental release. By coupling the processes of retardation, first-order degradation and convective-dispersive transport of the biocide bronopol and its degradation products, we demonstrated how natural attenuation reduces initial concentrations by more than a factor of hundred in the top  cm of the soil. A second application uses the UnsatChem module to explore the possible use of coal seam gas produced water for sustainable irrigation. Simulations with different irrigation waters (untreated, amended with surface water, and reverse osmosis treated) provided detailed results regarding chemical indicators of soil and plant health, notably SAR, EC and sodium concentrations. A third application uses the HP module to analyze trace metal transport involving cation exchange and surface complexation sorption reactions in a soil leached with coal seam gas produced water following some accidental water release scenario. Results show that the main process responsible for trace metal migration in soil is complexation of naturally present trace metals with inorganic ligands such as (bi)carbonate that enter the soil upon infiltration with alkaline produced water. The examples were selected to show how users can tailor the required model complexity to specific needs, such as for rapid screening or risk assessments of various chemicals nder generic soil conditions, or for more detailed site-specific analyses of actual subsurface pollution problems.",,"Mallants, Dirk|Simunek, Jirka|van Genuchten, Martinus Th.|Jacques, Diederik",WATER,hydrus|coal seam gas|contaminant transport|trace elements|hp1,10.3390/w9060385
393,WOS:000355932400009,2015,"Parameter sensitivity analysis and optimization of Noah land surface model with field measurements from Huaihe River Basin, China",MESOSCALE ETA-MODEL|WATERSHED MODEL|ENVIRONMENTAL-MODELS|UNCERTAINTY|CALIBRATION|IMPLEMENTATION|HYDROLOGY|SYSTEMS|IMPACT|EVAPORATION,"This study aims to identify the parameters that are most important in controlling the Noah land surface model (LSM), the analysis of parameter interactions, and the evaluation of the performance of parameter optimization using the parameter estimation software PEST. We found it necessary to analyze parameter sensitivity in order to properly simulate hydrological variables such as latent heat flux in the Huaihe River Basin, China. The parameters under study in the Noah LSM link thermodynamic and hydrological parts into a complete model. To our knowledge, this parameter interaction in the Noah LSM has never been studied before. There are, however, several studies concerning the influence of vegetation types and climate conditions on parameter sensitivity of the Noah LSM. Three sensitivity analysis methods, the including local sensitivity analysis method SENSAN, regional sensitivity analysis, and Sobol's method, were tested. Five experimental sites in the Huaihe River Basin were chosen to perform the simulations. The results show that the Noah LSM parameter sensitivities were impacted by the choice of the analysis method. The local method SENSAN often produced significant differences in results compared to the two global methods. The parameter interactions investigated made a significant contribution towards elucidating how one process influences another in the Noah LSM. The results show that parameters were not transferable solely based on vegetation types but also rely on climate conditions. According to the sensitivity analysis results, four sensitive parameters were chosen to be optimized using the PEST method. PEST is a widely used method for estimating parameters in models. Root-mean-square error was used to evaluate the effect of the optimization. Generally in all sites, the optimized parameters values perform better than the original parameter values.",,"Hou, Ting|Zhu, Yonghua|Lu, Haishen|Sudicky, Edward|Yu, Zhongbo|Ouyang, Fen",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,noah lsm|huaihe river|sensitivity analysis|rsa|sobol's method|pest,10.1007/s00477-015-1033-5
394,WOS:000289865900008,2011,Automatic differentiation strategy for the local sensitivity analysis of a one-dimensional hydraulic model,,"In this paper, automatic differentiation (AD) techniques are introduced and applied in the local sensitivity analysis of the state function handled by the one-dimensional hydraulic model, Mage. We have proposed the different steps to easily compute automatic derivatives of a given numerical model. More specifically, Tapenade software, in the tangent linear mode (TLM), has been used to calculate derivatives of the model outputs (discharge and water level) with respect to the bottom friction expressed in terms of Strickler relation. We have shown the independent contribution of the main stream and floodplain Strickler coefficients on discharges and water levels. Furthermore, numerical comparison has shown that derivatives computed using the AD tool are more accurate than those using the forward divided differences scheme."," Copyright (C) 2010 John Wiley & Sons, Ltd.","Souhar, O.|Faure, J. -B.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,automatic differentiation|divided differences|hydraulic models|sensitivity analysis,10.1002/fld.2263
395,WOS:000267193100001,2009,Evaluating uncertainty in integrated environmental models: A review of concepts and tools,RAINFALL-RUNOFF MODELS|GENERALIZED POLYNOMIAL CHAOS|WATER-QUALITY MODELS|SENSITIVITY-ANALYSIS|RISK-ASSESSMENT|AUTOMATIC CALIBRATION|GLOBAL OPTIMIZATION|RELIABILITY METHODS|HYDROLOGIC-MODELS|DIFFERENTIAL-EQUATIONS,"This paper reviews concepts for evaluating integrated environmental models and discusses a list of relevant software-based tools. A simplified taxonomy for sources of uncertainty and a glossary of key terms with ""standard'' definitions are provided in the context of integrated approaches to environmental assessment. These constructs provide a reference point for cataloging  different model evaluation tools. Each tool is described briefly (in the auxiliary material) and is categorized for applicability across seven thematic model evaluation methods. Ratings for citation count and software availability are also provided, and a companion Web site containing download links for tool software is introduced. The paper concludes by reviewing strategies for tool interoperability and offers guidance for both practitioners and tool developers.",,"Matott, L. Shawn|Babendreier, Justin E.|Purucker, S. Thomas",WATER RESOURCES RESEARCH,,10.1029/2008WR007301
396,WOS:000363071000055,2015,Implementation of the expert decision system for environmental assessment in composite materials selection for automotive components,ANALYTICAL HIERARCHY PROCESS|KNOWLEDGE-BASED SYSTEM|NATURAL FIBER COMPOSITES|BUMPER BEAM|DESIGN,"Conventional materials selection system was replaced with sophisticated software tools by rapid changing technology. The growing environmental concerns and regulations widely among the industry, especially in automobiles, force us to explore the natural fiber materials as a replacement for synthetic materials which is in common use. As a result of extensive research and development, new natural fiber reinforced composite materials are emerging and the database of materials growing exponentially. The decision of selecting optimized materials was complicated, as it involves diversified choice of materials, coupled with various influencing criteria for the selection process. To abstain from deciding inappropriate materials, the technology of expert system software tools can help us in the appropriate materials selection. The objective of this research was to explore the implementation of Analytical Hierarchy Process (AHP) using the expert choice software tool for deciding optimum natural fiber reinforced composite materials by considering main criteria and sub-criteria in the hierarchical model. The final judgement was performed with different scenarios of sensitivity analysis, giving priority to the environmental factors and sustainability. The result shows that the natural fiber composite material hemp and polypropylene gained the higher rank in the selection process and almost compliant with the requirements of industrial product design specification and can be recommended to automotive component manufacturers to enforce green technology. (C) ", Elsevier Ltd. All rights reserved.,"Ali, B. A. Ahmed|Sapuan, S. M.|Zainudin, E. S.|Othman, M.",JOURNAL OF CLEANER PRODUCTION,expert system|decision method|materials selection|analytical hierarchy process|environmental factor|natural fiber composites,10.1016/j.jclepro.2015.05.084
397,WOS:000253663600007,2008,A computational algorithm for the multiple generation of nonlinear mathematical models and stability study,,"This paper presents an algorithm that generates families of mathematical models with nonlinear parameters, and includes the study of linear models, based on the experimental data of the intervening variables. The implementation of this algorithm has been named poly-model and is based on the application of the Gauss-Newton algorithm for obtaining the parameters of nonlinear models [Verdu F. Un Algoritmo para la construccion multiple de modelos matematicos no lineales y el estudio de su estabilidad. Doctoral Tesis. Universidad de Alicante, ]. One of its characteristics is a search among different nonlinear models within the parameters; unlike the methods found in the scientific literature [Camacho Rosales J. Estadistica con SPSS para windows. Ed. Ra-Ma, ; Mathsoft Inc. Splus-. Guide to Statistics. Seattle, ], the user does not intervene in their generation. A pruning criteria has also been introduced that is based on the stability analysis of models generated from perturbations, applying studies carried out by the authors and published in [Verdu F, Villacampa Y. A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data. Advances in Engineering Software, ]. Object-oriented Pascal has been used in Delphi .", (c) 2007 Published by Elsevier Ltd.,"Verdu, F.|Villacampa, Y.",ADVANCES IN ENGINEERING SOFTWARE,modelling|nonlinear regression|sensitivity analysis,10.1016/j.advengsoft.2007.03.004
398,WOS:000400594100014,2017,Uncertainty Estimation in Flood Inundation Mapping: An Application of Non-parametric Bootstrapping,MONTHLY STREAMFLOW PREDICTION|ARTIFICIAL NEURAL-NETWORKS|CONFIDENCE-INTERVALS|MODEL CALIBRATION|DESIGN FLOODS|RUNOFF|RISK|PRECIPITATION|OPTIMIZATION|PARAMETERS,"Disaster prevention planning is affected in a significant way by a lack of in-depth understanding of the numerous uncertainties involved with flood delineation and related estimations. Currently, flood inundation extent is represented as a deterministic map without in-depth consideration of the inherent uncertainties associated with variables such as precipitation, streamflow, topographic representation, modelling parameters and techniques, and geospatial operations. The motivation of this study is to estimate uncertainties in flood inundation mapping based on a non-parametric bootstrapping method. The uncertainty is addressed through the application of non-parametric bootstrap sampling to the hydrodynamic modelling software, HEC-RAS, integrated with Geographic Information System (GIS). This approach was used to simulate different water levels and flow rates corresponding to different return periods from the available database. The study area was the Langat River Basin in Malaysia. The results revealed that the inundated land and infrastructure are subject to a flooding hazard of high-frequency events and that the flood damage potential is increasing significantly for residential areas and valuable land-use classes with higher return periods. The proposed methodology, as well as the study outcomes, of this paper could be beneficial to policymakers, water resources managers, insurance companies and other flood-related stakeholders."," Copyright (c) 2017 John Wiley & Sons, Ltd.","Faghih, M.|Mirzaei, M.|Adamowski, J.|Lee, J.|El-Shafie, A.",RIVER RESEARCH AND APPLICATIONS,flood mapping|uncertainty analysis|non-parametric bootstrap sampling|generalized extreme value distribution,10.1002/rra.3108
399,WOS:000086420500002,2000,Assessing the impact of managed-care on the distribution of length-of-stay using Bayesian hierarchical models,CANCER CLINICAL-TRIAL|SURVIVAL-DATA|FRAILTY,"Hierarchical models provide a useful framework for the complexities encountered in policy-relevant research in which the impact of social programs is being assessed. Such complexities include multi-site data, censored data and over-dispersion. In this paper, Bayesian inference through Markov Chain Monte Carlo methods is used for the analysis of a complex hierarchical log-normal model that shows the impact of a managed care strategy aimed at limiting length of hospital stays. Parameters in this model allow for variability in baseline length-of-stay as well as the program effect across hospitals. The authors demonstrate elicitation and sensitivity analysis with respect to prior distributions. All calculations for the posterior and predictive distributions were obtained using the software BUGS.",,"Stangl, D|Huerta, G",LIFETIME DATA ANALYSIS,hierarchical log-normal model|prior elicitation|gibbs sampling|predictive distribution|health policy,10.1023/A:1009691326989
400,WOS:000321088500001,2013,A review of Bayesian belief networks in ecosystem service modelling,COLUMBIA RIVER BASIN|LAND MANAGEMENT ALTERNATIVES|DECISION-SUPPORT TOOLS|GROUNDWATER CONTAMINATION|UNCERTAINTY ANALYSIS|ADAPTIVE MANAGEMENT|RESOURCE MANAGEMENT|EXPERT KNOWLEDGE|AUSTRALIA|SYSTEMS,"A wide range of quantitative and qualitative modelling research on ecosystem services (ESS) has recently been conducted. The available models range between elementary, indicator-based models and complex process-based systems. A semi-quantitative modelling approach that has recently gained importance in ecological modelling is Bayesian belief networks (BBNs). Due to their high transparency, the possibility to combine empirical data with expert knowledge and their explicit treatment of uncertainties, BBNs can make a considerable contribution to the ESS modelling research. However, the number of applications of BBNs in ESS modelling is still limited. This review discusses a number of BBN-based ESS models developed in the last decade. A SWOT analysis highlights the advantages and disadvantages of BBNs in ESS modelling and pinpoints remaining challenges for future research. The existing BBN models are suited to describe, analyse, predict and value ESS. Nevertheless, some weaknesses have to be considered, including poor flexibility of frequently applied software packages, difficulties in eliciting expert knowledge and the inability to model feedback loops. (c) ", Elsevier Ltd. All rights reserved.,"Landuyt, Dries|Broekx, Steven|D'hondt, Rob|Engelen, Guy|Aertsens, Joris|Goethals, Peter L. M.",ENVIRONMENTAL MODELLING & SOFTWARE,bayesian belief networks|ecosystem services|expert based systems|graphical models,10.1016/j.envsoft.2013.03.011
401,WOS:000348756700001,2015,Pi 4U: A high performance computing framework for Bayesian uncertainty quantification of complex models,LIQUID WATER|EVOLUTIONARY STRATEGIES|PROBABILISTIC APPROACH|MARGINAL LIKELIHOOD|DYNAMICAL-SYSTEMS|INVERSE PROBLEMS|UPDATING MODELS|SIMULATION|OPTIMIZATION|RELIABILITY,"We present Pi U,() an extensible framework, for non-intrusive Bayesian Uncertainty Quantification and Propagation (UQ+P) of complex and computationally demanding physical models, that can exploit massively parallel computer architectures. The framework incorporates Laplace asymptotic approximations as well as stochastic algorithms, along with distributed numerical differentiation and task-based parallelism for heterogeneous clusters. Sampling is based on the Transitional Markov Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization tasks associated with the asymptotic approximations are treated via the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified subset simulation method is used for posterior reliability measurements of rare events. The framework accommodates scheduling of multiple physical model evaluations based on an adaptive load balancing library and shows excellent scalability. In addition to the software framework, we also provide guidelines as to the applicability and efficiency of Bayesian tools when applied to computationally demanding physical models. Theoretical and computational developments are demonstrated with applications drawn from molecular dynamics, structural dynamics and granular flow.", (C) 2014 Elsevier Inc. All rights reserved.,"Hadjidoukas, P. E.|Angelikopoulos, P.|Papadimitriou, C.|Koumoutsakos, P.",JOURNAL OF COMPUTATIONAL PHYSICS,uncertainty quantification|parallel computing|distributed computing|bayesian inference|reliability,10.1016/j.jcp.2014.12.006
402,WOS:000300128000003,2012,Spatio-temporal uncertainty in Spatial Decision Support Systems: A case study of changing land availability for bioenergy crops in Mozambique,AGENT-BASED MODEL|COVER CHANGE|BIO-ENERGY|DYNAMICS|VALIDATION|SCENARIOS|GIS|SCIENCE|EXAMPLE|TOOL,"Spatial Decision Support Systems (SDSSs) often include models that can be used to assess the impact of possible decisions. These models usually simulate complex spatio-temporal phenomena, with input variables and parameters that are often hard to measure. The resulting model uncertainty is, however, rarely communicated to the user, so that current SDSSs yield clear, but therefore sometimes deceptively precise outputs. Inclusion of uncertainty in SDSSs requires modeling methods to calculate uncertainty and tools to visualize indicators of uncertainty that can be understood by its users, having mostly limited knowledge of spatial statistics. This research makes an important step towards a solution of this issue. It illustrates the construction of the PCRaster Land Use Change model (PLUC) that integrates simulation, uncertainty analysis and visualization. It uses the PCRaster Python framework, which comprises both a spatio-temporal modeling framework and a Monte Carlo analysis framework that together produce stochastic maps, which can be visualized with the Aguila software, included in the PCRaster Python distribution package. This is illustrated by a case study for Mozambique in which it is evaluated where bioenergy crops can be cultivated without endangering nature areas and food production now and in the near future, when population and food intake per capita will increase and thus arable land and pasture areas are likely to expand. It is shown how the uncertainty of the input variables and model parameters effects the model outcomes. Evaluation of spatio-temporal uncertainty patterns has provided new insights in the modeled land use system about, e.g., the shape of concentric rings around cities. In addition, the visualization modes give uncertainty information in an comprehensible way for users without specialist knowledge of statistics, for example by means of confidence intervals for potential bioenergy crop yields. The coupling of spatio-temporal uncertainty analysis to the simulation model is considered a major step forward in the exposure of uncertainty in SDSSs. (C) ", Elsevier Ltd. All rights reserved.,"Verstegen, Judith Anne|Karssenberg, Derek|van der Hilst, Floor|Faaij, Andre",COMPUTERS ENVIRONMENT AND URBAN SYSTEMS,spatial decision support systems|uncertainty|spatial modeling|visualization|land use change|bioenergy,10.1016/j.compenvurbsys.2011.08.003
403,WOS:000235767800009,2006,Comparing deterministic and probabilistic risk assessments - A case study at a closed steel mill in southern Sweden,EXPOSURE ASSESSMENT|SOIL|SITE|UNCERTAINTY|VARIABILITY|MODELS|GREECE|LEAD,"Background, Aims and Scope. Contaminated land is a high priority environmental problem in most of Europe and North-America. Sweden is no exception and generic guideline values have been developed for the initial assessment, but site-specific assessments are also needed. The generic guideline values are not applicable when the exposure conditions are different from the typical Swedish conditions or when the site contains a particularly sensitive ecosystem. The Swedish guideline values have, like in man), other countries, been set by using deterministic point estimates for all variables and constants in the used multimedia model. The same approach is common also for site-specific assessments, and a limitation is that it fails to quantify variability and uncertainty. Probabilistic risk assessment provided a method to deal with this problem. Variability and uncertainty in the input parameters (variables or constants) are described by probability distributions, and likewise the output (risk or exposure) is presented as a probability distribution. A substantial number of probabilistic risk assessments for contaminated land at sites in North America, Europe and Asia have been published. However, an extensive review of the literature did not identify an), study where probabilistic risk assessment was applied to a site contaminated by an iron or steel industry. Here we will describe such a case, where we have compared a deterministic point estimate with a probabilistic risk assessment for six elements and benzo[a]pyrene. Methods. The site had different metallurgical plants in operation for more than  years. Most parts of the steel mill were closed by the mid s, and today the site is used by small-sized enterprises. The soil is contaminated with metals from the previous industrial operations. The present owner plans to develop the site and has therefore initiated extensive investigations of soil contamination. Sixty-two soil samples collected between  and  provided a good coverage of the whole site, and were analyzed for the content of different elements and polycyclic aromatic hydrocarbons (PAH). The exposure assessments were focused on six elements with high concentrations compared to the generic guideline values; arsenic (As), lead (Pb), cadmium (Ccl), chromium (Cr), copper (Cu) and zinc (Zn). In addition, benzo[a]pyrene was included due to the high toxicity and comparatively high concentrations. Variability and uncertainty were characterized in a Monte Carlo simulation of exposures (, iterations), and the exposures were evaluated with two land use scenarios; less sensitive use and sensitive use. Results and Discussion. The deterministic point estimates and the probabilistic estimates of the th percentile are in approximately the same ranges in the scenario of less sensitive land use. It is only the exposure for arsenic that is slightly above the toxicological reference value (TRV) in the deterministic assessment. In the probabilistic assessment, the exposure for all elements is below the TRV. The results for sensitive land use are applicable to a scenario where the site is developed for general housing. The deterministic point estimates and the probabilistic estimates of the th percentile are also here in approximately the same ranges, but the exposure exceeds the TRV for arsenic, cadmium and lead. Drinking water, vegetables grown on site and soil ingestion are the major exposure pathways for this scenario. In this assessment, the estimated intake distributions are applicable to a randomly selected individual. The probability distributions used here to characterize the different soil parameters are typically representing both variability and uncertainty, and the same is true the majority of the exposure variables. We therefore decided not to attempt to separate variability and uncertainty at this stage, but with additional data from a more in-depth site investigation it might be possible to achieve this. Conclusions and Outlook. To the best of our knowledge, this study is the first report on a probabilistic risk assessment on a former iron and steel works site. The materials handled by this industry were less toxic than for many other metallurgical operations, but contaminants may still severely limit the options for future land use. This case study shows that probabilistic exposure estimates for a set of soil contaminants can be quite similar to deterministic point estimates. The main difference is instead to be found in the additional information obtained with the probabilistic assessment. The sensitivity analyses show pathways and input variables that contribute most to variations in the total intake of each contaminant, e.g. dermal contact and ingestion of soil, vegetables and drinking water. This information can be used both in the planning of future land use and for active measures to reduce current exposure. The probabilistic assessment also provides information on the magnitude of exposure and the margin of safety. This information may facilitate risk communication between decision-makers and stakeholders. The presentation of results from probabilistic risk assessments is only briefly discussed in the literature and here we see a need for research and opportunities for enhancement. The choice of data analytical tools may then be of importance, since more complex multimedia models are rather difficult to decipher when implemented within traditional spreadsheet software. Some of the research needs are identified here and in a previous review article in this journal.",,"Sander, P|Oberg, T",JOURNAL OF SOILS AND SEDIMENTS,arsenic|contaminated soils|drinking water|exposure assessments|monte carlo simulation|multimedia models|probabilistic risk assessments|sensitivity analysis|steel mills|uncertainties,10.1065/jss2005.10.147
404,WOS:000330738700004,2014,Land use impacts on biodiversity from kiwifruit production in New Zealand assessed with global and national datasets,LIFE-CYCLE ASSESSMENT|WATER USE IMPACTS|INDICATORS|LCA|CONSEQUENCES|RICHNESS|PROPOSAL|FOREST|TAXA,"Habitat loss is a significant cause of biodiversity loss, but while its importance is widely recognized, there is no generally accepted method on how to include impacts on biodiversity from land use and land use changes in cycle assessment (LCA), and existing methods are suffering from data gaps. This paper proposes a methodology for assessing the impact of land use on biodiversity using ecological structures as opposed to information on number of species. Two forms of the model (global and local scales) were used to assess environmental quality, combining ecosystem scarcity, vulnerability, and conditions for maintaining biodiversity. A case study for New Zealand kiwifruit production is presented. As part of the sensitivity analysis, model parameters (area and vulnerability) were altered and New Zealand datasets were also used. When the biodiversity assessment was implemented using a global dataset, the importance of productivity values was shown to depend on the area the results were normalized against. While the area parameter played an important role in the results, the proposed alternative vulnerability scale had little influence on the final outcome. Overall, the paper successfully implements a model to assess biodiversity impacts in LCA using easily accessible, free-of-charge data and software. Comparing the model using global vs. national datasets showed that there is a potential loss of regional significance when using the generalized model with the global dataset. However, as a guide to assessing biodiversity impact, the model allows for consistent comparison of product systems on an international basis.",,"Coelho, Carla R. V.|Michelsen, Ottar",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,biodiversity|ecoregions|impact assessment|kiwifruit|land use|scale dependency,10.1007/s11367-013-0628-7
405,WOS:000413886300011,2017,Process simulation and techno economic analysis of renewable diesel production via catalytic decarboxylation of rubber seed oil - A case study in Malaysia,TECHNOECONOMIC ASSESSMENT,"This work describes the economic feasibility of hydroprocessed diesel fuel production via catalytic decarboxylation of rubber seed oil in Malaysia. A comprehensive techno-economic assessment is developed using Aspen HYSYS V. software for process modelling and economic cost estimates. The profitability profile and, minimum fuels selling price of this synthetic fuels production using rubber seed oil as biomass feedstock are assessed under a set of assumptions for what can be plausibly be achieved in -years framework. In this study, renewable diesel processing facility is modelled to be capable of processing , L of inedible oil per day and producing a total of  million litre of renewable diesel product per annual with assumed annual operational days of . With the forecasted renewable diesel retail price of . RM per kg, the pioneering renewable diesel project investment offers an assuring return of investment of .% and net return as high as . million RM. Sensitivity analysis conducted showed that renewable diesel production cost is most sensitive to rubber seed oil price and hydrogen gas price, reflecting on the relative importance of feedstock prices in the overall profitability profile. (C) ", Elsevier Ltd. All rights reserved.,"Cheah, Kin Wai|Yusup, Suzana|Singh, Haswin Kaur Gurdeep|Uemura, Yoshimitsu|Lam, Hon Loong",JOURNAL OF ENVIRONMENTAL MANAGEMENT,techno-economic|catalytic decarboxylation|rubber seed oil|renewable diesel|minimum fuel selling price|profitability profile,10.1016/j.jenvman.2017.05.053
406,WOS:000239466700009,2006,The comparison of four dynamic systems-based software packages: Translation and sensitivity analysis,WETNESS,"Dynamic model development for describing complex ecological systems continues to grow in popularity. For both academic research and project management, understanding the benefits and limitations of systems-based software could improve the accuracy of results and enlarge the user audience. A Surface Wetness Energy Balance (SWEB) model for canopy surface wetness has been translated into four software packages and their strengths and weaknesses were compared based on 'novice' user interpretations. We found expression-based models such as Simulink and GoldSim with Expressions were able to model the SWEB more accurately; however, stock and flow-based models such as STELLA, Madonna, and GoldSim with Flows provided the user a better conceptual understanding of the ecologic system. Although the original objective of this study was to identify an 'appropriate' software package for predicting canopy surface wetness using SWEB, our outcomes suggest that many factors must be considered by the stakeholders when selecting a model because the modeling software becomes part of the model and of the calibration process. These constraints may include user demographics, budget limitations, built-in sensitivity and optimization tools, and the preference of user friendliness vs. computational power. Furthermore, the multitude of closed proprietary software may present a disservice to the modeling community, creating model artifacts that originate somewhere deep inside the undocumented features of the software, and masking the underlying properties of the model. (c) ", Elsevier Ltd. All rights reserved.,"Rizzo, Donna M.|Mouser, Paula J.|Whitney, David H.|Mark, Charles D.|Magarey, Roger D.|Voinov, Alexey A.",ENVIRONMENTAL MODELLING & SOFTWARE,model comparison|dynamic simulation|system-based models|canopy surface energy balance,10.1016/j.envsoft.2005.07.009
407,WOS:000368207400029,2016,Life cycle assessment of organic versus conventional agriculture. A case study of lettuce cultivation in Greece,ENVIRONMENTAL IMPACTS|PLANT,"The environmental sustainability of an organic and a conventional lettuce cultivation system, situated at Northern Greece, was investigated. Data from all stages (i.e. irrigation, machinery used, and fertilizing) of lettuce cultivation were collected and their sustainability was assessed by means of the life cycle assessment (LCA) methodology. Two different functional units, namely per hectare of cultivation and per ton of lettuce produced, were used and the environmental impacts, on mid and endpoint level, and CO emissions were estimated by means of the SimaPro  LCA software. It was found that the environmental footprint and the CO emissions were lower by % and %, respectively, for organic than for the conventional lettuce cultivation, when sustainability was assessed per area (ha) of cultivation. On the contrary, conventional lettuce cultivation showed a better environmental performance than organic by % and % in terms of CO emissions and total environmental impacts, respectively, when the amount of lettuce produced is used as the functional unit of calculations. This is attributed to the fact that the organic system, due to its lower crop yields, requires significantly larger cultivation area to achieve the same crop production with conventional. Moreover, it was found that in all cases the irrigation stage primarily contributed to most impact categories, due to its high energy demands for ground water pumping and the fossil-dependent Greek electricity grid. In addition, in all cases the conventional lettuce cultivation system yielded a significantly high impact onto freshwater eutrophication, due to the use of chemical fertilizers, thus posing serious stresses on local freshwater ecosystems. A sensitivity analysis was carried out and alternative, more sustainable, scenarios were proposed. (C) ", Elsevier Ltd. All rights reserved.,"Foteinis, Spyros|Chatzisymeon, Efthalia",JOURNAL OF CLEANER PRODUCTION,environmental footprint|lca|sustainable agriculture|non-organic|organic farming|vegetables,10.1016/j.jclepro.2015.09.075
408,WOS:000304926200026,2012,Numerical Modelling of Waste Stabilization Ponds: Where Do We Stand?,DYNAMIC MATHEMATICAL-MODEL|AERATED LAGOON|TREATMENT EFFICIENCY|MATURATION PONDS|STEADY-STATE|PAPER-MILL|PREDICTION|REMOVAL|TRANSFORMATION|CALIBRATION,"Waste stabilization pond (WSP) technology has been an active area of research for the last three decades. In spite of its relative simplicity of design, operation and maintenance, the various processes taking place in WSP have not been entirely quantified. Lately, modelling has served as an important, low-cost tool for a better description and an improved understanding of the system. Although several papers on individual pond models have been published, there is no specific review on different models developed so far. This paper aims at filling this gap. Models are compared by focussing on their key features like the presence and comprehensiveness of a water quality sub-model in terms of aerobic/anoxic and anaerobic carbon removal and nutrient removal; the type of hydraulic sub-model used (D, D, D or D); the software used for implementation and simulation; and whether or not sensitivity analysis, calibration and validation were done. This paper also recommends future directions of research in this area. In-depth study of the published models reveals a clear evolution over time in the concept of modelling, from just hydraulic empirical models to D ones and from simple first-order water quality models to complex ones which describe key biochemical processes as a set of mathematical equations. Due to the inherent complexity, models tend to focus only on specific aspects whilst ignoring or simplifying others. For instance, many models have been developed that either focus solely on hydrodynamics or solely on biochemical processes. Models which integrate both aspects in detail are still rare. Furthermore, it is evident from the review of the different models that calibration and validation with full-scale WSP data is also scarce. Hence, we believe that there is a need for the development of a comprehensive, calibrated model for waste stabilization ponds that can reliably serve as a support tool for the improvement and optimization of pond design and performance.",,"Sah, Leena|Rousseau, Diederik P. L.|Hooijmans, Christine M.",WATER AIR AND SOIL POLLUTION,computational fluid dynamics (cfd)|hydrodynamics|modelling|waste stabilization pond (wsp)|water quality,10.1007/s11270-012-1098-4
409,WOS:000222887600002,2004,Uncertainty calculation in life cycle assessments - A combined model of simulation and approximation,FRAMEWORK|LCA,"Goal and Background. Uncertainty is commonly not taken into account in LCA studies, which downgrades their usability for decision support. One often stated reason is a lack of method. The aim of this paper is to develop a method for calculating the uncertainty propagation in LCAs in a fast and reliable manner Approach. The method is developed in a model that reflects the calculation of an LCA. For calculating the uncertainty, the model. combines approximation formulas and Monte Carlo Simulation. It is based on virtual data that distinguishes true values and random errors or uncertainty, and that hence allows one to compare the performance of error propagation formulas and simulation results. The model is developed for a linear chain of processes, but extensions for covering also branched and looped. product systems are made and described. Results. The paper proposes a combined use of approximation formulas and Monte Carlo simulation for calculating uncertainty, in LCAs, developed primarily for the sequential approach. During the calculation, a parameter observation controls the performance of the approximation formulas. Quantitative threshold values are given in the paper. The combination thus transcends drawbacks of simulation and approximation. Conclusions and Outlook. The uncertainty question is a true Jigsaw puzzle for LCAs and the method presented in this paper may serve as one piece in solving it. It may thus foster a sound use of uncertainty assessment in LCAs. Analysing a proper management of the input uncertainty, taking into account suitable sampling; and estimation techniques; using the approach for real case studies, implementing it in LCA software for automatically applying the proposed combined uncertainty model and, on the other hand., investigating about how people do decide, and should decide, when their decision relies on explicitly uncertain LCA outcomes - these all are neighbouring puzzle pieces inviting to further work.",,"Ciroth, A|Fleischer, G|Steinbach, J",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,approximation formula|error propagation|life cycle inventory (lci)|life cycle impact assessment (lcia)|models|monte carlo simulation|quantitative thresholds|uncertainties,10.1065/lca2004.05.158
410,WOS:000325144600007,2013,The life cycle assessment of an e-waste treatment enterprise in China,,"Electrical and electronic waste (e-waste) has become one of the fastest growing waste streams in the world, and many countries have established e-waste treatment enterprises to solve their e-waste problems. In this study, a life cycle assessment (LCA) was undertaken to quantitatively investigate the environmental impacts of an e-waste treatment enterprise in China. The LCA is constructed by SimaPro software version . and expressed with the Eco-indicator  life cycle impact assessment method. For a sensitivity analysis of the overall LCA results, the so-called CML method is used in order to estimate the influence of the choice of the assessment method on the result. According to the survey data, discarded TV sets accounted for the highest proportion of e-waste treated in the enterprise in . The e-waste treatment had little environmental impact, and at the same time large environmental benefits can be achieved mainly due to the recycled resources and reuse of some components. Based on the research results, it can be seen that recycled metal, especially copper, would be of more importance for environmental benefits. Relevant results and data from this study could provide decision support to enterprise managers and government sectors.",,"Song, Qingbin|Wang, Zhishi|Li, Jinhui|Zeng, Xianlai",JOURNAL OF MATERIAL CYCLES AND WASTE MANAGEMENT,e-waste|lca|environmental performance|e-waste treatment enterprise|china,10.1007/s10163-013-0152-7
411,WOS:000248617800003,2007,Automatic sensitivity analysis of a finite volume model for two-dimensional shallow water flows,DENSE URBAN AREA,"Given a numerical model for solving two-dimensional shallow water equations, we are interested in the robustness of the simulation by identifying the rate of change of the water depths and discharges with respect to a change in the bottom friction coefficients. Such a sensitivity analysis can be carried out by computing the corresponding derivatives. Automatic differentiation (AD) is an efficient numerical method, free of approximation errors, to evaluate derivatives of the objective function specified by the computer program, Rubar for example. In this paper AD software tool Tapenade is used to compute forward derivatives. Numerical tests were done to show the robustness of the model and to demonstrate the efficiency of these AD-derivatives.",,"Souhar, O.|Faure, J.-B.|Paquier, A.",ENVIRONMENTAL FLUID MECHANICS,automatic differentiation|hydraulic model|sensitivity analysis|uncertainty propagation|steady flow,10.1007/s10652-007-9028-5
412,WOS:000374807600014,2016,Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework,EQUIFINALITY,"Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification. This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA. (C) ", Elsevier Ltd. All rights reserved.,"Peckham, Scott D.|Kelbert, Anna|Hill, Mary C.|Hutton, Eric W. H.",COMPUTERS & GEOSCIENCES,model uncertainty|modeling frameworks|component-based modeling|optimization|inverse problems|nonlinear least squares|parameter estimation|longitudinal river elevation profiles,10.1016/j.cageo.2016.03.005
413,WOS:000314055300009,2013,Life cycle assessment of sponge nickel produced by gas atomisation for use in industrial hydrogenation catalysis applications,PERFORMANCE|MANAGEMENT|EMISSIONS,"This paper presents a cradle-to-grave comparative life cycle assessment (LCA) of new gas atomised (GA) sponge nickel catalysts and evaluates their performance against the current cast and crush standard currently used in the industrial hydrogenation of butyraldehyde to butanol. A comparative LCA has been made, accounting for the energy used and emissions throughout the entire life cycle of sponge nickel catalysts-ranging from the upstream production of materials (mainly aluminium and nickel), to the manufacturing, to the operation and finally to the recycling and disposal. The LCA was performed following ISO principles where possible, and subsequently implemented in the software package GaBi .. The CML impact assessment methodology was used, with primary focus on comparing catalysts for equivalent greenhouse gasses generated over their lifetime and their relative global warming potential and secondary focus on acidification potential. This is justified as the lifetime is dominated by energy use in the operational phase, and acidification is dominated by the production of nickel for which existing ISO collected data has been used. A sensitivity analysis was used to provide a number of scenarios and overall environmental performances of the various sponge nickels considered when compared to the existing industrial standard. It was found that the energy and emissions during the operation phase associated with a given catalyst significantly outweigh the primary production, manufacturing and recycling. Primary production of the nickel (and to a lesser extent molybdenum when used as a dopant) also has a significant environmental impact in terms of acidification potential, but this is offset by operational energy savings over the catalysts' estimated lifetime and end of life recyclability. Finally, the impact of activity improvement and lifetime duration of sponge nickel catalysts was determined as both total life cycle energy for operational use and as a total life cycle global warming potential. From this assessment, the newly developed, higher activity spongy nickel catalysts produced by gas atomisation could have a significantly lower environmental impact than the current industry standard cast and crush method. Given the potential environmental benefits of such catalysts, applications in other processes that require a catalyst should also be investigated.",,"Lavery, Nicholas P.|Jarvis, David J.|Brown, Stephen G. R.|Adkins, Nicholas J.|Wilson, Benjamin P.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,butyraldehyde to butanol reaction|cast and crush|celanese/rhone poulenc process|gas atomisation|life cycle assessment|sponge (raney) nickel,10.1007/s11367-012-0478-8
414,WOS:000312813800011,2012,Ecological Risk-O-Meter: a risk assessor and manager software tool for better decision making in ecosystems,SECURITY METER|QUANTIFY|MODEL,"Increased awareness of environmental issues and their effects on ecological systems and human health drive an interest in developing computational methods to reduce detrimental consequences. For example, there are concerns regarding chlorofluorocarbons and their impact on stratospheric ozone, radon and its effect on human health, coal mining and effects on habitat loss, as well as numerous other issues. However, these issues do not exist in a vacuum nor occur just one at a time. There is a need to assess social and ecological risks comprehensively and account for numerous, inter-related potential risks. Given limited funds available for addressing these issues, how can spending for purposes of environmental and ecological mitigation be optimized? What is the magnitude of overall ecological risk for a given region? Novel software, the Ecological Risk-o-Meter, addresses these questions and concerns. The software tool not only assesses the current environmental and ecological risks, but also takes into account potential solutions and provides guidance as to how spending can be optimized to reducing overall environmental risk. We demonstrate this new tool and show how to optimize the costs of risk reduction in recursive cycles based on feedbacks."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Sahinoglu, Mehmet|Simmons, Susan J.|Cahoon, Lawrence B.|Morton, Scott",ENVIRONMETRICS,ecological systems|vulnerability|threat|countermeasure|risk-o-meter,10.1002/env.2186
415,WOS:000246563500007,2007,Influence of functional unit on the life cycle assessment of traction batteries,SIMULATION,"Goal, Scope and Background. This paper describes the influence of the choice of the functional unit on the results of an environmental assessment of different battery technologies for electric and hybrid vehicles. Battery, hybrid and fuel cell electric vehicles are considered as being environmentally friendly. However, the batteries they use are sometimes said to be environmentally unfriendly. At the current state of technology different battery types can be envisaged: lead-acid, nickel-cadmium, nickel-metal hydride, lithium-ion and sodium-nickel chloride. The environmental impacts described in this paper are based on a life cycle assessment (LCA) approach. One of the first critical stages of LCA is the definition of an appropriate and specific functional unit for electric and hybrid vehicle application. Most of the known LCA studies concerning batteries were performed while choosing different functional units, although this choice can influence the final results. An adequate functional unit, allowing to compare battery technologies in their real life vehicle application should be chosen. The results of the LCA are important as they will be used as a decision support for the end-of-life vehicles directive //EC (Official journal of the European Communities L/ ). As a consequence, a thorough analysis is required to define an appropriate functional unit for the assessment of batteries for electric vehicles. This paper discusses this issue and will mainly focus on traction batteries for electric vehicles. Main Features. An overview of the different parameters to be considered in the definition of a functional unit to compare battery technologies for battery electric vehicle application is described and discussed. An LCA study is performed for the most relevant potential functional units. SimaPro  is used as a software tool and Eco-indicator  as an impact assessment method. The influence of the different selected functional units on the results (Eco-indicator Points) is discussed. The environmental impact of the different electric vehicle battery technologies is described. A sensitivity analysis illustrates the robustness of the obtained results. Results and Discussion. Five main parameters are considered in each investigated functional unit: an equal depth of discharge is assumed, a relative number of batteries required during the life of the vehicle is calculated, the energy losses in the battery and the additional vehicle consumption due to the battery mass is included and the same lifetime distance target is taken into account. On the basis of the energy content, battery mass, number of cycles and vehicle autonomy three suitable functional units are defined: 'battery packs with an identical mass', 'battery packs with an identical energy content' and 'battery packs with an identical one-charge range'. The results show that the differences in the results between these three functional units are small and imply less variation on the results than the other uncertainties inherent to LCA studies. On the other hand, the results obtained using other, less adequate, functional units can be quite different. Conclusions. When performing an LCA study, it's important to choose an appropriate functional unit. Most of the time, this choice is unambiguous. However, sometimes this choice is more complicated when different correlated parameters have to be considered, as it is the case for traction batteries. When using a realistic functional unit, the result is not influenced significantly by the choice of one out of the three suitable functional units. Additionally, the life cycle assessment allowed concluding that three electric vehicle battery technologies have a comparable environmental impact: lead-acid, nickel-cadmium and nickel-metal hydride. Lithium-ion and sodium-nickel chloride have lower environinental impacts than the three previously cited technologies when used in a typical battery electric vehicle application. Recommendations and Perspectives. The article describes the need to consider all relevant parameters for the choice of a functional unit for an electric vehicle battery, as this choice can influence the conclusions. A more standardised method to define the functional unit could avoid these differences and could make it possible to compare the results of different traction battery LCA studies more easily.",,"Matheys, Julien|Van Autenboer, Wout|Timmermans, Jean-Marc|Van Mierlo, Joeri|Van den Bossche, Peter|Maggetto, Gaston",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,battery|electric vehicle|end-of-life-vehicle|environmental impact|functional unit|life cycle assessment (lca)|sensitivity analysis,10.1065/lca2007.04.322
416,WOS:000240794000013,2006,Sensitivity analysis of differential-algebraic equations and partial differential equations,ADAPTIVE MESH REFINEMENT|SYSTEMS|SOFTWARE|OPTIMIZATION|ALGORITHMS,"Sensitivity analysis generates essential information for model development, design optimization, parameter estimation, optimal control, model reduction and experimental design. In this paper we describe the forward and adjoint methods for sensitivity analysis, and outline some of our recent work on theory, algorithms and software for sensitivity analysis of differential-algebraic equation (DAE) and time-dependent partial differential equation (PDE) systems. (c) ", Elsevier Ltd. All rights reserved.,"Petzold, Linda|Li, Shengtai|Cao, Yang|Serban, Radu",COMPUTERS & CHEMICAL ENGINEERING,sensitivity analysis|differential-algebraic equations|adjoint method,10.1016/j.compchemeng.2006.05.015
417,WOS:000331341400011,2014,Adaptive stochastic Galerkin FEM,GENERALIZED POLYNOMIAL CHAOS|FINITE-ELEMENT-METHOD|ELLIPTIC SPDES|CONVERGENCE|PDES,"A framework for residual-based a posteriori error estimation and adaptive mesh refinement and polynomial chaos expansion for general second order linear elliptic PDEs with random coefficients is presented. A parametric, deterministic elliptic boundary value problem on an infinite-dimensional parameter space is discretized by means of a Galerkin projection onto finite generalized polynomial chaos (gpc) expansions, and by discretizing each gpc coefficient by a FEM in the physical domain. An anisotropic residual-based a posteriori error estimator is developed. It contains bounds for both contributions to the overall error: the error due to gpc discretization and the error due to Finite Element discretization of the gpc coefficients in the expansion. The reliability of the residual estimator is established. Based on the explicit form of the residual estimator, an adaptive refinement strategy is presented which allows to steer the polynomial degree adaptation and the dimension adaptation in the stochastic Galerkin discretization, and, embedded in the gpc adaptation loop, also the Finite Element mesh refinement of the gpc coefficients in the physical domain. Asynchronous mesh adaptation for different gpc coefficients is permitted, subject to a minimal compatibility requirement on meshes used for different gpc coefficients. Details on the implementation with the open-source software framework ALEA are presented; it is generic, and is based on available stiffness and mass matrices of a FEM for the deterministic, nonparametric nominal problem evaluated in the FEniCS environment. Preconditioning of the resulting matrix equation and iterative solution are discussed. Numerical experiments in two spatial dimensions for membrane and plane stress boundary value problems on polygons are presented. They indicate substantial savings in total computational complexity due to FE mesh coarsening in high gpc coefficients.", (C) 2013 Elsevier B.V. All rights reserved.,"Eigel, Martin|Gittelson, Claude Jeffrey|Schwab, Christoph|Zander, Elmar",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,uncertainty quantification|stochastic finite element methods|operator equations|alea|fenics|adaptive methods,10.1016/j.cma.2013.11.015
418,WOS:000378954000023,2016,SEVIRI PrePro: A novel software tool for the pre-processing of SEVIRI geostationary orbit EO data products,ECOSYSTEMS,"The Spinning Enhanced Visible and. Infrared Imager (SEVIRI) is a geostationary orbit multispectral sensor on-board the Meteosat second Generation (MSG) platform, acquiring Earth Observation (EO) data over Earth's land surface from the optical to infrared parts of electromagnetic spectrum every  min. From the sensor a series of operational products are also provided to the user's community at no cost via EUMETSAT or LSA SAF portals. Herein, an open access stand-alone software product developed in Java programming language is presented for automating key pre-processing steps to all the SEVIRI operationally distributed products. The software tool, named Seviri PrePro, makes use of present day multi-core processors and is able to process very large datasets in a short time period, making it appropriate as well for use in a High Performance Computing (HPC) environment. The practical usefulness of the toolkit is also demonstrated herein using as a case study the SEVIRI evapotranspiration (ET) product. The development of SEVIRI PrePro is of significant importance to the SEVIRI users' community and is also very timely given that, to our knowledge, no similar software tool is freely distributed at present. Its use is anticipated to make a significant contribution to a large number of practical applications requiring use of SEVIRI data, including but not limited, weather forecasting and global climate monitoring at a range of geographical scales. (C) ", Elsevier Ltd. All rights reserved.,"Petropoulos, George P.|Anagnostopoulos, Vasileios",ENVIRONMENTAL MODELLING & SOFTWARE,earth observation|seviri|pre-processing|operational products|software tool,10.1016/j.envsoft.2016.03.015
419,WOS:000323981900016,2013,Analyzing the effects of geological and parameter uncertainty on prediction of groundwater head and travel time,MULTIPLE-POINT STATISTICS|GLUE METHODOLOGY|CAPTURE ZONE|FLOW|SIMULATION|MODEL|HETEROGENEITY|INCOHERENCE|CALIBRATION|DENMARK,"Uncertainty of groundwater model predictions has in the past mostly been related to uncertainty in the hydraulic parameters, whereas uncertainty in the geological structure has not been considered to the same extent. Recent developments in theoretical methods for quantifying geological uncertainty have made it possible to consider this factor in groundwater modeling. In this study we have applied the multiple-point geostatistical method (MPS) integrated in the Stanford Geostatistical Modeling Software (SGeMS) for exploring the impact of geological uncertainty on groundwater flow patterns for a site in Denmark. Realizations from the geostatistical model were used as input to a groundwater model developed from Modular three-dimensional finite-difference ground-water model (MODFLOW) within the Groundwater Modeling System (GMS) modeling environment. The uncertainty analysis was carried out in three scenarios involving simulation of groundwater head distribution and travel time. The first scenario implied  stochastic geological models all assigning the same hydraulic parameters for the same geological units. In the second scenario the same  geological models were subjected to model optimization, where the hydraulic parameters for each of them were estimated by calibration against observations of hydraulic head and stream discharge. In the third scenario each geological model was run with  randomized sets of parameters. The analysis documented that the uncertainty on the conceptual geological model was as significant as the uncertainty related to the embedded hydraulic parameters.",,"He, X.|Sonnenborg, T. O.|Jorgensen, F.|Hoyer, A. -S.|Moller, R. R.|Jensen, K. H.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-3245-2013
420,WOS:000351458000015,2015,Risk Analysis of Water Demand for Agricultural Crops under Climate Change,OPTIMIZATION HBMO ALGORITHM|RESERVOIR OPERATION|DESIGN|UNCERTAINTY|DISCRETE|NETWORKS|STRATEGY|IMPACTS|SYSTEM,"This paper assesses the risk of increase in water demand for a wide range of irrigated crops in an irrigation network located downstream of the Aidoghmoush Dam in East Azerbaijan by considering climate change conditions for the period -. Atmosphere-ocean global circulation models (AOGCMs) are used to simulate climatic variables such as temperature and precipitation. The Bayesian approach is used to consider uncertainties of AOGCMs. Climate change scenarios of climatic variables are first weighted by using the mean observed temperature-precipitation (MOTP) method, and related probability distribution functions are produced. Outputs of AOGCMs are used as input to water requirement models. Then, produced by using the Monte Carlo method,  samples (discrete values) from the probability distribution functions of monthly downscaled temperature and precipitation in the study area are extracted by using a software for sensitivity and uncertainty analysis. Time series of climatic variables in future periods are then generated (temperature variable to calculate potential evapotranspiration and rainfall variable to calculate effective rainfall). To estimate crop water requirements, crop evapotranspiration (from the product of potential evapotranspiration in the previous step and coefficient of crop computed) and effective precipitation (from time series of the previous step) are calculated. The Food and Agricultural Organization of the United Nations (FAO) methods, FAO- and Penman-Monteith, were used to compute crop and potential evapotranspiration, respectively. Because of lack of required data, potential evapotranspiration in future periods is computed through the relationship of temperature and potential evapotranspiration in the baseline period; the same procedure is conducted for temperature. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering  monthly time series of downscaled temperature and precipitation in future periods. The results indicate that risk of changes in crop water requirements increases by approximately % for a % risk, approximately % for a % risk, and approximately % for a % risk. Also, based on the current cultivated area, on average, the volume of water demand only for the aforementioned crops will be approximately .(() m()/year) with a risk of %, approximately (() m()/year) with a risk of %, and approximately (() m()/year) with a risk of %. Wheat and barley are more resistant and less sensitive to climate change than other crops considered.", (C) 2014 American Society of Civil Engineers.,"Ashofteh, Parisa-Sadat|Bozorg-Haddad, Omid|Marino, Miguel A.",JOURNAL OF HYDROLOGIC ENGINEERING,climate change|net water requirement|risk|uncertainty|monte carlo,10.1061/(ASCE)HE.1943-5584.0001053
421,WOS:000242178600020,2006,Colloid-facilitated solute transport in variably saturated porous media: Numerical model and experimental verification,ALLUVIAL GRAVEL AQUIFER|DEEP-BED FILTRATION|CONTAMINANT TRANSPORT|PARTICLE-TRANSPORT|LABORATORY COLUMN|ANION EXCLUSION|IONIC-STRENGTH|SAND COLUMNS|WATER|SOILS,"Strongly sorbing chemicals (e. g., heavy metals, radionuclides, pharmaceuticals, and explosives) in porous media are associated predominantly with the solid phase, which is commonly assumed to be stationary. However, recent field- and laboratory-scale observations have shown that in the presence of mobile colloidal particles (e.g., microbes, humic substances, clays, and metal oxides), colloids can act as pollutant carriers and thus provide a rapid transport pathway for strongly sorbing contaminants. To address this problem, we developed a one-dimensional numerical model based on the HYDRUS-D software package that incorporates mechanisms associated with colloid and colloid-facilitated solute transport in variably saturated porous media. The model accounts for transient variably saturated water flow, and for both colloid and solute movement due to advection, diffusion, and dispersion, as well as for solute movement facilitated by colloid transport. The colloid transport module additionally considers the processes of attachment/detachment to/from the solid phase and/or the air-water interface, straining, and/or size exclusion. Various blocking and depth dependent functions can be used to modify the attachment and straining coefficients. The solute transport module uses the concept of two-site sorption to describe nonequilibrium adsorption-desorption reactions to the solid phase. The module further assumes that contaminants can be sorbed onto surfaces of both deposited and mobile colloids, fully accounting for the dynamics of colloid movement between different phases. Application of the model is demonstrated using selected experimental data from published saturated column experiments, conducted to investigate the transport of Cd in the presence of Bacillus subtilis spores in alluvial gravel aquifer media. Numerical results simulating bacteria transport, as well as the bacteria-facilitated Cd transport, are compared with experimental results. A sensitivity analysis of the model to various parameters is also presented.",,"Simunek, Jirka|He, Changming|Pang, Liping|Bradford, S. A.",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0151
422,WOS:000313918200054,2013,Stochastic approach to municipal solid waste landfill life based on the contaminant transit time modeling using the Monte Carlo (MC) simulation,EARTHEN BARRIERS|COMPACTED CLAY|SATURATED SOIL|TRANSPORT|MIGRATION|DIFFUSION|SORPTION|SITES|DECAY,"The paper is concerned with application and benefits of MC simulation proposed for estimating the life of a modern municipal solid waste (MSW) landfill. The software Crystal Ball (R) (CB), simulation program that helps analyze the uncertainties associated with Microsoft (R) Excel models by MC simulation, was proposed to calculate the transit time contaminants in porous media. The transport of contaminants in soil is represented by the one-dimensional (D) form of the advection-dispersion equation (ADE). The computer program CONTRANS written in MATLAB language is foundation to simulate and estimate the thickness of landfill compacted clay liner. In order to simplify the task of determining the uncertainty of parameters by the MC simulation, the parameters corresponding to the expression Z taken from this program were used for the study. The tested parameters are: hydraulic gradient (HG), hydraulic conductivity (HC), porosity (POROS), linear thickness (TH) and diffusion coefficient (EDC). The principal output report provided by CB and presented in the study consists of the frequency chart, percentiles summary and statistics summary. Additional CB options provide a sensitivity analysis with tornado diagrams. The data that was used include available published figures as well as data concerning the Mittal Steel Poland (MSP) S.A. in Krakow, Poland. This paper discusses the results and show that the presented approach is applicable for any MSW landfill compacted clay liner thickness design.", (C) 2012 Elsevier B.V. All rights reserved.,"Bieda, Boguslaw",SCIENCE OF THE TOTAL ENVIRONMENT,poland|mc simulation|cb (r)|sensitivity analysis|advection dispersion equation|stochastic process,10.1016/j.scitotenv.2012.10.032
423,WOS:000415699500014,2017,Matrix-free algorithm for the optimization of multidisciplinary systems,EQUALITY CONSTRAINED OPTIMIZATION|NONLINEAR AEROELASTIC SYSTEMS|LARGE-SCALE OPTIMIZATION|INEXACT NEWTON METHOD|KRYLOV-SCHUR METHODS|DESIGN OPTIMIZATION|SENSITIVITY-ANALYSIS|BOUNDARY-CONDITIONS|ENGINEERING DESIGN|SQP METHOD,"Multidisciplinary engineering systems are usually modeled by coupling software components that were developed for each discipline independently. The use of disparate solvers complicates the optimization of multidisciplinary systems and has been a long-standing motivation for optimization architectures that support modularity. The individual discipline feasible (IDF) formulation is particularly attractive in this respect. IDF achieves modularity by introducing optimization variables and constraints that effectively decouple the disciplinary solvers during each optimization iteration. Unfortunately, the number of variables and constraints can be significant, and the IDF constraint Jacobian required by most conventional optimization algorithms is prohibitively expensive to compute. Furthermore, limited-memory quasi-Newton approximations, commonly used for large-scale problems, exhibit linear convergence rates that can struggle with the large number of design variables introduced by the IDF formulation. In this work, we show that these challenges can be overcome using a reduced-space inexact-Newton-Krylov algorithm. The proposed algorithm avoids the need for the explicit constraint Jacobian and Hessian by using a Krylov iterative method to solve the Newton steps. The Krylov method requires matrix-vector products, which can be evaluated in a matrix-free manner using second-order adjoints. The Krylov method also needs to be preconditioned, and a key contribution of this work is a novel and effective preconditioner that is based on approximating a monolithic solution of the (linearized) multidisciplinary system. We demonstrate the efficacy of the algorithm by comparing it with the popular multidisciplinary feasible formulation on two test problems.",,"Dener, Alp|Hicken, Jason E.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,multidisciplinary design optimization|individual discipline feasible|inexact-newton-krylov|matrix-free|second-order adjoint|preconditioning,10.1007/s00158-017-1734-0
424,WOS:000270236100003,2009,Estimating stream metabolism from oxygen concentrations: Effect of spatial heterogeneity,ORGANIC-MATTER|ECOSYSTEM METABOLISM|GAS-EXCHANGE|PRIMARY PRODUCTIVITY|SENSITIVITY-ANALYSIS|RIVER|RESPIRATION|REAERATION|VARIABILITY|TURNOVER,"Rivers are heterogeneous at various scales. River metabolism estimators based on oxygen time series provide average estimates of net oxygen production at the scale of a river reach. These estimators are derived for homogeneous river reaches. For this reason, they cannot be used to analyze how exactly they average over longitudinal variations in net production, reaeration, oxygen saturation concentration and flow velocity. We try to fill this gap by using a general analytical solution of the transport-reaction equation to () demonstrate how downstream oxygen concentration is affected by upstream concentration and (possible) longitudinally varying values of net production, reaeration, oxygen saturation concentration and flow velocity within a reach, and () derive how the net production estimate depends on varying upstream river parameters. In addition, we derive a new net production estimator that extends previously suggested estimators. The equations derived in this paper provide a general framework for understanding the assumptions underlying net production estimators. They are used to derive recommendations on the use of single station or two stations measurement layouts to get accurate river metabolism estimates. The estimator is implemented in the freely available statistics and graphics software package R (http://www.r-project.org). This makes it easily applicable to observed oxygen time series. Empirical evidence of the significance of heterogeneity in rivers is demonstrated by applying the estimator to four subsequent reaches of a river using oxygen measurements from the ends of all reaches.",,"Reichert, Peter|Uehlinger, Urs|Acuna, Vicenc",JOURNAL OF GEOPHYSICAL RESEARCH-BIOGEOSCIENCES,,10.1029/2008JG000917
425,WOS:000306765400006,2012,Protocol to support model selection and evaluation in a modular crop modelling framework: An application for simulating crop response to nitrogen supply,AGRICULTURAL SYSTEMS|GROWTH SIMULATION|USE EFFICIENCY|RADIATION|WHEAT|SOFTWARE|SOIL|KNOWLEDGE|DYNAMICS|SCIENCE,"Crop models require different structures for different applications. Modular and flexible crop modelling frameworks, such as the recently developed agricultural production and externalities simulator (APES), support the change of model structure. However, the assembly of different modules to create a model may not always result in the best model structure. We developed and tested a protocol for a systematic selection and evaluation of a crop growth model structure. The novelty of the presented protocol relies on a throughout analysis of the different modelling approaches (modules) and on how to assemble them to create new modelling solutions (i.e. model). We use a case study to demonstrate that we can explicitly express and test the different assumptions behind the choice of a specific modelling approach. Our case study refers to the simulation of crop growth in response to nitrogen management and the importance of an accurate simulation of the nitrogen uptake. Applying the proposed protocol, we identify the need to improve the initially selected nitrogen mineralisation module. We conclude that not only is the protocol suitable to provide guidance for systematic testing of different crop processes modelled, but also its use highlights the importance of the documentation of the modelling process and of the clarification of the uncertainty associated with the model structure.", (C) 2011 Elsevier B.V. All rights reserved.,"Adam, M.|Belhouchette, H.|Corbeels, M.|Ewert, F.|Perrin, A.|Casellas, E.|Celette, F.|Wery, J.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,crop model structure|modules|uncertainty analysis|nitrogen,10.1016/j.compag.2011.09.009
426,WOS:000258484000001,2008,Software framework for parameter updating and finite-element response sensitivity analysis,INELASTIC STRUCTURES|RELIABILITY|SERVICES|PROGRAM,"The finite-element software framework OpenSees is extended with parameter updating and response sensitivity capabilities to support client applications such as reliability, optimization, and system identification. Using software design patterns, member properties, applied loadings, and nodal coordinates can be identified and repeatedly updated in order to create customized finite-element model updating applications. Parameters are identified using a Chain of Responsibility software pattern, where objects in the finite-element model forward a parameterization request to component objects until the request is handled. All messages to identify and update parameters are passed through a Facade that decouples client applications from the finite-element domain of OpenSees. To support response sensitivity analysis, the Strategy design pattern facilitates multiple approaches to evaluate gradients of the structural response, whereas the Visitor pattern ensures that objects in the finite-element domain make the proper contributions to the equations that govern the response sensitivity. Examples demonstrate the software design and the steps taken by representative finite-element model updating and response sensitivity applications.",,"Scott, Michael H.|Haukaas, Terje",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2008)22:5(281)
427,WOS:000364452300007,2015,Uncertainties propagations in 1D hydraulic modeling,,"Numerical modeling tools, like Crue the D modeling software developed by CNR, are widely used to analyze hydraulic and hydrological behavior of rivers. Those tools are based on input parameters, with physical or numerical meaning; theses inputs are generally known with some uncertainties. The tool Promethee, developed by IRSN, is able to realize uncertainties propagations, and two kinds of sensibility analysis: the first one, a determinist method (Morris) based on screening, is able to identify factors which influenced outputs variability; the second one, a probabilistic method (FAST) based on variance analysis of outputs regarding inputs variances, performs inputs ranking in function of outputs sensibilities. Uncertainties propagations studies require an important computational capacity; to do so; the Promethee/Crue coupling is used. The coupled tool is able to parameter Crue files for the hydraulic computations, to run lots of computation, and then to analyze results with statistic tools. This coupled tool gives the possibility to realize sensitivity studies by probabilistic method, to parameter realistic and complex model rivers, and to study the influence of several inputs variations.",,"Nguyen, Tra-mi|Richet, Yann|Balayn, Pierre|Bardet, Lise",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,promethee|crue9|sensitivity analysis|fast|morris,10.1051/lhb/20150055
428,WOS:000344785900002,2014,Environmental assessment of thermal insulation composite material,LIFE-CYCLE ASSESSMENT|RECYCLED POLYMER HDPE|BUILDING-MATERIALS|WASTE MANAGEMENT|EMBODIED ENERGY|IMPACTS|GREEN|BLOCK|GLASS|LCA,"This paper presents life cycle assessment of planned mass production of the thermal insulation blocks (TIB) made of thermal insulation composite material (TICM) from secondary raw materials-glass and plastic. This material is being developed at Brno University of Technology, Faculty of Civil Engineering for use in structural details of (especially low energy or passive) buildings subjected to higher compressive loads. Two production modes depending on the quality of the input materials are compared. The assessment is conducted using GaBi  software tool with inbuilt Ecoinvent database. The results of the assessment are presented in individual impact categories according to used characterization model (CML -Dec. ). All the necessary energy and material flows are specified in detail for the purpose of the assessment. Cut-off allocation method is used for allocating the environmental impacts of recycled materials. Part of the assessment is sensitivity analysis of one variable parameter-amount of TIB produced per year. The results of the assessment show decisive impact of used electricity source on the overall results-. and . %, respectively, for both production modes. This is closely connected with quality of used secondary raw materials and design of the production line. Use of higher-quality materials, as well as changes of the designed production line can reduce the overall environmental impacts by almost  %. The results show possible improvements in the planned mass production of the TIB. They also find that further investigation will be required before the start of mass production, especially in connection with improving the environmental impacts of used electricity sources.",,"Struhala, Karel|Stranska, Zuzana|Pencik, Jan|Matejka, Libor",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,composite plastics|life cycle assessment|recycling|secondary raw materials,10.1007/s11367-014-0796-0
429,WOS:000330574400011,2014,Computer algebra systems coming of age: Dynamic simulation and optimization of DAE systems in Mathematica (TM),SENSITIVITY-ANALYSIS|CONSISTENT INITIALIZATION|PATH CONSTRAINTS|EQUATIONS|EFFICIENT|SOFTWARE|CRITERIA,"In this article, DAE parser and SQPsolver, new Computer Algebra System packages specialized in Differential-Algebraic Equations for Mathematical (TM) are presented. These packages joint capabilities for dynamical system analysis, simulation and dynamic optimization through the direct sequential approach are presented with examples and case studies highlighting applications of practical interest to chemical engineers. An overview of the relevant theoretical topics to each of the features of the packages are presented as well as implementation insights. This work paves the way for innovative R&D platforms both capable of solving practical problems of interest as well as offer seamless computational workflow. (C) ", Elsevier Ltd. All rights reserved.,"Navarro, A. K. W.|Vassiliadis, V. S.",COMPUTERS & CHEMICAL ENGINEERING,differential algebraic equations|simulation|parametric sensitivity|dynamic optimizationa,10.1016/j.compchemeng.2013.11.004
430,WOS:000256053900010,2008,A comparative life cycle analysis of two different juice packages,,"Packaging wastes have a portion of -% in total municipal solid waste (MSW) in Turkey, and they have to be evaluated from production to final disposal from the environmental point of view. The concern about the environmental impacts of packages has been dealt with using several approaches in environmental management, such as risk assessment, environmental impact assessment, environmental auditing, energy analysis, material flow analysis, and life cycle analysis (LCA). The main purpose of this research was to investigate the life cycle environmental impact of glass bottles and beverage cartons. This LCA study was performed by using SimaPro (PRe Consultants, The Netherlands) software. Individual and comparative life cycle analysis of two packages was performed depending on a consumer who lives in Eskisehir city. For that aim, SimaPro was used, and the data to run the software was gathered from package producers, the database of the software, and the literature. Life cycle comparisons of the two juice packages among themselves and also within themselves were carried out by using EcoIndicator  on the basis of climate change, ecotoxicity, acidification/eutrophication, and fossil fuels. Sensitivity analysis was performed to evaluate the effects of transportation. According to comparison figures, the environmental load of glass bottles is higher than beverage carton's load for all the impact categories. This result is also supported by the sensitivity analysis.",,"Banar, Mufide|Cokaygil, Zerrin",ENVIRONMENTAL ENGINEERING SCIENCE,lca|glass bottle|beverage carton|packaging waste|simapro7,10.1089/ees.2007.0079
431,WOS:000244915000009,2007,Topology optimization of material-nonlinear continuum structures by the element connectivity parameterization,DESIGN SENSITIVITY-ANALYSIS|ELASTOPLASTIC STRUCTURES|COMPLIANT MECHANISMS|PLASTIC-DEFORMATION|CONTACT|CRASHWORTHINESS,"The application of the element density-based topology optimization method to nonlinear continuum structures is limited to relatively simple problems such as bilinear elastoplastic material problems. Furthermore, it is very difficult to use analytic sensitivity when a commercial nonlinear finite element code is used. As an alternative to the element density formulation, the element connectivity parameterization (ECP) formulation is developed for the topology optimization of isotropic-hardening elastoplastic or hyperelastic continua by using commercial software. ECP varies the stiffness of zero-length linear elastic links that connect design domain-discretizing finite elements. Unloading was not considered. But the advantages of ECP in material-nonlinear problems were demonstrated: considerably simple analytic sensitivity calculation using a commercial code and simple link stiffness penalization regardless of nonlinear material behaviour."," Copyright (c) 2006 John Wiley & Sons, Ltd.","Yoon, Gil Ho|Kim, Yoon Young",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,topology optimization|material-nonlinearity|element connectivity parameterization,10.1002/nme.1843
432,WOS:000334003800020,2014,Environmental impact assessment based on dynamic fuzzy simulation,MODELS|SYSTEMS,"A new ""quick scan"" method for an expert-/stakeholder-based impact assessment approach is introduced. This approach aims to reduce the complexity of models, to simulate and visualize the system dynamics and to provide a basis for guided discussion with stakeholders. The approach is based on dynamic fuzzy models that can be understood easily and developed by experts and understood and adapted by stakeholders (""white box models""). This open modeling process also forms the basis of the credibility of the simulation results. The quick scan approach is supported by an interactive simulation tool that includes optimization and uncertainty analysis as open source software. (C) ", Elsevier Ltd. All rights reserved.,"Wieland, Ralf|Gutzler, Carsten",ENVIRONMENTAL MODELLING & SOFTWARE,quick scan|environmental impact assessment|fuzzy modeling|dynamic fuzzy simulation,10.1016/j.envsoft.2014.02.001
433,WOS:000324007200020,2013,Robust optimal dynamic production/pricing policies in a closed-loop system,INVENTORY CONTROL|OPTIMIZATION APPROACH|PRODUCT|RETURNS|MODEL|ENVIRONMENT|EXCHANGE|DEMAND|COSTS,"Hybrid manufacturing/remanufacturing systems play a key role in implementing closed-loop production systems which have been considered due to increasingly environmental concerns and latent profit of used products. Manufacturing and remanufacturing rates, selling price of new products, and acquisition price of used products are the most critical variables to optimize in such hybrid systems. In this paper, we develop a dynamic production/pricing problem, in which decisions should be made in each period confronting with uncertain demand and return. The manufacturer is able to control the demand and return by adjusting selling price and acquisition price respectively, also she can stock inventories of used and new products to deal with uncertainties. Modeling a nominal profit maximization problem, we go through robust optimization approach to reformulate it for the uncertain case. Final robust optimization model is obtained as a quadratic programming model over discrete periods which can be solved by optimization packages of QP. A numerical example is defined and sensitivity analysis is performed on both basic parameters and parameters associated with uncertainty to create managerial views.", (C) 2013 Elsevier Inc. All rights reserved.,"Mahmoudzadeh, Mahdi|Sadjadi, Seyed Jafar|Mansour, Saeed",APPLIED MATHEMATICAL MODELLING,closed-loop supply chain|robust optimization|production-pricing|dynamic pricing|quadratic programming|remanufacturing,10.1016/j.apm.2013.03.008
434,WOS:000173726400003,2002,Emerging issues in population viability analysis,SPOTTED OWL METAPOPULATION|INBREEDING DEPRESSION|ELASTICITY ANALYSIS|CONSERVATION BIOLOGY|GENTIANA-PNEUMONANTHE|MAMMALIAN POPULATIONS|SPECIES CONSERVATION|PLANT-POPULATIONS|GENETIC-VARIATION|ET-AL,"Population viability analysis (PVA) has become a commonly used tool in endangered species management. There is no single process that constitutes PVA, but all approaches have in common all assessment of a population's risk of extinction (or quasi extinction) or its projected population growth either under current conditions or expected from proposed management. As model sophistication increases, and software programs that facilitate PVA without the need for modeling expertise become more available, there is greater potential for the misuse of models and increased confusion over interpreting their results. Consequently, we discuss the practical use and limitations of PVA in conservation planning, and we discuss some emerging issues of PVA. We review extant issues that have become prominent in PTA, including spatially explicit modeling, sensitivity analysis, incorporating genetics into PVA, PVA in plants, and PVA software packages, but our coverage of emerging issues is not comprehensive. We conclude that PVA is a powerful tool in conservation biology for comparing alternative research plans and relative extinction risks among species, but the suggest caution in its use: () because PVA is a model, its validity depends on the appropriateness of the model's structure and data quality; () results should be presented with appropriate assessment of confidence; () model construction and results should be subject to external review, and () model structure, input, and results should be treated as hypotheses to be tested. We also suggest () restricting the definition of PVA to development of a formal quantitative model, () focusing more research on determining how pervasive density-dependence feedback is across species, and () not using PVA to determine minimum population size or () the specific probability of reaching extinction. The most appropriate use of PVA may be for comparing the relative effects of potential management actions on population growth or persistence.",,"Reed, JM|Mills, LS|Dunning, JB|Menges, ES|McKelvey, KS|Frye, R|Beissinger, |Anstett, MC|Miller, P",CONSERVATION BIOLOGY,,10.1046/j.1523-1739.2002.99419.x
435,WOS:000395108800010,2017,Pattern Mixture Models for Quantifying Missing Data Uncertainty in Longitudinal Invariance Testing,STRUCTURAL EQUATION MODELS|FACTORIAL INVARIANCE|DROP-OUT|PSYCHOLOGICAL-RESEARCH|NONIGNORABLE DROPOUT|SENSITIVITY-ANALYSIS|CLINICAL-TRIALS|INCOMPLETE DATA|MIXED MODELS|GROWTH,"Many psychology applications assess measurement invariance of a construct (e.g., depression) over time. These applications are often characterized by few time points (e.g., ), but high rates of dropout. Although such applications routinely assume that the dropout mechanism is ignorable, this assumption may not always be reasonable. In the presence of nonignorable dropout, fitting a conventional longitudinal factor model (LFM) to assess longitudinal measurement invariance can yield misleading inferences about the level of invariance, along with biased parameter estimates. In this article we develop pattern mixture longitudinal factor models (PM-LFMs) for quantifying uncertainty in longitudinal invariance testing due to an unknown, but potentially nonignorable, dropout mechanism. PM-LFMs are a kind of multiple group model wherein observed missingness patterns define groups, LFM parameters can differ across these pattern-groups subject to identification constraints, and marginal inference about longitudinal invariance is obtained by pooling across pattern-groups. When dropout is nonignorable, we demonstrate via simulation that conventional LFMs can indicate longitudinal noninvariance, even when invariance holds in the overall population; certain PM-LFMs are shown to ameliorate this problem. On the other hand, when dropout is ignorable, PM-LFMs are shown to provide results comparable to conventional LFMs. Additionally, we contrast PM-LFMs to a latent mixture approach for accommodating nonignorable dropoutwherein missingness patterns can differ across latent groups. In an empirical example assessing longitudinal invariance of a harsh parenting construct, we employ PM-LFMs to assess sensitivity of results to assumptions about nonignorable missingness. Software implementation and recommendations for practice are discussed.",,"Sterba, Sonya K.",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,longitudinal factor model|longitudinal invariance|nonignorable missing data|pattern mixture model,10.1080/10705511.2016.1250635
436,WOS:000409151600011,2017,"Simulation, identification and statistical variation in cardiovascular analysis (SISCA) - A software framework for multi-compartment lumped modeling",SENSITIVITY-ANALYSIS|BLOOD-FLOW|ARTERIAL|WINDKESSEL|PRESSURE|SYSTEM|HEART,"It has not yet been possible to obtain modeling approaches suitable for covering a wide range of real world scenarios in cardiovascular physiology because many of the system parameters are uncertain or even unknown. Natural variability and statistical variation of cardiovascular system parameters in healthy and diseased conditions are characteristic features for understanding cardiovascular diseases in more detail. This paper presents SISCA, a novel software framework for cardiovascular system modeling and its MATLAB implementation. The framework defines a multi-model statistical ensemble approach for dimension reduced, multi-compartment models and focuses on statistical variation, system identification and patient-specific simulation based on clinical data. We also discuss a data-driven modeling scenario as a use case example. The regarded dataset originated from routine clinical examinations and comprised typical pre and post surgery clinical data from a patient diagnosed with coarctation of aorta. We conducted patient and disease specific pre/post surgery modeling by adapting a validated nominal multi-compartment model with respect to structure and parametrization using metadata and MRI geometry. In both models, the simulation reproduced measured pressures and flows fairly well with respect to stenosis and stent treatment and by pre-treatment cross stenosis phase shift of the pulse wave. However, with post-treatment data showing unrealistic phase shifts and other more obvious inconsistencies within the dataset, the methods and results we present suggest that conditioning and uncertainty management of routine clinical data sets needs significantly more attention to obtain reasonable results in patient-specific cardiovascular modeling.",,"Huttary, Rudolf|Goubergrits, Leonid|Schutte, Christof|Bernhard, Stefan",COMPUTERS IN BIOLOGY AND MEDICINE,windkessel elements|lumped models|od modeling|multi-compartment modeling|cardiovascular simulation|distributed parameter modeling|clinical data set|coarctation of aorta|patient-specific models|disease-specific models|multiscale modeling,10.1016/j.compbiomed.2017.05.021
437,WOS:000278967500018,2010,SensSB: a software toolbox for the development and sensitivity analysis of systems biology models,,"SensSB (Sensitivity Analysis for Systems Biology) is an easy to use, MATLAB-based software toolbox, which integrates several local and global sensitivity methods that can be applied to a wide variety of biological models. In addition to addressing the sensitivity analysis problem, SensSB aims to cover all the steps involved during the modeling process. The main features of SensSB are: (i) derivative and variance-based global sensitivity analysis, (ii) pseudo-global identifiability analysis, (iii) optimal experimental design (OED) based on global sensitivities, (iv) robust parameter estimation, (v) local sensitivity and identifiability analysis, (vi) confidence intervals of the estimated parameters and (vii) OED based on the Fisher Information Matrix (FIM). SensSB is also able to import models in the Systems Biology Mark-up Language (SBML) format. Several examples from simple analytical functions to more complex biological pathways have been implemented and can be downloaded together with the toolbox. The importance of using sensitivity analysis techniques for identifying unessential parameters and designing new experiments is quantified by increased identifiability metrics of the models and decreased confidence intervals of the estimated parameters.",,"Rodriguez-Fernandez, Maria|Banga, Julio R.",BIOINFORMATICS,,10.1093/bioinformatics/btq242
438,WOS:000321313800102,2013,Techno-economic analysis of two bio-oil upgrading pathways,BIOMASS FAST PYROLYSIS|HYDROGEN-PRODUCTION|GREEN GASOLINE|MODEL|FUELS,"We evaluate the economic feasibility for two bio-oil upgrading pathways: two-stage hydrotreating followed by fluid catalytic cracking (FCC) or single-stage hydrotreating followed by hydrocracking. In the hydrotreating/FCC pathway, two options are available as the hydrogen source for hydrotreating: merchant hydrogen or hydrogen from natural gas reforming. The primary products of the hydrotreating/FCC pathway are commodity chemicals whereas the primary products for the hydrotreating/hydrocracking pathway are transportation fuels and hydrogen. The two pathways are modeled using Aspen Plus (R) for a  metric tons/day facility. Equipment sizing and cost calculations are based on Aspen Economic Evaluation (R) software. The bio-oil yield via fast pyrolysis is assumed to be % of biomass. We calculate the internal rate of return (IRR) for each pathway as a function of feedstock cost, fixed capital investment (FCI), hydrogen and catalyst costs, and facility revenues. The results show that a facility employing the hydrotreating/ FCC pathway with hydrogen production via natural gas reforming option generates the highest IRR of .%. Sensitivity analysis demonstrates that product yield, FCI, and biomass cost have the greatest impacts on facility IRR. Monte-Carlo analysis shows that two-stage hydrotreating and FCC of the aqueous phase bio-oil with hydrogen produced via natural gas reforming has a relatively low risk for project investment.", (c) 2013 Published by Elsevier B.V.,"Zhang, Yanan|Brown, Tristan R.|Hu, Guiping|Brown, Robert C.",CHEMICAL ENGINEERING JOURNAL,fast pyrolysis|bio-oil upgrading|commodity chemicals|transportation fuels|hydrogen,10.1016/j.cej.2013.01.030
439,WOS:000355049700021,2015,Life cycle assessment of integrated waste management systems for alternative legacy scenarios of the London Olympic Park,MUNICIPAL SOLID-WASTE|GLOBAL WARMING CONTRIBUTIONS|GLASS-PACKAGING DISPOSAL|GREENHOUSE GASES|ANAEROBIC-DIGESTION|ENERGY RECOVERY|TO-ENERGY|LCA|INCINERATION|TRANSITIONS,"This paper presents the results of the life cycle assessment (LCA) of  integrated waste management systems (IWMSs) for  potential post-event site design scenarios of the London Olympic Park. The aim of the LCA study is to evaluate direct and indirect emissions resulting from various treatment options of municipal solid waste (MSW) annually generated on site together with avoided emissions resulting from energy, materials and nutrients recovery. IWMSs are modelled using GaBi v. Product Sustainability software and results are presented based on the CML (v.Nov-) characterisation method. The results show that IWMSs with advanced thermal treatment (ATT) and incineration with energy recovery have the lowest Global Warming Potential (GWP) than IWMSs where landfill is the primary waste treatment process. This is due to higher direct emissions and lower avoided emissions from the landfill process compared to the emissions from the thermal treatment processes. LCA results demonstrate that significant environmental savings are achieved through substitution of virgin materials with recycled ones. The results of the sensitivity analysis carried out for IWMS  shows that increasing recycling rate by %, % and % compared to the baseline scenario can reduce GWP by %, % and % respectively. Sensitivity analysis also shows how changes in waste composition affect the overall result of the system. The outcomes. of such assessments provide decision-makers with fundamental information regarding the environmental impacts of different waste treatment options necessary for sustainable waste management planning. (C)  The Authors.", Published by Elsevier Ltd.,"Parkes, Olga|Lettieri, Paola|Bogle, I. David L.",WASTE MANAGEMENT,environmental assessment|life cycle assessment (lca)|integrated waste management systems (iwms),10.1016/j.wasman.2015.03.017
440,WOS:000303381300019,2012,Chloride migration in groundwater for a tannery belt in Southern India,NATURAL RECHARGE|AQUIFER PARAMETERS|SOLUTE TRANSPORT|TAMIL-NADU|TERRAIN|BASIN|WATER|SITE,"Groundwater in a tannery belt in Southern India is being polluted by the discharge of untreated effluents from  operating tanneries. Total dissolved solids and chloride (Cl-) measurements in open wells in the tannery cluster vary from , to , and , to , mg/l, respectively. A mass transport model was constructed using Visual MODFLOW Premium . software to investigate the chloride migration in an area of . km(). Input to the chloride migration model was a groundwater flow model that considered steady and transient conditions. This model was calibrated with field observations; and sensitivity analysis was carried out whereby model parameters, viz., conductivity, dispersivity, and source concentration were altered slightly, and the effect on calibration statistics was evaluated. Results indicated that hydraulic conductivity played a more sensitive role than did dispersivity. The Cl- migration was mainly through advection rather than dispersion. It was found that even if the pollutant load reduced to % of the present level, the Cl- concentration in groundwater, even after  years, would not be reduced to the permissible limit of drinking water in the tannery belt.",,"Mondal, N. C.|Singh, V. P.",ENVIRONMENTAL MONITORING AND ASSESSMENT,shallow aquifer|tannery industry|groundwater pollution|chloride migration|southern india,10.1007/s10661-011-2156-x
441,WOS:000356741300001,2015,Multi-objective model auto-calibration and reduced parameterization: Exploiting gradient-based optimization tool for a hydrologic model,AUTOMATIC CALIBRATION|MULTICRITERIA METHODS|SENSITIVITY-ANALYSIS|GLOBAL OPTIMIZATION|SOIL-MOISTURE|SWAT|MODULE|INFORMATION|VALIDATION|ALGORITHM,"Multi-objective model optimization methods have been extensively studied based on evolutionary algorithms, but less on gradient-based algorithms. This study demonstrates a framework for multi-objective model calibration/optimization using gradient-based optimization tools. Model-independent software Parameter ESTimation (PEST) was used to auto-calibrate ISWAT, a modified version of the distributed hydrologic model Soil and Water Assessment Tool (SWAT), in the Shenandoah River watershed. The time-series processor TSPROC was used to combine multiple objectives into the auto-calibration process. Two sets of roughness coefficients for main channels, one assigned and calibrated according on soil types and one determined via empirical equations, were examined for stream discharge simulation. Five different weighting alternatives were investigated for their effects on ISWAT calibrations. Results showed that using Manning's roughness coefficients obtained from empirical equations improves simulation results and calibration efficiency. Applying a two-step weighting alternative to different observation groups would provide the best calibration results. (C) ", Elsevier Ltd. All rights reserved.,"Wang, Yan|Brubaker, Kaye",ENVIRONMENTAL MODELLING & SOFTWARE,pest|swat|tsproc|multi-objectives|auto-calibration,10.1016/j.envsoft.2015.04.001
442,WOS:000414818700006,2017,Assessment of environmental impacts and operational costs of the implementation of an innovative source-separated urine treatment,WASTE-WATER TREATMENT|LIFE-CYCLE ASSESSMENT|TREATMENT PLANTS|NUTRIENT MANAGEMENT|REMOVAL|ALTERNATIVES|SYSTEM|FOCUS,"Innovative treatment technologies and management methods are necessary to valorise the constituents of wastewater, in particular nutrients from urine (highly concentrated and can have significant impacts related to artificial fertilizer production). The FP project, ValuefromUrine, proposed a new two-step process (called VFU) based on struvite precipitation and microbial electrolysis cell (MEC) to recover ammonia, which is further transformed into ammonium sulphate. The environmental and economic impacts of its prospective implementation in the Netherlands were evaluated based on life cycle assessment (LCA) methodology and operational costs. In order to tackle the lack of stable data from the pilot plant and the complex effects on wastewater treatment plant (WWTP), process simulation was coupled with LCA and costs assessment using the Python programming language. Additionally, particular attention was given to the propagation and analysis of inputs uncertainties. Five scenarios of VFU implementation were compared to the conventional treatment of  m() of wastewater. Inventory data were obtained from SUMO software for the WWTP operation. LCA was based on Brightway software (using ecoinvent database and ReCiPe method). The results, based on  iterations sampled from inputs distributions (foreground parameters, ecoinvent background data and market prices), showed a significant advantage of VFU technology, both at a small and decentralized scale and at a large and centralized scale (% confidence intervals not including zero values). The benefits mainly concern the production of fertilizers, the decreased efforts at the WWTP, the water savings from toilets flushing, as well as the lower infrastructure volumes if the WWTP is redesigned (in case of significant reduction of nutrients load in wastewater). The modelling approach, which could be applied to other case studies, improves the representativeness and the interpretation of results (e.g. complex relationships, global sensitivity analysis) but requires additional efforts (computing and engineering knowledge, longer calculation time). Finally, the sustainability assessment should be refined in the future with the development of the technology at larger scale to update these preliminary conclusions before its commercialization. (C) ", Elsevier Ltd. All rights reserved.,"Igos, Elorri|Besson, Mathilde|Gutierrez, Tomas Navarrete|de Faria, Ana Barbara Bisinella|Benetto, Enrico|Barna, Ligia|Ahmadi, Aras|Sperandio, Mathieu",WATER RESEARCH,source-separated urine treatment|process simulation|sustainability assessment|innovative technology|integrated modelling,10.1016/j.watres.2017.09.016
443,WOS:000418736700067,2017,"A Practical, Robust Methodology for Acquiring New Observation Data Using Computationally Expensive Groundwater Models",EMPIRICAL ORTHOGONAL FUNCTIONS|BAYESIAN EXPERIMENTAL-DESIGN|REDUCED-ORDER MODEL|PREDICTIVE UNCERTAINTY|PARAMETER-IDENTIFICATION|GENETIC ALGORITHM|NETWORK DESIGN|DATA-WORTH|REDUCTION|FLOW,"Regional groundwater flow models play an important role in decision making regarding water resources; however, the uncertainty embedded in model parameters and model assumptions can significantly hinder the reliability of model predictions. One way to reduce this uncertainty is to collect new observation data from the field. However, determining where and when to obtain such data is not straightforward. There exist a number of data-worth and experimental design strategies developed for this purpose. However, these studies often ignore issues related to real-world groundwater models such as computational expense, existing observation data, high-parameter dimension, etc. In this study, we propose a methodology, based on existing methods and software, to efficiently conduct such analyses for large-scale, complex regional groundwater flow systems for which there is a wealth of available observation data. The method utilizes the well-established d-optimality criterion, and the minimax criterion for robust sampling strategies. The so-called Null-Space Monte Carlo method is used to reduce the computational burden associated with uncertainty quantification. And, a heuristic methodology, based on the concept of the greedy algorithm, is proposed for developing robust designs with subsets of the posterior parameter samples. The proposed methodology is tested on a synthetic regional groundwater model, and subsequently applied to an existing, complex, regional groundwater system in the Perth region of Western Australia. The results indicate that robust designs can be obtained efficiently, within reasonable computational resources, for making regional decisions regarding groundwater level sampling. Plain Language Summary Water supply for the public, industry, and the environment can be heavily reliant on groundwater resources. Therefore, decision makers must be able to make predictions about how a groundwater system will respond to management options. These predictions often contain a significant degree of uncertainty. This uncertainty must be reduced in order for decision makers to make optimal use of groundwater resources with minimal risk to the environment. One way to reduce this uncertainty is to obtain more information about the nature of the groundwater system by collecting new measurement data from the study site. However, it is often not clear where and when to collect this data. This study proposes a new methodology for collecting data in an optimal fashion so that the information acquired is maximized. The method incorporates any existing information, examines the characteristics of uncertainty, and alleviates the high computing costs associated with conducting the necessary calculations. The procedure is applied to a regional groundwater system in the Perth area of Western Australia. The results are consistent with the hydrogeologic conceptualization of the Perth system, and provide important insight into where new observation wells could be constructed to obtain information about the hydraulic nature of faults.",,"Siade, Adam J.|Hall, Joel|Karelse, Robert N.",WATER RESOURCES RESEARCH,groundwater modeling|uncertainty assessment|experimental design|calibration|monitoring network,10.1002/2017WR020814
444,WOS:000379138500008,2016,A probabilistic projection of the transient flow equations with random system parameters and internal boundary conditions,FREQUENCY-RESPONSE METHOD|POLYNOMIAL CHAOS|WATER-HAMMER|PIPELINES|DESIGN,"This paper presents a novel probabilistic approach based on the polynomial chaos expansion that can model the uncertainty propagation from the beginning of a waterhammer simulation and not as an afterthought. Uncertainties are considered in pipe diameter, friction coefficient, and wave speed, as well as internal boundary conditions of leaks and blockages. The polynomial chaos expansion solver results are in an excellent agreement with those calculated by using a model employing the traditional method of characteristics. The probabilistic polynomial chaos approach has the advantage of being robust and more efficient than other non-intrusive methods such as Monte Carlo simulation, which requires thousands of iterations for sharp solutions. The polynomial chaos approach is further extended to solve for randomness in frequency domain using the transfer matrix method with results of comparable accuracy. With further developments, this probabilistic approach can be integrated within existing network modelling software for practical hydraulic engineering problems.",,"Sattar, Ahmed M. A.",JOURNAL OF HYDRAULIC RESEARCH,blockages|leaks|pipelines|polynomial chaos expansion|probabilistic analysis|random variable|transient flow|waterhammer equations,10.1080/00221686.2016.1140682
445,WOS:000275585100011,2010,Development of expert system modeling based decision support system for swine manure management,LOSSES,"Animal waste has always been considered as a resource for agricultural input as biofertilizer. However, the management is becoming more stringent due to environmental regulations. Livestock producers are faced with different manure management options that may be implemented into their operations. Given the expansion of the livestock industry, the implementation of environmental regulations, and the increasing importance of social and health issues, the selection of optimal manure management systems is becoming a strategically important task. Increasingly, integrated decision support systems (DSSs) are becoming necessary to assist decision makers in their evaluation of different manure management alternatives, like, liquid system, semi-solid system, solid system and bio-gas or bio-energy system based on combinations of different manure management sub-systems (collection, storage and application). To address this situation, a user-friendly computer program called Integrated Swine Manure Management (ISMM) is being developed for the Canadian Prairie provinces. Decision criteria including environmental, agronomic, social and health, greenhouse gas emission, and economic factors have been considered for the selection. design, and operation of the DSS. The expert system modeling is based on Visual Basic programming. Decision on adopting a particular combination of systems components is based on performance rating of the overall system. The program is interactive so that weighting factors for the different decision criteria can be varied to suit site-specific considerations. In this paper, the systems approach for development of an integrated liquid manure management system is discussed. Using a case study, sensitivity analysis of different combinations of management components is also reported for systems performance. The decision software compared satisfactorily with other available DSS packages.", (C) 2010 Elsevier B.V. All rights reserved.,"Karmakar, S.|NKetia, M.|Lague, C.|Agnew, J.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,manure management|systems engineering approach|expert system|decision support system (dss)|decision criteria,10.1016/j.compag.2009.12.009
446,WOS:000371559900011,2016,Prediction of fixed-bed breakthrough curves for H2S adsorption from biogas: Importance of axial dispersion for design,,"An axially dispersed plug flow model with non-linear isotherm based on the linear driving force (LDF) approximation was used to predict the fixed-bed breakthrough curves for HS adsorption from biogas on sewage sludge thermally treated. The model was implemented and solved numerically by Comsol Multiphysics software. The predicted breakthrough curves matched very well the experimental data and were clearly better than those predictions obtained in our previous work by Aspen Adsorption assuming ideal plug flow. The comparison between the present and previous models, as well as a sensitivity analysis of the model and operational parameters, revealed that the overall mass transfer coefficient is usually underestimated when axial dispersion is neglected in a scale-up from lab scale, and hence, the importance of axial dispersion for design purposes of HS fixed-bed adsorption.", (C) 2015 Elsevier B.V. All rights reserved.,"Aguilera, P. G.|Gutierrez Ortiz, F. J.",CHEMICAL ENGINEERING JOURNAL,breakthrough curves|fixed bed column reactor|h2s adsorption|comsol modeling,10.1016/j.cej.2015.12.075
447,WOS:000304453300004,2012,Standardized uncertainty analysis for hydrometry: a review of relevant approaches and implementation examples,,"The water-centric community has continuously made efforts to identify, assess and implement rigorous uncertainty analyses for routine hydrological measurements. This paper reviews some of the most relevant efforts and subsequently demonstrates that the Guide to the expression of uncertainty in measurement (GUM) is a good candidate for estimation of uncertainty intervals for hydrometry. The demonstration is made by implementing the GUM to typical hydrometric applications and comparing the analysis results with those obtained using the Monte Carlo method. The results show that hydrological measurements would benefit from the adoption of the GUM as the working standard, because of its soundness, the availability of software for practical implementation and potential for extending the GUM to hydrological/hydraulic numerical simulations.",,"Muste, Marian|Lee, Kyutae|Bertrand-Krajewski, Jean-Luc",HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES,uncertainty analysis|hydrometric measurements|monte carlo|uncertainty estimation|sensitivity analysis,10.1080/02626667.2012.675064
448,WOS:000305299400013,2012,Adjoint sensitivity in PDE constrained least squares problems as a multiphysics problem,,"Purpose - The purpose of this paper is to provide a framework for the implementation of an adjoint sensitivity formulation for least-squares partial differential equations constrained optimization problems exploiting a multiphysics finite elements package. The estimation of the diffusion coefficient in a Poisson-type diffusion equation is used as an example. Design/methodology/approach - The authors derive the adjoint formulation in a continuous setting allowing to attribute to the direct and adjoint states the role of different fields to be solved for. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. Having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. This sensitivity can then be used to efficiently solve the least-squares problem. Findings - The authors derived the adjoint formulation in a continuous setting allowing the direct and adjoint states to be attributed the role of different fields to be solved. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. It is found that, having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. Research limitations/implications - This paper implies that modern multiphysics finite elements packages provide a flexible and extendable software environment for the experimentation with different adjoint formulations. Such tools are therefore expected to become increasingly important in solving notoriously difficult partial differential equation (PDE)-constrained least-squares problems. The framework also provides the possibility of experimentation with different regularization techniques (total variation and multiscale techniques for instance) to handle the ill-posedness of the problem. Originality/value - In this paper the adjoint sensitivity computation is casted as a multiphysics problem allowing for a flexible and extendable implementation.",,"Lahaye, Domenico|Mulckhuyse, Wouter",COMPEL-THE INTERNATIONAL JOURNAL FOR COMPUTATION AND MATHEMATICS IN ELECTRICAL AND ELECTRONIC ENGINEERING,sensitivity analysis|differential equations|computation|computer software|adjoint sensitivity|non-linear least squares problems|multiphysics finite elements software,10.1108/03321641211209780
449,WOS:000299195200036,2011,Full scale 3D-modelling of the coupled gas migration and heat dissipation in a planned repository for radioactive waste in the Callovo-Oxfordian clay,HYDRAULIC CONDUCTIVITY,"An important question related to the long-term safety performance of a repository for long-lived medium and high-level radioactive waste in the Callovo-Oxfordian clay unit is the impact of heat and gas generated in the waste emplacement areas on the gas and water pressure and on the water saturation in the backfilled repository and in the host rock. The current design of such a repository consists of a multitude of different underground structures, such as emplacement drifts for waste canisters and other types of waste packages, access and ventilation drifts, and access shafts in the central part of the repository. The individual underground structures exhibit different thermo-hydraulic and geometrical properties yielding a large and complex system for the flow and transport of gas, water and heat. A detailed D modelling of the entire repository would require a tremendous computational effort, even when using high performance simulator codes. A newly developed method (Poller et al., ) allows for the D modelling of the two-phase gas-water flow and thermal evolution in the entire repository/host-rock system in a simplified manner. Besides accounting for both the detailed structures at local scale and the global geometry of the drift network, it also allows for an assessment of the gas phase pressure as well as the hydrogen and heat fluxes developing over the complete lifetime of the repository system. In this paper, the results of a reference scenario are presented. The assessment focuses on the two dominant processes, i.e. the dissolution and diffusion of the generated hydrogen, and the advective migration of the forming hydrogen gas phase in space and time (up to  million years). Further, the main findings of a sensitivity analysis on different features, physical processes and parameter uncertainty are presented. (C) ", Elsevier Ltd. All rights reserved.,"Enssle, Carl Philipp|Croise, Jean|Puller, Andreas|Mayer, Gerhard|Wendling, Jacques",PHYSICS AND CHEMISTRY OF THE EARTH,radioactive waste repository|clay|hydrogen and heat transport|two-phase flow|tough2-mp|numerical simulation,10.1016/j.pce.2011.07.033
450,WOS:000375818700005,2016,River-to-sea pressure retarded osmosis: Resource utilization in a full-scale facility,POWER-GENERATION|SALINITY GRADIENTS|REVERSE ELECTRODIALYSIS|OSMOTIC POWER|SEAWATER DESALINATION|ENERGY EFFICIENCY|RO-PRO|WATER|SYSTEM|PERFORMANCE,"Pressure retarded osmosis (PRO) is a technology that could be utilized to recover energy from the mixing of freshwater with seawater. This source of renewable energy is sizeable and in the past decade several investigations analyzed its potential. The vast majority of studies focused on mass transfer problems across the membrane in order to improve membrane productivity and just recently studies started to look at membrane module efficiencies and parasitic loads within the PRO facility. In this article, the net specific energy production from a facility-scale PRO system was determined and optimized by using a novel simulation method that integrates parasitic loads and efficiencies of the PRO facility components and combines the model with an optimization software in a linked system optimization scheme. It was found that the overall net specific energy that may be recovered by a river-to-sea PRO facility is approximately . kWh per m() of permeate. Furthermore, a sensitivity analysis was performed to elucidate the relationship between net specific energy and power density as functions of membrane area, flow rates, and operating pressures. In general, in order to maximize resource recovery, a low power density, thus a low membrane productivity, must be accepted.", (C) 2016 Elsevier B.V. All rights reserved.,"O'Toole, Galen|Jones, Lori|Coutinho, Chris|Hayes, Corey|Napoles, Monica|Achilli, Andrea",DESALINATION,renewable energy|salinity gradient power|pressure retarded osmosis|net specific energy|power density|facility analysis,10.1016/j.desal.2016.01.012
451,WOS:000407603800012,2017,Sensitivity analysis of DEM prediction for sliding wear by single iron ore particle,DISCRETE ELEMENT METHOD|BALL MILLS|SIMULATION|MODEL|PERFORMANCE|CONTACT|MECHANISMS|MOTION|STEEL|LIFE,"Purpose - Sliding wear is a common phenomenon in the iron ore handling industry. Large-scale handling of iron ore bulk-solids causes a high amount of volume loss from the surfaces of bulk-solids-handling equipment. Predicting the sliding wear volume from equipment surfaces is beneficial for efficient maintenance of worn equipment. Recently, the discrete element method (DEM) simulations have been utilised to predict the wear by bulk-solids. However, the sensitivity of wear prediction subjected to DEM parameters has not been systemically investigated at single particle level. To ensure the wear predictions by DEM are accurate and stable, this study aims to conduct the sensitivity analysis at the single particle level. Design/methodology/approach - In this research, pin-on-disc wear tests are modelled to predict the sliding wear by individual iron ore particles. The Hertz-Mindlin ( no slip) contact model is implemented to simulate interactions between particle ( pin) and geometry ( disc). To quantify the wear from geometry surface, a sliding wear equation derived from Archard's wear model is adopted in the DEM simulations. The accuracy of the pin-on-disc wear test simulation is assessed by comparing the predicted wear volume with that of the theoretical calculation. The stability is evaluated by repetitive tests of a reference case. At the steady-state wear, the sensitivity analysis is done by predicting sliding wear volumes using the parameter values determined by iron ore-handling conditions. This research is carried out using the software EDEM (R) ... Findings - Numerical errors occur when a particle passes a joint side of geometry meshes. However, this influence is negligible compared to total wear volume of a wear revolution. A reference case study demonstrates that accurate and stable results of sliding wear volume can be achieved. For the sliding wear at steady state, increasing particle density or radius causes more wear, whereas, by contrast, particle Poisson's ratio, particle shear modulus, geometry mesh size, rotating speed, coefficient of restitution and time step have no impact on wear volume. As expected, increasing indentation force results in a proportional increase. For maintaining wear characteristic and reducing simulation time, the geometry mesh size is recommended. To further reduce simulation time, it is inappropriate using lower particle shear modulus. However, the maximum time step can be increased to % T-R without compromising simulation accuracy. Research limitations/implications - The applied coefficient of sliding wear is determined based on theoretical and experimental studies of a spherical head of iron ore particle. To predict realistic volume loss in the iron ore-handling industry, this coefficient should be experimentally determined by taking into account the non-spherical shapes of iron ore particles. Practical implications - The effects of DEM parameters on sliding wear are revealed, enabling the selections of adequate values to predict sliding wear in the iron ore-handling industry. Originality/value - The accuracy and stability to predict sliding wear by using EDEM (R) .. are verified. Besides, this research accelerates the calibration of sliding wear prediction by DEM.",,"Chen, Guangming|Schott, Dingena L.|Lodewijks, Gabriel",ENGINEERING COMPUTATIONS,discrete element method|pin-on-disc|bulk-solids-handling|wear prediction,10.1108/EC-07-2016-0265
452,WOS:000302118200009,2012,GIS-based applications of sensitivity analysis for sewer models,URBAN DRAINAGE SYSTEMS|CALIBRATION|PERFORMANCE|INFRASTRUCTURE|IDENTIFICATION|UNCERTAINTIES|VULNERABILITY|IMPACT,"Sensitivity analysis (SA) evaluates the impact of changes in model parameters on model predictions. Such an analysis is commonly used when developing or applying environmental models to improve the understanding of underlying system behaviours and the impact and interactions of model parameters. The novelty of this paper is a geo-referenced visualization of sensitivity indices for model parameters in a combined sewer model using geographic information system (GIS) software. The result is a collection of maps for each analysis, where sensitivity indices (calculated for model parameters of interest) are illustrated according to a predefined symbology. In this paper, four types of maps (an uncertainty map, calibration map, vulnerability map, and design map) are created for an example case study. This article highlights the advantages and limitations of GIS-based SA of sewer models. The conclusion shows that for all analyzed applications, GIS-based SA is useful for analyzing, discussing and interpreting the model parameter sensitivity and its spatial dimension. The method can lead to a comprehensive view of the sewer system.",,"Mair, M.|Sitzenfrei, R.|Kleidorfer, M.|Moederl, M.|Rauch, W.",WATER SCIENCE AND TECHNOLOGY,capacity design|combined sewer system|gis applications|model calibration|uncertainty assessment|vulnerability assessment,10.2166/wst.2012.954
453,WOS:000245438200012,2007,The boundary-quality penalty: a quantitative method for approximating species responses to fragmentation in reserve selection,UNCERTAINTY ANALYSIS|SITE SELECTION|DESIGN|NETWORKS|BIODIVERSITY|LANDSCAPE|MODELS|CONSERVATION|PERSISTENCE|PROBABILITIES,"Aggregation of reserve networks is generally considered desirable for biological and economic reasons: aggregation reduces negative edge effects and facilitates metapopulation dynamics, which plausibly leads to improved persistence of species. Economically, aggregated networks are less expensive to manage than fragmented ones. Therefore, many reserve-design methods use qualitative heuristics, such as distance-based criteria or boundary-length penalties to induce reserve aggregation. We devised a quantitative method that introduces aggregation into reserve networks. We call the method the boundary-quality penalty (BQP) because the biological value of a land unit (grid cell) is penalized when the unit occurs close enough to the edge of a reserve such that a fragmentation or edge effect would reduce population densities in the reserved cell. The BQP can be estimated for any habitat model that includes neighborhood (connectivity) effects, and it can be introduced into reserve selection software in a standardized manner We used the BQP in a reserve-design case study of the Hunter Valley of southeastern Australia. The BQP resulted in a more highly aggregated reserve network structure. The degree of aggregation required was specified by observed (albeit modeled) biological responses to fragmentation. Estimating the effects of fragmentation on individual species and incorporating estimated effects in the objective function of reserve-selection algorithms is a coherent and defensible way to select aggregated reserves. We implemented the BQP in the context of the Zonation method, but it could as well be implemented into any other spatially explicit reserve-planning framework.",,"Moilanen, Atte|Wintle, Brendan A.",CONSERVATION BIOLOGY,boundary length|connectivity|edge effect|fragmentation|habitat model|model averaging|reserve selection|site-selection algorithm|zonation,10.1111/j.1523-1739.2006.00625.x
454,WOS:000186310600007,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: II - Numerical validation and applications,VARIATIONAL DATA ASSIMILATION|CHEMISTRY DATA ASSIMILATION|AIR-QUALITY MODEL|OZONE|IMPLEMENTATION|CODE,"The Kinetic PreProcessor KPP was extended to generate the building blocks needed for the direct and adjoint sensitivity analysis of chemical kinetic systems. An overview of the theoretical aspects of sensitivity calculations and a discussion of the KPP software tools is presented in the companion paper. In this work the correctness and efficiency of the KPP generated code for direct and adjoint sensitivity studies are analyzed through an extensive set of numerical experiments. Direct-decoupled Rosenbrock methods are shown to be cost-effective for providing sensitivities at low and medium accuracies. A validation of the discrete-adjoint evaluated gradients is performed against the finite difference estimates. The accuracy of the adjoint gradients is measured using a reference gradient value obtained with a standard direct-decoupled method. The accuracy is studied for both constant step size and variable step size integration of the forward/adjoint model and the consistency between the discrete and continuous adjoint models is analyzed. Applications of the KPP-. software package to direct and adjoint sensitivity studies, variational data assimilation, and parameter identification are considered for the comprehensive chemical mechanism SAPRC-. (C) ", Elsevier Ltd. All rights reserved.,"Daescu, DN|Sandu, A|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,sensitivity analysis|data assimilation|parameter identification|optimization,10.1016/j.atmosenv.2003.08.020
455,WOS:000312654600026,2013,SPSens: a software package for stochastic parameter sensitivity analysis of biochemical reaction networks,COUPLED CHEMICAL-REACTIONS|SYSTEMS,"SPSens is a software package for the efficient computation of stochastic parameter sensitivities of biochemical reaction networks. Parameter sensitivity analysis is a valuable tool that can be used to study robustness properties, for drug targeting, and many other purposes. However its application to stochastic models has been limited when Monte Carlo methods are required due to extremely high computational costs. SPSens provides efficient, state of the art sensitivity analysis algorithms in a single software package so that sensitivity analysis can be easily performed on stochastic models of biochemical reaction networks. SPSens implements the algorithms in C and estimates sensitivities with respect to both infinitesimal and finite perturbations to system parameters, in many cases reducing variance by orders of magnitude compared to basic methods. Included among the features of SPSens are serial and parallel command line versions, an interface with Matlab, and several example problems.",,"Sheppard, Patrick W.|Rathinam, Muruhan|Khammash, Mustafa",BIOINFORMATICS,,10.1093/bioinformatics/bts642
456,WOS:000412192800042,2017,Dynamic optimization of beer fermentation: Sensitivity analysis of attainable performance vs. product flavour constraints,VERTICAL ELECTRICAL FURNACE|PERLITE GRAIN EXPANSION|BATCH DISTILLATION|OPTIMAL OPERATION|FORMULATION|SIMULATION|MODEL,"The declining alcohol industry in the UK and the concurrent surge in supply and variety of beer products has created extremely competitive environment for breweries, many of which are pursuing the benefits of process intensification and optimization. To gain insight into the brewing process, an investigation into the influence of by-product threshold levels on obtainable fermentation performance has been performed, by computing optimal operating temperature profiles for a range of constraint levels on by-product concentrations in the final product. The DynOpt software package has been used, converting the continuous control vector optimization problem into nonlinear programming (NLP) form via collocation on finite elements, which has then been solved with an interior point algorithm. This has been performed for increasing levels of time discretization, by means of a range of initializing solution profiles, for a wide spectrum of imposed by-product flavour constraints. Each by-product flavour threshold affects process performance in a unique way. Results indicate that the maximum allowable diacetyl concentration in the final product has very strong influence on batch duration, with lower limits requiring considerably longer batches. The maximum allowable ethyl acetate concentration is shown to dictate the attainable ethanol concentration, and lower limits adversely affect the desired high alcohol content in the final product. (C) ", Elsevier Ltd. All rights reserved.,"Rodman, Alistair D.|Gerogiorgis, Dimitrios I.",COMPUTERS & CHEMICAL ENGINEERING,beer fermentation|dynamic optimization|multi-objective optimization|orthogonal collocation on finite elements|sensitivity analysis|flavour constraints,10.1016/j.compchemeng.2017.06.024
457,WOS:000386579700009,2016,Vehicle lightweighting through the use of molybdenum-bearing advanced high-strength steels (AHSS),,"The past two decades have seen growing pressure on vehicle manufacturers to reduce the environmental impact of their vehicles. One effective way to improve fuel efficiency and lower tailpipe emissions is to use advanced high-strength steels (AHSS) that offer equal strength and crash resistance at lower mass. The present study assesses the life cycle environmental impacts of two steel grades considered for the B-pillar in the Ford Fusion: A press-hardened boron steel design as used in the previous model of the vehicle and a hydroformed component made from a mix of the molybdenum-bearing dual phase steels DP and DP. Information related to the component masses and grades was provided by Ford. Process models for the steelmaking process, finishing, forming, vehicle use and end of life were created in the GaBi LCA software tool. Sensitivity analyses were conducted on the impact of the hydroforming process for the new component, for which only proxy data were available and on the mix of DP and DP in the B-pillar. Results have been presented for the environmental impact categories deemed most relevant to vehicle use. The life cycle assessment showed that the new DP/DP B-pillar design has a lower impact for the environmental impact categories assessed. Overall, the global warming potential (GWP) of the new DP/DP design was  % lower than the boron steel design over the full life cycle of the vehicle. The use phase was found to be the major source of environmental impacts, accounting for  % of the life cycle GWP impact. The  kg weight saving accounts for the majority of the difference in impacts between the two B-pillar designs. Impacts from manufacturing were also lower for the new design for all of the impact categories assessed despite the higher alloy content of the steel. A sensitivity analysis of the hydroforming process showed that even if impacts from forming were  % greater than for press hardening, the GWP from production of the new B-pillar design would still be lower than the boron steel version. The molybdenum-bearing DP/DP B-pillar was found to have lower life cycle and production impacts than the previous boron steel design. The assessment indicates that significant improvements in the environmental impacts associated with the body structure of vehicles could be made through the increased use of AHSS in vehicles without compromising crash performance.",,"Hardwick, Alexander Patrick|Outteridge, Tim",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,automotive|high-strength steel|lightweighting|molybdenum,10.1007/s11367-015-0967-7
458,WOS:000253118500008,2008,"Borehole Optimisation System (BOS) - A case study assessing options for abstraction of urban groundwater in Nottingham, UK",TRIASSIC SANDSTONE AQUIFER|BIODEGRADATION|CONTAMINATION|BIOTRANSFORMATION|TRANSFORMATIONS|TRICHLOROETHENE|HYDROCARBONS|ATTENUATION|RECHARGE|MTBE,"The recognition that urban groundwater is a potentially valuable resource for potable and industrial uses due to growing pressures on perceived less polluted rural groundwater has led to a requirement to assess the groundwater contamination risk in urban areas from industrial contaminants such as chlorinated solvents. The development of a probabilistic risk based management tool that predicts groundwater quality at potential new urban boreholes is beneficial in determining the best sites for future resource development. The Borehole Optimisation System (BOS) is a custom Geographic Information System (GIs) application that has been developed with the objective of identifying the optimum locations for new abstraction boreholes. BOS can be applied to any aquifer subject to variable contamination risk. The system is described in more detail by Tait et al. [Tait, N.G., Davison, J.J., Whittaker, J.J., Lehame, S.A. Lerner, D.N., a. Borehole Optimisation System (BOS) - a GIs based risk analysis tool for optimising the use of urban groundwater. Environmental Modelling and Software , -]. This paper applies the BOS model to an urban Permo-Triassic Sandstone aquifer in the city centre of Nottingham, UK. The risk of pollution in potential new boreholes from the industrial chlorinated solvent tetrachloroethene (PCE) was assessed for this region. The risk model was validated against contaminant concentrations from  actual field boreholes within the study area. In these studies the model generally underestimated contaminant concentrations. A sensitivity analysis showed that the most responsive model parameters were recharge, effective porosity and contaminant degradation rate. Multiple simulations were undertaken across the study area in order to create surface maps indicating areas of low PCE concentrations, thus indicating the best locations to place new boreholes. Results indicate that northeastern, eastern and central regions have the lowest potential PCE concentrations in abstraction groundwater and therefore are the best sites for locating new boreholes. These locations coincide with aquifer areas that are confined by low permeability Mercia Mudstone deposits. Conversely southern and northwestern areas are unconfined and have shallower depth to groundwater. These areas have the highest potential PCE concentrations. These studies demonstrate the applicability of BOS as a tool for informing decision makers on the development of urban groundwater resources. (c) ", Elsevier Ltd. All rights reserved.,"Tait, N. G.|Davison, R. M.|Leharne, S. A.|Lerner, D. N.",ENVIRONMENTAL MODELLING & SOFTWARE,borehole optimisation system|gis|pce|probabilistic risk modelling|urban groundwater,10.1016/j.envsoft.2007.09.001
459,WOS:000275555800009,2010,Benchmarking multidisciplinary design optimization algorithms,COLLABORATIVE OPTIMIZATION,"A comparison of algorithms for multidisciplinary design optimization (MDO) is performed with the aid of a new software framework. This framework, pyMDO, was developed in Python and is shown to be an excellent platform for comparing the performance of the various MDO methods. pyMDO eliminates the need for reformulation when solving a given problem using different MDO methods: once a problem has been described, it can automatically be cast into any method. In addition, the modular design of pyMDO allows rapid development and benchmarking of new methods. Results generated from this study provide a strong foundation for identifying the performance trends of various methods with several types of problems.",,"Tedford, Nathan P.|Martins, Joaquim R. R. A.",OPTIMIZATION AND ENGINEERING,multidisciplinary design optimization|decomposition algorithms|nonlinear programming|sensitivity analysis,10.1007/s11081-009-9082-6
460,WOS:000322557200002,2013,Comparison of sediment transport computations using hydrodynamic versus hydrologic models in the Simiyu River in Tanzania,BED-LOAD TRANSPORT|SENSITIVITY-ANALYSIS|PARAMETERS|SWAT|TOOL,"This paper presents the results of a study that compares the sediment routing of the Simiyu River using the hydrologic model, Soil and Water Assessment Tool (SWAT) and the D hydrodynamic simulation software for Rivers and Estuaries (SOBEK-RE) model. Routing in SWAT is completed using the simplified Bagnold's equation and in the SOBEK-RE model is undertaken using the Saint Venant equation. The upstream boundary conditions for the routing modules were derived from the subcatchments sediment yields that were estimated by SWAT using the Modified Universal Soil Loss Equation (MUSLE). The sediment loads extrapolated or interpolated from the sediment rating curve for the catchment outlet were used for calibration and validation purposes. The SWAT model predicted an erosion rate of . Mt/yr. The total sediment load transported to the main outlet of the catchment simulated by the SWAT and SOBEK-RE models was equal to . and . Mt/yr, respectively. Thus the models computed a net erosion in the channels of . Mt/yr (SWAT) and . Mt/yr (SOBEK-RE). When comparing the results of the models for the different reaches of the main channel and main tributaries, the models showed different results both in magnitude and in sign (erosion/deposition). However, in a situation where data is scarce (such as grain size, channel geometry), the more complex hydrodynamic model does not necessarily lead to more reliable results. (c) ", Elsevier Ltd. All rights reserved.,"van Griensven, Ann|Popescu, Loana|Abdelhamid, M. R.|Ndomba, Preksedis Marco|Beevers, Lindsay|Betrie, Getnet D.",PHYSICS AND CHEMISTRY OF THE EARTH,sediment routing|sediment transport|swat|sober-re|simiyu river basin,10.1016/j.pce.2013.02.003
461,WOS:000362848700009,2015,A Multi-Attribute Decision Analysis for Decommissioning Offshore Oil and Gas Platforms,UTILITY MEASUREMENT,"The  oil and gas platforms off the coast of southern California are reaching the end of their economic lives. Because their decommissioning involves large costs and potential environmental impacts, this became an issue of public controversy. As part of a larger policy analysis conducted for the State of California, we implemented a decision analysis as a software tool (PLATFORM) to clarify and evaluate decision strategies against a comprehensive set of objectives. Key options selected for in-depth analysis are complete platform removal and partial removal to  feet below the water line, with the remaining structure converted in place to an artificial reef to preserve the rich ecosystems supported by the platform's support structure. PLATFORM was instrumental in structuring and performing key analyses of the impacts of each option (e.g., on costs, fishery production, air emissions) and dramatically improved the team's productivity. Sensitivity analysis found that disagreement about preferences, especially about the relative importance of strict compliance with lease agreements, has much greater effects on the preferred option than does uncertainty about specific outcomes, such as decommissioning costs. It found a near-consensus of stakeholders in support of partial removal and ""rigs-to-reefs"" program. The project's results played a role in the decision to pass legislation enabling an expanded California ""rigs-to-reefs"" program that includes a mechanism for sharing cost savings between operators and the state.", (C) 2015 SETAC,"Henrion, Max|Bernstein, Brock|Swamy, Surya",INTEGRATED ENVIRONMENTAL ASSESSMENT AND MANAGEMENT,decision analysis|decommissioning|multi-attribute utility|oil and gas platforms|rigs-to-reefs,10.1002/ieam.1693
462,WOS:000303082000011,2012,An environmental and economic analysis for geotube coastal structures retaining dredge material,EROSION,"This paper investigates the environmental and economic sensitivity of coastal structures for two different construction methods: a traditional rubble mound structure and a geotube coastal structure using dredged material. The analysis is undertaken for two projects: a small scale coastal protection project using a revetment and a medium size capital harbour expansion using a breakwater. This work provides further insight into previously published work by Sheehan et al. () on the economic aspects of geotube technology and identifies the optimum method of construction for each type of coastal structure. An economic sensitivity analysis is undertaken on the key logistical parameters involved in the construction of these coastal structures. An environmental sensitivity analysis focuses on the CO emissions produced from the construction of the coastal structures for both construction methods. These sensitivity analyses are undertaken using a decision support software program (DMMAP), developed to assist users at the planning stages of a project to achieve sustainable dredge material management. The key logistical parameters are analysed to generate environmental and economic ranking tables. The analyses highlight that the size of the structure and the distance to the source of the quarry material are crucial factors in determining the optimum construction method. This work shows that geotubes are a viable alternative to traditional rubble mound coastal structures. It also shows that traditional construction methods may be more economical than geotube structures when considering small coastal structures. In general, the larger the scale of the project the greater the potential savings in CO emissions and cost that can be achieved through the use of geotube technology. Geotubes, with the use of dredge material, may provide a sustainable beneficial use for dredge material and offer a serious economic and environmental alternative to traditional rubble mound structures.", (C) 2012 Elsevier B.V. All rights reserved.,"Sheehan, C.|Harrington, J.",RESOURCES CONSERVATION AND RECYCLING,geotube|revetment|breakwater|coastal structures|dredging|beneficial use,10.1016/j.resconrec.2012.01.011
463,WOS:000313165600011,2013,Life cycle assessment of a waste lubricant oil management system,LCA,"This paper compares  waste lubricant oil (WLO) systems ( management alternatives and a system in use in Portugal) using a life cycle assessment (LCA). The alternatives tested use various mild processing techniques and recovery options: recycling during expanded clay production, recycling and electric energy production, re-refining, energy recovery during cement production, and energy recovery during expanded clay production. The proposed  alternatives and the actual present day situation were analyzed using LCA software UMBERTO ., applied to eight environmental impact categories. The LCA included an expansion system to accommodate co-products. The results show that mild processing with low liquid gas fuel consumption and re-refining is the best option to manage WLO with regard to abiotic depletion, eutrophication, global warming, and human toxicity environmental impacts. A further environmental option is to treat the WLO using the same mild processing technique, but then send it to expanded clay recycling to be used as a fuel in expanded clay production, as this is the best option regarding freshwater sedimental ecotoxicity, freshwater aquatic ecotoxicity, and acidification. It is recommended that there is a shift away from recycling and electric energy production. Although sensitivity analysis shows re-refining and energy recovery in expanded clay production are sensitive to unit location and substituted products emission factors, the LCA analysis as a whole shows that both options are good recovery options; re-refining is the preferable option because it is closer to the New Waste Framework Directive waste hierarchy principle.",,"Pires, Ana|Martinho, Graca",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,energy recovery|life cycle assessment|re-refining|waste lubricant oils,10.1007/s11367-012-0455-2
464,WOS:000348201000009,2015,Sensitivity analyses and simulations of a full-scale experimental membrane bioreactor system using the activated sludge model No. 3 (ASM3),RETENTION TIME|OPERATION|WATER,"An ASM-based model was implemented in the numerical software MATHEMATICA where sensitivity analyses and simulations of a membrane bioreactor (MBR) system were carried out. These results were compared with those obtained using the commercial simulator WEST. Predicted values did not show significant variations between both software and simulations showed that the most influential operational conditions were influent flow rate and concentrations and bioreactor volumes. On the other hand, sensitivity analyses were carried out with both software programs for the same five outputs: COD, ammonium and nitrate concentrations in the effluent, total suspended solids concentration and oxygen uptake rate in the aerobic bioreactor. Similar results were in general obtained in both cases and according to these analyses, the most significant inputs over the model predictions were growth and storage heterotrophic biomass yields and decay coefficient. Other parameters related to the hydrolysis process or to the autotrophic biomass also significantly influenced model outputs.",,"Ruiz, L. M.|Rodelas, P.|Perez, J. I.|Gomez, M. A.",JOURNAL OF ENVIRONMENTAL SCIENCE AND HEALTH PART A-TOXIC/HAZARDOUS SUBSTANCES & ENVIRONMENTAL ENGINEERING,simulation|modeling|sensitivity analysis|mbr|asm3,10.1080/10934529.2015.981122
465,WOS:000269966500005,2009,Thermodynamics of Autothermal Wood Gasification,BIOMASS|PERFORMANCE|CHAR,"This work extensively studies the thermodynamics of air blown autothermal wood gasification at adiabatic conditions. To this end, the software package HSC Chemistry (R) was used to determine the composition of the synthesis gas at thermodynamic equilibrium. This software operates on the basis of Gibb's energy minimization. In the model, dry and ash-free wood has been represented by CH.O-(.). Dry air has been modeled as a mixture of oxygen, nitrogen and argon. As the calculations were carried out with respect to adiabatic conditions, it was necessary to determine the adiabatic flame temperature via beat and mass balances. This approach of adopting adiabatic conditions, though generally not taken into consideration in thermodynamic studies, is seen as beneficial, providing additional information with regard to the gasifier operating point. A sensitivity analysis was conducted. The influence of parameters; like equivalence ratio, water content of wood fuel and air preheating on adiabatic flame temperature and cold gas efficiency is discussed. For air at ambient temperature, the highest cold gas efficiency is achieved with an equivalence ratio of about .. This was dependent to some small degree on the water content of the fuel wood. With increased air preheating, the maximum cold gas efficiency is increased and shifted to lower equivalence ratios. For wet wood, it transpired to be more efficient to use the sensible beat from the synthesis gas for drying of the fuel rather than preheating of the air. Finally, calculated values are compared to measurements from a circulating fluidised bed gasifier."," (C) 2009 American institute of Chemical Engineers Environ Prog, 28: 347-354, 2009","Mevissen, Nora|Schulzke, Tim|Unger, Christoph A.|Mac an Bhaird, Sean",ENVIRONMENTAL PROGRESS & SUSTAINABLE ENERGY,thermodynamic equilibrium|simulation|solid carbon boundary|cold gas efficiency,10.1002/ep.10393
466,WOS:000292726900029,2011,Development of a screening method to assess flood risk on Danish national roads and highway systems,,"A method to assess flood risk on Danish national roads in a large area in the middle and southern part of Jutland, Denmark, was developed for the Danish Road Directorate. Flood risk has gained renewed focus due to the climate changes in recent years and extreme rain events are expected to become more frequent in the future. The assessment was primarily based on a digital terrain model (DTM) covering , km() in a . x . m grid. The high-resolution terrain model was chosen in order to get an accurate estimation of the potential flooding in the road area and in the immediate vicinity, but also put a high requirement on the methods, hardware and software applied. The outcome of the analysis was detailed maps (as GIS layers) illustrating the location of depressions with depths, surface area and volume data for each depression. Furthermore, preferential flow paths, catchment boundaries and ranking of each depression were calculated. The ranking was based on volume of depressions compared with upstream catchment and a sensitivity analysis of the runoff coefficient. Finally, a method for assessing flood risk at a more advanced level (hydrodynamic simulation of surface and drainage) was developed and used on a specific blue spot as an example. The case study shows that upstream catchment, depressions, drainage system, and use of hydrodynamic calculations have a great influence on the result. Upstream catchments can contribute greatly to the flooding.",,"Nielsen, N. H.|Larsen, M. R. A.|Rasmussen, S. F.",WATER SCIENCE AND TECHNOLOGY,flood risk maps|predicting flood risk of highway systems|high-resolution dtm|gis analysis|sensitivity analysis|climate change|screening method|decision support tool,10.2166/wst.2011.157
467,WOS:000318057900004,2013,A long-term sensitivity analysis of the denitrification and decomposition model,NITROUS-OXIDE EMISSIONS|DRAINED ILLINOIS AGROECOSYSTEMS|GREENHOUSE-GAS EMISSIONS|SOIL ORGANIC-CARBON|DNDC MODEL|AGRICULTURAL SOILS|N2O EMISSIONS|COMPUTATIONAL EXPERIMENTS|MECHANISTIC MODEL|RAINFALL EVENTS,"Although sensitivity analysis (SA) was conducted on the DeNitrification-DeComposition (DNDC) model, a global SA over a long period of time is lacking. We used a method of Bayesian analysis of computer code outputs (BACCO) with the Gaussian emulation machine for sensitivity analysis software (GEM-SA) to conduct a long-term SA of DNDC for predicting the annual change of soil organic carbon (dSOC), nitrous oxide emission (NO) and grain yield of spring wheat. Twenty seven non-weather input parameters with wide ranges were selected for SA using weather data recorded from Three Hills, Alberta over  years (-). The SA had two steps: ) a preliminary BACCO GEM-SA was conducted to identify a more accurate emulator sampling method and to screen out parameters with insignificant influence on model outcomes; and ) final BACCO GEM-SA was conducted with optimal input design set for emulator training runs varying only the significant input parameters. Results indicated that the Maximin Latin Hypercube sampling method outperformed the LP-x method with higher emulator accuracy. Most of the  input parameters contributed little to the three outputs by the first step BACCO GEM-SA. In the second step of BACCO GEM-SA there were only three (in the case of dSOC) and six (in the cases of NO and yield) input parameters whose influence contributed to more than % of the total output variances by their total effects. Among the selected parameters, initial soil organic carbon and clay content are very important and were important in determining results for all three outputs. Sensitivities of some parameters, such as clay content and urea fertilizer amount changed dramatically over the years. This indicates that a single year SA may overestimate or underestimate a long-term parameter effect on the model prediction. The two-step procedure with the BACCO GEM-SA method improved the accuracy of SA and provided important information for model validation and parameterization.", Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.,"Qin, Xiaobo|Wang, Hong|Li, Yu'e|Li, Yong|McConkey, Brian|Lemke, Reynald|Li, Changsheng|Brandt, Kelsey|Gao, Qingzhu|Wan, Yunfan|Liu, Shuo|Liu, Yuntong|Xu, Chao",ENVIRONMENTAL MODELLING & SOFTWARE,dndc|long-term|global sensitivity analysis|bacco gem-sa,10.1016/j.envsoft.2013.01.005
