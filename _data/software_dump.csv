,id,year,title,keywords,abstract,copyright,AU,SO,DE,DOI
0,WOS:000367774700005,2015,Chaospy: An open source tool for designing methods of uncertainty quantification,,"The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables. (C)  The Authors.", Published by Elsevier B.V.,"Feinberg, Jonathan|Langtangen, Hans Petter",JOURNAL OF COMPUTATIONAL SCIENCE,uncertainty quantification|polynomial chaos expansions|monte carlo simulation|rosenblatt transformations|python package,10.1016/j.jocs.2015.08.008
1,WOS:000374807600014,2016,Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework,EQUIFINALITY,"Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification. This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA. (C) ", Elsevier Ltd. All rights reserved.,"Peckham, Scott D.|Kelbert, Anna|Hill, Mary C.|Hutton, Eric W. H.",COMPUTERS & GEOSCIENCES,model uncertainty|modeling frameworks|component-based modeling|optimization|inverse problems|nonlinear least squares|parameter estimation|longitudinal river elevation profiles,10.1016/j.cageo.2016.03.005
2,WOS:000240794000013,2006,Sensitivity analysis of differential-algebraic equations and partial differential equations,ADAPTIVE MESH REFINEMENT|SYSTEMS|SOFTWARE|OPTIMIZATION|ALGORITHMS,"Sensitivity analysis generates essential information for model development, design optimization, parameter estimation, optimal control, model reduction and experimental design. In this paper we describe the forward and adjoint methods for sensitivity analysis, and outline some of our recent work on theory, algorithms and software for sensitivity analysis of differential-algebraic equation (DAE) and time-dependent partial differential equation (PDE) systems. (c) ", Elsevier Ltd. All rights reserved.,"Petzold, Linda|Li, Shengtai|Cao, Yang|Serban, Radu",COMPUTERS & CHEMICAL ENGINEERING,sensitivity analysis|differential-algebraic equations|adjoint method,10.1016/j.compchemeng.2006.05.015
3,WOS:000332292700001,2014,Multiobjective Combinatorial Auctions in Transportation Procurement,OF-THE-ART,"This paper presents a multiobjective winner determination combinatorial auction mechanism for transportation carriers to present multiple transport lanes and bundle the lanes as packet bids to the shippers for the purposes of ocean freight. This then allows the carriers to maximize their network of resources and pass some of the cost savings onto the shipper. Specifically, we formulate three multi-objective optimization models (weighted objective model, preemptive goal programming, and compromise programming) under three criteria of cost, marketplace fairness, and the marketplace confidence in determining the winning packages. We develop solutions on the three models and perform a sensitivity analysis to show the options the shipper can use depending on the existing conditions at the point of awarding the transport lanes.",,"Ignatius, Joshua|Hosseini-Motlagh, Seyyed-Mahdi|Goh, Mark|Sepehri, Mohammad Mehdi|Mustafa, Adli|Rahman, Amirah",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/951783
4,WOS:000230722000011,2005,"Assessing the potential of thermal infrared satellite surveys for monitoring seismically active areas: The case of Kocaell (Izmit) earthquake, August 17, 1999",AFTERSHOCK DISTRIBUTION|IMPENDING EARTHQUAKES|SENSITIVITY-ANALYSIS|SURFACE-TEMPERATURE|AUTOMATED DETECTION|TURKEY|GREECE|FAULT|PREDICTION|PRECURSOR,"Space-time anomalies of Earth's emitted radiation in the thermal infrared spectral range (TIR) measured from satellite months to weeks before the occurrence of earthquakes, have been interpreted, by several authors, as pre-seismic signals. The claimed connection of TIR emission with seismic activity has been considered, up to now, with some caution by the scientific community mainly for the insufficiency of the validation data-sets and the scarce importance attached by those authors to other causes (e.g. meteorological) that, rather than seismic activity, could be responsible for the observed TIR signal fluctuations. In this paper, a robust satellite data analysis technique is described which pen-nits us to identify anomalous space-time TIR signal transients even in very variable observational (satellite view angle, land topography and coverage, etc.) and natural (e.g. meteorological) conditions. A statistically well-founded definition of TIR anomaly is given and proposed as a suitable tool for satellite TIR surveys in seismically active regions. Eight years of Meteosat TIR observations have been analyzed in order to characterize the TIR signal behavior at each specific observation time and location. Space-time TIR signal transients have then been analyzed, both in the presence (validation) and in the absence of (confutation) seismic events, looking for possible space-time relationships. The devastating earthquake which occurred in Turkey (Izmit.. August , M(S)similar to .) in  has been considered as a test case for validation, relatively unperturbed periods (no earthquakes with M > ) were taken for confutation purposes. Quite intense (S/N > .) and rare, spatially extensive and time persistent, TIR signal transients were identified appearing eight days before the Izmit main shock in Greece, moving to Turkey on August  and disappearing, moving back to Greece, some days after. Possible implications of such results, together with present limitations of the proposed technique, will also be discussed in the light of the improved performances expected by its extension to other existing or future satellite packages.", (c) 2005 Elsevier Inc. All rights reserved.,"Tramutoli, |Cuomo, |Filizzola, C|Pergola, N|Pietrapertosa, C",REMOTE SENSING OF ENVIRONMENT,earthquake|thermal anomalies|thermal infrared satellite|avhrr|meteosat|msg|seviri|north anatolian fault|kocaeli-izmit,10.1016/j.rse.2005.04.006
5,WOS:000262275400013,2008,Evaluating the effect of scale in flood inundation modelling in urban environments,SCANNING LASER ALTIMETRY|DIFFUSION-WAVE TREATMENT|RASTER-BASED MODEL|RESOLUTION|SIMULATION|VEGETATION|1D,"Cellular-based approaches for flood inundation modelling have been extensively calibrated and evaluated for this prediction of flood flows on rural river reaches. However, there has only been limited application of these approaches to urban environments, where the need for flood management is greatest. Practical application of two-dimensional (D) flood inundation models is often limited by computation time and processing power on standard desktop PCs when attempting to resolve flows on the high-resolution grids necessary to replicate urban features. Consequently, it is necessary to evaluate the effectiveness of coarse grids to represent flood flows through urban environments. To examine these effects, LOSFLOOD-FP, a D storage cell model, is applied to hypothetical flooding scenarios in Greenfields, Glasgow, Grid resampling techniques in GIS software packages are evaluated and a bilinear griddling technique appears to provide the most accurate and physically intuitive results. A gridding method maintaining sharp elevation changes at building interfaces and neighbouring land is presented and estimates of the discretization noise associated with the coarse resolution grids suggest little improvement over current gridding methods. The variation in model results from the friction sensitivity analysis suggests a non-stationary response to Manning's n with changing model resolution. Model results suggests that a coarse resolution model for urban applications is limited by the representation or urban media in coarse model grids. Furthermore, critical length scales related to building dimensions and building separation distances exist in urban areas that determine maximum possible grid resolutions for hydraulic models of urban flooding."," Copyright (C) 2008 John Wiley & Sons, Ltd.","Fewtrell, T. J.|Bates, P. D.|Horritt, M.|Hunter, N. M.",HYDROLOGICAL PROCESSES,friction sensitivity|hydraulic modelling|scale|urban flooding,10.1002/hyp.7148
6,WOS:000173726400003,2002,Emerging issues in population viability analysis,SPOTTED OWL METAPOPULATION|INBREEDING DEPRESSION|ELASTICITY ANALYSIS|CONSERVATION BIOLOGY|GENTIANA-PNEUMONANTHE|MAMMALIAN POPULATIONS|SPECIES CONSERVATION|PLANT-POPULATIONS|GENETIC-VARIATION|ET-AL,"Population viability analysis (PVA) has become a commonly used tool in endangered species management. There is no single process that constitutes PVA, but all approaches have in common all assessment of a population's risk of extinction (or quasi extinction) or its projected population growth either under current conditions or expected from proposed management. As model sophistication increases, and software programs that facilitate PVA without the need for modeling expertise become more available, there is greater potential for the misuse of models and increased confusion over interpreting their results. Consequently, we discuss the practical use and limitations of PVA in conservation planning, and we discuss some emerging issues of PVA. We review extant issues that have become prominent in PTA, including spatially explicit modeling, sensitivity analysis, incorporating genetics into PVA, PVA in plants, and PVA software packages, but our coverage of emerging issues is not comprehensive. We conclude that PVA is a powerful tool in conservation biology for comparing alternative research plans and relative extinction risks among species, but the suggest caution in its use: () because PVA is a model, its validity depends on the appropriateness of the model's structure and data quality; () results should be presented with appropriate assessment of confidence; () model construction and results should be subject to external review, and () model structure, input, and results should be treated as hypotheses to be tested. We also suggest () restricting the definition of PVA to development of a formal quantitative model, () focusing more research on determining how pervasive density-dependence feedback is across species, and () not using PVA to determine minimum population size or () the specific probability of reaching extinction. The most appropriate use of PVA may be for comparing the relative effects of potential management actions on population growth or persistence.",,"Reed, JM|Mills, LS|Dunning, JB|Menges, ES|McKelvey, KS|Frye, R|Beissinger, |Anstett, MC|Miller, P",CONSERVATION BIOLOGY,,10.1046/j.1523-1739.2002.99419.x
7,WOS:000323457200002,2013,Comparative study on simulation performances of CORSIM and VISSIM for urban street network,ACTUATED SIGNAL SYSTEM|MICROSCOPIC SIMULATION|MODEL CALIBRATION|TRAFFIC FLOW|VALIDATION|BEHAVIOR,"With the progress of simulation technologies, many transportation simulation packages were developed. However, little information is available to the users in applying these models to the most appropriate situations, or even seldom with the simulation accuracy of the individual model. This study conducts a comparative analysis of two popular simulation models (VISSIM and CORSIM), based on their simulation performances on an urban transportation network. Road network and field traffic data from North Bund, Hongkou District, Shanghai, China were used as the simulation background and input. Sensitivity analysis was carried out to compare the performance of both models based on four key indices, namely software usability, average control delay, average queuing length, and cross-sectional traffic volume. Advantages of each simulator were identified based on comparison analyses of simulations with different levels of congestion and intersection geospatial scales. The main performance difference was found lying in the default parameter configuration within the models, including driver behavior settings, traffic environment settings, and vehicle types, etc. Consequently, it was recommended that analysts should choose their appropriate tools based on intersection type and level of saturation within the simulation case.", (c) 2013 Elsevier B.V. All rights reserved.,"Sun, Daniel (Jian)|Zhang, Lihui|Chen, Fangxi",SIMULATION MODELLING PRACTICE AND THEORY,micro-simulation model|urban transportation network|comparative study|signal intersections|sensitivity analysis,10.1016/j.simpat.2013.05.007
8,WOS:000330487700021,2014,"A modified (S-1,S) inventory system for deteriorating items with Poisson demand and non-zero lead time",OPTIMAL REPLENISHMENT POLICY|PRODUCTION QUANTITY MODELS|STOCK-DEPENDENT DEMAND|OPTIMAL SELLING PRICE|VARYING DEMAND|LOT-SIZE|EOQ MODEL|SHORTAGES,"An inventory system is considered for continuous decaying items with non-zero lead time and stochastic demand when shortages are allowed and all unsatisfied demands are back-logged. In this research we consider orders as separate packages where replenishment is one-for-one and a modified base stock policy is applied. In this paper, a penalty cost is introduced for stochastic inventory models with decaying items when less than one unit of the product is delivered to the customers. The objective of the warehouse is to maximize his average profit. Since the concavity analysis of the model is extremely complicated, an upper bound is introduced and an algorithm is presented for finding the optimal solution. Finally, a numerical example is presented and sensitivity analysis is carried out for a number of important parameters.", (C) 2013 Elsevier Inc. All rights reserved.,"Alizadeh, M.|Eskandari, H.|Sajadifar, S. M.",APPLIED MATHEMATICAL MODELLING,inventory|deteriorating items|poisson demand|upper bound,10.1016/j.apm.2013.07.014
9,WOS:000260920100004,2008,ON THE SENSITIVITY OF DESIRABILITY FUNCTIONS FOR MULTIRESPONSE OPTIMIZATION,,"Desirability functions have been one of the most important multiresponse optimization technique since the early eighties. Main reasons for this popularity might be counted as the convenience of the implementation of the method and it's availability in many experimental design software packages. Technique itself involves somehow subjective parameters such as the importance coefficients between response characteristics that are used to calculate overall desirability, weights used in determining the shape of each individual response and the size of the specification band of the response. However, the impact of these sensitive parameters on the solution set is mostly uninvestigated. This paper proposes a procedure to analyze the sensitivity of the important characteristic parameters of desirability functions and their impact on pareto-optimal solution set. The proposed procedure uses the experimental design tools on the solution space and estimates a prediction equation on the overall desirability to identify the sensitive parameters. For illustration, a classical desirability example is selected from the literature and results are given along with the discussion.",,"Aksezer, Caglar S.",JOURNAL OF INDUSTRIAL AND MANAGEMENT OPTIMIZATION,desirability functions|parametric sensitivity analysis|multiresponse optimization,10.3934/jimo.2008.4.685
10,WOS:000174593500001,2002,Use of the most likely failure point method for risk estimation and risk uncertainty analysis,,"The most likely failure point (MLFP) method, developed within the field of structural reliability analysis (where it is known as the FORM/SORM method) is a technique for estimating the risk (probability) that a calculated quantity Q exceeds a set limit Q(lim) when some or all of the inputs to the calculation are uncertain. It can be used as an efficient stand-alone method for this type of risk calculation. However, for application within the field of toxic hazards, it is proposed as a means for performing sensitivity analyses, possibly in parallel with a risk calculation carried out by conventional methods. The basis of the method is outlined and its use is demonstrated by means of an example calculation of the risk arising froth an installation containing chlorine. The calculation uses, as a consequence model, commercial software for the prediction of dense gas transport. The risk estimate is shown to be acceptably close to that obtained by the Monte Carlo method. The use of a proposed screening procedure utilising the sensitivity formulas that the method provides, in order to identify the most significant uncertainties, is demonstrated. The identification of a single set of input values containing sufficient information to summarise (at least approximately) the entire risk analysis is considered to be an important feature of the method and is proposed as the basis of a means for assessing the validity of the consequence model.", (C) 2002 Elsevier Science B.V All rights reserved.,"Mitchell, B",JOURNAL OF HAZARDOUS MATERIALS,risk|toxic|hazard|uncertainty|sensitivity,10.1016/S0304-3894(01)00378-8
11,WOS:000253333700007,2008,Hardware and software efficacy in assessment of fine root diameter distributions,LENGTH|MORPHOLOGY|IMAGES,"Fine roots constitute the majority of root system surface area and thus most of the nutrient and water absorption surface. Fine roots are, however, the least understood of all plant roots. A sensitivity analysis of several software programs capable of providing root diameter distribution analyses was undertaken to determine if this software was capable of discriminating % changes in diameters of roots in the .-. mm diameter range. Digital images produced by drawing discrete lines, by scanning wires of various diameters, and by scanning roots from several legume species were analyzed and compared. None of the three packages were able to adequately analyze these images. Each introduced artifacts into the data that were severe enough to confound interpretation of the resulting diameter class length histograms at resolutions from  to  pixels (px) mm(-), and root diameters from . to .  mm or larger. One package was, however, clearly superior to the other two for routine digital analysis. All three packages require additional development before they are suitable for routine analysis of fine roots. Due to the  px mm- resolution ceiling with currently available scanners, the smallest roots for which this level of discrimination is possible is .mm diameter. For many agricultural and forest species, up to % of their total root length is less than . mm in diameter. It is concluded that both hardware and software constraints currently inhibit the sensitivity of investigations into fine root diameter shifts in response to environmental conditions.", Published by Elsevier B.V.,"Zobel, Richard W.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,fine roots|high resolution|scanner|digital image analysis|diameter distribution|root length,10.1016/j.compag.2007.08.002
12,WOS:000324007200020,2013,Robust optimal dynamic production/pricing policies in a closed-loop system,INVENTORY CONTROL|OPTIMIZATION APPROACH|PRODUCT|RETURNS|MODEL|ENVIRONMENT|EXCHANGE|DEMAND|COSTS,"Hybrid manufacturing/remanufacturing systems play a key role in implementing closed-loop production systems which have been considered due to increasingly environmental concerns and latent profit of used products. Manufacturing and remanufacturing rates, selling price of new products, and acquisition price of used products are the most critical variables to optimize in such hybrid systems. In this paper, we develop a dynamic production/pricing problem, in which decisions should be made in each period confronting with uncertain demand and return. The manufacturer is able to control the demand and return by adjusting selling price and acquisition price respectively, also she can stock inventories of used and new products to deal with uncertainties. Modeling a nominal profit maximization problem, we go through robust optimization approach to reformulate it for the uncertain case. Final robust optimization model is obtained as a quadratic programming model over discrete periods which can be solved by optimization packages of QP. A numerical example is defined and sensitivity analysis is performed on both basic parameters and parameters associated with uncertainty to create managerial views.", (C) 2013 Elsevier Inc. All rights reserved.,"Mahmoudzadeh, Mahdi|Sadjadi, Seyed Jafar|Mansour, Saeed",APPLIED MATHEMATICAL MODELLING,closed-loop supply chain|robust optimization|production-pricing|dynamic pricing|quadratic programming|remanufacturing,10.1016/j.apm.2013.03.008
13,WOS:000379138500008,2016,A probabilistic projection of the transient flow equations with random system parameters and internal boundary conditions,FREQUENCY-RESPONSE METHOD|POLYNOMIAL CHAOS|WATER-HAMMER|PIPELINES|DESIGN,"This paper presents a novel probabilistic approach based on the polynomial chaos expansion that can model the uncertainty propagation from the beginning of a waterhammer simulation and not as an afterthought. Uncertainties are considered in pipe diameter, friction coefficient, and wave speed, as well as internal boundary conditions of leaks and blockages. The polynomial chaos expansion solver results are in an excellent agreement with those calculated by using a model employing the traditional method of characteristics. The probabilistic polynomial chaos approach has the advantage of being robust and more efficient than other non-intrusive methods such as Monte Carlo simulation, which requires thousands of iterations for sharp solutions. The polynomial chaos approach is further extended to solve for randomness in frequency domain using the transfer matrix method with results of comparable accuracy. With further developments, this probabilistic approach can be integrated within existing network modelling software for practical hydraulic engineering problems.",,"Sattar, Ahmed M. A.",JOURNAL OF HYDRAULIC RESEARCH,blockages|leaks|pipelines|polynomial chaos expansion|probabilistic analysis|random variable|transient flow|waterhammer equations,10.1080/00221686.2016.1140682
14,WOS:000251831400002,2008,"Simulation modeling for efficient groundwater management in Balasore coastal basin, India",WATER-RESOURCES|AQUIFERS,"The Balasore coastal groundwater basin in Orissa, India is under a serious threat of overdraft and seawater intrusion. The overexploitation resulted in abandoning many shallow tubewells in the basin. The main intent of this study is the development of a -D groundwater flow and transport model of the basin using the Visual MODFLOW package for analyzing the aquifer response to various pumping strategies. The simulation model was calibrated and validated satisfactorily. Using the validated model, the groundwater response to five pumping scenarios under existing cropping conditions was simulated. The results of the sensitivity analysis indicated that the Balasore aquifer system is more susceptible to the river seepage, recharge from rainfall and interflow than the horizontal and vertical hydraulic conductivities and specific storage. Finally, based on the modeling results, salient management strategies are suggested for the long-term sustainability of vital groundwater resources of the Balasore groundwater basin. The most promising management strategy for the Balasore basin could be: a reduction in the pumpage from the second aquifer by % in the downstream region and an increase in the pumpage to % from the first and second aquifer at potential locations.",,"Rejani, R.|Jha, Madan K.|Panda, S. N.|Mull, R.",WATER RESOURCES MANAGEMENT,simulation|hydrogeology|coastal basin,10.1007/s11269-006-9142-z
15,WOS:000344387000014,2014,Non-stationary extreme value analysis in a changing climate,DIFFERENTIAL EVOLUTION|MODEL PROJECTIONS|RETURN LEVELS|SIMULATIONS|EVENTS|TEMPERATURE|VARIABILITY|ENSEMBLE|IMPACTS|RECORDS,"This paper introduces a framework for estimating stationary and non-stationary return levels, return periods, and risks of climatic extremes using Bayesian inference. This framework is implemented in the Non-stationary Extreme Value Analysis (NEVA) software package, explicitly designed to facilitate analysis of extremes in the geosciences. In a Bayesian approach, NEVA estimates the extreme value parameters with a Differential Evolution Markov Chain (DE-MC) approach for global optimization over the parameter space. NEVA includes posterior probability intervals (uncertainty bounds) of estimated return levels through Bayesian inference, with its inherent advantages in uncertainty quantification. The software presents the results of non-stationary extreme value analysis using various exceedance probability methods. We evaluate both stationary and non-stationary components of the package for a case study consisting of annual temperature maxima for a gridded global temperature dataset. The results show that NEVA can reliably describe extremes and their return levels.",,"Cheng, Linyin|AghaKouchak, Amir|Gilleland, Eric|Katz, Richard W.",CLIMATIC CHANGE,,10.1007/s10584-014-1254-5
16,WOS:000312654600026,2013,SPSens: a software package for stochastic parameter sensitivity analysis of biochemical reaction networks,COUPLED CHEMICAL-REACTIONS|SYSTEMS,"SPSens is a software package for the efficient computation of stochastic parameter sensitivities of biochemical reaction networks. Parameter sensitivity analysis is a valuable tool that can be used to study robustness properties, for drug targeting, and many other purposes. However its application to stochastic models has been limited when Monte Carlo methods are required due to extremely high computational costs. SPSens provides efficient, state of the art sensitivity analysis algorithms in a single software package so that sensitivity analysis can be easily performed on stochastic models of biochemical reaction networks. SPSens implements the algorithms in C and estimates sensitivities with respect to both infinitesimal and finite perturbations to system parameters, in many cases reducing variance by orders of magnitude compared to basic methods. Included among the features of SPSens are serial and parallel command line versions, an interface with Matlab, and several example problems.",,"Sheppard, Patrick W.|Rathinam, Muruhan|Khammash, Mustafa",BIOINFORMATICS,,10.1093/bioinformatics/bts642
17,WOS:000088879600005,2000,Scales and similarities in runoff processes with respect to geomorphometry,SPATIAL VARIABILITY|CATCHMENT|SOIL,"Numerous investigations using various techniques have been carried out towards a more detailed understanding of relationships and interactions between catchment morphometry and rainfall-runoff processes. Recently, this research question has become more relevant through the need for accurate, yet simple, computer models simulating the water balance of large areas. Moreover, advances in the analysis of landform morphometry through the availability of high-resolution digital elevation models (DEMs) and powerful geographical information systems (GIS) have enhanced research efforts with this aim. In this study several computer techniques and models were applied to investigate the effects of geomorphometry on rainfall-runoff processes at different scales. The sensitivity of dynamic hydrological processes to comparatively static boundary conditions requires different methods for modelling, analysis and visualization of different kinds of data appropriate to different scales. Therefore an approach integrating several geocomputational concepts, including spatial analysis of different types of geodata, static modelling of spatial structures, dynamic four-dimensional modelling of hydrological processes and statistical techniques was chosen. Geomorphometric analysis of the study sites was carried out with GIS packages (including ARC/INFO and GRASS), special purpose software and self-developed tools. Soil-morphometry relationships were modelled within a GIS environment. Hydrological models (SAKE and TOPMODEL) were then used to simulate rainfall-runoff processes, and finally statistical tools and sensitivity analysis were applied to gain an insight into the hydrological significance of the various geomorphometric properties. The results demonstrate the importance of small subregions of the catchment, particularly those having low slope angles, low flow lengths and concavities. The spatial distribution of soil types significantly influences modelled runoff. Spatial distributions of soil types are partly related to morphometry and can be captured using soil-morphometry models. Further results show that catchments which differ significantly in morphometry show different runoff responses and different hydrological sensitivity to changes in boundary conditions. A crude derivation of geomorphometric-hydrological landform types could be reached. Therefore, geomorphometric classifications of catchment types could form a basis for representative hydrological modelling at the large scale. Models describing soil distribution in relation to geomorphometry could assist regionalization of spatial heterogeneity and structure of soil parameters relevant in hydrological modelling. Moreover, quantification of geomorphometric catchment structure, e.g. in terms of contributing areas, is needed to describe significant geomorphometric catchment characteristics."," Copyright (C) 2000 John Wiley & Sons, Ltd.","Schmidt, J|Hennrich, K|Dikau, R",HYDROLOGICAL PROCESSES,hydrological modelling|geomorphometry|gis|dem|soil-morphometry relationship,10.1002/1099-1085(20000815/30)14:11/12<1963::AID-HYP48>3.0.CO;2-M
18,WOS:000277447500011,2010,partDSA: deletion/substitution/addition algorithm for partitioning the covariate space in prediction,REGRESSION,"Motivation: Until now, much of the focus in cancer has been on biomarker discovery and generating lists of univariately significant genes, as well as epidemiological and clinical measures. These approaches, although significant on their own, are not effective for elucidating the synergistic qualities of the numerous components in complex diseases. These components do not act one at a time, but rather in concert with numerous others. A compelling need exists to develop analytically sound and computationally advanced methods that elucidate a more biologically meaningful understanding of the mechanisms of cancer initiation and progression by taking these interactions into account. Results: We propose a novel algorithm, partDSA, for prediction when several variables jointly affect the outcome. In such settings, piecewise constant estimation provides an intuitive approach by elucidating interactions and correlation patterns in addition to main effects. As well as generating 'and' statements similar to previously described methods, partDSA explores and chooses the best among all possible 'or' statements. The immediate benefit of partDSA is the ability to build a parsimonious model with 'and' and 'or' conjunctions that account for the observed biological phenomena. Importantly, partDSA is capable of handling categorical and continuous explanatory variables and outcomes. We evaluate the effectiveness of partDSA in comparison to several adaptive algorithms in simulations; additionally, we perform several data analyses with publicly available data and introduce the implementation of partDSA as an R package. Availability: http://cran.r-project.org/web/packages/partDSA/index.html Contact: annette.molinaro@yale.edu Supplementary information: Supplementary data are available at Bioinformatics online.",,"Molinaro, Annette M.|Lostritto, Karen|van der Laan, Mark",BIOINFORMATICS,,10.1093/bioinformatics/btq142
19,WOS:000239466700009,2006,The comparison of four dynamic systems-based software packages: Translation and sensitivity analysis,WETNESS,"Dynamic model development for describing complex ecological systems continues to grow in popularity. For both academic research and project management, understanding the benefits and limitations of systems-based software could improve the accuracy of results and enlarge the user audience. A Surface Wetness Energy Balance (SWEB) model for canopy surface wetness has been translated into four software packages and their strengths and weaknesses were compared based on 'novice' user interpretations. We found expression-based models such as Simulink and GoldSim with Expressions were able to model the SWEB more accurately; however, stock and flow-based models such as STELLA, Madonna, and GoldSim with Flows provided the user a better conceptual understanding of the ecologic system. Although the original objective of this study was to identify an 'appropriate' software package for predicting canopy surface wetness using SWEB, our outcomes suggest that many factors must be considered by the stakeholders when selecting a model because the modeling software becomes part of the model and of the calibration process. These constraints may include user demographics, budget limitations, built-in sensitivity and optimization tools, and the preference of user friendliness vs. computational power. Furthermore, the multitude of closed proprietary software may present a disservice to the modeling community, creating model artifacts that originate somewhere deep inside the undocumented features of the software, and masking the underlying properties of the model. (c) ", Elsevier Ltd. All rights reserved.,"Rizzo, Donna M.|Mouser, Paula J.|Whitney, David H.|Mark, Charles D.|Magarey, Roger D.|Voinov, Alexey A.",ENVIRONMENTAL MODELLING & SOFTWARE,model comparison|dynamic simulation|system-based models|canopy surface energy balance,10.1016/j.envsoft.2005.07.009
20,WOS:000250314200004,2007,"Efficient algorithms for life cycle assessment, input-output analysis, and Monte-Carlo analysis",UNCERTAINTY|INVENTORIES|OPTIONS|SYSTEM|MODEL,"Goal, Scope, and Background. As Life Cycle Assessment (LCA) and Input-Output Analysis (IOA) systems increase in size, computation times and memory usage can increase rapidly. The use of efficient methods of solution allows the use of a wide range of analysis techniques. Some techniques, such as Monte-Carlo Analysis, may be limited if computational times are too slow. Discussion of Methods. In this article, I describe algorithms that substantially reduce computation times and memory usage for solving LCA and IOA systems and performing Monte-Carlo analysis. The algorithms are based on well-established iterative methods of solving linear systems and exploit the power series expansion of the Leontief inverse. The algorithms are further enhanced by using sparse matrix algebra. Results and Discussion. The algorithms presented in this article reduce computational time and memory usage by orders of magnitude, while still retaining a high degree of accuracy. For a  x  LCA system, the algorithm reduced computation time from s to .s while retaining an accuracy of (-)%. Storage was reduced from  megabytes to . megabytes. The algorithm was used to perform a Monte-Carlo analysis on the same system with , samples in s. I also discuss various issues of power series convergence for general LCA and IOA systems and show that convergence will generally hold due to the mathematical structure of LCA and IOA systems. Conclusions. By exploiting the mathematical structure of LCA and IOA iterative techniques substantially reduced the computational times required for solving LCA and IOA systems and for performing Monte-Carlo simulations. This allows more widespread implementation analysis techniques, such as Monte-Carlo analysis, in LCA and IOA. Recommendations and Perspectives. It is suggested that algorithms, such as the ones described in this article, should be implemented in LCA packages. Various checks can be used to verify that computational errors are kept to a minimum.",,"Peters, Glen P.",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,algorithms|input-output analysis (ioa)|matrix inverse|monte-carlo|numerical approaches|power series expansion|sensitivity analysis,10.1065/lca2006.06.254
21,WOS:000224375900007,2004,A tool for risk-based management of surface water quality,SENSITIVITY-ANALYSIS|UNCERTAINTY|MODELS|EUTROPHICATION|PREDICTION|PHOSPHORUS|SYSTEMS|FUTURE,"Water quality Risk Analysis Tool (WaterRAT) is software for supporting decision-making in surface water quality management. The philosophy behind the software is that uncertainty in water quality model predictions is inevitably high due to model equation error, parameter error, and limited definition of boundary conditions and management objectives. Using sensitivity and uncertainty analyses based on Monte Carlo simulation and first order methods, WaterRAT allows the modeller to identify the significant uncertainties, and evaluate the degree to which they control decision-making risk. WaterRAT has a library of river and lake water quality models of varying complexity, and these can be applied at a wide range of temporal and spatial scales, allowing the model design to be responsive to both the modelling task and the data constraints. (C) ", Elsevier Ltd. All rights reserved.,"McIntyre, NR|Wheater, HS",ENVIRONMENTAL MODELLING & SOFTWARE,water quality|uncertainty|risk|decision-making,10.1016/j.envsoft.2003.12.003
22,WOS:000388119400013,2016,ASSESSMENT OF INDUSTRIAL SOLID WASTE USING THE INTELLIGENT DECISION SYSTEM (IDS) METHOD,EVIDENTIAL REASONING APPROACH|MANAGEMENT|UNCERTAINTY|SAFETY,"About  tons of daily solid waste disposal is one of the consequences of the speedy industrial expansion in the province of Khuzestan in the south west of Iran. There are more often than not diverse criteria for assessing the resulted pollution loads from solid waste disposal. In this paper, a new application for the Intelligent Decision System (IDS) is demonstrated for industrial solid waste assessment. IDS software is a Windows-based package for handling Multiple Criteria Decision Making (MCDM) problems considering both qualitative and quantitative criteria under uncertainties. The basis of IDS is a recently developed theory named the Evidential Reasoning ( ER) approach. The major features, superiority and excellence of IDS will be clarified through its application to the ranking of the industrial units located in the Khuzestan Province. Moreover, as a complimentary assessment, a sensitivity analysis is carried out in which the effect of decision maker's attitude toward risk on the total utility which each industry would gain is investigated. The results show that Ahwaz, Abadan, and Khoramshahr are respectively the three most polluting cities in the Khuzestan province. It is concluded that IDS can be utilized not only to handle problems which traditional methods can work out, but also to arrange and evaluate more difficult decision problems that traditional methods are not sufficiently expert of handling.",,"Abed-Elmdoust, Armaghan|Kerachian, Reza",ENVIRONMENTAL ENGINEERING AND MANAGEMENT JOURNAL,evidential reasoning approach|industrial solid waste assessment|intelligent decision system|khuzestan province|multiple-criteria-decision-making (mcdm),10.30638/eemj.2016.191
23,WOS:000401878000001,2017,Minmax regret combinatorial optimization problems with investments,INTERVAL DATA|ALGORITHM,"A new minmax regret optimization model in a system with uncertain parameters is proposed. In this model it is allowed to make investments before a minmax regret solution is implemented in order to modify the source or the nature of the existing uncertainty. Therefore, it is allowed to spend resources in order to change the basic cost structure of the system and take advantage of the modified system to find a robust solution. Some properties of this model allow us to have proper Mathematical Programming formulations that can be solved by standard optimization packages. As a practical application we consider the shortest path problem in a network in which it is possible to modify the uncertainty intervals for the arc costs by investing in the system. We also give an approximate algorithm and generalize some existing results on constant factor approximations. (C) ", Elsevier Ltd. All rights reserved.,"Conde, Eduardo|Leal, Marina",COMPUTERS & OPERATIONS RESEARCH,minmax regret models|robustness and sensitivity analysis|shortest path problem,10.1016/j.cor.2017.03.007
24,WOS:000321088500001,2013,A review of Bayesian belief networks in ecosystem service modelling,COLUMBIA RIVER BASIN|LAND MANAGEMENT ALTERNATIVES|DECISION-SUPPORT TOOLS|GROUNDWATER CONTAMINATION|UNCERTAINTY ANALYSIS|ADAPTIVE MANAGEMENT|RESOURCE MANAGEMENT|EXPERT KNOWLEDGE|AUSTRALIA|SYSTEMS,"A wide range of quantitative and qualitative modelling research on ecosystem services (ESS) has recently been conducted. The available models range between elementary, indicator-based models and complex process-based systems. A semi-quantitative modelling approach that has recently gained importance in ecological modelling is Bayesian belief networks (BBNs). Due to their high transparency, the possibility to combine empirical data with expert knowledge and their explicit treatment of uncertainties, BBNs can make a considerable contribution to the ESS modelling research. However, the number of applications of BBNs in ESS modelling is still limited. This review discusses a number of BBN-based ESS models developed in the last decade. A SWOT analysis highlights the advantages and disadvantages of BBNs in ESS modelling and pinpoints remaining challenges for future research. The existing BBN models are suited to describe, analyse, predict and value ESS. Nevertheless, some weaknesses have to be considered, including poor flexibility of frequently applied software packages, difficulties in eliciting expert knowledge and the inability to model feedback loops. (c) ", Elsevier Ltd. All rights reserved.,"Landuyt, Dries|Broekx, Steven|D'hondt, Rob|Engelen, Guy|Aertsens, Joris|Goethals, Peter L. M.",ENVIRONMENTAL MODELLING & SOFTWARE,bayesian belief networks|ecosystem services|expert based systems|graphical models,10.1016/j.envsoft.2013.03.011
25,WOS:000330491600049,2014,Improvement of the R-SWAT-FME framework to support multiple variables and multi-objective functions,MISSISSIPPI RIVER-BASIN|RAINFALL-RUNOFF MODELS|AUTOMATIC CALIBRATION|PARAMETER-ESTIMATION|UNCERTAINTY ANALYSIS|SENSITIVITY-ANALYSIS|BAYESIAN-APPROACH|CATCHMENT MODELS|LAND-USE|POLLUTION,"Application of numerical models is a common practice in the environmental field for investigation and prediction of natural and anthropogenic processes. However, process knowledge, parameter identifiability, sensitivity, and uncertainty analyses are still a challenge for large and complex mathematical models such as the hydrological/water quality model, Soil and Water Assessment Tool (SWAT). In this study, the previously developed R program language-SWAT-Flexible Modeling Environment (R-SWAT-FME) was improved to support multiple model variables and objectives at multiple time steps (i.e., daily, monthly, and annually). This expansion is significant because there is usually more than one variable (e.g., water, nutrients, and pesticides) of interest for environmental models like SWAT. To further facilitate its easy use, we also simplified its application requirements without compromising its merits, such as the user-friendly interface. To evaluate the performance of the improved framework, we used a case study focusing on both streamflow and nitrate nitrogen in the Upper Iowa River Basin (above Marengo) in the United States. Results indicated that the R-SWAT-FME performs well and is comparable to the built-in auto-calibration tool in multi-objective model calibration. Overall, the enhanced R-SWAT-FME can be useful for the SWAT community, and the methods we used can also be valuable for wrapping potential R packages with other environmental models.", Published by Elsevier B.V.,"Wu, Yiping|Liu, Shuguang",SCIENCE OF THE TOTAL ENVIRONMENT,calibration|fme|monte carlo|r|sensitivity and uncertainty analyses|swat,10.1016/j.scitotenv.2013.07.048
26,WOS:000351458000015,2015,Risk Analysis of Water Demand for Agricultural Crops under Climate Change,OPTIMIZATION HBMO ALGORITHM|RESERVOIR OPERATION|DESIGN|UNCERTAINTY|DISCRETE|NETWORKS|STRATEGY|IMPACTS|SYSTEM,"This paper assesses the risk of increase in water demand for a wide range of irrigated crops in an irrigation network located downstream of the Aidoghmoush Dam in East Azerbaijan by considering climate change conditions for the period -. Atmosphere-ocean global circulation models (AOGCMs) are used to simulate climatic variables such as temperature and precipitation. The Bayesian approach is used to consider uncertainties of AOGCMs. Climate change scenarios of climatic variables are first weighted by using the mean observed temperature-precipitation (MOTP) method, and related probability distribution functions are produced. Outputs of AOGCMs are used as input to water requirement models. Then, produced by using the Monte Carlo method,  samples (discrete values) from the probability distribution functions of monthly downscaled temperature and precipitation in the study area are extracted by using a software for sensitivity and uncertainty analysis. Time series of climatic variables in future periods are then generated (temperature variable to calculate potential evapotranspiration and rainfall variable to calculate effective rainfall). To estimate crop water requirements, crop evapotranspiration (from the product of potential evapotranspiration in the previous step and coefficient of crop computed) and effective precipitation (from time series of the previous step) are calculated. The Food and Agricultural Organization of the United Nations (FAO) methods, FAO- and Penman-Monteith, were used to compute crop and potential evapotranspiration, respectively. Because of lack of required data, potential evapotranspiration in future periods is computed through the relationship of temperature and potential evapotranspiration in the baseline period; the same procedure is conducted for temperature. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering  monthly time series of downscaled temperature and precipitation in future periods. The results indicate that risk of changes in crop water requirements increases by approximately % for a % risk, approximately % for a % risk, and approximately % for a % risk. Also, based on the current cultivated area, on average, the volume of water demand only for the aforementioned crops will be approximately .(() m()/year) with a risk of %, approximately (() m()/year) with a risk of %, and approximately (() m()/year) with a risk of %. Wheat and barley are more resistant and less sensitive to climate change than other crops considered.", (C) 2014 American Society of Civil Engineers.,"Ashofteh, Parisa-Sadat|Bozorg-Haddad, Omid|Marino, Miguel A.",JOURNAL OF HYDROLOGIC ENGINEERING,climate change|net water requirement|risk|uncertainty|monte carlo,10.1061/(ASCE)HE.1943-5584.0001053
27,WOS:000403211900001,2017,Epidemic model formulation and analysis for diarrheal infections caused by salmonella,SENSITIVITY-ANALYSIS|ECONOMIC BURDEN|UNCERTAINTY|DISEASE,"Epidemic modeling can be used to gain better understanding of infectious diseases, such as diarrhea. In the presented research, a continuous mathematical model has been formulated for diarrhea caused by salmonella. This model has been analyzed and simulated to be established in a functioning form. Elementary model analysis, such as working out the disease-free state and basic reproduction number, has been done for this model. The basic reproduction number has been calculated using the next generation matrix method. Stability analysis of the model has been done using the Routh-Hurwitz method. Sensitivity analysis and parameter estimation have been completed for the system too using MATLAB packages that work on the Latin Hypercube Sampling and Partial Rank Correlation Coefficient methods. It was established that as long as R- < , there will be no epidemic. Upon simulation using assumed parameter values, the results produced comprehended the epidemic theory and practical situations. The system was proven stable using the Routh-Hurwitz criterion and parameter estimation was successfully completed. Salmonella diarrhea has been successfully modeled and analyzed in this research. This model has been flexibly built and it can be integrated onto certain platforms to be used as a predictive system to prevent further infections of salmonella diarrhea.",,"Chaturvedi, Ojaswita|Jeffrey, Mandu|Lungu, Edward|Masupe, Shedden",SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL,salmonella|epidemic modeling|model analysis|routh-hurwitz|sensitivity analysis|parameter estimation,10.1177/0037549716685409
28,WOS:000330574400011,2014,Computer algebra systems coming of age: Dynamic simulation and optimization of DAE systems in Mathematica (TM),SENSITIVITY-ANALYSIS|CONSISTENT INITIALIZATION|PATH CONSTRAINTS|EQUATIONS|EFFICIENT|SOFTWARE|CRITERIA,"In this article, DAE parser and SQPsolver, new Computer Algebra System packages specialized in Differential-Algebraic Equations for Mathematical (TM) are presented. These packages joint capabilities for dynamical system analysis, simulation and dynamic optimization through the direct sequential approach are presented with examples and case studies highlighting applications of practical interest to chemical engineers. An overview of the relevant theoretical topics to each of the features of the packages are presented as well as implementation insights. This work paves the way for innovative R&D platforms both capable of solving practical problems of interest as well as offer seamless computational workflow. (C) ", Elsevier Ltd. All rights reserved.,"Navarro, A. K. W.|Vassiliadis, V. S.",COMPUTERS & CHEMICAL ENGINEERING,differential algebraic equations|simulation|parametric sensitivity|dynamic optimizationa,10.1016/j.compchemeng.2013.11.004
29,WOS:000323644600031,2013,Nitrous Oxide Emissions from Cropland: a Procedure for Calibrating the DayCent Biogeochemical Model Using Inverse Modelling,CARBON-DIOXIDE|SOIL|N2O|DENITRIFICATION|SIMULATIONS|COLORADO|SYSTEMS|DNDC,"DayCent is a biogeochemical model of intermediate complexity widely used to simulate greenhouse gases (GHG), soil organic carbon and nutrients in crop, grassland, forest and savannah ecosystems. Although this model has been applied to a wide range of ecosystems, it is still typically parameterized through a traditional ""trial and error"" approach and has not been calibrated using statistical inverse modelling (i.e. algorithmic parameter estimation). The aim of this study is to establish and demonstrate a procedure for calibration of DayCent to improve estimation of GHG emissions. We coupled DayCent with the parameter estimation (PEST) software for inverse modelling. The PEST software can be used for calibration through regularized inversion as well as model sensitivity and uncertainty analysis. The DayCent model was analysed and calibrated using NO flux data collected over  years at the Iowa State University Agronomy and Agricultural Engineering Research Farms, Boone, IA. Crop year  data were used for model calibration and  data were used for validation. The optimization of DayCent model parameters using PEST significantly reduced model residuals relative to the default DayCent parameter values. Parameter estimation improved the model performance by reducing the sum of weighted squared residual difference between measured and modelled outputs by up to  %. For the calibration period, simulation with the default model parameter values underestimated mean daily NO flux by  %. After parameter estimation, the model underestimated the mean daily fluxes by  %. During the validation period, the calibrated model reduced sum of weighted squared residuals by  % relative to the default simulation. Sensitivity analysis performed provides important insights into the model structure providing guidance for model improvement.",,"Rafique, Rashad|Fienen, Michael N.|Parkin, Timothy B.|Anex, Robert P.",WATER AIR AND SOIL POLLUTION,daycent model|inverse modelling|parameter estimation (pest)|nitrous oxide|sensitivity analysis|automatic calibration|validation,10.1007/s11270-013-1677-z
30,WOS:000231058700001,2005,A user's guide to the brave new world of designing simulation experiments,COMPUTER EXPERIMENTS|SENSITIVITY-ANALYSIS|SAMPLING CRITERIA|ROBUST DESIGN|OUTPUT|MODEL|OPTIMIZATION|METHODOLOGY|METAMODELS|MANAGEMENT,"Many simulation practitioners can get more from their analyses by using the statistical theory on design of experiments (DOE) developed specifically for exploring computer models. We discuss a toolkit of designs for simulators with limited DOE expertise who want to select a design and an appropriate analysis for their experiments. Furthermore, we provide a research agenda listing problems in the design of simulation experiments-as opposed to real-world experiments-that require more investigation. We consider three types of practical problems: () developing a basic understanding of a particular simulation model or system, () finding robust decisions or policies as opposed to so-called optimal solutions, and () comparing the merits of various decisions or policies. Our discussion emphasizes aspects that are typical for simulation, such as having many more factors than in real-world experiments, and the sequential nature of the data collection. Because the same problem type may be addressed through different design types, we discuss quality attributes of designs, such as the ease of design construction, the flexibility for analysis, and efficiency considerations. Moreover, the selection of the design type depends on the metamodel (response surface) that the analysts tentatively assume; for example, complicated metamodels require more simulation runs. We present several procedures to validate the metamodel estimated from a specific design, and we summarize a case study illustrating several of our major themes. We conclude with a discussion of areas that merit more work to achieve the potential benefits-either via new research or incorporation into standard simulation or statistical packages.",,"Kleijnen, JPC|Sanchez, SM|Lucas, TW|Cioppa, TM",INFORMS JOURNAL ON COMPUTING,simulation|design of experiments|metamodels|latin hypercube|sequential bifurcation|robust design,10.1287/ijoc.1050.0136
31,WOS:000290190800017,2011,Advances in concrete arch dams shape optimization,DESIGN,"This paper presents an efficient methodology to find the optimum shape of arch dams. In order to create the geometry of arch dams a new algorithm based on Hermit Splines is proposed. A finite element based shape sensitivity analysis for design-dependent loadings involving body force, hydrostatic pressure and earthquake loadings is implemented. The sensitivity analysis is performed using the concept of mesh design velocity. In order to consider the practical requirements in the optimization model such as construction stages, many geometrical and behavioral constrains are included in the model in comparison with previous researches. The optimization problem is solved via the sequential quadratic programming (SQP) method. The proposed methods are applied successfully to an Iranian arch dam, and good results are achieved. By using such methodology, efficient software for shape optimization of concrete arch dams for practical and reliable design now is available.", (C) 2011 Elsevier Inc. All rights reserved.,"Akbari, Jalal|Ahmadi, Mohammad Taghi|Moharrami, Hamid",APPLIED MATHEMATICAL MODELLING,arch dam|shape sensitivity analysis|finite element modeling|shape optimization,10.1016/j.apm.2011.01.020
32,WOS:000309496000040,2012,Estimation of surface shortwave radiation components under all sky conditions: Modeling and sensitivity analysis,PHOTOSYNTHETICALLY ACTIVE RADIATION|DISCRETE-ORDINATE-METHOD|SIMPLE PHYSICAL MODEL|SOLAR-RADIATION|INDEPENDENT PIXEL|GLOBAL IRRADIANCE|CLIMATE RESEARCH|MODIS DATA|SATELLITE|CLOUDS,"Clouds are the most important modulator of the amount of solar energy absorbed by the earth-atmosphere system. Traditional one-dimensional (D) plane-parallel atmospheric radiative transfer models which use the independent pixel approximation (IPA) can only consider two extreme conditions, i.e., either cloud-free or overcast cases. In this paper, two cloud fraction related factors (hemispherical effective cloud fraction and regional cloud fraction) are calculated and incorporated into MODTRAN  (one of the most popular radiative transfer packages) to simulate the surface shortwave radiation components and the top-of-atmosphere (TOA) radiance for all possible solar-cloud-viewing geometries. The accuracy of this modified solar radiative transfer model (named as MODTRAN-CF) is consistent with its prototype (MODTRAN ) which has been widely used and validated in radiative transfer modeling. Some field measurements are used to validate the superiority of MODTRAN-CF. For further understanding and simplifying of this physical model, a global sensitivity analysis (GSA) method is employed to analyze the effect of model parameters on each surface shortwave radiation component. Five parameters including solar zenith angle, surface albedo, hemispherical effective cloud fraction, ground altitude and atmospheric visibility show non-negligible impacts on almost all surface shortwave fluxes, which indicates that these five parameters should be carefully considered in the future modeling of the surface shortwave radiation fluxes. Two cloud optical thickness related parameters (cloud extinction coefficient and cloud thickness) exhibit obvious importance only under cloudy illumination condition especially with optically thin clouds. These findings on the improved model will enhance our knowledge on how to accurately model the surface shortwave radiation fluxes under all sky conditions.", (C) 2012 Elsevier Inc. All rights reserved.,"Chen, Ling|Yan, Guangjian|Wang, Tianxing|Ren, Huazhong|Calbo, Josep|Zhao, Jing|McKenzie, Richard",REMOTE SENSING OF ENVIRONMENT,modtran-cf|hemispherical effective cloud fraction|global sensitivity analysis,10.1016/j.rse.2012.04.006
33,WOS:000278898800009,2010,Sensitivity coefficients for matrix-based LCA,LIFE-CYCLE ASSESSMENT|UNCERTAINTY|PRODUCT,"Background, aim, and scope Matrix-based life cycle assessment (LCA) is part of the standard ingredients of modern LCA tools. An important aspect of matrix-based LCA that is straightforward to carry out, but that requires a careful mathematical handling, is the inclusion of sensitivity coefficients based on differentiating the matrix-based formulas. Materials and methods We briefly review the basic equations for LCA and the basic theory of sensitivity coefficients. Results We present the complete set of sensitivity coefficients from inventory to weighting through characterization and normalization. We show the specific formulas for perturbation analysis, uncertainty analysis, and key issue analysis. We also provide an example using the ecoinvent data. Discussion The limitations of the present approach include the restriction to small changes and uncertainties and the ignorance of correlation between input uncertainties. In contrast to common thinking, there is no restriction to normally distributed uncertainties: Every uncertainty distribution for which a variance can be defined can be submitted to the analytical uncertainty analysis. Conclusions This paper provides a useful set of tables for a number of purposes related to uncertainty and sensitivity analysis. Recommendations and perspectives Although the formulas derived are not simple, they are straightforward to implement in software for LCA. Once this is done, the use of these formulas can become routine practice, enabling a key issue analysis and speeding up perturbation and uncertainty analysis.",,"Heijungs, Reinout",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,derivatives|life cycle interpretation|matrix-based lca|sensitivity|taylor series expansion|uncertainty,10.1007/s11367-010-0158-5
34,WOS:000340951200003,2014,Numerical and intelligent modeling of triaxial strength of anisotropic jointed rock specimens,FUZZY INFERENCE SYSTEM|INTACT ROCKS|CONTINUUM|MASSES,"The strength of anisotropic rock masses can be evaluated through either theoretical or experimental methods. The latter is more precise but also more expensive and time-consuming especially due to difficulties of preparing high-quality samples. Numerical methods, such as finite element method (FEM), finite difference method (FDM), distinct element method (DEM), etc. have been regarded as precise and low-cost theoretical approaches in different fields of rock engineering. On the other hand, applicability of intelligent approaches such as fuzzy systems, neural networks and decision trees in rock mechanics problems has been recognized through numerous published papers. In current study, it is aimed to theoretically evaluate the strength of anisotropic rocks with through-going discontinuity using numerical and intelligent methods. In order to do this, first, strength data of such rocks are collected from the literature. Then FlAC, a commercially well-known software for FDM analysis, is applied to simulate the situation of triaxial test on anisotropic jointed specimens. Reliability of this simulation in predicting the strength of jointed specimens has been verified by previous researches. Therefore, the few gaps of the experimental data are filled by numerical simulation to prevent unexpected learning errors. Furthermore, a sensitivity analysis is carried out based on the numerical process applied herein. Finally, two intelligent methods namely feed forward neural network and a newly developed fuzzy modeling approach are utilized to predict the strength of above-mentioned specimens. Comparison of the results with experimental data demonstrates that the intelligent models result in desirable prediction accuracy.",,"Asadi, Mojtaba|Bagheripour, Mohammad Hossein",EARTH SCIENCE INFORMATICS,numerical modeling|artificial neural networks|fuzzy systems|strength anisotropy|jointed rock,10.1007/s12145-013-0137-z
35,WOS:000414818700006,2017,Assessment of environmental impacts and operational costs of the implementation of an innovative source-separated urine treatment,WASTE-WATER TREATMENT|LIFE-CYCLE ASSESSMENT|TREATMENT PLANTS|NUTRIENT MANAGEMENT|REMOVAL|ALTERNATIVES|SYSTEM|FOCUS,"Innovative treatment technologies and management methods are necessary to valorise the constituents of wastewater, in particular nutrients from urine (highly concentrated and can have significant impacts related to artificial fertilizer production). The FP project, ValuefromUrine, proposed a new two-step process (called VFU) based on struvite precipitation and microbial electrolysis cell (MEC) to recover ammonia, which is further transformed into ammonium sulphate. The environmental and economic impacts of its prospective implementation in the Netherlands were evaluated based on life cycle assessment (LCA) methodology and operational costs. In order to tackle the lack of stable data from the pilot plant and the complex effects on wastewater treatment plant (WWTP), process simulation was coupled with LCA and costs assessment using the Python programming language. Additionally, particular attention was given to the propagation and analysis of inputs uncertainties. Five scenarios of VFU implementation were compared to the conventional treatment of  m() of wastewater. Inventory data were obtained from SUMO software for the WWTP operation. LCA was based on Brightway software (using ecoinvent database and ReCiPe method). The results, based on  iterations sampled from inputs distributions (foreground parameters, ecoinvent background data and market prices), showed a significant advantage of VFU technology, both at a small and decentralized scale and at a large and centralized scale (% confidence intervals not including zero values). The benefits mainly concern the production of fertilizers, the decreased efforts at the WWTP, the water savings from toilets flushing, as well as the lower infrastructure volumes if the WWTP is redesigned (in case of significant reduction of nutrients load in wastewater). The modelling approach, which could be applied to other case studies, improves the representativeness and the interpretation of results (e.g. complex relationships, global sensitivity analysis) but requires additional efforts (computing and engineering knowledge, longer calculation time). Finally, the sustainability assessment should be refined in the future with the development of the technology at larger scale to update these preliminary conclusions before its commercialization. (C) ", Elsevier Ltd. All rights reserved.,"Igos, Elorri|Besson, Mathilde|Gutierrez, Tomas Navarrete|de Faria, Ana Barbara Bisinella|Benetto, Enrico|Barna, Ligia|Ahmadi, Aras|Sperandio, Mathieu",WATER RESEARCH,source-separated urine treatment|process simulation|sustainability assessment|innovative technology|integrated modelling,10.1016/j.watres.2017.09.016
36,WOS:000322354200016,2013,Comparison of bacon packaging on a life cycle basis: a case study,WASTE|IDENTIFICATION|MANAGEMENT|SYSTEM|FOOD,"This study was conducted to compare the environmental effect of weight reduction and different material composition of packages using life cycle analysis of a traditional bacon package (L-board bacon package) compared to a new light weighted bacon package (OLB bacon package). A sensitivity analysis of the main components for the L-board and OLB bacon package is included to confirm either the potential environmental benefits obtained by weight reduction of the traditional bacon package or changing packaging materials. The L-board bacon package is composed of polyethylene/wax coated paper/polyethylene with an overwrap pouch. The new lightweight OLB-board bacon package is composed of reverse printed oriented polypropylene/expanded polystyrene with adhesive, along with a lighter weight overwrap pouch. Environmental impacts were characterized by life cycle assessment with SimaPro software. The general principles, framework of this study, goal and scope definition of the problem, inventory analysis, and interpretation of the results were conducting according to the ISO  family of standards. The new light weighted OLB-board bacon package shows lower environmental burden than the traditional bacon package in most impact indicators, except the mineral extraction indicator. Based on a sensitivity analysis, it was found that changing the material of the original bacon package reduces more the environmental burden of the final bacon package than reducing the weight of the original bacon board material. (C) ", Elsevier Ltd. All rights reserved.,"Kang, DongHo|Sgriccia, Nikki|Selke, Susan|Auras, Rafael",JOURNAL OF CLEANER PRODUCTION,wax coated paper|polypropylene|polystyrene|light weighting|bacon|pouch,10.1016/j.jclepro.2013.05.008
37,WOS:000366034200088,2015,Comprehensive Sensitivity Analysis in NLP Models in PSE Applications Using Space-Filling DOE Strategy,OPTIMIZATION|DESIGN,"Sensitivity analysis is an integral step in the interpretation of the solutions of optimization models, particularly when there are uncertainties in the numerical values of model parameters. Conventional approaches to sensitivity analysis rely on the use of shadow prices in linear models and Lagrange multipliers in non-linear models. Modern commercial optimization software packages are able to automatically generate such sensitivity coefficients to allow rapid post-optimality analysis. However, in the case of non-linear models, Lagrange multipliers have two distinct limitations. First, they represent only changes in the optimal value of an objective function with respect to small changes in parameter values, and thus remain valid only near the immediate vicinity of the nominal design point. Secondly, each Lagrange multiplier gives only the effect of the change of one parameter, assuming that all other parameters remain at their nominal values. Hence, they provide no information about joint effects or interactions caused by simultaneous changes in parameter values. In this paper, we present a strategy based on design of experiments (DOE) to generate a sensitivity surface, which we define as the mapping of the optimal model solution against a range of values of the optimization model parameters. Space-filling designs are used as a basis to generate proxy regression models with quadratic and interaction terms, in order to capture curvature of the sensitivity surface. The resulting proxy model contains more information than is available in conventional sensitivity analysis. In particular, this approach shows curvature and interaction effects that are not reflected when Lagrange multipliers are used. We present case studies based on problems drawn from process systems engineering (PSE) literature to illustrate this comprehensive sensitivity analysis strategy.",,"Tan, Raymond R.|Aviso, Kathleen B.|Uy, Oscar M.|Varbanov, PS|Klemes, JJ|Alwi, SRW|Yong, JY|Liu, ","PRES15: PROCESS INTEGRATION, MODELLING AND OPTIMISATION FOR ENERGY SAVING AND POLLUTION REDUCTION",,10.3303/CET1545088
38,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, Jean-Luc",WATER SCIENCE AND TECHNOLOGY,calibration|epistemology|field data|modelling|sensitivity analysis|separate and combined sewers|stormwater|verification,10.2166/wst.2007.090
39,WOS:000356196000011,2015,PUQ; A code for non-intrusive uncertainty propagation in computer simulations,STRENGTH|MAXIMUM|SCIENCE|CHAOS,"We present a software package for the non-intrusive propagation of uncertainties in input parameters through computer simulation codes or mathematical models and associated analysis; we demonstrate its use to drive micromechanical simulations using a phase field approach to dislocation dynamics. The PRISM uncertainty quantification framework (PUQ) offers several methods to sample the distribution of input variables and to obtain surrogate models (or response functions) that relate the uncertain inputs with the quantities of interest (QoIs); the surrogate models are ultimately used to propagate uncertainties. PUQ requires minimal changes in the simulation code, just those required to annotate the QoI(s) for its analysis. Collocation methods include Monte Carlo, Latin Hypercube and Smolyak sparse grids and surrogate models can be obtained in terms of radial basis functions and via generalized polynomial chaos. PUQ uses the method of elementary effects for sensitivity analysis in Smolyak runs. The code is available for download and also available for cloud computing in nanoHUB. PUQ orchestrates runs of the nanoPLASTICITY tool at nanoHUB where users can propagate uncertainties in dislocation dynamics simulations using simply a web browser, without downloading or installing any software. Program summary Program title: PUQ Catalogue identifier: AEWP_v_ Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWP_v_.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.:  No. of bytes in distributed program, including test data, etc.:  Distribution format: tar.gz Programming language: Python, C. Computer: Workstations. Operating system: Linux, Mac OSX. Classification: ., ., ., External routines: SciPy, Matplotlib, hpy Nature of problem: Uncertainty propagation and creation of response surfaces. Solution method: Generalized Polynomial Chaos (gPC) using Smolyak sparse grids. Running time: PUQ performs uncertainty quantification and sensitivity analysis by running a simulation multiple times using different values for input parameters. Its run time will be the product of the run time of the chosen simulation code and the number of runs required to achieve the desired accuracy.", (C) 2015 Elsevier B.V. All rights reserved.,"Hunt, Martin|Haley, Benjamin|McLennan, Michael|Koslowski, Marisol|Murthy, Jayathi|Strachan, Alejandro",COMPUTER PHYSICS COMMUNICATIONS,uncertainty quantification|surrogate model|sensitivity analysis,10.1016/j.cpc.2015.04.011
40,WOS:000256053900010,2008,A comparative life cycle analysis of two different juice packages,,"Packaging wastes have a portion of -% in total municipal solid waste (MSW) in Turkey, and they have to be evaluated from production to final disposal from the environmental point of view. The concern about the environmental impacts of packages has been dealt with using several approaches in environmental management, such as risk assessment, environmental impact assessment, environmental auditing, energy analysis, material flow analysis, and life cycle analysis (LCA). The main purpose of this research was to investigate the life cycle environmental impact of glass bottles and beverage cartons. This LCA study was performed by using SimaPro (PRe Consultants, The Netherlands) software. Individual and comparative life cycle analysis of two packages was performed depending on a consumer who lives in Eskisehir city. For that aim, SimaPro was used, and the data to run the software was gathered from package producers, the database of the software, and the literature. Life cycle comparisons of the two juice packages among themselves and also within themselves were carried out by using EcoIndicator  on the basis of climate change, ecotoxicity, acidification/eutrophication, and fossil fuels. Sensitivity analysis was performed to evaluate the effects of transportation. According to comparison figures, the environmental load of glass bottles is higher than beverage carton's load for all the impact categories. This result is also supported by the sensitivity analysis.",,"Banar, Mufide|Cokaygil, Zerrin",ENVIRONMENTAL ENGINEERING SCIENCE,lca|glass bottle|beverage carton|packaging waste|simapro7,10.1089/ees.2007.0079
41,WOS:000257487300002,2008,Materials integrity in microsystems: a framework for a petascale predictive-science-based multiscale modeling and simulation system,STRESSED GRAIN-GROWTH|CHAOS REPRESENTATIONS|POLYCRYSTALLINE MATERIALS|ELASTIC PROPERTIES|ADAPTIVE ANALYSIS|DESIGN|UNCERTAINTY|FORMULATION|METALS|ERRORS,"Microsystems have become an integral part of our lives and can be found in homeland security, medical science, aerospace applications and beyond. Many critical microsystem applications are in harsh environments, in which long-term reliability needs to be guaranteed and repair is not feasible. For example, gyroscope microsystems on satellites need to function for over  years under severe radiation, thermal cycling, and shock loading. Hence a predictive-science-based, verified and validated computational models and algorithms to predict the performance and materials integrity of microsystems in these situations is needed. Confidence in these predictions is improved by quantifying uncertainties and approximation errors. With no full system testing and limited sub-system testings, petascale computing is certainly necessary to span both time and space scales and to reduce the uncertainty in the prediction of long-term reliability. This paper presents the necessary steps to develop predictive-science-based multiscale modeling and simulation system. The development of this system will be focused on the prediction of the long-term performance of a gyroscope microsystem. The environmental effects to be considered include radiation, thermo-mechanical cycling and shock. Since there will be many material performance issues, attention is restricted to creep resulting from thermal aging and radiation-enhanced mass diffusion, material instability due to radiation and thermo-mechanical cycling and damage and fracture due to shock. To meet these challenges, we aim to develop an integrated multiscale software analysis system that spans the length scales from the atomistic scale to the scale of the device. The proposed software system will include molecular mechanics, phase field evolution, micromechanics and continuum mechanics software, and the state-of-the-art model identification strategies where atomistic properties are calibrated by quantum calculations. We aim to predict the long-term (in excess of  years) integrity of the resonator, electrode base, multilayer metallic bonding pads, and vacuum seals in a prescribed mission. Although multiscale simulations are efficient in the sense that they focus the most computationally intensive models and methods on only the portions of the space-time domain needed, the execution of the multiscale simulations associated with evaluating materials and device integrity for aerospace microsystems will require the application of petascale computing. A component-based software strategy will be used in the development of our massively parallel multiscale simulation system. This approach will allow us to take full advantage of existing single scale modeling components. An extensive, pervasive thrust in the software system development is verification, validation, and uncertainty quantification (UQ). Each component and the integrated software system need to be carefully verified. An UQ methodology that determines the quality of predictive information available from experimental measurements and packages the information in a form suitable for UQ at various scales needs to be developed. Experiments to validate the model at the nanoscale, microscale, and macroscale are proposed. The development of a petascale predictive-science-based multiscale modeling and simulation system will advance the field of predictive multiscale science so that it can be used to reliably analyze problems of unprecedented complexity, where limited testing resources can be adequately replaced by petascale computational power, advanced verificaion, validation, and UQ methodologies.",,"To, Albert C.|Liu, Wing Kam|Olson, Gregory B.|Belytschko, Ted|Chen, Wei|Shephard, Mark S.|Chung, Yip-Wah|Ghanem, Roger|Voorhees, Peter W.|Seidman, David N.|Wolverton, Chris|Chen, J. S.|Moran, Brian|Freeman, Arthur J.|Tian, Rong|Luo, Xiaojuan|Lautenschlager, Eric|Challoner, A. Dorian",COMPUTATIONAL MECHANICS,multiscale modeling|petascale computing|microsystems|mems,10.1007/s00466-008-0267-1
42,WOS:000408861800155,2017,Improving Thermal Comfort of Low-Income Housing in Thailand through Passive Design Strategies,TROPICAL HUMID REGION|BUILDINGS|STANDARDS|ADAPTATION|HOUSES,"In Thailand, the delivery of adequate low-income housing has historically been overshadowed by politics with cost and quantity being prioritised over quality, comfort and resilience. In a country that experiences hot and humid temperatures throughout the year, buildings need to be adaptable to the climate to improve the thermal comfort of inhabitants. This research is focused on identifying areas for improving the thermal performance of these housing designs. Firstly, dynamic thermal simulations were run on a baseline model using the adaptive thermal comfort model CIBSE TM for assessment. The three criteria defined in CIBSE TM were used to assess the frequency and severity of overheating in the buildings. The internal temperature of the apartments was shown to exceed the thermal comfort threshold for these criteria throughout the year. The internal operating daily temperatures of the apartment remain high, ranging from a maximum of . degrees C to a minimum of . degrees C. Based on these findings, five criteria were selected to be analysed for sensitivity to obtain the key parameters that influence the thermal performance and to suggest possible areas for improvement. The computer software package Integrated Environmental SolutionsVirtual Environment (IES-VE) was used to perform building energy simulations. Once the baseline conditions were identified, the software packages SimLab. and RStudio were used to carry out the sensitivity analysis. These results indicated that roof material and the presence of a balcony have the greatest influence on the system. Incorporating insulation into the roof reduced the mean number of days of overheating by .%. Removing the balcony increased the number of days of overheating by .% due to significant reductions in internal ventilation.",,"Bhikhoo, Nafisa|Hashemi, Arman|Cruickshank, Heather",SUSTAINABILITY,thermal comfort|low income housing|thailand|tropical climates|dynamic thermal simulations|sensitivity analysis,10.3390/su9081440
43,WOS:000299195200036,2011,Full scale 3D-modelling of the coupled gas migration and heat dissipation in a planned repository for radioactive waste in the Callovo-Oxfordian clay,HYDRAULIC CONDUCTIVITY,"An important question related to the long-term safety performance of a repository for long-lived medium and high-level radioactive waste in the Callovo-Oxfordian clay unit is the impact of heat and gas generated in the waste emplacement areas on the gas and water pressure and on the water saturation in the backfilled repository and in the host rock. The current design of such a repository consists of a multitude of different underground structures, such as emplacement drifts for waste canisters and other types of waste packages, access and ventilation drifts, and access shafts in the central part of the repository. The individual underground structures exhibit different thermo-hydraulic and geometrical properties yielding a large and complex system for the flow and transport of gas, water and heat. A detailed D modelling of the entire repository would require a tremendous computational effort, even when using high performance simulator codes. A newly developed method (Poller et al., ) allows for the D modelling of the two-phase gas-water flow and thermal evolution in the entire repository/host-rock system in a simplified manner. Besides accounting for both the detailed structures at local scale and the global geometry of the drift network, it also allows for an assessment of the gas phase pressure as well as the hydrogen and heat fluxes developing over the complete lifetime of the repository system. In this paper, the results of a reference scenario are presented. The assessment focuses on the two dominant processes, i.e. the dissolution and diffusion of the generated hydrogen, and the advective migration of the forming hydrogen gas phase in space and time (up to  million years). Further, the main findings of a sensitivity analysis on different features, physical processes and parameter uncertainty are presented. (C) ", Elsevier Ltd. All rights reserved.,"Enssle, Carl Philipp|Croise, Jean|Puller, Andreas|Mayer, Gerhard|Wendling, Jacques",PHYSICS AND CHEMISTRY OF THE EARTH,radioactive waste repository|clay|hydrogen and heat transport|two-phase flow|tough2-mp|numerical simulation,10.1016/j.pce.2011.07.033
44,WOS:000337157100006,2014,Error propagation methods for LCA-a comparison,LIFE-CYCLE ASSESSMENT|INPUT-OUTPUT MODELS|UNCERTAINTY ANALYSIS|DECISION-MAKING|ASSESSMENTS|INVENTORIES|PRODUCTS|IMPACT,"The analysis of uncertainty in life cycle assessment (LCA) studies has been a topic for more than  years, and many commercial LCA programs now feature a sampling approach called Monte Carlo analysis. Yet, a full Monte Carlo analysis of a large LCA system, for instance containing the , unit processes of ecoinvent v., is rarely carried out by LCA practitioners. One reason for this is computation time. An alternative faster than Monte Carlo method is analytical error propagation by means of a Taylor series expansion; however, this approach suffers from being explained in the literature in conflicting ways, hampering implementation in most software packages for LCA. The purpose of this paper is to compare the two different approaches from a theoretical and practical perspective. In this paper, we compare the analytical and sampling approaches in terms of their theoretical background and their mathematical formulation. Using three case studies-one stylized, one real-sized, and one input-output (IO)-based-we approach these techniques from a practical perspective and compare them in terms of speed and results. Depending on the precise question, a sampling or an analytical approach provides more useful information. Whenever they provide the same indicators, an analytical approach is much faster but less reliable when the uncertainties are large. For a good analysis, analytical and sampling approaches are equally important, and we recommend practitioners to use both whenever available, and we recommend software suppliers to implement both.",,"Heijungs, Reinout|Lenzen, Manfred",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,analytical methods|gaussian error propagation|ioa|lca|monte carlo|sampling methods|uncertainty,10.1007/s11367-014-0751-0
45,WOS:000404559900023,2017,Simulating the Fate and Transport of Coal Seam Gas Chemicals in Variably-Saturated Soils Using HYDRUS,HYDRAULIC CONDUCTIVITY|VADOSE ZONE|REACTIVE TRANSPORT|TRANSFORMATION PRODUCTS|NITROSAMINE FORMATION|CONSTRUCTED WETLANDS|SENSITIVITY ANALYSIS|AGRICULTURAL SOILS|SOLUTE TRANSPORT|FIELD CONDITIONS,"The HYDRUS-D and HYDRUS (D/D) computer software packages are widely used finite element models for simulating the one-, and two- or three-dimensional movement of water, heat, and multiple solutes in variably-saturated media, respectively. While the standard HYDRUS models consider only the fate and transport of individual solutes or solutes subject to first-order degradation reactions, several specialized HYDRUS add-on modules can simulate far more complex biogeochemical processes. The objective of this paper is to provide a brief overview of the HYDRUS models and their add-on modules, and to demonstrate possible applications of the software to the subsurface fate and transport of chemicals involved in coal seam gas extraction and water management operations. One application uses the standard HYDRUS model to evaluate the natural soil attenuation potential of hydraulic fracturing chemicals and their transformation products in case of an accidental release. By coupling the processes of retardation, first-order degradation and convective-dispersive transport of the biocide bronopol and its degradation products, we demonstrated how natural attenuation reduces initial concentrations by more than a factor of hundred in the top  cm of the soil. A second application uses the UnsatChem module to explore the possible use of coal seam gas produced water for sustainable irrigation. Simulations with different irrigation waters (untreated, amended with surface water, and reverse osmosis treated) provided detailed results regarding chemical indicators of soil and plant health, notably SAR, EC and sodium concentrations. A third application uses the HP module to analyze trace metal transport involving cation exchange and surface complexation sorption reactions in a soil leached with coal seam gas produced water following some accidental water release scenario. Results show that the main process responsible for trace metal migration in soil is complexation of naturally present trace metals with inorganic ligands such as (bi)carbonate that enter the soil upon infiltration with alkaline produced water. The examples were selected to show how users can tailor the required model complexity to specific needs, such as for rapid screening or risk assessments of various chemicals nder generic soil conditions, or for more detailed site-specific analyses of actual subsurface pollution problems.",,"Mallants, Dirk|Simunek, Jirka|van Genuchten, Martinus Th.|Jacques, Diederik",WATER,hydrus|coal seam gas|contaminant transport|trace elements|hp1,10.3390/w9060385
46,WOS:000335707200019,2014,FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module,LATIN HYPERCUBE SAMPLES|REINFORCEMENT CORROSION|INPUT VARIABLES|CONCRETE|SIZE|SIMULATION|CARBONATION|DURABILITY|FRACTURE|MODEL,"The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of ""randomizing"" computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique - simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper. (C) ", Elsevier Ltd. All rights reserved.,"Novak, Drahomir|Vorechovsky, Miroslav|Teply, Bretislav",ADVANCES IN ENGINEERING SOFTWARE,statistical analysis|sensitivity|reliability|monte carlo simulation|latin hypercube sampling|simulated annealing|random fields|material degradation,10.1016/j.advengsoft.2013.06.011
47,WOS:000237124500022,2006,Automatic calibration and predictive uncertainty analysis of a semidistributed watershed model,RAINFALL-RUNOFF MODELS|GLOBAL OPTIMIZATION|HYDROLOGIC-MODELS|PARAMETER-ESTIMATION|ALGORITHM|MULTIPLE|SCHEME,"Semidistributed models are commonly calibrated manually, but software for automatic calibration is now available. We present a two-stage routine for automatic calibration of the semidistributed watershed model Soil and Water Assessment Tool ( SWAT) that finds the best values for the model parameters, preserves spatial variability in essential parameters, and leads to a measure of the model prediction uncertainty. In the first stage, a modified global Shuffled Complex Evolution (SCE-UA) method was employed to find the ""best'' values for the lumped model parameters. In the second stage, the spatial variability of the original model parameters was restored and a local search method ( a variant of Levenberg - Marquart method) was used to find a more distributed set of parameters using the results of the previous stage as starting values. A method called ""regularization'' was adopted to prevent the parameters from taking extreme values. In addition, we applied a nonlinear calibration-constrained method to develop confidence intervals for annual and -d average flow predictions. We calibrated stream flow in the Etowah River measured at Canton, GA ( a watershed area of  km()) for the years  to  and used the years  to  for validation. The Parameter Estimator ( PEST) software was used to conduct the two-stage automatic calibration and prediction uncertainty analysis. Calibration for daily and monthly flow produced a very good fit to the measured data. Nash-Sutcliffe coefficients for daily and monthly flow over the calibration period were . and ., respectively. They were . and ., respectively, for the validation period. The nonlinear prediction uncertainty analysis worked well for long-term ( annual) flow in that our prediction confidence intervals included or were very near to the observed flow for most years. It did not work well for short-term (-d average) flows in that the prediction confidence intervals did not include the observed flow, especially for low and high flow conditions.",,"Lin, ZL|Radcliffe, DE",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0025
48,WOS:000307721100006,2012,Evolutionary topology optimization of periodic composites for extremal magnetic permeability and electrical permittivity,LEVEL-SET|STRUCTURAL OPTIMIZATION|DESIGN|HOMOGENIZATION|METAMATERIALS|SHAPE|MICROSTRUCTURES,"This paper presents a bidirectional evolutionary structural optimization (BESO) method for designing periodic microstructures of two-phase composites with extremal electromagnetic permeability and permittivity. The effective permeability and effective permittivity of the composite are obtained by applying the homogenization technique to the representative periodic base cell (PBC). Single or multiple objectives are defined to maximize or minimize the electromagnetic properties separately or simultaneously. The sensitivity analysis of the objective function is conducted using the adjoint method. Based on the established sensitivity number, BESO gradually evolves the topology of the PBC to an optimum. Numerical examples demonstrate that the electromagnetic properties of the resulting D and D microstructures are very close to the theoretical Hashin-Shtrikman (HS) bounds. The proposed BESO algorithm is computationally efficient as the solution usually converges in less than  iterations. The proposed BESO method can be implemented easily as a post-processor to standard commercial finite element analysis software packages, e.g. ANSYS which has been used in this study. The resulting topologies are clear black-and-white solutions (with no grey areas). Some interesting topological patterns such as Vigdergauz-type structure and Schwarz primitive structure have been found which will be useful for the design of electromagnetic materials.",,"Huang, X.|Xie, Y. M.|Jia, B.|Li, Q.|Zhou, S. W.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,topology optimization|bidirectional evolutionary structural optimization (beso)|homogenization|effective permeability|effective permittivity,10.1007/s00158-012-0766-8
49,WOS:000347582800052,2015,Development of the integrated fuzzy analytical hierarchy process with multidimensional scaling in selection of natural wastewater treatment alternatives,TREATMENT SYSTEMS|NETWORK PROCESS|OPTIMIZATION|MANAGEMENT|WETLANDS|AHP,"Multi-criteria decision-making in selection of wastewater treatment alternatives has been explored widely, while few past studies comprehensively addressed the integration of various aspects (e.g., environmental, economical, ecological and management, and technical factors), which is a priority for decision-makers. This paper develops the integrated fuzzy analytical hierarchy process (AHP) with multidimensional scaling (MDS) approach to improve current methods for determining the optimal alternative. The integrated method incorporates the weights computed by AHP into the fuzzy matter-element, and allows evaluators to understand the relative importance of each index or criterion at a high level. This is followed by the MDS method to determine the optimal alternative directly through the coordinates associated with each alternative in a two-dimensional configuration. The method was evaluated via specific programming language software packages, and was applied to select natural wastewater treatment alternatives in a case study. Results indicate the stabilization pond was the optimal alternative among five natural wastewater treatment systems. Sensitivity analysis was conducted and reflects the importance of weighing on alternative selection.", (C) 2014 Elsevier B.V. All rights reserved.,"Ouyang, Xiaoguag|Guo, Fen|Shan, Dan|Yu, Huanyun|Wang, Jian",ECOLOGICAL ENGINEERING,analytical hierarchy process|fuzzy matter-element|multidimensional scaling|wastewater treatment alternative|sensitivity analysis,10.1016/j.ecoleng.2014.11.006
50,WOS:000221832000028,2004,"Robust satellite techniques for seismically active areas monitoring: a sensitivity analysis on September 7, 1999 Athens's earthquake",IMPENDING EARTHQUAKES|PRECURSOR|GREECE|TURKEY|AUGUST|CHINA|IZMIT,"Space-time TIR anomalies, observed from months to weeks before the occurrence of earthquakes, have been suggested, by several authors, as pre-seismic signals. A robust approach (RAT) has recently been proposed (and successfully applied in the field of monitoring major natural and environmental risks) which permits a statistically based definition of TIR anomaly even in the presence of highly variable contributions from atmospheric (e.g. transmittance), surface (e.g. emissivity and morphology) and observational (time/season, but also solar and satellite zenithal angles) conditions. In this paper the actual potential of satellite TIR surveys is evaluated on the basis of several years of NOAA/AVHRR and METEOSAT observations over Europe. TIR anomalies, possibly associated to the Athens's earthquake which occurred on September , , have been particularly considered in order to evaluate the capability of the proposed approach to filter-out noisy contributions to the measured TIR signal due to variable, observational and meteorological, conditions. This study demonstrated the capability of the proposed method to isolate (if any) possible pre-seismic anomalous TIR patterns from the most important noisy contributions to the measured signal. The advantages offered by the use of geo-stationary (quite doubling the achievable signal-to-noise ratio) instead of polar satellite packages result also quite evident after the tests performed in the case of Athens's earthquake. Even if it was not the aim of this paper to confirm or confute the existence of pre-seismic TIR anomalies (an extended number of test-cases should be analyzed before), results here achieved surely encourage the continuation of the studies in this direction permitting, moreover, to devise suitable strategies in order to obtain more firm answers to this fascinating hypothesis. (C) ", Elsevier Ltd. All rights reserved.,"Filizzola, C|Pergola, N|Pietrapertosa, C|Tramutoli, ",PHYSICS AND CHEMISTRY OF THE EARTH,earthquake|satellite thermal infrared|avhrr|meteosat|msg,10.1016/j.pce.2003.11.019
51,WOS:000377452800009,2016,The Influence of Model Structure Uncertainty on Water Quality Assessment,RUNOFF|APPLICABILITY|TRANSPORT|SUPPORT,"Physically-based mathematical water quality models are known as potentially effective tools to simulate the temporal and spatial variations of water quality variables along rivers. Each model relies on specific sets of assumptions and equations to simulate the physico-biochemical processes, which influence on its simulation results. This paper aims to improve the insight in the uncertainties related to state-of-the-art river physico-biochemical water quality modelling. Sensitivity analysis is applied to the processes implemented in three most popular commercial software packages: MIKE, InfoWorks RS and InfoWorks ICM. This is done for the Molse Neet river case study. Firstly, the physico-biochemical processes are screened to obtain a preliminary assessment on the critical processes and to determine the processes that require more detailed comparison. Then, local sensitivity analysis is carried out to specify the sensitive parameters and processes. Results show that the hydrodynamic results, heat transfer rate and reaeration simulations cause large differences in model simulation outputs for water temperature and dissolved oxygen concentrations. The ignorance of processes related to sediment transport, phytoplankton and bacteria has a significant influence on the higher values of organic matter and lower values of dissolved oxygen concentrations. The three models show consensus on the main pollutant sources explaining organic matter and nitrate concentrations, but disagree on the main factors explaining the DO concentrations.",,"Thanh Thuy Nguyen, |Willems, Patrick",WATER RESOURCES MANAGEMENT,infoworksicm|infoworksrs|mike11|model structure uncertainty|river water quality model|sensitivity analysis,10.1007/s11269-016-1330-x
52,WOS:000374807600006,2016,Sensitivity of a third generation wave model to wind and boundary condition sources and model physics: A case study from the South Atlantic Ocean off Brazil coast,NONLINEAR ENERGY-TRANSFER|SHALLOW-WATER|DISSIPATION|SPECTRUM|PARAMETERIZATIONS|COMPUTATIONS|PERFORMANCE|SWAN,"Three different packages describing the white capping dissipation process, and the corresponding energy input from wind to wave were used to study the surface wave dynamics in South Atlantic Ocean, close to the Brazilian coast. A host of statistical parameters were computed to evaluate the performance of wave model in terms of simulated bulk wave parameters. Wave measurements from a buoy deployed off Santa Catarina Island, Southern Brazil and data along the tracks of Synthetic Aperture Radars were compared with simulated bulk wave parameters; especially significant wave height, for skill assessment of different packages. It has been shown that using a single parameter representing the performance of source and sink terms in the wave model, or relying on data from only one period of simulations for model validation and skill assessment would be misleading. The model sensitivity to input parameters such as time step and grid size were addressed using multiple datasets. The wind data used for the simulation were obtained from two different sources, and provided the opportunity to evaluate the importance of input data quality. The wind speed extracted from remote sensing satellites was compared to wind datasets used for wave modeling. The simulation results showed that the wind quality and its spatial resolution is highly correlated to the quality of model output. Two different sources of wave information along the open boundaries of the model domain were used for skill assessment of a high resolution wave model for the study area. It has been shown, based on the sensitivity analysis, that the effect of using different boundary conditions would decrease as the distance from the open boundary increases; however, the difference were still noticeable at the buoy location which was located - km away from the model boundaries; but restricted to the narrow band of the low frequency wave spectrum. (C) ", Elsevier Ltd. All rights reserved.,"Siadatmousavi, S. Mostafa|Jose, Felix|da Silva, Graziela Miot",COMPUTERS & GEOSCIENCES,white capping|wave spectrum|model uncertainties|santa catarina island,10.1016/j.cageo.2015.09.025
53,WOS:000222887600002,2004,Uncertainty calculation in life cycle assessments - A combined model of simulation and approximation,FRAMEWORK|LCA,"Goal and Background. Uncertainty is commonly not taken into account in LCA studies, which downgrades their usability for decision support. One often stated reason is a lack of method. The aim of this paper is to develop a method for calculating the uncertainty propagation in LCAs in a fast and reliable manner Approach. The method is developed in a model that reflects the calculation of an LCA. For calculating the uncertainty, the model. combines approximation formulas and Monte Carlo Simulation. It is based on virtual data that distinguishes true values and random errors or uncertainty, and that hence allows one to compare the performance of error propagation formulas and simulation results. The model is developed for a linear chain of processes, but extensions for covering also branched and looped. product systems are made and described. Results. The paper proposes a combined use of approximation formulas and Monte Carlo simulation for calculating uncertainty, in LCAs, developed primarily for the sequential approach. During the calculation, a parameter observation controls the performance of the approximation formulas. Quantitative threshold values are given in the paper. The combination thus transcends drawbacks of simulation and approximation. Conclusions and Outlook. The uncertainty question is a true Jigsaw puzzle for LCAs and the method presented in this paper may serve as one piece in solving it. It may thus foster a sound use of uncertainty assessment in LCAs. Analysing a proper management of the input uncertainty, taking into account suitable sampling; and estimation techniques; using the approach for real case studies, implementing it in LCA software for automatically applying the proposed combined uncertainty model and, on the other hand., investigating about how people do decide, and should decide, when their decision relies on explicitly uncertain LCA outcomes - these all are neighbouring puzzle pieces inviting to further work.",,"Ciroth, A|Fleischer, G|Steinbach, J",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,approximation formula|error propagation|life cycle inventory (lci)|life cycle impact assessment (lcia)|models|monte carlo simulation|quantitative thresholds|uncertainties,10.1065/lca2004.05.158
54,WOS:000368731900010,2016,A pseudo-statistical approach to treat choice uncertainty: the example of partitioning allocation methods,LIFE-CYCLE ASSESSMENT|LCA|PROPAGATION|INVENTORY|SCENARIOS|SYSTEMS,"Purpose Despite efforts to treat uncertainty due to methodological choices in life cycle assessment (LCA) such as standardization, one-at-a-time (OAT) sensitivity analysis, and analytical and statistical methods, no method exists that propagate this source of uncertainty for all relevant processes simultaneously with data uncertainty through LCA. This study aims to develop, implement, and test such a method, for the particular example of the choice of partitioning methods for allocation in LCA, to be used in LCA calculations and software. Methods Monte Carlo simulations were used jointly with the CMLCA software for propagating into distributions of LCA results, uncertainty due to the choice of allocation method together with uncertainty of unit process data. In this study, a methodological preference is assigned to each partitioning method, applicable to multi-functional processes in the system. The allocation methods are sampled per process according to these preferences. A case study on rapeseed oil focusing on three greenhouse gas (GHG) emissions and their global warming impacts is presented to illustrate the method developed. The results of the developed method are compared with those for the same case similarly quantifying uncertainty of unit process data but accompanied by separate scenarios for the different partitioning choices. Results and discussion The median of the inventory flows (emissions) for separate scenarios varies due to the partitioning choices and unit process data uncertainties. Inventory variations are reflected in the global warming results. Results for the approach of this study vary with the methodological preference assigned to the different allocation methods per multi-functional process and with the continuous distribution of unit process data. The method proved feasible and implementable. However, absolute uncertainties only further increased. Therefore, it should be further researched to reflect relative uncertainties, more relevant for comparative LCAs. Conclusions Propagation of uncertainties due to the choice of partitioning methods and to unit process data into LCA results is enabled by the proposed method, while capturing variability due to both sources. It is a practical proposal to tackle unresolved debates about partitioning choices increasing robustness and transparency of LCA results. Assigning a methodological preference to each allocation method of multi-functional processes in the system enables pseudo-statistical propagation of uncertainty due to allocation. Involving stakeholders in determining these methodological preferences allows for participatory approaches. Eventually, this method could be expanded to also cover other ways of dealing with allocation and to other methodological choices in LCA.",,"Beltran, Angelica Mendoza|Heijungs, Reinout|Guinee, Jeroen|Tukker, Arnold",INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT,allocation|lca|methodological choices|monte carlo|uncertainty,10.1007/s11367-015-0994-4
55,WOS:000305299400013,2012,Adjoint sensitivity in PDE constrained least squares problems as a multiphysics problem,,"Purpose - The purpose of this paper is to provide a framework for the implementation of an adjoint sensitivity formulation for least-squares partial differential equations constrained optimization problems exploiting a multiphysics finite elements package. The estimation of the diffusion coefficient in a Poisson-type diffusion equation is used as an example. Design/methodology/approach - The authors derive the adjoint formulation in a continuous setting allowing to attribute to the direct and adjoint states the role of different fields to be solved for. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. Having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. This sensitivity can then be used to efficiently solve the least-squares problem. Findings - The authors derived the adjoint formulation in a continuous setting allowing the direct and adjoint states to be attributed the role of different fields to be solved. They are one-way coupled through the mismatch between measured and direct states acting as a source term in the adjoint equation. It is found that, having solved for the direct and adjoint state, the sensitivity of the cost function with respect to the design variables can then be obtained by a suitable post-processing procedure. Research limitations/implications - This paper implies that modern multiphysics finite elements packages provide a flexible and extendable software environment for the experimentation with different adjoint formulations. Such tools are therefore expected to become increasingly important in solving notoriously difficult partial differential equation (PDE)-constrained least-squares problems. The framework also provides the possibility of experimentation with different regularization techniques (total variation and multiscale techniques for instance) to handle the ill-posedness of the problem. Originality/value - In this paper the adjoint sensitivity computation is casted as a multiphysics problem allowing for a flexible and extendable implementation.",,"Lahaye, Domenico|Mulckhuyse, Wouter",COMPEL-THE INTERNATIONAL JOURNAL FOR COMPUTATION AND MATHEMATICS IN ELECTRICAL AND ELECTRONIC ENGINEERING,sensitivity analysis|differential equations|computation|computer software|adjoint sensitivity|non-linear least squares problems|multiphysics finite elements software,10.1108/03321641211209780
56,WOS:000245389900003,2007,A non-intrusive stochastic Galerkin approach for modeling uncertainty propagation in deformation processes,PARTIAL-DIFFERENTIAL-EQUATIONS|CONTINUUM SENSITIVITY METHOD|METAL-FORMING PROCESS|POLYNOMIAL CHAOS|DESIGN,"Large deformation processes are inherently complex considering the non-linear phenomena that need to be accounted for. Stochastic analysis of these processes is a formidable task due to the numerous sources of uncertainty and the various random input parameters. As a result, uncertainty propagation using intrusive techniques requires tortuous analysis and overhaul of the internal structure of existing deterministic analysis codes. In this paper, we present an approach called non-intrusive stochastic Galerkin (NISG) method, which can be directly applied to presently available deterministic legacy software for modeling deformation processes with minimal effort for computing the complete probability distribution of the underlying stochastic processes. The method involves finite element discretization of the random support space and piecewise continuous interpolation of the probability distribution function over the support space with deterministic function evaluations at the element integration points. For the hyperelastic-viscoplastic large deformation problems considered here with varying levels of randomness in the input and boundary conditions, the NISG method provides highly accurate estimates of the statistical quantities of interest within a fraction of the time required using existing Monte Carlo methods. (c) ", Elsevier Ltd. All rights reserved.,"Acharjee, Swagato|Zabaras, Nicholas",COMPUTERS & STRUCTURES,uncertainty|deformation processes|stochastic galerkin methods|stochastic modeling,10.1016/j.compstruc.2006.10.004
57,WOS:000275585100011,2010,Development of expert system modeling based decision support system for swine manure management,LOSSES,"Animal waste has always been considered as a resource for agricultural input as biofertilizer. However, the management is becoming more stringent due to environmental regulations. Livestock producers are faced with different manure management options that may be implemented into their operations. Given the expansion of the livestock industry, the implementation of environmental regulations, and the increasing importance of social and health issues, the selection of optimal manure management systems is becoming a strategically important task. Increasingly, integrated decision support systems (DSSs) are becoming necessary to assist decision makers in their evaluation of different manure management alternatives, like, liquid system, semi-solid system, solid system and bio-gas or bio-energy system based on combinations of different manure management sub-systems (collection, storage and application). To address this situation, a user-friendly computer program called Integrated Swine Manure Management (ISMM) is being developed for the Canadian Prairie provinces. Decision criteria including environmental, agronomic, social and health, greenhouse gas emission, and economic factors have been considered for the selection. design, and operation of the DSS. The expert system modeling is based on Visual Basic programming. Decision on adopting a particular combination of systems components is based on performance rating of the overall system. The program is interactive so that weighting factors for the different decision criteria can be varied to suit site-specific considerations. In this paper, the systems approach for development of an integrated liquid manure management system is discussed. Using a case study, sensitivity analysis of different combinations of management components is also reported for systems performance. The decision software compared satisfactorily with other available DSS packages.", (C) 2010 Elsevier B.V. All rights reserved.,"Karmakar, S.|NKetia, M.|Lague, C.|Agnew, J.",COMPUTERS AND ELECTRONICS IN AGRICULTURE,manure management|systems engineering approach|expert system|decision support system (dss)|decision criteria,10.1016/j.compag.2009.12.009
58,WOS:000304453300004,2012,Standardized uncertainty analysis for hydrometry: a review of relevant approaches and implementation examples,,"The water-centric community has continuously made efforts to identify, assess and implement rigorous uncertainty analyses for routine hydrological measurements. This paper reviews some of the most relevant efforts and subsequently demonstrates that the Guide to the expression of uncertainty in measurement (GUM) is a good candidate for estimation of uncertainty intervals for hydrometry. The demonstration is made by implementing the GUM to typical hydrometric applications and comparing the analysis results with those obtained using the Monte Carlo method. The results show that hydrological measurements would benefit from the adoption of the GUM as the working standard, because of its soundness, the availability of software for practical implementation and potential for extending the GUM to hydrological/hydraulic numerical simulations.",,"Muste, Marian|Lee, Kyutae|Bertrand-Krajewski, Jean-Luc",HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES,uncertainty analysis|hydrometric measurements|monte carlo|uncertainty estimation|sensitivity analysis,10.1080/02626667.2012.675064
59,WOS:000365335000031,2016,A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes,MULTIOBJECTIVE OPTIMIZATION|EVOLUTIONARY ALGORITHMS|FLOWSHEET OPTIMIZATION|PROCESS SIMULATORS|GENETIC ALGORITHM|DESIGN|METHODOLOGY|PLANTS|PERSPECTIVES|INTEGRATION,"Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab (TM) is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented. (C) ", Elsevier Ltd. All rights reserved.,"Otte, Daniel|Lorenz, Hilke-Marie|Repke, Jens-Uwe",COMPUTERS & CHEMICAL ENGINEERING,optimization|molecular-inspired parallel tempering|multicomponent separation process|retrofit|stochastic algorithm|chemcad (tm),10.1016/j.compchemeng.2015.08.023
60,WOS:000264171200019,2009,A Life Cycle Comparison of Alternative Cheese Packages,,"A comparative life cycle assessment (LCA) between three different cheese packages (P: completely polypropylene (PP), P: tin and polyethylene (PE), and P: carton and PE) has been carried out for the production, distribution and waste disposal (% landfill) phase. A package for  kg of cheese was selected as the functional unit. SimaPro software (PReConsultants, The Netherlands) was used for the LCA study. The EcoIndicator  method was selected for comparison of the packages. The comparisons show that the total environmental performance of the cheese package types in order from worst to best is P, P, and P. This conclusion was supported by a sensitivity analysis, which was conducted by using different impact assessment methods.",,"Banar, Muefide|Cokaygil, Zerrin",CLEAN-SOIL AIR WATER,ecoindicator 99|food technology|landfilling|life cycle assessment|packaging|simapro7,10.1002/clen.200700185
61,WOS:000413886300011,2017,Process simulation and techno economic analysis of renewable diesel production via catalytic decarboxylation of rubber seed oil - A case study in Malaysia,TECHNOECONOMIC ASSESSMENT,"This work describes the economic feasibility of hydroprocessed diesel fuel production via catalytic decarboxylation of rubber seed oil in Malaysia. A comprehensive techno-economic assessment is developed using Aspen HYSYS V. software for process modelling and economic cost estimates. The profitability profile and, minimum fuels selling price of this synthetic fuels production using rubber seed oil as biomass feedstock are assessed under a set of assumptions for what can be plausibly be achieved in -years framework. In this study, renewable diesel processing facility is modelled to be capable of processing , L of inedible oil per day and producing a total of  million litre of renewable diesel product per annual with assumed annual operational days of . With the forecasted renewable diesel retail price of . RM per kg, the pioneering renewable diesel project investment offers an assuring return of investment of .% and net return as high as . million RM. Sensitivity analysis conducted showed that renewable diesel production cost is most sensitive to rubber seed oil price and hydrogen gas price, reflecting on the relative importance of feedstock prices in the overall profitability profile. (C) ", Elsevier Ltd. All rights reserved.,"Cheah, Kin Wai|Yusup, Suzana|Singh, Haswin Kaur Gurdeep|Uemura, Yoshimitsu|Lam, Hon Loong",JOURNAL OF ENVIRONMENTAL MANAGEMENT,techno-economic|catalytic decarboxylation|rubber seed oil|renewable diesel|minimum fuel selling price|profitability profile,10.1016/j.jenvman.2017.05.053
62,WOS:000175661100006,2002,MIOS: A decision tool for determination of optimal irrigated cropping pattern of a multicrop system under water scarcity constraint,DEFICIT IRRIGATION|ALLOCATION|OPTIMIZATION|MODEL,"This paper develops a dynamic programming optimisation model that considers the competition of crops in a season, both for irrigation water and cultivated area. Decision variables are the cultivated area and water allocated to each crop. The objective function of the model is based on crop-water production functions, production costs and crop prices. The model is solved using the CSUDP generalised dynamic programming package for conditions existing in northern Tunisia where a large fraction of the irrigation water is stored in a reservoir mainly during the winter period. The model gives the optimal distribution of area and water to each crop and the profit. Sensitivity analysis of the optimal solution to costs, area and changes in crop prices is carried out. The analysis results show that changes in prices and costs have a large impact on profits. Results indicate that the model can be a valuable tool for regional agencies or irrigation authorities in determining seasonal cropping pattern for a region at the beginning of the season."," Copyright (C) 2002 John Wiley Sons, Ltd.","Kipkorir, EC|Sahli, A|Raes, D",IRRIGATION AND DRAINAGE,dynamic programming|optimisation|cropping pattern|multicrop system|tunisia|irrigation,10.1002/ird.47
63,WOS:000383298800020,2016,Designing and implementing a multi-core capable integrated urban drainage modelling Toolkit:Lessons from CityDrain3,WASTE-WATER SYSTEM|CLIMATE-CHANGE|IDENTIFIABILITY ANALYSIS|SENSITIVITY-ANALYSIS|STORMWATER MODELS|STORAGE TANK|SEWER SYSTEM|CITY DRAIN|MANAGEMENT|UNCERTAINTY,"Integrated urban drainage modelling combines different aspects of the urban water system into a common framework. With increasing pressures of a changing climate, urban growth and economic constraints, the need for wider spread integration is necessary in the interest of a sustainable future. Greater complexity results in greater computational burden but modelling packages will, likewise, need to be flexible enough to allow incorporation of new algorithms. With advancements in modern information technology, a parallel implementation of such a modelling toolkit is mandatory while still leaving its users the flexibility of extensions. The design and implementation of the integrated modelling framework CityDrain shows that it is possible to write research code that is high-performance and extensible by many research projects. Three use case scenarios are presented to showcase the application of CityDrain. The performance advantage of parallelization (up to  times compared to its predecessor) and the scalability of the framework are also demonstrated. (C) ", Elsevier Ltd. All rights reserved.,"Burger, Gregor|Bach, Peter M.|Urich, Christian|Leonhardt, Gunther|Kleidorfer, Manfred|Rauch, Wolfgang",ADVANCES IN ENGINEERING SOFTWARE,integrated urban drainage|modelling|simulation framework|object-oriented design|multi-core|parallel computing,10.1016/j.advengsoft.2016.08.004
64,WOS:000087885500003,2000,Buckling design optimization of complex built-up structures with shape and size variables,SENSITIVITY ANALYSIS,"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper.",,"Gu, YX|Zhao, GZ|Zhang, HW|Kang, Z|Grandhi, RV",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,shape optimization|size optimization|buckling|natural frequency|dynamic response|built-up structures,10.1007/s001580050101
65,WOS:000236011600030,2006,A standard interface between simulation programs and systems analysis software,TRANSPORT PARAMETERS|AQUATIC SYSTEMS|UNCERTAINTY|MODELS|MINIMIZATION|FLOW,"A simple interface between simulation programs and systems analytical software is proposed. This interface is designed to facilitate linkage of environmental simulation programs with systems analytical software and thus can contribute to remedying the deficiency in applying systems analytical techniques to environmental modelling studies. The proposed concept, consisting of a text file interface combined with a batch mode simulation program call, is independent of model structure, operating system and programming language. It is open for implementation by academic and commercial simulation and systems analytical software developers and is very simple to implement. Its practicability is demonstrated by implementations for three environmental simulation packages (AQUASIM, SWAT and LEACHM) and two systems analytical program packages (UNCSIM, SUR). The properties listed above and the demonstration of the ease of implementation of the approach are prerequisites for the stimulation of a widespread implementation of the proposed interface that would be beneficial for the dissemination of systems analytical techniques in the environmental and engineering sciences. Furthermore, such a development could stimulate the transfer of systems analytical techniques between different fields of application.",,"Reichert, P",WATER SCIENCE AND TECHNOLOGY,systems analytical techniques|environmental simulation programs|statistical inference|sensitivity analysis|identifiability analysis|uncertainty analysis,10.2166/wst.2006.029
66,WOS:000301013200010,2012,"Automating calibration, sensitivity and uncertainty analysis of complex models using the R package Flexible Modeling Environment (FME): SWAT as an example",WATER ASSESSMENT-TOOL|RAINFALL-RUNOFF MODELS|HYDROLOGIC-MODELS|PARAMETER UNCERTAINTY|GLOBAL OPTIMIZATION|BAYESIAN-APPROACH|CATCHMENT MODELS|RIVER-BASIN|SOIL|PREDICTION,"Parameter optimization and uncertainty issues are a great challenge for the application of large environmental models like the Soil and Water Assessment Tool (SWAT), which is a physically-based hydrological model for simulating water and nutrient cycles at the watershed scale. In this study, we present a comprehensive modeling environment for SWAT, including automated calibration, and sensitivity and uncertainty analysis capabilities through integration with the R package Flexible Modeling Environment (FME). To address challenges (e.g., calling the model in R and transferring variables between Fortran and R) in developing such a two-language coupling framework, ) we converted the Fortran-based SWAT model to an R function (R-SWAT) using the RFortran platform, and alternatively ) we compiled SWAT as a Dynamic Link Library (DLL). We then wrapped SWAT (via R-SWAT) with FME to perform complex applications including parameter identifiability, inverse modeling, and sensitivity and uncertainty analysis in the R environment. The final R-SWAT-FME framework has the following key functionalities: automatic initialization of R, running Fortran-based SWAT and R commands in parallel, transferring parameters and model output between SWAT and R, and inverse modeling with visualization. To examine this framework and demonstrate how it works, a case study simulating streamflow in the Cedar River Basin in Iowa in the United Sates was used, and we compared it with the built-in auto-calibration tool of SWAT in parameter optimization. Results indicate that both methods performed well and similarly in searching a set of optimal parameters. Nonetheless, the R-SWAT-FME is more attractive due to its instant visualization, and potential to take advantage of other R packages (e.g., inverse modeling and statistical graphics). The methods presented in the paper are readily adaptable to other model applications that require capability for automated calibration, and sensitivity and uncertainty analysis.", Published by Elsevier Ltd.,"Wu, Yiping|Liu, Shuguang",ENVIRONMENTAL MODELLING & SOFTWARE,calibration|fme|monte carlo|r|sensitivity and uncertainty analysis|swat,10.1016/j.envsoft.2011.11.013
67,WOS:000404133200071,2017,Addressing Large-Scale Energy Retrofit of a Building Stock via Representative Building Samples: Public and Private Perspectives,COST-OPTIMALITY|MULTIOBJECTIVE OPTIMIZATION|RESIDENTIAL BUILDINGS|PERFORMANCE|IMPACT|CONSUMPTION|METHODOLOGY|CATEGORY|ENVELOPE|POLICIES,"Scientific literature about energy retrofit focuses on single buildings, but the investigation of whole building stocks is particularly worthy because it can yield substantial energy, environmental and economic benefits. Hence, how to address large-scale energy retrofit of existing building stocks? The paper handles this issue by employing a methodology that provides a robust energy analysis of building categories. This is denoted as SLABE, ""Simulation-based Large-scale uncertainty/sensitivity Analysis of Building Energy performance"". It was presented by the same authors and is here enhanced to investigate a whole and heterogeneous building stock that includes various categories. Each category is represented via a Representative Building Sample (RBS), which is defined through Latin hypercube sampling and uncertainty analysis. Hence, optimal retrofit packages are found in function of building location, intended use and construction type. Two families of optimal solutions are achieved. The first one collects the most energy-efficient (and thus sustainable) solutions, among the ones that produce global cost savings, thereby addressing the public perspective. The second one collects cost-optimal solutions thereby addressing the private perspective. EnergyPlus is employed as a simulation tool and coupled with MATLAB (R) for data analysis and processing. The methodology is applied to a significant share of the Italian public administration building stock, which includes several building categories depending on location, use destination and construction type. The outcomes show huge potential energy and economic savings, and could support a deep energy renovation of the Italian building stock.",,"Ascione, Fabrizio|Bianco, Nicola|De Stasio, Claudio|Mauro, Gerardo Maria|Vanoli, Giuseppe Peter",SUSTAINABILITY,building energy performance|energy simulations|building stock|retrofit|building sampling|representative building sample|large-scale analysis|cost-optimal|public incentives,10.3390/su9060940
68,WOS:000292418000035,2011,Quantification of model-form and predictive uncertainty for multi-physics simulation,,"Traditional uncertainty quantification in multi-physics design problems involves the propagation of parametric uncertainties in input variables such as structural or aerodynamic properties through a single, or series of models constructed to represent the given physical scenario. These models are inherently imprecise, and thus introduce additional sources of error to the design problem. In addition, there often exists multiple models to represent the given situation, and complete confidence in selecting the most accurate model among the model set considered is beyond the capability of the user. Thus, quantification of the errors introduced by this modeling process is a necessary step in the complete quantification of the uncertainties in multi-physics design problems. In this work, a modeling uncertainty quantification framework was developed to quantify to quantify both the model-form and predictive uncertainty in a design problem through the use of existing methods as well as newly developed modifications to existing methods in the literature. The applicability of this framework to a problem involving full-scale simulation was then demonstrated using the AGARD . Weakened Wing and three different aeroelastic simulation packages to quantify the flutter conditions of the wing. (C) ", Elsevier Ltd. All rights reserved.,"Riley, Matthew E.|Grandhi, Ramana V.",COMPUTERS & STRUCTURES,uncertainty quantification|model-form uncertainty|predictive uncertainty|multi-physics simulation,10.1016/j.compstruc.2010.10.004
69,WOS:000255505900032,2008,A sensitivity analysis of typical life insurance contracts with respect to the technical basis,,"In [Christiansen, M.C., . A sensitivity analysis concept for life insurance with respect to a valuation basis of infinite dimension. Insurance: Math. Econom. doi: ./j.insmatheco...] a sensitivity analysis concept was introduced for the prospective reserve of individual life insurance contracts as functional of the technical basis parameters such as interest rate, mortality probability, disability probability, et cetera. On the basis of that concept, the present paper gives in addition the sensitivities of the premium level. Applying these approaches, an extensive sensitivity analysis is carried out: A study of the basic life insurance contract types 'pure endowment insurance', 'temporary life insurance', 'annuity insurance' and 'disability insurance' identifies their diverse characteristics, in particular their weakest points concerning fluctuations of the technical basis. An investigation of combinations of these insurance contract types shows what synergy effects can be expected by creating insurance packages.", (C) 2007 Elsevier B.V. All rights reserved.,"Christiansen, Marcus C.",INSURANCE MATHEMATICS & ECONOMICS,life insurance|variations in the technical basis|prospective reserve|premium level|sensitivity analysis|insurance packages,10.1016/j.insmatheco.2007.08.005
70,WOS:000252516900010,2008,Self-adjoint sensitivity analysis of lossy dielectric structures with electromagnetic time-domain simulators,MICROWAVE IMAGE-RECONSTRUCTION|OPTIMAL-DESIGN METHOD|GRIDS,"We present an efficient self-adjoint approach for the computation of response derivatives in lossy inhomogencous structures with time-domain electromagnetic solvers. Our approach yields the responses and their derivatives with only one system analysis regardless of the number of optimizable parameters. The only requirement is to access the field solution at the perturbation grid points. The computation is performed as an independent post-process outside the solver. This makes our approach easy to implement as stand-alone software, which aids microwave design based on commercial computer-aided design packages. We show that our sensitivity analysis approach yields Jacobians of second-order accuracy for lossy dielectric structures. The approach is verified through -D, -D and -D examples using the time-domain field solutions obtained with solvers based on the finite-difference time-domain (FDTD) and transmission line modeling (TLM) methods."," Copyright (c) 2007 John Wiley & Sons, Ltd.","Song, Yunpeng|Li, Ying|Nikolova, Natalia K.|Bakr, Mohamed H.",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,time-domain analysis|sensitivity analysis|adjoint-variable methods|tlm method|fdtd method,10.1002/jnm.659
71,WOS:000322557200002,2013,Comparison of sediment transport computations using hydrodynamic versus hydrologic models in the Simiyu River in Tanzania,BED-LOAD TRANSPORT|SENSITIVITY-ANALYSIS|PARAMETERS|SWAT|TOOL,"This paper presents the results of a study that compares the sediment routing of the Simiyu River using the hydrologic model, Soil and Water Assessment Tool (SWAT) and the D hydrodynamic simulation software for Rivers and Estuaries (SOBEK-RE) model. Routing in SWAT is completed using the simplified Bagnold's equation and in the SOBEK-RE model is undertaken using the Saint Venant equation. The upstream boundary conditions for the routing modules were derived from the subcatchments sediment yields that were estimated by SWAT using the Modified Universal Soil Loss Equation (MUSLE). The sediment loads extrapolated or interpolated from the sediment rating curve for the catchment outlet were used for calibration and validation purposes. The SWAT model predicted an erosion rate of . Mt/yr. The total sediment load transported to the main outlet of the catchment simulated by the SWAT and SOBEK-RE models was equal to . and . Mt/yr, respectively. Thus the models computed a net erosion in the channels of . Mt/yr (SWAT) and . Mt/yr (SOBEK-RE). When comparing the results of the models for the different reaches of the main channel and main tributaries, the models showed different results both in magnitude and in sign (erosion/deposition). However, in a situation where data is scarce (such as grain size, channel geometry), the more complex hydrodynamic model does not necessarily lead to more reliable results. (c) ", Elsevier Ltd. All rights reserved.,"van Griensven, Ann|Popescu, Loana|Abdelhamid, M. R.|Ndomba, Preksedis Marco|Beevers, Lindsay|Betrie, Getnet D.",PHYSICS AND CHEMISTRY OF THE EARTH,sediment routing|sediment transport|swat|sober-re|simiyu river basin,10.1016/j.pce.2013.02.003
72,WOS:000327903400013,2013,Addressing ten questions about conceptual rainfall-runoff models with global sensitivity analyses in R,MULTIOBJECTIVE CALIBRATION|ENVIRONMENTAL-MODEL|CATCHMENT MODEL|CLIMATE-CHANGE|UNCERTAINTY|PERFORMANCE|INDEXES|PARAMETERS|AUSTRALIA|HYDROLOGY,"Sensitivity analysis (SA) is generally recognized as a worthwhile step to diagnose and remedy difficulties in identifying model parameters, and indeed in discriminating between model structures. An analysis of papers in three journals indicates that SA is a standard omission in hydrological modeling exercises. We provide some answers to ten reasonably generic questions using the Morris and Sobol SA methods, including to what extent sensitivities are dependent on parameter ranges selected, length of data period, catchment response type, model structures assumed and climatic forcing. Results presented demonstrate the sensitivity of four target functions to parameter variations of four rainfall runoff models of varying complexity (- parameters). Daily rainfall, streamflow and pan evaporation data are used from four -year data sets and from five catchments in the Australian Capital Territory (ACT) region. Similar results are obtained using the Morris and Sobol methods. It is shown how modelers can easily identify parameters that are insensitive, and how they might improve identifiability. Using a more complex objective function, however, may not result in all parameters becoming sensitive. Crucially, the results of the SA can be influenced by the parameter ranges selected. The length of data period required to characterize the sensitivities assuredly is a minimum of five years. The results confirm that only the simpler models have well-identified parameters, but parameter sensitivities vary between catchments. Answering these ten questions in other case studies is relatively easy using freely available software with the Hydromad and Sensitivity packages in R.", (C) 2013 Elsevier B.V. All rights reserved.,"Shin, Mun-Ju|Guillaume, Joseph H. A.|Croke, Barry F. W.|Jakeman, Anthony J.",JOURNAL OF HYDROLOGY,sensitivity analysis|rainfall-runoff model|identifiability,10.1016/j.jhydrol.2013.08.047
73,WOS:000272519900022,2009,Predicting physical properties of emerging compounds with limited physical and chemical data: QSAR model uncertainty and applicability to military munitions,OCTANOL/WATER PARTITION-COEFFICIENTS|RISK-ASSESSMENT|LOG-P|SOLUBILITY,"Reliable, up-front information on physical and biological properties of emerging materials is essential before making a decision and investment to formulate, synthesize, scale-up, test, and manufacture a new material for use in both military and civilian applications. Multiple quantitative structure-activity relationships (QSARs) software tools are available for predicting a material's physical/chemical properties and environmental effects. Even though information on emerging materials is often limited, QSAR software output is treated without sufficient uncertainty analysis. We hypothesize that uncertainty and variability in material properties and uncertainty in model prediction can be too large to provide meaningful results. To test this hypothesis, we predicted octanol water partitioning coefficients (log P) for multiple, similar compounds with limited physical-chemical properties using six different commercial log P calculators (KOWWIN, MarvinSketch, ACD/Labs, ALogP, CLogP, SPARC). Analysis was done for materials with largely uncertain properties that were similar, based on molecular formula, to military compounds (RDX, BTTN, TNT) and pharmaceuticals (Carbamazepine, Gemfibrizol). We have also compared QSAR modeling results for a well-studied pesticide and pesticide breakdown product (Atrazine, DDE). Our analysis shows variability due to structural variations of the emerging chemicals may be several orders of magnitude. The model uncertainty across six software packages was very high ( orders of magnitude) for emerging materials while it was low for traditional chemicals (e.g. Atrazine). Thus the use of QSAR models for emerging materials screening requires extensive model validation and coupling QSAR output with available empirical data and other relevant information.", Published by Elsevier Ltd.,"Bennett, Erin R.|Clausen, Jay|Linkov, Eugene|Linkov, Igor",CHEMOSPHERE,qsar|epi suite (tm)|explosives|rdx|tnt,10.1016/j.chemosphere.2009.09.003
