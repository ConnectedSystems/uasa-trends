,id,year,title,keywords,abstract,copyright,AU,SO,DE,DOI
0,WOS:000245063400008,2007,Parameter estimation and uncertainty analysis for a watershed model,RAINFALL-RUNOFF MODELS|SHUFFLED COMPLEX EVOLUTION|GROUNDWATER-FLOW MODEL|MONTE-CARLO METHODS|BAYESIAN-APPROACH|METROPOLIS ALGORITHM|PREDICTION INTERVALS|CATCHMENT MODELS|MARKOV-CHAINS|CALIBRATION,"Where numerical models are employed as an aid to environmental management, the uncertainty associated with predictions made by such models must be assessed. A number of different methods are available to make such an assessment. This paper explores the use of three such methods, and compares their performance when used in conjunction with a lumped parameter model for surface water flow (HSPF) in a large watershed. Linear (or first-order) uncertainty analysis has the advantage that it can be implemented with virtually no computational burden. While the results of such an analysis can be extremely useful for assessing parameter uncertainty in a relative sense, and ascertaining the degree of correlation between model parameters, its use in analyzing predictive uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods are far more robust, and can produce reliable estimates of parameter and predictive uncertainty. As well as this, they can provide the modeler with valuable qualitative information on the shape of parameter and predictive probability distributions; these shapes can be quite complex, especially where local objective function optima lie within those parts of parameter space that are considered probable after calibration has been undertaken. Nonlinear calibration-constrained optimization can also provide good estimates of parameter and predictive uncertainty, even in situations where the objective function surface is complex. Furthermore, they can achieve these estimates using far fewer model runs than MCMC methods. However, they do not provide the same amount of qualitative information on the probability structure of parameter space as do MCMC methods, a situation that can be partially rectified by combining their use with an efficient gradient-based search method that is specifically designed to locate different local optima. All methods of parameter and predictive uncertainty analysis discussed herein are implemented using freely-available software. Hence similar studies, or extensions of the present study, can be easily undertaken in other modeling contexts by other modelers. (c) ", Elsevier Ltd. All rights reserved.,"Gallagher, Mark|Doherty, John",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty analysis|parameter estimation|mathematical modeling|markov chain monte carlo|model calibration,10.1016/j.envsoft.2006.06.007
1,WOS:000089556100004,2000,Estimating labor productivity using probability inference neural network,ESTIMATING CONSTRUCTION PRODUCTIVITY,"This paper discusses the derivation of a probabilistic neural network classification model and its application in the construction industry. The probability inference neural network (PINN) model is based on the same concepts as those of the learning vector quantization method combined with a probabilistic approach. The classification and prediction networks are combined in an integrated network, which required the development of a different training and recall algorithm. The topology and algorithm of the developed model was presented and explained in detail. Portable computer software was developed to implement the training, testing, and recall for PINN. The PINN was tested on real historical productivity data at a local construction company and compared to the classic feedforward back-propagation neural network model. This showed marked improvement in performance and accuracy. In addition, the effectiveness of PINN for estimating labor production rates in the context of the application domain was validated through sensitivity analysis.",,"Lu, M|AbouRizk, SM|Hermann, UH",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2000)14:4(241)
3,WOS:000378360600027,2016,Operational snow mapping with simplified data assimilation using the seNorge snow model,WATER EQUIVALENT|COVERED AREA|SWISS ALPS|DEPTH|PREDICTION|CALIBRATION|CHALLENGES|RADIATION|NORWAY|SCHEME,"Frequently updated maps of snow conditions are useful for many applications, e.g., for avalanche and flood forecasting services, hydropower energy situation analysis, as well as for the general public. Numerical snow models are often applied in snow map production for operational hydrological services. However, inaccuracies in the simulated snow maps due to model uncertainties and the lack of suitable data assimilation techniques to correct them in near-real time may often reduce the usefulness of the snow maps in operational use. In this paper the revised seNorge snow model (v...) for snow mapping is described, and a simplified data assimilation procedure is introduced to correct detected snow model biases in near real-time. The data assimilation procedure is theoretically based on the Bayesian updating paradigm and is meant to be pragmatic with modest computational and input data requirements. Moreover, it is flexible and can utilize both point-based snow depth and satellite-based areal snow-covered area observations, which are generally the most common data-sources of snow observations. The model and analysis codes as well as the ""R"" statistical software are freely available. All these features should help to lower the challenges and hurdles hampering the application of data-assimilation techniques in operational hydrological modeling. The steps of the data assimilation procedure (evaluation, sensitivity analysis, optimization) and their contribution to significantly increased accuracy of the snow maps are demonstrated with a case from eastern Norway in winter /.", (C) 2016 Elsevier B.V. All rights reserved.,"Saloranta, Tuomo M.",JOURNAL OF HYDROLOGY,snow|modeling|snow mapping|data assimilation,10.1016/j.jhydrol.2016.03.061
5,WOS:000358997900004,2015,Sensitivity Analysis for Bayesian Hierarchical Models,LINEAR MIXED MODELS|LOCAL INFLUENCE|MARGINAL DENSITIES|INFERENCE|APPROXIMATIONS|PERTURBATION,"Prior sensitivity examination plays an important role in applied Bayesian analyses. This is especially true for Bayesian hierarchical models, where interpretability of the parameters within deeper layers in the hierarchy becomes challenging. In addition, lack of information together with identifiability issues may imply that the prior distributions for such models have an undesired influence on the posterior inference. Despite its importance, informal approaches to prior sensitivity analysis are currently used. They require repetitive re-fits of the model with ad-hoc modified base prior parameter values. Other formal approaches to prior sensitivity analysis suffer from a lack of popularity in practice, mainly due to their high computational cost and absence of software implementation. We propose a novel formal approach to prior sensitivity analysis, which is fast and accurate. It quantifies sensitivity without the need for a model re-fit. Through a series of examples we show how our approach can be used to detect high prior sensitivities of some parameters as well as identifiability issues in possibly over-parametrized Bayesian hierarchical models.",,"Roos, Malgorzata|Martins, Thiago G.|Held, Leonhard|Rue, Havard",BAYESIAN ANALYSIS,base prior|formal local sensitivity measure|bayesian robustness|calibration|hellinger distance|bayesian hierarchical models|identifiability|overparametrisation,10.1214/14-BA909
6,WOS:000321439500015,2013,Application of Bayesian Networks in Quantitative Risk Assessment of Subsea Blowout Preventer Operations,OFFSHORE SAFETY ASSESSMENT|RELIABILITY-ANALYSIS|ORGANIZATIONAL-FACTORS|FAULT-TREES|SYSTEMS|METHODOLOGY|FACILITIES|ACCIDENTS|SECURITY|PLATFORM,"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.",,"Cai, Baoping|Liu, Yonghong|Liu, Zengkai|Tian, Xiaojie|Zhang, Yanzhen|Ji, Renjie",RISK ANALYSIS,bayesian networks|quantitative risk assessment|subsea blowout preventer,10.1111/j.1539-6924.2012.01918.x
7,WOS:000368869200001,2016,A GUI platform for uncertainty quantification of complex dynamical models,RAINFALL-RUNOFF MODELS|GLOBAL SENSITIVITY MEASURES|AUTOMATIC CALIBRATION|OPTIMIZATION|INDEXES|DESIGN|MACHINE|OUTPUT,"Uncertainty quantification (UQ) refers to quantitative characterization and reduction of uncertainties present in computer model simulations. It is widely used in engineering and geophysics fields to assess and predict the likelihood of various outcomes. This paper describes a UQ platform called UQ-PyL (Uncertainty Quantification Python Laboratory), a flexible software platform designed to quantify uncertainty of complex dynamical models. UQ-PyL integrates different kinds of UQ methods, including experimental design, statistical analysis, sensitivity analysis, surrogate modeling and parameter optimization. It is written in Python language and runs on all common operating systems. UQ-PyL has a graphical user interface that allows users to enter commands via pull-down menus. It is equipped with a model driver generator that allows any computer model to be linked with the software. We illustrate the different functions of UQ-PyL by applying it to the uncertainty analysis of the Sacramento Soil Moisture Accounting Model. We will also demonstrate that UQ-PyL can be applied to a wide range of applications. (C)  The Authors.", Published by Elsevier Ltd.,"Wang, Chen|Duan, Qingyun|Tong, Charles H.|Di, Zhenhua|Gong, Wei",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification|design of experiments|sensitivity analysis|surrogate modeling|parameter optimization|uq-pyl,10.1016/j.envsoft.2015.11.004
8,WOS:000172361300016,2001,Including optimisation in the conception of fabric structures,SENSITIVITY ANALYSIS|VARIATIONAL APPROACH|ADJOINT SYSTEMS|OPTIMIZATION|DOMAIN,"For the conception of fabric structure, many specialised software have been developed in the last  years. Therefore, designing a fabric structure remains difficult, as textile behaviour is complex. To improve the static analysis stage, two new CAD tools have been developed and are presented in this paper. The first is a design sensitivity analysis module to give designers a better structural behaviour understanding. The second is an optimisation module, which determine the optimal adjustments leading to a structure satisfying the different requirements. All of these are illustrated by some examples.", (C) 2001 Civil-Comp Ltd. and Elsevier Science Ltd. All rights reserved.,"Sindel, F|Nouri-Baranger, T|Trompette, P",COMPUTERS & STRUCTURES,fabric structures|static analysis|design sensitivity analysis|optimisation,10.1016/S0045-7949(01)00079-7
10,WOS:000330916900008,2014,Detecting the causes of ill-conditioning in structural finite element models,,"In , version . of the finite element-based structural analysis package Oasys GSA was released. A new feature in this release was the estimation of the -norm condition number kappa() (K)= parallel to K parallel to()parallel to K- parallel to() of the stiffness matrix K of structural models by using a -norm estimation algorithm of Higham and Tisseur to estimate parallel to K- parallel to(). The condition estimate is reported as part of the information provided to engineers when they carry out linear/static analysis of models and a warning is raised if the condition number is found to be large. The inclusion of this feature prompted queries from users asking how the condition number impacted the analysis and, in cases where the software displayed an ill conditioning warning, how the ill conditioning could be ""fixed"". We describe a method that we have developed and implemented in the software that enables engineers to detect sources of ill conditioning in their models and rectify them. We give the theoretical background and illustrate our discussion with real-life examples of structural models to which this tool has been applied and found useful. Typically, condition numbers of stiffness matrices reduce from O(()) for erroneous models to O(()) or less for the corrected model. (C) ", Elsevier Ltd. All rights reserved.,"Kannan, Ramaseshan|Hendry, Stephen|Higham, Nicholas J.|Tisseur, Francoise",COMPUTERS & STRUCTURES,ill-conditioning|condition number|stiffness matrix|structural analysis|modelling errors,10.1016/j.compstruc.2013.11.014
11,WOS:000340977000075,2014,Groundwater fluxes in a shallow seasonal wetland pond: The effect of bathymetric uncertainty on predicted water and solute balances,MASS-BALANCE|DEPENDENT ECOSYSTEMS|LAKES|MODEL|DISCHARGE|STORAGE|RN-222|VOLUME|CALIBRATION|AUSTRALIA,"The successful management of groundwater dependent shallow seasonal wetlands requires a sound understanding of groundwater fluxes. However, such fluxes are hard to quantify. Water volume and solute mass balance models can be used in order to derive an estimate of groundwater fluxes within such systems. This approach is particularly attractive, as it can be undertaken using measurable environmental variables, such as; rainfall, evaporation, pond level and salinity. Groundwater fluxes estimated from such an approach are subject to uncertainty in the measured variables as well as in the process representation and in parameters within the model. However, the shallow nature of seasonal wetland ponds means water volume and surface area can change rapidly and non-linearly with depth, requiring an accurate representation of the wetland pond bathymetry. Unfortunately, detailed bathymetry is rarely available and simplifying assumptions regarding the bathymetry have to be made. However, the implications of these assumptions are typically not quantified. We systematically quantify the uncertainty implications for eight different representations of wetland bathymetry for a shallow seasonal wetland pond in South Australia. The predictive uncertainty estimation methods provided in the Model-Independent Parameter Estimation and Uncertainty Analysis software (PEST) are used to quantify the effect of bathymetric uncertainty on the modelled fluxes. We demonstrate that bathymetry can be successfully represented within the model in a simple parametric form using a cubic Sexier curve, allowing an assessment of bathymetric uncertainty due to measurement error and survey detail on the derived groundwater fluxes compared with the fixed bathymetry models. Findings show that different bathymetry conceptualisations can result in very different mass balance components and hence process conceptualisations, despite equally good fits to observed data, potentially leading to poor management decisions for the wetlands. Model predictive uncertainty increases with the crudity of the bathymetry representation, however, approximations that capture the general shape of the wetland pond such as a power law or Bezier curve show only a small increase in prediction uncertainty compared to the full dGPS surveyed bathymetry, implying these may be sufficient for most modelling purposes.", (C) 2014 Elsevier B.V. All rights reserved.,"Trigg, Mark A.|Cook, Peter G.|Brunner, Philip",JOURNAL OF HYDROLOGY,wetland ponds|bathymetry|uncertainty|pest|solute balance|bezier curve,10.1016/j.jhydrol.2014.06.020
12,WOS:000369512800012,2016,Integration of a Three-Dimensional Process-Based Hydrological Model into the Object Modeling System,JGRASS-NEWAGE SYSTEM|DISTRIBUTED MODEL|FRAMEWORK|ENERGY|BUDGETS|BASIN|WATER|TIME|FLOW|TERRAIN,"The integration of a spatial process model into an environmental modeling framework can enhance the model's capabilities. This paper describes a general methodology for integrating environmental models into the Object Modeling System (OMS) regardless of the model's complexity, the programming language, and the operating system used. We present the integration of the GEOtop model into the OMS version . and illustrate its application in a small watershed. OMS is an environmental modeling framework that facilitates model development, calibration, evaluation, and maintenance. It provides innovative techniques in software design such as multithreading, implicit parallelism, calibration and sensitivity analysis algorithms, and cloud-services. GEOtop is a physically based, spatially distributed rainfall-runoff model that performs three-dimensional finite volume calculations of water and energy budgets. Executing GEOtop as an OMS model component allows it to: () interact directly with the open-source geographical information system (GIS) uDig-JGrass to access geo-processing, visualization, and other modeling components; and () use OMS components for automatic calibration, sensitivity analysis, or meteorological data interpolation. A case study of the model in a semi-arid agricultural catchment is presented for illustration and proof-of-concept. Simulated soil water content and soil temperature results are compared with measured data, and model performance is evaluated using goodness-of-fit indices. This study serves as a template for future integration of process models into OMS.",,"Formetta, Giuseppe|Capparelli, Giovanna|David, Olaf|Green, Timothy R.|Rigon, Riccardo",WATER,watershed model|environmental modeling framework|automatic calibration|software integration,10.3390/w8010012
16,WOS:000315974500021,2013,Distributed computation of large scale SWAT models on the Grid,PERSPECTIVES|SYSTEMS|TOOL,"The increasing interest in larger spatial and temporal scale models and high resolution input data processing comes at a price of higher computational demand. This price is evidently even higher when common modeling routines such as calibration and uncertainty analysis are involved. Likewise, methods and techniques for reducing computation time in large scale socio-environmental modeling software is growing. Recent advancements in distributed computing such as Grid infrastructure have provided further opportunity to this effort. In the interest of gaining computational efficiency, we developed generic tools and techniques for enabling the Soil and Water Assessment Tool (SWAT) model application to run on the EGEE (Enabling Grids for E-science projects in Europe) Grid. Various program components/scripts were written to split a large scale hydrological model of the Soil and Water Assessment Tool (SWAT), to submit the split models to the Grid, and to collect and merge results into single output format. A three-step procedure was applied to take advantage of the Grid. Firstly, a python script was run in order to split the SWAT model into several sub-models. Then, individual sub-models were submitted in parallel for execution on the Grid. Finally, the outputs of the sub-basins were collected and the reach routing process was performed with another script executing a modified SWAT program. We conducted experimental simulations with multiple temporal and spatial scale hydrological models on the Grid infrastructure. Results showed that, in spite of computing overheads, parallel computation of socio-environmental models on the Grid is beneficial for model applications especially with large spatial and temporal scales. In the end, we conclude by recommending methods for further reducing computational overheads while running large scale model applications on the Grid. (c) ", Elsevier Ltd. All rights reserved.,"Yalew, S.|van Griensven, A.|Ray, N.|Kokoszkiewicz, L.|Betrie, G. D.",ENVIRONMENTAL MODELLING & SOFTWARE,distributed computing|grid computing|hydrological models|swat|gridification,10.1016/j.envsoft.2012.08.002
18,WOS:000187422700003,2004,MULINO-DSS: a computer tool for sustainable use of water resources at the catchment scale,EXAMPLE|DESIGN|MODELS,"MULINO, an ongoing project financed by the European Commission, has released the prototype of a Decision Support System software (mDSS) for the sustainable management of water resources at the catchment scale. The software integrates socio-economic and environmental modelling, with geo-spatial information and multi-criteria analysis. The policy background refers to the EU Water Framework Directive. The challenging multi-disciplinary context was approached by developing an innovative and dynamic implementation of the DPSIR framework, originally proposed by the European Environmental Agency. In mDSS integrated assessment modelling provides the values of quantitative indicators to be used for transparent and participated decisions, through the application of value functions, weights and decision rules chosen by the end user. Simple routines for the sensitivity analysis and comparison of alternative weight vectors also provides effective decision support by exploring and finding compromises between conflicting interests/perspectives in a multi-stakeholder context.", (C) 2003 Published by Elsevier B.V on behalf of IMACS.,"Giupponi, C|Mysiak, J|Fassio, A|Cogan, ",MATHEMATICS AND COMPUTERS IN SIMULATION,dss software|water resources|sustainable use|catchment|modelling,10.1016/j.matcom.2003.07.003
19,WOS:000188167100002,2003,Uncertainty analysis of hydrologic and water quality predictions for a small watershed using SWAT2000,SOIL HYDRAULIC-PROPERTIES|RECHARGE RATE ESTIMATION|SPATIAL VARIABILITY|ARID ENVIRONMENTS|SOLUTE TRANSPORT|RISK-ASSESSMENT|PART 1|MODEL|CONSTRAINTS|VARIABLES,"Hydrologic and water quality (H/WQ) models are being used with increasing frequency to devise alternative pollution control strategies. It has been recognized that such models may have a large degree of uncertainty associated with their predictions, and that this uncertainty can significantly impact the utility of the model. In this study, ARRAMIS (Advanced Risk & Reliability Assessment Model) software package was used to analyze the uncertainty of the SWAT (Soil and Water Assessment Tool) outputs concerning nutrients and sediment losses from agricultural lands. ARRAMIS applies Monte Carlo simulation technique connected with Latin hypercube sampling (LHS) scheme. This technique is applied to the Warner Creek watershed located in the Piedmont physiographic region of Maryland, and it provides an interval estimate of a range of values with an associated probability instead of a point estimate of a particular pollutant constituent. Uncertainty of model outputs was investigated using LHS scheme with restricted pairing for the model input sampling. Probability distribution functions (pdfs) for each of the  model simulations were constructed from these results. Model output distributions of interest in this analysis were stream flow, sediment, organic nitrogen (organic-N), organic phosphorus (organic-P), nitrate, ammonium, and mineral phosphorus (mineral-P) transported with water. Developed probability distribution functions for the model provided information with desirable probability. Results indicate that consideration of input parameter uncertainty produces % less mean stream flow along with approximately .% larger sediment loading than obtained using mean input parameters. On the contrary, mean of outputs regarding nutrients such as nitrate, ammonia, organic-N, and organic-P (but not mineral-P) were almost the same as the one using mean input parameters. The uncertainty in predicted stream flow and sediment loading is large, but that for nutrient loadings is the same as that of the corresponding input parameters. This study concluded that using a best possible distribution for the input parameters to reflect the impact of soils and land use diversity in a small watershed on SWAT model outputs may be more accurate than using average values for each input parameter.",,"Sohrabi, TM|Shirmohammadi, A|Chu, TW|Montas, H|Nejadhashemi, AP",ENVIRONMENTAL FORENSICS,uncertainty|swat2000|latin hypercube sampling|monte carlo|nonpoint pollution|nutrient,10.1080/714044368
20,WOS:000269046900012,2009,IPH-TRIM3D-PCLake: A three-dimensional complex dynamic model for subtropical aquatic ecosystems,,"This paper presents IPH-TRIMD-PCLake, a three-dimensional complex dynamic model for subtropical aquatic ecosystems. It combines a spatially explicit hydrodynamic model with a water-quality and biotic model of ecological interactions. The software, which is freely available for research purposes, has a graphical user-friendly interface and a flexible design that allows the user to vary the complexity of the model. It also has built-in analysis tools such as Monte Carlo sensitivity analysis, a genetic algorithm for calibration, and plotting tools. (C) ", Elsevier Ltd. All rights reserved.,"Fragoso, Carlos R.|van Nes, Egbert H.|Janse, Jan H.|Marques, David da Motta",ENVIRONMENTAL MODELLING & SOFTWARE,aquatic ecosystem model|cascading trophic effects|subtropical ecosystems|3d model,10.1016/j.envsoft.2009.05.006
21,WOS:000178643700005,2002,A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data,SYSTEMS,"In the construction of mathematical models from experimental data, it is possible to determine equations that model relations using several methodologies [Un nuevo algoritmo para la modelizacion de sistemas altamente estructurados (); Env. Model. Software  () ; Guide to Statistics , (); Regression models (); Ecosystems and Sustainable Development II ()]. These methodologies build equations that are in line with the experimental data and they analyse a dimension of adjustment and a dimension of error or distance between the experimental data and the data that is produced by the model. There are studies of sensitivity of the sample of data, as found by Bolado and Alonso [Proceedings SAMO ]. The authors consider that it is useful to obtain new parameters that relate the sensitivity of the equations to the variations that are produced by the experimental data. This will allow the selection of the model according to new criteria. On the one hand, the authors present a theoretical study of sensitivity of the models according to different points of view. On the other hand, they discuss a computing algorithm that allows the analysis of sensitivity (and stability) of the mathematical equations, which are built from any methodology. An interface has been incorporated into this algorithm to allow a graphic visualisation of the effects that are produced when modifications of the model are carried out.", (C) 2002 Elsevier Science Ltd. All rights reserved.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,sensitivity analysis|environmental modelling|monte carlo,10.1016/S0965-9978(02)00023-6
27,WOS:000251683400002,2007,Anisotropic finite element modeling for patient-specific mandible,AUTOMATIC MESH GENERATION|MEDICAL IMAGES|QUALITY ASSESSMENT|CANCELLOUS BONE|CORTICAL BONE|CT|ALGORITHM|CAD|SEGMENTS|NUMBERS,"This paper presents an ad hoc modular software tool to quasi-automatically generate patient-specific three-dimensional (D) finite element (FE) model of the human mandible. The main task is taking into account the complex geometry of the individual mandible, as well as the inherent highly anisotropic material law. At first, by computed tomography data (CT), the individual geometry of the complete range of mandible was well reproduced, also the separation between cortical and cancellous bone. Then, taking advantage of the inherent shape nature as 'curve' long bone, the algorithm employed a pair of B-spline curves running along the entire upper and lower mandible borders as auxiliary baselines, whose directions are also compatible with that of the trajectory of maximum material stiffness throughout the cortical bone of the mandible. And under the guidance of this pair of auxiliary baselines, a sequence of B-spline surfaces were interpolated adaptively as curve cross-sections to cut the original geometry. Following, based on the produced curve contours and the corresponding curve cross-section surfaces, quite well structured FE volume meshes were constructed, as well as the inherent trajectory vector fields of the anisotropic material (orthotropic for cortical bone and transversely isotropic for cancellous bone). Finally, a sensitivity analysis comprising various D FE simulations was carried out to reveal the relevance of elastic anisotropy for the load carrying behavior of the mandible.", (C) 2007 Elsevier Ireland Ltd. All rights reserved.,"Liao, Sheng-Hui|Tong, Ruo-Feng|Dong, Jin-Xiang",COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE,anisotropic material|automatic mesh generator (amg)|complete mandible model|patient-specific|finite element method,10.1016/j.cmpb.2007.09.009
28,WOS:000233938100033,2005,Utility of dynamic-landscape metapopulation models for sustainable forest management,BIRDS|INDICATORS|VIABILITY|FRAGMENTATION|BIODIVERSITY|FIRE,"We evaluated the utility of combining metapopulation models with landscape-level forest-dynamics models to assess the sustainability of forest management practices. We used the Brown Creeper (Certhia americana) in the boreal forests of northern Ontario as a case study. We selected the Brown Creeper as a potential indicator of sustainability because it is relatively common in the region but is dependent on snags and old trees for nesting and foraging; hence, it may be sensitive to timber harvesting. For the modeling we used RAMAS Landscape, a software package that integrates RAMAS GIS, population-modeling software, and LANDIS, forest-dynamics modeling software. Predictions about the future floristic composition and structure of the landscape tinder a variety of management and natural disturbance scenarios were derived using LANDIS. We modeled eight alternative forest management scenarios, ranging in intensity from no timber harvesting and a natural fire regime to intensive timber harvesting with salvage logging after fire. We predicted the response of The Brown Creeper metapopulation over a -year period and used future population size and expected minimum population size to compare the sustainability of the various management scenarios. The modeling methods were easy to apply and model predictions were sensitive to the differences among management scenarios, indicating that these methods may be useful for assessing and ranking the sustainability of forest management options. Primary concerns about the method are the practical difficulties associated with incorporating fire stochasticity in prediction uncertainty and the number of model assumptions that must be made and tested with sensitivity analysis. We wrote new software to bell) quantify the contribution of landscape stochasticity to model prediction uncertainty.",,"Wintle, BA|Bekessy, SA|Venier, LA|Pearce, JL|Chisholm, RA",CONSERVATION BIOLOGY,brown creeper|landscape ecology|population model|population viability analysis|succession model,10.1111/j.1523-1739.2005.00276.x
29,WOS:000366034200088,2015,Comprehensive Sensitivity Analysis in NLP Models in PSE Applications Using Space-Filling DOE Strategy,OPTIMIZATION|DESIGN,"Sensitivity analysis is an integral step in the interpretation of the solutions of optimization models, particularly when there are uncertainties in the numerical values of model parameters. Conventional approaches to sensitivity analysis rely on the use of shadow prices in linear models and Lagrange multipliers in non-linear models. Modern commercial optimization software packages are able to automatically generate such sensitivity coefficients to allow rapid post-optimality analysis. However, in the case of non-linear models, Lagrange multipliers have two distinct limitations. First, they represent only changes in the optimal value of an objective function with respect to small changes in parameter values, and thus remain valid only near the immediate vicinity of the nominal design point. Secondly, each Lagrange multiplier gives only the effect of the change of one parameter, assuming that all other parameters remain at their nominal values. Hence, they provide no information about joint effects or interactions caused by simultaneous changes in parameter values. In this paper, we present a strategy based on design of experiments (DOE) to generate a sensitivity surface, which we define as the mapping of the optimal model solution against a range of values of the optimization model parameters. Space-filling designs are used as a basis to generate proxy regression models with quadratic and interaction terms, in order to capture curvature of the sensitivity surface. The resulting proxy model contains more information than is available in conventional sensitivity analysis. In particular, this approach shows curvature and interaction effects that are not reflected when Lagrange multipliers are used. We present case studies based on problems drawn from process systems engineering (PSE) literature to illustrate this comprehensive sensitivity analysis strategy.",,"Tan, Raymond R.|Aviso, Kathleen B.|Uy, Oscar M.|Varbanov, PS|Klemes, JJ|Alwi, SRW|Yong, JY|Liu, ","PRES15: PROCESS INTEGRATION, MODELLING AND OPTIMISATION FOR ENERGY SAVING AND POLLUTION REDUCTION",,10.3303/CET1545088
34,WOS:000349876500012,2015,"Understanding the Day Cent model: Calibration, sensitivity, and identifiability through inverse modeling",NITROUS-OXIDE EMISSIONS|EVALUATING PARAMETER IDENTIFIABILITY|DAYCENT ECOSYSTEM MODEL|SOIL ORGANIC-MATTER|ERROR REDUCTION|2 STATISTICS|AUTOMATIC CALIBRATION|PRODUCTION SYSTEMS|N2O EMISSIONS|WATER-FLOW,"The ability of biogeochemical ecosystem models to represent agro-ecosystems depends on their correct integration with field observations. We report simultaneous calibration of  DayCent model parameters using multiple observation types through inverse modeling using the PEST parameter estimation software. Parameter estimation reduced the total sum of weighted squared residuals by % and improved model fit to crop productivity, soil carbon, volumetric soil water content, soil temperature, NO, and soil NO- compared to the default simulation. Inverse modeling substantially reduced predictive model error relative to the default model for all model predictions, except for soil NO- and NH+. Post-processing analyses provided insights into parameter-observation relationships based on parameter correlations, sensitivity and identifiability. Inverse modeling tools are shown to be a powerful way to systematize and accelerate the process of biogeochemical model interrogation, improving our understanding of model function and the underlying ecosystem biogeochemical processes that they represent. (C)  The Authors. Published by", Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).,"Necpalova, Magdalena|Anex, Robert P.|Fienen, Michael N.|Del Grosso, Stephen J.|Castellano, Michael J.|Sawyer, John E.|Iqbal, Javed|Pantoja, Jose L.|Barker, Daniel W.",ENVIRONMENTAL MODELLING & SOFTWARE,daycent model|inverse modeling|pest|sensitivity analysis|parameter identifiability|parameter correlations,10.1016/j.envsoft.2014.12.011
35,WOS:000251865000019,2008,Synthetic microarray data generation with RANGE and NEMO,RECONSTRUCTION,"Motivation: For testing and sensitivity analysis purposes, it is beneficial to have known transcription networks of sufficient size and variability during development of microarray data and network deconvolution algorithms. Description of such networks in a simple language translatable to Systems Biology Markup Language would allow generation of model data for the networks. Results: Described herein is software (RANGE: RAndom Network GEnerator) to generate large random transcription networks in the NEMO (NEtwork MOtif) language. NEMO is recognized by a grammar for transcription network motifs using lex and yacc to output Systems Biology Markup Language models for either specified or randomized gene input functions. These models of known networks may be input to a biochemical simulator, allowing the generation of synthetic microarray data.",,"Long, James|Roth, Mitchell",BIOINFORMATICS,,10.1093/bioinformatics/btm529
36,WOS:000335707200019,2014,FReET: Software for the statistical and reliability analysis of engineering problems and FReET-D: Degradation module,LATIN HYPERCUBE SAMPLES|REINFORCEMENT CORROSION|INPUT VARIABLES|CONCRETE|SIZE|SIMULATION|CARBONATION|DURABILITY|FRACTURE|MODEL,"The objective of the paper is to present methods and software for the efficient statistical, sensitivity and reliability assessment of engineering problems. Attention is given to small-sample techniques which have been developed for the analysis of computationally intensive problems. The paper shows the possibility of ""randomizing"" computationally intensive problems in the manner of the Monte Carlo type of simulation. In order to keep the number of required simulations at an acceptable level, Latin Hypercube Sampling is utilized. The technique is used for both random variables and random fields. Sensitivity analysis is based on non-parametric rank-order correlation coefficients. Statistical correlation is imposed by the stochastic optimization technique - simulated annealing. A hierarchical sampling approach has been developed for the extension of the sample size in Latin Hypercube Sampling, enabling the addition of simulations to a current sample set while maintaining the desired correlation structure. The paper continues with a brief description of the user-friendly implementation of the theory within FReET commercial multipurpose reliability software. FReET-D software is capable of performing degradation modeling, in which a large number of reinforced concrete degradation models can be utilized under the main FReET software engine. Some of the interesting applications of the software are referenced in the paper. (C) ", Elsevier Ltd. All rights reserved.,"Novak, Drahomir|Vorechovsky, Miroslav|Teply, Bretislav",ADVANCES IN ENGINEERING SOFTWARE,statistical analysis|sensitivity|reliability|monte carlo simulation|latin hypercube sampling|simulated annealing|random fields|material degradation,10.1016/j.advengsoft.2013.06.011
37,WOS:000087626100009,2000,ECOP: an economic model to assess the willow short rotation coppice global profitability in a case of small scale gasification pathway in Belgium,WOOD,"This paper presents a software package developed to assess the economic profitability of an original way to produce renewable energy: the small scale gasification of willow cultivated as short rotation coppice (SRC) in Belgium. The theoretical bases of the model (process hypotheses and economic indicators) are firstly presented together with the most relevant characteristics of the energy production route (SRC management and wood production, storage and conversion). A reference case is then defined which combines the most influencing parameters (reference interest rate, rotation length, subsidies, harvest mode, SRC yield, power of the electricity generator and annual production of electricity). A sensitivity analysis on these parameters highlighted that the project profitability, from the net present value point of view, is very sensitive to the reference interest rate, to the subsidies (of the conversion unit but probably also of any other kind of subsidies), to the SRC yield and to the generator power, all other parameters remaining constant. The rotation length has only a low influence, at least in the range of classic values ( to  years). To harvest the wood in stems (with delayed chipping) seems also to be the most interesting option.", (C) 2000 Elsevier Science Ltd. All rights reserved.,"Goor, F|Jossart, JM|Ledent, JF",ENVIRONMENTAL MODELLING & SOFTWARE,,10.1016/S1364-8152(00)00014-1
38,WOS:000256840200073,2007,"MEDOR, a didactic tool to support interpretation of bioassay data after internal contamination by actinides",UNCERTAINTIES|INHALATION|(PUO2)-PU-239|WORKERS|MODEL|LUNG,"A didactic software, MEthodes DOsimetriques de REference (MEDOR), is being developed to provide help in the interpretation of biological data. Its main purpose is to evaluate the pertinence of the application of different models. This paper describes its first version that is focused on inhalation exposure to actinide aerosols. With this tool, sensitivity analysis on different parameters of the ICRP models can be easily done for aerosol deposition, in terms of activity and particle number, actinide biokinetics and doses. The user can analyse different inhalation cases showing either that dose per unit intake cannot be applied if the aerosol contains a low number of particles or that an inhibition of the late pulmonary clearance by particle transport can occur which contributes to a - fold increase in effective dose as compared with application of default parameters. This underlines the need to estimate systematically the number of deposited particles, as well as to do chest monitoring as long as possible.",,"Miele, A.|Blanchin, N.|Raynaud, P.|Quesne, B.|Giraud, J. M.|Fottorino, R.|Berard, P.|Ansoborlo, E.|Franck, D.|Blanchardon, E.|Vathaire, C. Challeton-de|Lebaron-Jacobs, L.|Poncy, J. L.|Piechowski, J.|Fritsch, P.",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm288
39,WOS:000287437100013,2011,"NCNA: Integrated platform for constructing, visualizing, analyzing and sharing human-mediated nitrogen biogeochemical networks",,"Human alterations to the nitrogen (N) cycle are closely associated with global environmental and climate change. New tools are necessary to model and analyze the highly complex N cycles emerging from human-mediated ecosystems. We developed a new software. NCNA, to provide three functions: a) rigorous reconstruction of quasi-empirical models (QEMs), b) computer-aided interface for data collection and automatic sensitivity analysis, and c) automatic generation, visualization and network environ analysis (NEA) of N cycling networks. (c) ", Elsevier Ltd. All rights reserved.,"Min, Yong|Gong, Wei|Jin, Xiaogang|Chang, Jie|Gu, Baojing|Han, Zhen|Ge, Ying",ENVIRONMENTAL MODELLING & SOFTWARE,quasi-empirical model|reconstruction|visualization|network environ analysis|urbanization,10.1016/j.envsoft.2010.11.002
40,WOS:000385907200039,2016,MINFIT: A Spreadsheet-Based Tool for Parameter Estimation in an Equilibrium Speciation Software Program,SURFACE COMPLEXATION MODELS|URANIUM(VI) ADSORPTION|CONSISTENT MODEL|TITRATION DATA|LAYER MODEL|SORPTION|OXIDE|MONTMORILLONITE|PHOSPHATE|HEMATITE,"Determination of equilibrium constants describing chemical reactions in the aqueous phase and at solid-water interface relies on inverse modeling and parameter estimation. Although there are existing tools available, the steep learning curve prevents the wider community of environmental engineers and chemists to adopt those tools. Stemming from classical chemical equilibrium codes, MINEQL+ has been one of the most widely used chemical equilibrium software programs. We developed a spreadsheet-based tool, which we are calling MINFIT, that interacts with MINEQL+ to perform parameter estimations that optimize model fits to experimental data sets. MINFIT enables automatic and convenient screening of a large number of parameter sets toward the optimal solutions by calling MINEQL+ to perform iterative forward calculations following either exhaustive equidistant grid search or randomized search algorithms. The combined use of the two algorithms can securely guide the searches for the global optima. We developed interactive interfaces so that the optimization processes are transparent. Benchmark examples including both aqueous and surface complexation problems illustrate the parameter estimation and associated sensitivity analysis. MINFIT is accessible at http://minfit.strikingly.com.",,"Xie, Xiongfei|Giammar, Daniel E.|Wang, Zimeng",ENVIRONMENTAL SCIENCE & TECHNOLOGY,,10.1021/acs.est.6b03399
41,WOS:000295845900012,2011,Groundwater drawdown at Nankou site of Beijing Plain: model development and calibration,FLOW,"Water shortage and groundwater pollution have become two primary environmental concerns to Beijing since the s. The local aquifers, as the dominant sources for domestic and agricultural water supply, are depleting due to groundwater abstraction and continuous drought in recent years with rapid urbanization and increasing water consumption. Therefore, understanding the hydrogeological system is fundamental for a sustainable water resources management. In this article, the numerical analysis of a -D regional groundwater flow model for the Nankou area is presented. The hydrogeological system is reproduced according to sparsely distributed boreholes data. The numerical analysis is carried out using the scientific software OpenGeoSys, which is based on the finite element method. The model calibration and sensitivity analysis are accomplished with inverse methods by applying a model independent parameter estimation system (PEST). The results of the calibrated model show reasonable agreements with observed water levels. The transient groundwater flow simulations reflect the observed drawdown of the last  years and show the formation of a depression cone in an intensively pumped area.",,"Sun, Feng|Shao, Haibing|Kalbacher, Thomas|Wang, Wenqing|Yang, Zhongshan|Huang, Zhenfang|Kolditz, Olaf",ENVIRONMENTAL EARTH SCIENCES,groundwater modeling|opengeosys|pest|nankou,10.1007/s12665-011-0957-4
42,WOS:000295845300022,2011,Modeling the effects of completion techniques and formation heterogeneity on CO2 sequestration in shallow and deep saline aquifers,STORAGE,"This work studied the effect of completion techniques and reservoir heterogeneity on CO storage and injectivity in saline aquifers using a compositional reservoir simulator, CMG-GEM. Two reservoir models were built based on the published data to represent a deep saline aquifer and a shallow aquifer. The effect of various completion conditions on CO storage was then discussed, including partial perforation of the reservoir net pay (partial completion), well geometry, orientation, location, and length. The heterogeneity effect was addressed by considering three parameters: mean permeability, the vertical to horizontal permeability ratio, and permeability variation. Sensitivity analysis was carried out using iSIGHT software (design of experiments) to determine the dominant factors affecting CO storage capacity and injectivity. Simulation results show that the most favorable option is the perforation of all layers with horizontal wells - m long set in the upper layers. Mean permeability has the most effect on CO storage capacity and injectivity; k(v)/k(h) affects CO injectivity storage capacity more than permeability variation, V-k. More CO can be stored in the heterogeneous reservoirs with low mean permeability; however, high injectivity can be achieved in the uniform reservoirs with high mean permeability.",,"Yang, Fang|Bai, Baojun|Dunn-Norman, Shari",ENVIRONMENTAL EARTH SCIENCES,co2 sequestration|saline aquifer|completion|heterogeneity|reservoir simulation,10.1007/s12665-011-0908-0
43,WOS:000358060800012,2015,A software development framework for structural optimization considering non linear static responses,DYNAMIC TOPOLOGY OPTIMIZATION|EQUIVALENT LOADS,"In the real world, structural systems may not have linear static characteristics. However, structural optimization has been developed based on static responses because sensitivity analysis regarding static finite element analysis is developed quite well. Analyses other than static analysis are heavily required in the engineering community these days. Techniques for such analyses have been extensively developed and many software systems using the finite element method are easily available in the market. On the other hand, development of structural optimization using such analyses is fairly slow due to many obstacles. One obstacle is that it is very difficult and expensive to consider the nonlinearities or dynamic effects in the way of conventional optimization. Recently, the equivalent static loads method for non linear static response structural optimization (ESLSO) has been proposed for structural optimization with various responses: linear dynamic response, nonlinear static response, and nonlinear dynamic response. In ESLSO, finite element analysis other than static analysis is performed, equivalent static loads (ESLs) are generated, linear static response structural optimization is carried out with the ESLs and the process iterates. A software system for the automatic use of ESLSO is developed and described. One of the advantages of ESLSO is that it can use well developed commercial software systems for structural analysis and linear static response structural optimization. Various analysis and optimization systems are integrated in the developed system. The structure of the system is systematically defined and the software is developed by the C++ language on the Windows operating system.",,"Lee, Hyun-Ah|Park, Gyung-Jin",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization|equivalent static loads (esls)|equivalent static loads method for non linear static response structural optimization (eslso),10.1007/s00158-015-1228-x
46,WOS:000321813200001,2013,Modelling of groundwater infiltration into sewer systems,,"Groundwater infiltration into urban sewers represents a problem that influences costs and management of technical systems. The hydrodynamic groundwater software MODFLOW is used to analyse the influencing variables of the infiltration processes. Besides the hydraulic conductivity of the soil and the piezometric head in the vicinity of the sewer pipe, properties of the sewer trench, the shape and the size of leaks are important influencing factors. A non-linear-regression method is applied to develop a one-dimensional approach in accordance with the MODFLOW results and Darcy's law. Monte Carlo simulations and the developed one-dimensional model are used to assess the leak area and the range of pressure loss in the vicinity of the pipe leaks. By additional sensitivity analysis it was found that the infiltration factor and the conductivity of the backfill are very important for the calculation of the leak area.",,"Karpf, Christian|Krebs, Peter",URBAN WATER JOURNAL,infiltration|sewer|modelling|parameter|sensitivity|monte carlo simulations,10.1080/1573062X.2012.724077
47,WOS:000309496000040,2012,Estimation of surface shortwave radiation components under all sky conditions: Modeling and sensitivity analysis,PHOTOSYNTHETICALLY ACTIVE RADIATION|DISCRETE-ORDINATE-METHOD|SIMPLE PHYSICAL MODEL|SOLAR-RADIATION|INDEPENDENT PIXEL|GLOBAL IRRADIANCE|CLIMATE RESEARCH|MODIS DATA|SATELLITE|CLOUDS,"Clouds are the most important modulator of the amount of solar energy absorbed by the earth-atmosphere system. Traditional one-dimensional (D) plane-parallel atmospheric radiative transfer models which use the independent pixel approximation (IPA) can only consider two extreme conditions, i.e., either cloud-free or overcast cases. In this paper, two cloud fraction related factors (hemispherical effective cloud fraction and regional cloud fraction) are calculated and incorporated into MODTRAN  (one of the most popular radiative transfer packages) to simulate the surface shortwave radiation components and the top-of-atmosphere (TOA) radiance for all possible solar-cloud-viewing geometries. The accuracy of this modified solar radiative transfer model (named as MODTRAN-CF) is consistent with its prototype (MODTRAN ) which has been widely used and validated in radiative transfer modeling. Some field measurements are used to validate the superiority of MODTRAN-CF. For further understanding and simplifying of this physical model, a global sensitivity analysis (GSA) method is employed to analyze the effect of model parameters on each surface shortwave radiation component. Five parameters including solar zenith angle, surface albedo, hemispherical effective cloud fraction, ground altitude and atmospheric visibility show non-negligible impacts on almost all surface shortwave fluxes, which indicates that these five parameters should be carefully considered in the future modeling of the surface shortwave radiation fluxes. Two cloud optical thickness related parameters (cloud extinction coefficient and cloud thickness) exhibit obvious importance only under cloudy illumination condition especially with optically thin clouds. These findings on the improved model will enhance our knowledge on how to accurately model the surface shortwave radiation fluxes under all sky conditions.", (C) 2012 Elsevier Inc. All rights reserved.,"Chen, Ling|Yan, Guangjian|Wang, Tianxing|Ren, Huazhong|Calbo, Josep|Zhao, Jing|McKenzie, Richard",REMOTE SENSING OF ENVIRONMENT,modtran-cf|hemispherical effective cloud fraction|global sensitivity analysis,10.1016/j.rse.2012.04.006
52,WOS:000352642900010,2015,"A computational framework for dynamic data-driven material damage control, based on Bayesian inference and model selection",APPLICATIONS SYSTEMS|CREEP DAMAGE|SIMULATIONS|MECHANICS|GROWTH,"In the present study, a general dynamic data-driven application system (DDDAS) is developed for real-time monitoring of damage in composite materials using methods and models that account for uncertainty in experimental data, model parameters, and in the selection of the model itself. The methodology involves (i) data data from uniaxial tensile experiments conducted on a composite material; (ii) continuum damage mechanics based material constitutive models; (iii) a Bayesian framework for uncertainty quantification, calibration, validation, and selection of models; and (iv) general Bayesian filtering, as well as Kalman and extended Kalman filters. A software infrastructure is developed and implemented in order to integrate the various parts of the DDDAS. The outcomes of computational analyses using the experimental data prove the feasibility of the Bayesian-based methods for model calibration, validation, and selection. Moreover, using such DDDAS infrastructure for real-time monitoring of the damage and degradation in materials results in results in an improved prediction of failure in the system."," Copyright (C) 2014 John Wiley & Sons, Ltd.","Prudencio, E. E.|Bauman, P. T.|Faghihi, D.|Ravi-Chandar, K.|Oden, J. T.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,bayesian model selection|extended kalman filter|dynamic data-driven application systems|material damage,10.1002/nme.4669
53,WOS:000405522600004,2017,Estimation of vertical water fluxes from temperature time series by the inverse numerical computer program FLUX-BOT,FLOW|DIFFUSIVITY|HEAT,"The application of heat as a hydrological tracer has become a standard method for quantifying water fluxes between groundwater and surface water. The typical application is to estimate vertical water fluxes in the shallow subsurface beneath streams or lakes. For this purpose, time series of temperatures in the surface water and in the sediment are measured and evaluated by a vertical D representation of heat transport by advection and conduction. Several analytical solutions exist to calculate the vertical water flux from the measured temperatures. Although analytical solutions can be easily implemented, they are restricted to specific boundary conditions such as a sinusoidal upper temperature boundary. Numerical solutions offer higher flexibility in the selection of the boundary conditions. This, in turn, reduces the effort of data preprocessing, such as the extraction of the diurnal temperature variation from the raw data. Here, we present software to estimate water fluxes based on temperaturesFLUX-BOT. FLUX-BOT is a numerical code written in MATLAB that calculates vertical water fluxes in saturated sediments based on the inversion of measured temperature time series observed at multiple depths. FLUX-BOT applies a centred Crank-Nicolson implicit finite difference scheme to solve the one-dimensional heat advection-conduction equation. FLUX-BOT includes functions for the inverse numerical routines, functions for visualizing the results, and a function for performing uncertainty analysis. We present applications of FLUX-BOT to synthetic and to real temperature data to demonstrate its performance.",,"Munz, Matthias|Schmidt, Christian",HYDROLOGICAL PROCESSES,heat tracing|numerical solution|surface water groundwater interaction|temperature time series|vertical water flux,10.1002/hyp.11198
54,WOS:000407370700097,2017,Modeling Nitrogen Dynamics in a Waste Stabilization Pond System Using Flexible Modeling Environment with MCMC,SENSITIVITY-ANALYSIS|CONSTRUCTED WETLAND|WATER TREATMENT|PREDICTIVE UNCERTAINTY|NUTRIENT RECOVERY|GLUE METHODOLOGY|BAYESIAN METHOD|UNITED-STATES|REMOVAL|PERFORMANCE,"This study presents an approach for obtaining realization sets of parameters for nitrogen removal in a pilot-scale waste stabilization pond (WSP) system. The proposed approach was designed for optimal parameterization, local sensitivity analysis, and global uncertainty analysis of a dynamic simulation model for the WSP by using the R software package Flexible Modeling Environment (R-FME) with the Markov chain Monte Carlo (MCMC) method. Additionally, generalized likelihood uncertainty estimation (GLUE) was integrated into the FME to evaluate the major parameters that affect the simulation outputs in the study WSP. Comprehensive modeling analysis was used to simulate and assess nine parameters and concentrations of ON-N, NH-N and NO-N. Results indicate that the integrated FME-GLUE-based model, with good Nash-Sutcliffe coefficients (.-.) and correlation coefficients (.-.), successfully simulates the concentrations of ON-N, NH-N and NO-N. Moreover, the Arrhenius constant was the only parameter sensitive to model performances of ON-N and NH-N simulations. However, Nitrosomonas growth rate, the denitrification constant, and the maximum growth rate at  degrees C were sensitive to ON-N and NO-N simulation, which was measured using global sensitivity.",,"Mukhtar, Hussnain|Lin, Yu-Pin|Shipin, Oleg V.|Petway, Joy R.",INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH,flexiblemodeling environment|waste stabilization pond|nitrogen dynamic|parameterization|sensitivity|mcmc|glue|global uncertainty,10.3390/ijerph14070765
59,WOS:000326061300005,2013,"A methodology for optimal MSW management, with an application in the waste transportation of Attica Region, Greece",MUNICIPAL SOLID-WASTE|SYSTEM|GENERATION|COLLECTION|MODEL,"The paper describes a software system capable of formulating alternative optimal Municipal Solid Wastes (MSWs) management plans, each of which meets a set of constraints that may reflect selected objections and/or wishes of local communities. The objective function to be minimized in each plan is the sum of the annualized capital investment and annual operating cost of all transportation, treatment and final disposal operations involved, taking into consideration the possible income from the sale of products and any other financial incentives or disincentives that may exist. For each plan formulated, the system generates several reports that define the plan, analyze its cost elements and yield an indicative profile of selected types of installations, as well as data files that facilitate the geographic representation of the optimal solution in maps through the use of GIS. A number of these reports compare the technical and economic data from all scenarios considered at the study area, municipality and installation level constituting in effect sensitivity analysis. The generation of alternative plans offers local authorities the opportunity of choice and the results of the sensitivity analysis allow them to choose wisely and with consensus. The paper presents also an application of this software system in the capital Region of Attica in Greece, for the purpose of developing an optimal waste transportation system in line with its approved waste management plan. The formulated plan was able to: (a) serve  Municipalities and Communities that generate nearly  million t/y of comingled MSW with distinctly different waste collection patterns, (b) take into consideration several existing waste transfer stations (WTS) and optimize their use within the overall plan, (c) select the most appropriate sites among the potentially suitable (new and in use) ones, (d) generate the optimal profile of each WTS proposed, and (e) perform sensitivity analysis so as to define the impact of selected sets of constraints (limitations in the availability of sites and in the capacity of their installations) on the design and cost of the ensuing optimal waste transfer system. The results show that optimal planning offers significant economic savings to municipalities, while reducing at the same time the present levels of traffic, fuel consumptions and air emissions in the congested Athens basin. (C) ", Elsevier Ltd. All rights reserved.,"Economopoulou, M. A.|Economopoulou, A. A.|Economopoulos, A. P.",WASTE MANAGEMENT,municipal solid wastes|msw|optimal msw transportation|optimal msw treatment|optimal msw management|software for optimal msw management|regional msw management plans|national msw management plans,10.1016/j.wasman.2013.06.016
60,WOS:000285122400004,2010,Efficient solution for Galerkin-based polynomial chaos expansion systems,STOCHASTIC PROJECTION METHOD|FINITE-ELEMENT SYSTEMS|ITERATIVE SOLUTION|LINEAR-SYSTEMS|FLUID-FLOW|MODELS,"Iterative solvers and preconditioners are widely used for handling the linear system of equations arising from stochastic finite element method (SFEM) formulations, e.g. galerkin-based polynomial chaos (G-P-C) Expansion method. Especially, Preconditioned Conjugate Gradient (PCG) solver and the Incomplete Cholesky (IC) preconditioner are shown to be adequate choices within this context. In this study, approaches for the automated adjustment of the input parameters for these tools are to be introduced. The proposed algorithms aim to enable the use of the PCG solver and IC preconditioner in a black-box fashion. As a result, the requirement of the expertise for using these tools is removed to a certain extend. Furthermore, these algorithms can be used also for the implementation purposes of SFEM's within general purpose software by increasing the ease of the use of these tools and hence leading to an improved user-comfort. (C) ", Elsevier Ltd. All rights reserved.,"Panayirci, H. M.",ADVANCES IN ENGINEERING SOFTWARE,stochastic finite elements|polynomial chaos expansion|computational efficiency|iterative solvers|preconditioners|uncertainty quantification,10.1016/j.advengsoft.2010.09.004
67,WOS:000383298800020,2016,Designing and implementing a multi-core capable integrated urban drainage modelling Toolkit:Lessons from CityDrain3,WASTE-WATER SYSTEM|CLIMATE-CHANGE|IDENTIFIABILITY ANALYSIS|SENSITIVITY-ANALYSIS|STORMWATER MODELS|STORAGE TANK|SEWER SYSTEM|CITY DRAIN|MANAGEMENT|UNCERTAINTY,"Integrated urban drainage modelling combines different aspects of the urban water system into a common framework. With increasing pressures of a changing climate, urban growth and economic constraints, the need for wider spread integration is necessary in the interest of a sustainable future. Greater complexity results in greater computational burden but modelling packages will, likewise, need to be flexible enough to allow incorporation of new algorithms. With advancements in modern information technology, a parallel implementation of such a modelling toolkit is mandatory while still leaving its users the flexibility of extensions. The design and implementation of the integrated modelling framework CityDrain shows that it is possible to write research code that is high-performance and extensible by many research projects. Three use case scenarios are presented to showcase the application of CityDrain. The performance advantage of parallelization (up to  times compared to its predecessor) and the scalability of the framework are also demonstrated. (C) ", Elsevier Ltd. All rights reserved.,"Burger, Gregor|Bach, Peter M.|Urich, Christian|Leonhardt, Gunther|Kleidorfer, Manfred|Rauch, Wolfgang",ADVANCES IN ENGINEERING SOFTWARE,integrated urban drainage|modelling|simulation framework|object-oriented design|multi-core|parallel computing,10.1016/j.advengsoft.2016.08.004
69,WOS:000299324200005,2012,Yield improvement analysis with parameter-screening factorials,SENSITIVITY-ANALYSIS|SYSTEM|MODEL,"This paper presents a technique for the critical parameter analysis of the disk drive manufacturing process. The objective of the work is to improve the manufacturing yield by tuning the parameters that significantly affect the yield. Several techniques were studied including the sensitivity analysis framework, which is currently used at several disk drive plants. From our initial experiments, we found that the sensitivity analysis results were not sufficiently good and the interactions between parameters were not identified. We then designed a new technique based on factorial designs, the parameter-screening factorials algorithm. Our method can work with a large number of inputs within reasonable computing time, and can identify both the parameter and the interaction effects. The results can be obtained more quickly and are better in comparison with the currently used technique. Moreover, by applying the technique to the full list instead of the pre-selected list of the manufacturing parameters, we discovered that the parameters watch list previously identified by the experts should be adjusted to include some extra parameters. After the results were validated by the experts, we designed software that automates the critical parameter analysis process. The software should greatly benefit the daily yield analysis at the disk drive manufacturing plant greatly.", (C) 2011 Elsevier B.V. All rights reserved.,"Yamwong, Worraluk|Achalakul, Tiranee",APPLIED SOFT COMPUTING,critical parameter identification|yield improvement analysis|factorial designs,10.1016/j.asoc.2011.11.021
70,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, Jean-Luc",WATER SCIENCE AND TECHNOLOGY,calibration|epistemology|field data|modelling|sensitivity analysis|separate and combined sewers|stormwater|verification,10.2166/wst.2007.090
71,WOS:000414081000003,2017,Multi-scale equation of state computations for confined fluids,EQUILIBRIUM,"Fluid properties of five binary mixtures relevant to shale gas and light tight oil in confined nano-channels are studied. Canonical (NVT) Monte Carlo simulations are used to determine internal energies of departure of pure fluids using the RASPA software system (Dubbeldam et al., ). The linear mixing rule proposed by Lucia et al. () is used to determine internal energies of departure for mixtures, U-M(D), in confined spaces and compared to U-M(D) from direct NVT Monte Carlo simulation. The sensitivity of the mixture energy parameter, a(M), for the Gibbs-Helmholtz constrained (GHC) equation, confined fluid molar volume, V-M, and bubble point pressure are studied as a function of uncertainty in U-M(D). Results show that the sensitivity of confined fluid molar volume to % uncertainty in U-M(D) is less than % and that the GHC equation predicts physically meaningful reductions in bubble point pressure for light tight oils. (C) ", Elsevier Ltd. All rights reserved.,"Thomas, Edward|Lucia, Angelo",COMPUTERS & CHEMICAL ENGINEERING,confined fluids|monte carlo simulation|ghc equation of state|sensitivity analysis|bubble point pressure,10.1016/j.compchemeng.2017.05.028
72,WOS:000403211900001,2017,Epidemic model formulation and analysis for diarrheal infections caused by salmonella,SENSITIVITY-ANALYSIS|ECONOMIC BURDEN|UNCERTAINTY|DISEASE,"Epidemic modeling can be used to gain better understanding of infectious diseases, such as diarrhea. In the presented research, a continuous mathematical model has been formulated for diarrhea caused by salmonella. This model has been analyzed and simulated to be established in a functioning form. Elementary model analysis, such as working out the disease-free state and basic reproduction number, has been done for this model. The basic reproduction number has been calculated using the next generation matrix method. Stability analysis of the model has been done using the Routh-Hurwitz method. Sensitivity analysis and parameter estimation have been completed for the system too using MATLAB packages that work on the Latin Hypercube Sampling and Partial Rank Correlation Coefficient methods. It was established that as long as R- < , there will be no epidemic. Upon simulation using assumed parameter values, the results produced comprehended the epidemic theory and practical situations. The system was proven stable using the Routh-Hurwitz criterion and parameter estimation was successfully completed. Salmonella diarrhea has been successfully modeled and analyzed in this research. This model has been flexibly built and it can be integrated onto certain platforms to be used as a predictive system to prevent further infections of salmonella diarrhea.",,"Chaturvedi, Ojaswita|Jeffrey, Mandu|Lungu, Edward|Masupe, Shedden",SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL,salmonella|epidemic modeling|model analysis|routh-hurwitz|sensitivity analysis|parameter estimation,10.1177/0037549716685409
76,WOS:000404133200071,2017,Addressing Large-Scale Energy Retrofit of a Building Stock via Representative Building Samples: Public and Private Perspectives,COST-OPTIMALITY|MULTIOBJECTIVE OPTIMIZATION|RESIDENTIAL BUILDINGS|PERFORMANCE|IMPACT|CONSUMPTION|METHODOLOGY|CATEGORY|ENVELOPE|POLICIES,"Scientific literature about energy retrofit focuses on single buildings, but the investigation of whole building stocks is particularly worthy because it can yield substantial energy, environmental and economic benefits. Hence, how to address large-scale energy retrofit of existing building stocks? The paper handles this issue by employing a methodology that provides a robust energy analysis of building categories. This is denoted as SLABE, ""Simulation-based Large-scale uncertainty/sensitivity Analysis of Building Energy performance"". It was presented by the same authors and is here enhanced to investigate a whole and heterogeneous building stock that includes various categories. Each category is represented via a Representative Building Sample (RBS), which is defined through Latin hypercube sampling and uncertainty analysis. Hence, optimal retrofit packages are found in function of building location, intended use and construction type. Two families of optimal solutions are achieved. The first one collects the most energy-efficient (and thus sustainable) solutions, among the ones that produce global cost savings, thereby addressing the public perspective. The second one collects cost-optimal solutions thereby addressing the private perspective. EnergyPlus is employed as a simulation tool and coupled with MATLAB (R) for data analysis and processing. The methodology is applied to a significant share of the Italian public administration building stock, which includes several building categories depending on location, use destination and construction type. The outcomes show huge potential energy and economic savings, and could support a deep energy renovation of the Italian building stock.",,"Ascione, Fabrizio|Bianco, Nicola|De Stasio, Claudio|Mauro, Gerardo Maria|Vanoli, Giuseppe Peter",SUSTAINABILITY,building energy performance|energy simulations|building stock|retrofit|building sampling|representative building sample|large-scale analysis|cost-optimal|public incentives,10.3390/su9060940
77,WOS:000331916100026,2014,Life cycle assessment of corn-based ethanol production in Argentina,TILLAGE SYSTEMS|BIOENERGY PRODUCTION|UNITED-STATES|LAND-USE|EMISSIONS|BIOFUELS|IMPACTS|PAMPAS|ENERGY|DYNAMICS,"The promotion of biofuels as energy for transportation in the world is mainly driven by the perspective of oil depletion, the concerns about energy security and global warming. In Argentina, the legislation has imposed the use of biofuels in blend with fossil fuels ( to %) in the transport sector. The aim of this paper is to assess the environmental impact of corn-based ethanol production in the province of Santa Fe in Argentina based on the life cycle assessment methodology. The studied system includes from raw materials production to anhydrous ethanol production using dry milling technology. The system is divided into two subsystems: agricultural system and refinery system. The treatment of stillage is considered as well as the use of co-products (distiller's dried grains with solubles), but the use and/or application of the produced biofuel is not analyzed: a cradle-to-gate analysis is presented. As functional unit,  MJ of anhydrous ethanol at biorefinery is chosen. Two life cycle impact assessment methods are selected to perform the study: Eco-indicator  and ReCiPe. SimaPro is the life cycle assessment software used. The influence of the perspectives on the model is analyzed by sensitivity analysis for both methods. The two selected methods identify the same relevant processes. The use of fertilizers and resources, seeds production, harvesting process, corn drying, and phosphorus fertilizers and acetamide-anillide-compounds production are the most relevant processes in agricultural system. For refinery system, corn production, supplied heat and burned natural gas result in the higher contributions. The use of distiller's dried grains with solubles has an important positive environmental impact.", (C) 2013 Elsevier B.V. All rights reserved.,"Pieragostini, Carla|Aguirre, Pio|Mussati, Miguel C.",SCIENCE OF THE TOTAL ENVIRONMENT,life cycle assessment|corn-based ethanol|eco-indicator 99|recipe|sensitivity analysis|perspectives analysis,10.1016/j.scitotenv.2013.11.012
79,WOS:000259363300011,2008,Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment,EXPERT JUDGMENT|PROBABILITY-DISTRIBUTIONS|COMPREHENSIVE REALISM|INHALED FORMALDEHYDE|PHARMACOKINETIC DATA|CLIMATE-CHANGE|LUNG-CANCER|F344 RAT|INFORMATION|ELICITATION,"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure.",,"Small, Mitchell J.",RISK ANALYSIS,bayesian belief network|cancer risk assessment|distributional method|expert judgment|genotoxicity|mode of action|uncertainty analysis|weight of evidence,10.1111/j.1539-6924.2008.01134.x
80,WOS:000353971300004,2015,Enhancing the Characterization of Epistemic Uncertainties in PM2.5 Risk Analyses,PARTICULATE AIR-POLLUTION|LONG-TERM EXPOSURE|UNITED-STATES|FOLLOW-UP|6 CITIES|MORTALITY|FINE|COHORT|VETERANS|QUALITY,"The Environmental Benefits Mapping and Analysis Program (BenMAP) is a software tool developed by the U.S. Environmental Protection Agency (EPA) that is widely used inside and outside of EPA to produce quantitative estimates of public health risks from fine particulate matter (PM.). This article discusses the purpose and appropriate role of a risk analysis tool to support risk management deliberations, and evaluates the functions of BenMAP in this context. It highlights the importance in quantitative risk analyses of characterization of epistemic uncertainty, or outright lack of knowledge, about the true risk relationships being quantified. This article describes and quantitatively illustrates sensitivities of PM. risk estimates to several key forms of epistemic uncertainty that pervade those calculations: the risk coefficient, shape of the risk function, and the relative toxicity of individual PM. constituents. It also summarizes findings from a review of U.S.-based epidemiological evidence regarding the PM. risk coefficient for mortality from long-term exposure. That review shows that the set of risk coefficients embedded in BenMAP substantially understates the range in the literature. We conclude that BenMAP would more usefully fulfill its role as a risk analysis support tool if its functions were extended to better enable and prompt its users to characterize the epistemic uncertainties in their risk calculations. This requires expanded automatic sensitivity analysis functions and more recognition of the full range of uncertainty in risk coefficients.",,"Smith, Anne E.|Gans, Will",RISK ANALYSIS,benmap|epidemiology|health risk|pm2|5|risk analysis|uncertainty,10.1111/risa.12236
81,WOS:000227978000007,2005,"Investigating uncertainty and sensitivity in integrated, multimedia environmental models: tools for FRAMES-3MRA",FRAMEWORK|SYSTEM,"Elucidating uncertainty and sensitivity structures in environmental models can be a difficult task, even for low-order, single-medium constructs driven by a unique set of site-specific data. Quantitative assessment of integrated, multimedia models that simulate hundreds of sites, spanning multiple geographical and ecological regions, will ultimately require a comparative approach using several techniques, coupled with sufficient computational power. The Framework for Risk Analysis in Multimedia Environmental Systems - Multimedia, Multipathway, and Multireceptor Risk Assessment (FRAMES-MRA) is an important software model being developed by the United States Environmental Protection Agency for use in risk assessment of hazardous waste management facilities. The MRA modeling system includes a set of  science modules that collectively simulate release, fate and transport, exposure, and risk associated with hazardous contaminants disposed of in land-based waste management units (WMU). The MRA model encompasses  multi-dimensional input variables, over  of which are explicitly stochastic. Design of SuperMUSE, a  GHz PC-based, Windows-based Supercomputer for Model Uncertainty and Sensitivity Evaluation is described. Developed for MRA and extendable to other computer models, an accompanying platform-independent, Java-based parallel processing software toolset is also discussed. For MRA, comparison of stand-alone PC versus SuperMUSE simulation executions showed a parallel computing overhead of only . seconds/simulation, a relative cost increase of .% over average model runtime. Parallel computing software tools represent a critical aspect of exploiting the capabilities of such modeling systems. The Java toolset developed here readily handled machine and job management tasks over the Windows cluster, and is currently capable of completing over  million MRA model simulations per month on SuperMUSE. Preliminary work is reported for an example uncertainty analysis of Benzene disposal that describes the relative importance of various exposure pathways in driving risk levels for ecological receptors and human health. Incorporating landfills, waste piles, aerated tanks, surface impoundments, and land application units, the site-based data used in the analysis included  facilities across the United States representing  site-WMU combinations.", Published by Elsevier Ltd.,"Babendreier, JE|Castleton, KJ",ENVIRONMENTAL MODELLING & SOFTWARE,multimedia model|parallel computing|pc-based supercomputing|uncertainty analysis|sensitivity analysis|benzene disposal|java,10.1016/j.envsoft.2004.09.013
83,WOS:000317821300006,2013,Sensitivity Analysis of Multiple Informant Models When Data Are Not Missing at Random,MARITAL SATISFACTION|DROP-OUT|SYMPTOMS|ADOPTION|PARENT|CHILD,"Missing data are common in studies that rely on multiple informant data to evaluate relationships among variables for distinguishable individuals clustered within groups. Estimation of structural equation models using raw data allows for incomplete data, and so all groups can be retained for analysis even if only  member of a group contributes data. Statistical inference is based on the assumption that data are missing completely at random or missing at random. Importantly, whether or not data are missing is assumed to be independent of the missing data. A saturated correlates model that incorporates correlates of the missingness or the missing data into an analysis and multiple imputation that might also use such correlates offer advantages over the standard implementation of SEM when data are not missing at random because these approaches could result in a data analysis problem for which the missingness is ignorable. This article considers these approaches in an analysis of family data to assess the sensitivity of parameter estimates and statistical inferences to assumptions about missing data, a strategy that could be easily implemented using SEM software.",,"Blozis, Shelley A.|Ge, Xiaojia|Xu, Shu|Natsuaki, Misaki N.|Shaw, Daniel S.|Neiderhiser, Jenae M.|Scaramella, Laura V.|Leve, Leslie D.|Reiss, David",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,auxiliary variables|missing data|missing not at random|multiple imputation|multiple informant data,10.1080/10705511.2013.769393
86,WOS:000414958700005,2017,Spatial analysis and health risk assessment of heavy metals concentration in drinking water resources,MONTE-CARLO-SIMULATION|SURFACE-WATER|SENSITIVITY-ANALYSIS|GROUNDWATER|CONTAMINATION|POLLUTION|CHINA|RIVER|GIS|EXPOSURE,"The heavy metals available in drinking water can be considered as a threat to human health. Oncogenic risk of such metals is proven in several studies. Present study aimed to investigate concentration of the heavy metals including As, Cd, Cr, Cu, Fe, Hg, Mn, Ni, Pb, and Zn in  water supply wells and  water reservoirs within the cities Ardakan, Meibod, Abarkouh, Bafgh, and Bahabad. The spatial distribution of the concentration was carried out by the software ArcGIS. Such simulations as non-carcinogenic hazard and lifetime cancer risk were conducted for lead and nickel using Monte Carlo technique. The sensitivity analysis was carried out to find the most important and effective parameters on risk assessment. The results indicated that concentration of all metals in  wells (except iron in  cases) reached the levels mentioned in EPA, World Health Organization, and Pollution Control Department standards. Based on the spatial distribution results at all studied regions, the highest concentrations of metals were derived, respectively, for iron and zinc. Calculated HQ values for non-carcinogenic hazard indicated a reasonable risk. Average lifetime cancer risks for the lead in Ardakan and nickel in Meibod and Bahabad were shown to be . x (-), . x (-), and  x (-), respectively, demonstrating high carcinogenic risk compared to similar standards and studies. The sensitivity analysis suggests high impact of concentration and BW in carcinogenic risk.",,"Fallahzadeh, Reza Ali|Ghaneian, Mohammad Taghi|Miri, Mohammad|Dashti, Mohamad Mehdi",ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH,groundwater|metals|health risk assessment|monte carlo simulation|sensitivity analysis|geographic information systems,10.1007/s11356-017-0102-3
97,WOS:000247276500009,2007,Application of non-linear automatic optimization techniques for calibration of HSPF,GLOBAL OPTIMIZATION|MODELS,"Development of TMDLs (total maximum daily loads) is often facilitated by using the software system BASINS (Better Assessment Science Integrating point and Nonpoint Sources). One of the key elements of BASINS is the watershed model HSPF (Hydrological Simulation Program Fortran) developed by USEPA. Calibration of HSPF is a very tedious and time consuming task, more than  parameters are involved in the calibration process. In the current research, three non-linear automatic optimization techniques are applied and compared, as well an efficient way to calibrate HSPF is suggested. Parameter optimization using local and global optimization techniques for the watershed model is discussed. Approaches to automatic calibration of HSPF using the nonlinear parameter estimator PEST (Parameter Estimation Tool) with its Gauss-Marquardt-L evenberg (GMI-) method, Random multiple Search Method (RSM), and Shuffled Complex Evolution method developed at the University of Arizona (SCE-UA) are presented. Sensitivity analysis was conducted and the most and the least sensitive parameters were identified. It was noted that sensitivity depends on number of adjustable parameters. As more parameters were optimized simultaneously - a wider range of parameter values can maintain the model in the calibrated state. Impact of GML, RSM, and SCE-UA variables on ability to find the global minimum of the objective function (OF) was studied and the best variables are suggested. All three methods proved to be more efficient than manual HSPF calibration. Optimization results obtained by these methods are very similar, although in most cases RSM out performs GML and SCE-UA outperforms RSM. GML is a very fast method, it can perform as well as SCE-UA when the variables are properly adjusted, initial guess is good and insensitive parameters are eliminated from the optimization process. SCE-UA is very robust and convenient to use. Logical definition of key variables in most cases leads to the global minimum.",,"Iskra, Igor|Droste, Ronald",WATER ENVIRONMENT RESEARCH,calibration|hspf|pest|gauss-marquardt-levenberg method|sce-ua,10.2175/106143007X156862
100,WOS:000259464100004,2008,Improving user assessment of error implications in digital elevation models,ACCURACY ASSESSMENT|SPATIAL DATABASES|FUZZY-SETS|SLOPE|UNCERTAINTY|DEM|CONSEQUENCES|PREDICTION|SIMULATION|PROGRAM,"A digital representation of a terrain Surface is an approximation of reality and is inherently prone to some degree of error and uncertainty. Research in uncertainty analysis has produced a vast range of methods for investigating error and its propagation. However, the complex and varied methods proposed by researchers and academics create ambiguity for the dataset user. In this study, existing methods are combined and simplified to present a prototype tool to enable any digital elevation model (DEM) user to access and apply uncertainty analysis. The effect of correlated gridded DEM error is investigated, using stochastic conditional simulation to generate multiple equally likely representations of an actual terrain surface. Propagation of data uncertainty to the slope derivative, and the impact on a landslide susceptibility model are assessed. Two frameworks are developed to examine the probable and possible uncertainties in classifying the landslide hazard: probabilistic and fuzzy. The entire procedure is automated using publicly available software and user requirements are minimised. A case study example shows the resultant code can be used to quantify, visualise and demonstrate the propagation of error in a DEM. As a tool for uncertainty analysis the method can improve user assessment of error and its implications. (C) ", Elsevier Ltd. All rights reserved.,"Darnell, Amii R.|Tate, Nicholas J.|Brunsdon, Chris",COMPUTERS ENVIRONMENT AND URBAN SYSTEMS,error|uncertainty|stochastic simulation|digital elevation models|propagation,10.1016/j.compenvurbsys.2008.02.003
101,WOS:000383683800015,2016,Scalable subsurface inverse modeling of huge data sets with an application to tracer concentration breakthrough data from magnetic resonance imaging,COMPONENT GEOSTATISTICAL APPROACH|GENERALIZED COVARIANCE FUNCTIONS|HETEROGENEOUS POROUS-MEDIA|PARAMETER-ESTIMATION|UNCERTAINTY QUANTIFICATION|HYDRAULIC CONDUCTIVITY|TEMPORAL MOMENTS|EQUATIONS|TRANSPORT|SYSTEMS,"Characterizing subsurface properties is crucial for reliable and cost-effective groundwater supply management and contaminant remediation. With recent advances in sensor technology, large volumes of hydrogeophysical and geochemical data can be obtained to achieve high-resolution images of subsurface properties. However, characterization with such a large amount of information requires prohibitive computational costs associated with ""big data'' processing and numerous large-scale numerical simulations. To tackle such difficulties, the principal component geostatistical approach (PCGA) has been proposed as a ""Jacobian-free'' inversion method that requires much smaller forward simulation runs for each iteration than the number of unknown parameters and measurements needed in the traditional inversion methods. PCGA can be conveniently linked to any multiphysics simulation software with independent parallel executions. In this paper, we extend PCGA to handle a large number of measurements (e.g.,  or more) by constructing a fast preconditioner whose computational cost scales linearly with the data size. For illustration, we characterize the heterogeneous hydraulic conductivity (K) distribution in a laboratory-scale -D sand box using about  million transient tracer concentration measurements obtained using magnetic resonance imaging. Since each individual observation has little information on the K distribution, the data were compressed by the zeroth temporal moment of breakthrough curves, which is equivalent to the mean travel time under the experimental setting. Only about  forward simulations in total were required to obtain the best estimate with corresponding estimation uncertainty, and the estimated K field captured key patterns of the original packing design, showing the efficiency and effectiveness of the proposed method.",,"Lee, Jonghyun|Yoon, Hongkyu|Kitanidis, Peter K.|Werth, Charles J.|Valocchi, Albert J.",WATER RESOURCES RESEARCH,,10.1002/2015WR018483
102,WOS:000276271600005,2010,Application of generic data assimilation tools (DATools) for flood forecasting purposes,,"This paper describes the generic data assimilation software tool DATools. DATools can be used as standalone or within Delft-FEWS. DATools is completely configurable via XML configuration. DATools is built up of three components: a Filter, a Stochastic Modeler, and a Stochastic Observer. Configuration of all these three parts is explained in detail. At the moment two data assimilation filters are available within DATools: () ensemble Kalman Filter and () the residual resampling filter. Results of a twin experiment with both filters with DATtools show similar results as a previous study performed with custom implementations. It is also shown that DATools can function inside Delft-FEWS software used for operational flood forecasting. Applying EnKF to a D hydrodynamic SOBEK-RE model of the river Rhine within the operational system FEWS-NL Rhine and Meuse improves the forecasts at the Lobith gaugin station and downstream of Lobith. DATools has been coupled with the HBV-, SOBEK, and REW models and will be coupled to MODFLOW, Delft-D, and the geotechnical model MSetlle in the near future. Uncertainty analysis with this tool is also possible and calibration will be added later this year. (C) ", Elsevier Ltd. All rights reserved.,"Weerts, Albrecht H.|El Serafy, Ghada Y.|Hummel, Stef|Dhondia, Juzer|Gerritsen, Herman",COMPUTERS & GEOSCIENCES,data assimilation|hydrology|ensemble kalman filter|residual resampling filter|operational system|delft-fews,10.1016/j.cageo.2009.07.009
105,WOS:000168591400008,2001,MCE-RISK: integrating multicriteria evaluation and CIS for risk decision-making in natural hazards,GEOGRAPHICAL INFORMATION-SYSTEMS,"During the past two decades there have been a wide range of applications for decision-making linking multicriteria evaluation (MCE) and geographic information systems (GIS). However, limited literature reports the development of MCE-GIS software, and the comparison of various MCE-GIS approaches. This paper introduces an MCE-GIS program called MCE-RISK for risk-based decision-making. It consists of a series of modules for data standardisation, weighting, MCE-GIS methods. and sensitivity analysis. The program incorporates different MCE-GIS methods. including weighted linear combination (WLC), the technique for order preference by similarity to ideal solution (TOPSIS), and compromise programming (CP), enabling comparisons between different methods for the same decision problem to be made. An example of decision-making for determining priority areas for a bushfire hazard reduction burning is examined. After implementing the alternative MCE-GIS methods, and comparing final outputs and the computational difficulty involved in the analysis, WLC is recommended. Some caveats on using MCE-GIS methods art: also dis cussed. Although the development of MCE-RISK and its application reported in this paper are specific to risk-based decisionmaking in natural hazards, the program can be used for other environmental decision applications. such as environmental impact assessment and land-use planning."," (C) 2001 Elsevier Science Ltd, All rights reserved.","Chen, KP|Blong, R|Jacobson, C",ENVIRONMENTAL MODELLING & SOFTWARE,risk decision-making|multicriteria evaluation|cis|bushfire|prescribed burning,10.1016/S1364-8152(01)00006-8
107,WOS:000384333000001,2016,"Gulf war contamination assessment for optimal monitoring and remediation cost-benefit analysis, Kuwait",RAUDHATAIN,"Site characterization was performed on an area of  km() around the strategically vital freshwater aquifers of the Al-Rawdhatain and Umm Al-Aish to assess the status of groundwater pollution as the result of Iraq invasion to Kuwait in . Advanced data analysis and visualization software (EVS-Pro) was used for groundwater contamination assessment analytes: total petroleum hydrocarbon (TPH) and total dissolved solids (TDS). This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. Based on the ""minimum-maximum plume technology'' analysis, the nominal plume area with a threshold of . mg/kg TPH is estimated at about . km . This is the difference between the maximum and minimum predicted plume sizes. EVS-Pro also computed . x () and . x () for the plume volumes and masses (dollars per volume and mass), respectively. Also, new sampling locations were determined for further detailed site assessments based on the confidence and uncertainty analysis, which is more defensible and cost-optimized approach. This will reduce the number of samples needed (saves time and money) and provide a superior assessment of the analytes distribution. These tools prove to be effective in assessing remediation costs of clean-up versus benefits obtained and in developing a cost-effective monitoring programme for insights into processes controlling subsurface contaminant transport that impact water quality.",,"Yihdego, Yohannes|Al-Weshah, Radwan A.",ENVIRONMENTAL EARTH SCIENCES,visualization|data analysis|site investigation|plume|monitoring|remediation|hydrocarbon|clean-up cost|pollution,10.1007/s12665-016-6025-3
108,WOS:000303035400007,2012,Comparison of different uncertainty techniques in urban stormwater quantity and quality modelling,FORMAL BAYESIAN METHOD|SENSITIVITY-ANALYSIS|STREAMFLOW SIMULATION|HYDRAULIC-PROPERTIES|GLUE APPROACH|WATER|OPTIMIZATION|CALIBRATION|PARAMETER|QUANTIFICATION,"Urban drainage models are important tools used by both practitioners and scientists in the field of stormwater management. These models are often conceptual and usually require calibration using local datasets. The quantification of the uncertainty associated with the models is a must, although it is rarely practiced. The International Working Group on Data and Models, which works under the IWA/IAHR Joint Committee on Urban Drainage, has been working on the development of a framework for defining and assessing uncertainties in the field of urban drainage modelling. A part of that work is the assessment and comparison of different techniques generally used in the uncertainty assessment of the parameters of water models. This paper compares a number of these techniques: the Generalized Likelihood Uncertainty Estimation (GLUE), the Shuffled Complex Evolution Metropolis algorithm (SCEM-UA), an approach based on a multi-objective auto-calibration (a multialgorithm, genetically adaptive multi-objective method, AMALGAM) and a Bayesian approach based on a simplified Markov Chain Monte Carlo method (implemented in the software MICA). To allow a meaningful comparison among the different uncertainty techniques, common criteria have been set for the likelihood formulation, defining the number of simulations, and the measure of uncertainty bounds. Moreover, all the uncertainty techniques were implemented for the same case study, in which the same stormwater quantity and quality model was used alongside the same dataset. The comparison results for a well-posed rainfall/runoff model showed that the four methods provide similar probability distributions of model parameters, and model prediction intervals. For ill-posed water quality model the differences between the results were much wider; and the paper provides the specific advantages and disadvantages of each method. In relation to computational efficiency (i.e. number of iterations required to generate the probability distribution of parameters), it was found that SCEM-UA and AMALGAM produce results quicker than GLUE in terms of required number of simulations. However, GLUE requires the lowest modelling skills and is easy to implement. All non-Bayesian methods have problems with the way they accept behavioural parameter sets, e.g. GLUE, SCEM-UA and AMALGAM have subjective acceptance thresholds, while MICA has usually problem with its hypothesis on normality of residuals. It is concluded that modellers should select the method which is most suitable for the system they are modelling (e.g. complexity of the model's structure including the number of parameters), their skill/knowledge level, the available information, and the purpose of their study. (C) ", Elsevier Ltd. All rights reserved.,"Dotto, Cintia B. S.|Mannina, Giorgio|Kleidorfer, Manfred|Vezzaro, Luca|Henrichs, Malte|McCarthy, David T.|Freni, Gabriele|Rauch, Wolfgang|Deletic, Ana",WATER RESEARCH,urban drainage models|uncertainties|parameter probability distributions|bayesian inference|glue|scem-ua|mica|amalgam|mcmc|multi-objective auto-calibration,10.1016/j.watres.2012.02.009
110,WOS:000283094800003,2010,Uncertainty analysis for estimation of landfill emissions and data sensitivity for the input variation,,"Results of research and practical experience confirm that stabilization of GHG concentrations will require a tremendous effort. One of the sectors identified as a significant source of methane (CH()) emissions are solid waste disposal sites (SWDS). Landfills are the key source of CH() emissions in the emissions inventory of Slovakia, and the actual emission factors are estimated with a high uncertainty level. The calculation of emission uncertainty of the landfills using the more sophisticated Tier  Monte Carlo method is evaluated in this article. The software package that works with the probabilistic distributions and their combination was developed with this purpose in mind. The results, sensitivity analysis, and computational methodology of the CH() emissions from SWDS are presented in this paper.",,"Szemesova, J.|Gera, M.",CLIMATIC CHANGE,,10.1007/s10584-010-9919-1
112,WOS:000168416500008,2001,A computational methodology for shape optimization of structures in frictionless contact,SUPERCONVERGENT PATCH RECOVERY|SENSITIVITY ANALYSIS|FORMULATION|ALGORITHMS|SOLIDS,"This paper presents a computational methodology for shape optimization of structures in frictionless contact. which provides a basis for developing user-friendly and efficient shape optimization software. For evaluation it has been implemented as a subsystem of a general finite element software. The overall design and main principles of operation of this software are outlined. The parts connected to shape optimization are described in more detail. The key building blocks are: analytic sensitivity analysis, an adaptive finite element method, an accurate contact solver. and a sequential convex programing optimization algorithm. Results for three model application examples are presented, in which the contact pressure and the effective stress are optimized.", (C) 2001 Elsevier Science B.V. All rights reserved.,"Hilding, D|Torstenfelt, B|Klarbring, A",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,shape optimization|frictionless contact|finite element method|sensitivity analysis|adaptive meshing,10.1016/S0045-7825(00)00310-8
114,WOS:000241177900002,2006,Design parameterization and tool integration for CAD-based mechanism optimization,,"This paper presents an open and integrated tool environment that enables engineers to effectively search, in a CAD solid model form, for a mechanism design with optimal kinematic and dynamic performance. In order to demonstrate the feasibility of such an environment, design parameterization that supports capturing design intents in product solid models must be available, and advanced modeling, simulation, and optimization technologies implemented in engineering software tools must be incorporated. In this paper, the design parameterization capabilities developed previously have been applied to support design optimization of engineering products, including a High Mobility Multi-purpose Wheeled Vehicle (HMMWV). In the proposed environment, Pro/ENGINEER and SolidWorks are supported for product model representation, DADS (Dynamic Analysis and Design System) is employed for dynamic simulation of mechanical systems including ground vehicles, and DOT (Design Optimization Tool) is included for a batch mode design optimization. In addition to the commercial tools, a number of software modules have been implemented to support the integration; e.g., interface modules for data retrieval, and model update modules for updating CAD and simulation models in accordance with design changes. Note that in this research, the overall finite difference method has been adopted to support design sensitivity analysis. (C) ", Elsevier Ltd. All rights reserved.,"Chang, Kuang-Hua|Joo, Sung-Hwan",ADVANCES IN ENGINEERING SOFTWARE,design optimization|design parameterization|computer-aided design|dynamic simulation|tool integration,10.1016/j.advengsoft.2006.05.005
117,WOS:000400267300014,2017,Hierarchical approach to hydrological model calibration,WATER-QUALITY MODELS|UNCERTAINTY ANALYSIS|RIVER-BASIN|SWAT|OPTIMIZATION|FLOW|EQUIFINALITY|PREDICTIONS|SENSITIVITY|VALIDATION,"Hydrological models have been widely used for water resources management. Successful application of hydrological models depends on careful calibration and uncertainty analysis. Spatial unit of water balance calculations may differ widely in different models from grids to hydrological response units (HRU). The Soil and Water Assessment Tool (SWAT) software uses HRU as the spatial unit. SWAT simulates hydrological processes at sub-basin level by deriving HRUs by thresholding areas of soil type, land use, and slope combinations. This may ignore some important areas, which may have great impact on hydrological processes in the watershed. In this study, a hierarchical HRU approach was developed in order to increase model performance and reduce computational complexity simultaneously. For hierarchical optimization, HRUs are first divided into two-HRU types and are optimized with respect to some relevant influence parameters. Then, each HRU is further divided into two. Each child HRU inherits the optimum parameter values of the parent HRU as its initial value. This approach decreases the total calibration time while obtaining a better result. The performance of the hierarchical methodology is demonstrated on two basins, namely Sarisu-Eylikler and Namazgah Dam Lake Basins in Turkey. In Sarisu-Eylikler, we obtained good results by a combination of curve number (CN), soil hydraulic conductivity, and slope for generating HRUs, while in Namazgah use of only CN gave better results.",,"Ozdemir, Ayfer|Leloglu, Ugur Murat|Abbaspour, Karim C.",ENVIRONMENTAL EARTH SCIENCES,swat|calibration|hydrological response unit|sufi2|optimization,10.1007/s12665-017-6560-6
118,WOS:000413245200004,2017,A user-friendly software package for VIC hydrologic model development,GLOBAL SENSITIVITY-ANALYSIS|PRECIPITATION PRODUCTS|MULTISITE CALIBRATION|SOIL-MOISTURE|RUNOFF-MODEL|WATER|BASIN|SIMULATION|DATASET|FLUXES,"The Variable Infiltration Capacity (VIC) hydrologic and river routing model simulates the water and energy fluxes that occur near the land surface and provides useful information regarding the quantity and timing of available water within a watershed system. However, despite its popularity, wider adoption is hampered by the considerable effort required to prepare model inputs and calibrate the model parameters. This study presents a user-friendly software package, named VIC-Automated Setup Toolkit (VIC-ASSIST), accessible through an intuitive MATLAB graphical user interface. VIC-ASSIST enables users to navigate the model building process through prompts and automation, with the intention to promote the use of the model for practical, educational, and research purposes. The automated processes include watershed delineation, climate and geographical input set-up, model parameter calibration, sensitivity analysis, and graphical output generation. We demonstrate the package's utilities in various case studies. (C) ", Elsevier Ltd. All rights reserved.,"Wi, Sungwook|Ray, Patrick|Demaria, Eleonora M. C.|Steinschneider, Scott|Brown, Casey",ENVIRONMENTAL MODELLING & SOFTWARE,vic hydrologic model|vic setup assistant tool|matlab graphic user interface|automatic calibration|sensitivity analysis,10.1016/j.envsoft.2017.09.006
120,WOS:000388155500003,2016,GTApprox: Surrogate modeling for industrial design,SENSITIVITY-ANALYSIS|GAUSSIAN-PROCESSES|REGRESSION|REGULARIZATION|ALGORITHM|SELECTION|MACHINE|SPLINES|SAMPLES|EXPERTS,"We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. (C) ", Elsevier Ltd. All rights reserved.,"Belyaev, Mikhail|Burnaev, Evgeny|Kapushev, Ermek|Panov, Maxim|Prikhodko, Pavel|Vetrov, Dmitry|Yarotsky, Dmitry",ADVANCES IN ENGINEERING SOFTWARE,approximation|surrogate model|surrogate-based optimization,10.1016/j.advengsoft.2016.09.001
126,WOS:000270759400017,2009,Quantifying predictive uncertainty for a mountain-watershed model,HYDROLOGIC-MODELS|AUTOMATIC CALIBRATION|GLOBAL OPTIMIZATION|SWAT MODEL|VALIDATION|SENSITIVITY,"Watershed models require calibration before they are utilized as a decision-making tool. This paper describes a rigorous sensitivity analysis, automated parameter estimation and evaluation of prediction uncertainty for a Watershed Analysis Risk Management Framework (WARMF) model of the Turkey Creek Watershed. Sensitivity analysis was conducted using UCODE calibration and uncertainty-analysis software. Simulated stream flow is strongly sensitive to  of the  parameters evaluated: hydraulic conductivity, field capacity, total porosity, precipitation weighting factor, evaporation magnitude, evaporation skewness and snow melting rates; and parameter sensitivity is dependent on site-specific climate and soil conditions. Simulated stream flow matched observed stream flow fairly well with an R() value of ., Nash-Sutcliffe coefficient of efficiency (NSE) value of . and Root Mean Squared Error (RMSE) of . m()/s. The calibrated model was used to predict changes in stream flow that would result from changes in land use, including development of forested areas in parts of the watershed to commercial and residential areas. As expected, new development resulted in increased peak flows and reduced low flows. Uncertainty associated with all model parameters, including those not estimated by calibration by enhancing the parameter variance/covariance matrix, was considered when evaluating prediction uncertainties. Seventy percent of the time, predicted flows had uncertainties less than % with more of the uncertainty during low flow conditions.", (C) 2009 Elsevier B.V. All rights reserved.,"Geza, Mengistu|Poeter, Eileen P.|McCray, John E.",JOURNAL OF HYDROLOGY,sensitivity analysis|automatic calibration|prediction uncertainty|ucode|warmf,10.1016/j.jhydrol.2009.07.025
127,WOS:000405513900006,2017,Bayesian inference of earthquake parameters from buoy data using a polynomial chaos-based surrogate,NUMERICAL TIDAL MODEL|UNCERTAINTY QUANTIFICATION|DIFFERENTIAL-EQUATIONS|FRICTION COEFFICIENTS|TSUNAMI|SIMULATIONS,"This work addresses the estimation of the parameters of an earthquake model by the consequent tsunami, with an application to the Chile  event. We are particularly interested in the Bayesian inference of the location, the orientation, and the slip of an Okada-based model of the earthquake ocean floor displacement. The tsunami numerical model is based on the GeoClaw software while the observational data is provided by a single DARTa""c buoy. We propose in this paper a methodology based on polynomial chaos expansion to construct a surrogate model of the wave height at the buoy location. A correlated noise model is first proposed in order to represent the discrepancy between the computational model and the data. This step is necessary, as a classical independent Gaussian noise is shown to be unsuitable for modeling the error, and to prevent convergence of the Markov Chain Monte Carlo sampler. Second, the polynomial chaos model is subsequently improved to handle the variability of the arrival time of the wave, using a preconditioned non-intrusive spectral method. Finally, the construction of a reduced model dedicated to Bayesian inference is proposed. Numerical results are presented and discussed.",,"Giraldi, Loic|Le Maitre, Olivier P.|Mandli, Kyle T.|Dawson, Clint N.|Hoteit, Ibrahim|Knio, Omar M.",COMPUTATIONAL GEOSCIENCES,uncertainty quantification|bayesian inference|polynomial chaos expansion|noise model|low-rank representation|shallow water equation|tsunami|earthquake inversion,10.1007/s10596-017-9646-z
132,WOS:000411869000009,2017,A Scenario Based Impact Assessment of Trace Metals on Ecosystem of River Ganges Using Multivariate Analysis Coupled with Fuzzy Decision-Making Approach,WATER-QUALITY MANAGEMENT|HEAVY-METALS|INDIA|BASIN|FISH,"The growing consciousness about the health risks associated with environmental pollutants has brought a major shift in global concern towards prevention of hazardous/trace metals discharge in water bodies. Majority of these trace metals gets accumulated in the body of aquatic lives, which are considered as potential indicators of hazardous content. This results in an ecological imbalance in the form of poisoning, diseases and even death of fish and other aquatic lives, and ultimately affect humans through food chain. Trace metals such as Cd, Cr, Cu, Mn, Ni, Pb and Zn originated from various industrial operations containing metallic solutions and agricultural practices, have been contributing significantly to cause aquatic pollution. The present study develops a novel approach of expressing sustainability of river's ecosystem based on health of the fish by coupling fuzzy sensitivity analysis into multivariate analysis. A systematic methodology has been developed by generating monoplot, two dimensional biplot and rotated component matrix (using 'Analyze it' and 'SPSS' software), which can simultaneously identify critical trace metals and their industrial sources, critical sampling stations, and adversely affected fish species along with their interrelationships. A case study of assessing the impact of trace metals on the aquatic life of river Ganges, India has also been presented to demonstrate effectiveness of the model. The clusters pertaining to various water quality parameters have been identified using Principal Component Analysis (PCA) to determine actual sources of pollutants and their impact on aquatic life. The fuzzy sensitivity analysis reveals the cause-effect relationship of these critical parameters. The study suggests pollution control agencies to enforce appropriate regulations on the wastewater dischargers responsible for polluting river streams with a particular kind of trace metal(s).",,"Srinivas, R.|Singh, Ajit Pratap|Sharma, Rishikesh",WATER RESOURCES MANAGEMENT,aquatic ecosystem|fuzzy decision-making|impact assessment|multivariate analysis|river ecosystem|water quality,10.1007/s11269-017-1738-y
133,WOS:000241572400011,2006,Optimal control of open-channel flow using adjoint sensitivity analysis,MANNINGS ROUGHNESS COEFFICIENTS|CONTAMINANT RELEASES|HAZARD MITIGATION|WAVE CONTROL|IDENTIFICATION|OPTIMIZATION|RIVERS|MODEL,"An optimal flow control methodology based on adjoint sensitivity analysis for controlling nonlinear open channel flows with complex geometries is presented. The adjoint equations, derived from the nonlinear Saint-Venant equations, are generally capable of evaluating the time-dependent sensitivities with respect to a variety of control variables under complex flow conditions and cross-section shapes. The internal boundary conditions of the adjoint equations at a confluence (junction) derived by the variational approach make the flow control model applicable to solve optimal flow control problems in a channel network over a watershed. As a result, an optimal flow control software package has been developed, in which two basic modules, i.e., a hydrodynamic module and a bound constrained optimization module using the limited-memory quasi-Newton algorithm, are integrated. The effectiveness and applicability of this integrated optimal control tool are demonstrated thoroughly by implementing flood diversion controls in rivers, from one reach with a single or multiple floodgates (with or without constraints), to a channel network with multiple floodgates. This new optimal flow control model can be generally applied to make optimal decisions in real-time flood control and water resource management in a watershed.",,"Ding, Yan|Wang, Sam S. Y.",JOURNAL OF HYDRAULIC ENGINEERING-ASCE,open channel flow|sensitivity analysis|floods|geometry|optimization,10.1061/(ASCE)0733-9429(2006)132:11(1215)
134,WOS:000395529100004,2017,Monte Carlo Approach for Uncertainty Analysis of Acoustic Doppler Current Profiler Discharge Measurement by Moving Boat,ADCP VELOCITY|FRAMEWORK|VARIANCE,"This paper presents a method using Monte Carlo simulations for assessing uncertainty of moving-boat acoustic Doppler current profiler (ADCP) discharge measurements using a software tool known as QUant, which was developed for this purpose. Analysis was performed on  data sets from four Water Survey of Canada gauging stations in order to evaluate the relative contribution of a range of error sources to the total estimated uncertainty. The factors that differed among data sets included the fraction of unmeasured discharge relative to the total discharge, flow nonuniformity, and operator decisions about instrument programming and measurement cross section. As anticipated, it was found that the estimated uncertainty is dominated by uncertainty of the discharge in the unmeasured areas, highlighting the importance of appropriate selection of the site, the instrument, and the user inputs required to estimate the unmeasured discharge. The main contributor to uncertainty was invalid data, but spatial inhomogeneity in water velocity and bottom-track velocity also contributed, as did variation in the edge velocity, uncertainty in the edge distances, edge coefficients, and the top and bottom extrapolation methods. To a lesser extent, spatial inhomogeneity in the bottom depth also contributed to the total uncertainty, as did uncertainty in the ADCP draft at shallow sites. The estimated uncertainties from QUant can be used to assess the adequacy of standard operating procedures. They also provide quantitative feedback to the ADCP operators about the quality of their measurements, indicating which parameters are contributing most to uncertainty, and perhaps even highlighting ways in which uncertainty can be reduced. Additionally, QUant can be used to account for self-dependent error sources such as heading errors, which are a function of heading. The results demonstrate the importance of a Monte Carlo method tool such as QUant for quantifying random and bias errors when evaluating the uncertainty of moving-boat ADCP measurements.",,"Moore, Stephanie A.|Jamieson, Elizabeth C.|Rainville, Francois|Rennie, Colin D.|Mueller, David S.",JOURNAL OF HYDRAULIC ENGINEERING,moving-boat acoustic doppler current profiler (adcp)|uncertainty|monte carlo|probabilistic|stream gauging procedures,10.1061/(ASCE)HY.1943-7900.0001249
135,WOS:000417943600002,2017,Parameter sensitivity analysis of a 1-D cold region lake model for land-surface schemes,DIURNAL MIXED-LAYER|GLOBAL SENSITIVITY|CLIMATE MODELS|WATER-QUALITY|ENVIRONMENTAL SYSTEMS|UNCERTAINTY ANALYSES|HYDROLOGICAL MODELS|GENERAL-CIRCULATION|ORGANIC-MATTER|GREAT-LAKES,"Lakes might be sentinels of climate change, but the uncertainty in their main feedback to the atmosphere heat- exchange fluxes - is often not considered within climate models. Additionally, these fluxes are seldom measured, hindering critical evaluation of model output. Analysis of the Canadian Small Lake Model (CSLM), a one-dimensional integral lake model, was performed to assess its ability to reproduce diurnal and seasonal variations in heat fluxes and the sensitivity of simulated fluxes to changes in model parameters, i.e., turbulent transport parameters and the light extinction coefficient (K-d). A C++ open-source software package, Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), was used to perform sensitivity analysis (SA) and identify the parameters that dominate model behavior. The generalized likelihood uncertainty estimation (GLUE) was applied to quantify the fluxes' uncertainty, comparing daily-averaged eddy-covariance observations to the output of CSLM. Seven qualitative and two quantitative SA methods were tested, and the posterior likelihoods of the modeled parameters, obtained from the GLUE analysis, were used to determine the dominant parameters and the uncertainty in the modeled fluxes. Despite the ubiquity of the equifinality issue - different parameter-value combinations yielding equivalent results-the answer to the question was unequivocal: K-d, a measure of how much light penetrates the lake, dominates sensible and latent heat fluxes, and the uncertainty in their estimates is strongly related to the accuracy with which K-d is determined. This is important since accurate and continuous measurements of K-d could reduce modeling uncertainty.",,"Guerrero, Jose-Luis|Pernica, Patricia|Wheater, Howard|Mackay, Murray|Spence, Chris",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-21-6345-2017
138,WOS:000235218600050,2005,Comparison of deterministic and Monte Carlo methods in shielding design,,"In shielding calculation, deterministic methods have some advantages and also some disadvantages relative to other kind of codes, such as Monte Carlo. The main advantage is the short computer time needed to find solutions while the disadvantages are related to the often-used build-up factor that is extrapolated from high to low energies or with unknown geometrical conditions, which can lead to significant errors in shielding results. The aim of this work is to investigate how good are some deterministic methods to calculating low-energy shielding, using attenuation coefficients and build-up factor corrections. Commercial software MicroShield . has been used as the deterministic code while MCNP has been used as the Monte Carlo code. Point and cylindrical sources with slab shield have been defined allowing comparison between the capability of both Monte Carlo and deterministic methods in a day-by-day shielding calculation using sensitivity analysis of significant parameters, such as energy and geometrical conditions.",,"Oliveira, AD|Oliveira, C",RADIATION PROTECTION DOSIMETRY,,10.1093/prd/nci187
139,WOS:000168569900007,2001,COOPT - a software package for optimal control of large-scale differential-algebraic equation systems,,"This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential-algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals, The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering. (C)  IMACS.", Published by Elsevier Science B.V. All rights reserved.,"Serban, R|Petzold, LR",MATHEMATICS AND COMPUTERS IN SIMULATION,differential-algebraic equations|sensitivity analysis|optimal control|sequential quadratic programming methods,10.1016/S0378-4754(01)00289-0
140,WOS:000246812000012,2007,Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation,SURFACE PARAMETERIZATION SCHEMES|DISTRIBUTED MODEL|MULTIOBJECTIVE OPTIMIZATION|ENVIRONMENTAL SYSTEMS|HYDROLOGICAL MODELS|UNCERTAINTY|CALIBRATION|EFFICIENT|PROJECT|OUTPUT,"This study seeks to identify sensitivity tools that will advance our understanding of lumped hydrologic models for the purposes of model improvement, calibration efficiency and improved measurement schemes. Four sensitivity analysis methods were tested: () local analysis using parameter estimation software (PEST), () regional sensitivity analysis (RSA), () analysis of variance (ANOVA), and () Sobol's method. The methods' relative efficiencies and effectiveness have been analyzed and compared. These four sensitivity methods were applied to the lumped Sacramento soil moisture accounting model (SAC-SMA) coupled with SNOW-. Results from this study characterize model sensitivities for two medium sized watersheds within the Juniata River Basin in Pennsylvania, USA. Comparative results for the  sensitivity methods are presented for a -year time series with  h,  h, and  h time intervals. The results of this study show that model parameter sensitivities are heavily impacted by the choice of analysis method as well as the model time interval. Differences between the two adjacent watersheds also suggest strong influences of local physical characteristics on the sensitivity methods' results. This study also contributes a comprehensive assessment of the repeatability, robustness, efficiency, and ease-of-implementation of the four sensitivity methods. Overall ANOVA and Sobol's method were shown to be superior to RSA and PEST. Relative to one another, ANOVA has reduced computational requirements and Sobol's method yielded more robust sensitivity rankings.",,"Tang, Y.|Reed, P.|Wagener, T.|van Werkhoven, K.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-11-793-2007
142,WOS:000237124500022,2006,Automatic calibration and predictive uncertainty analysis of a semidistributed watershed model,RAINFALL-RUNOFF MODELS|GLOBAL OPTIMIZATION|HYDROLOGIC-MODELS|PARAMETER-ESTIMATION|ALGORITHM|MULTIPLE|SCHEME,"Semidistributed models are commonly calibrated manually, but software for automatic calibration is now available. We present a two-stage routine for automatic calibration of the semidistributed watershed model Soil and Water Assessment Tool ( SWAT) that finds the best values for the model parameters, preserves spatial variability in essential parameters, and leads to a measure of the model prediction uncertainty. In the first stage, a modified global Shuffled Complex Evolution (SCE-UA) method was employed to find the ""best'' values for the lumped model parameters. In the second stage, the spatial variability of the original model parameters was restored and a local search method ( a variant of Levenberg - Marquart method) was used to find a more distributed set of parameters using the results of the previous stage as starting values. A method called ""regularization'' was adopted to prevent the parameters from taking extreme values. In addition, we applied a nonlinear calibration-constrained method to develop confidence intervals for annual and -d average flow predictions. We calibrated stream flow in the Etowah River measured at Canton, GA ( a watershed area of  km()) for the years  to  and used the years  to  for validation. The Parameter Estimator ( PEST) software was used to conduct the two-stage automatic calibration and prediction uncertainty analysis. Calibration for daily and monthly flow produced a very good fit to the measured data. Nash-Sutcliffe coefficients for daily and monthly flow over the calibration period were . and ., respectively. They were . and ., respectively, for the validation period. The nonlinear prediction uncertainty analysis worked well for long-term ( annual) flow in that our prediction confidence intervals included or were very near to the observed flow for most years. It did not work well for short-term (-d average) flows in that the prediction confidence intervals did not include the observed flow, especially for low and high flow conditions.",,"Lin, ZL|Radcliffe, DE",VADOSE ZONE JOURNAL,,10.2136/vzj2005.0025
145,WOS:000295716700008,2011,Uncertainty propagation or box propagation,ORDINARY DIFFERENTIAL-EQUATIONS|INITIAL-VALUE PROBLEMS|TAYLOR-SERIES METHOD|VALIDATED SOLUTIONS|LORENZ MODEL|ODES|ODES/DAES,This paper discusses the use of recently developed techniques and software in the numerical propagation of uncertainties in initial coordinates and/or parameters for initial value problems. We present an approach based on several validated numerical integration techniques but focusing on the propagation of boxes. The procedure uses a multivariable high order Taylor series development of the solution of the system whose Taylor coefficients are calculated via extended automatic differentiation rules for all the basic operations. These techniques are implemented in the recent free-software TIDES. The classical two-body and Lorenz problems are chosen as examples to show the benefits of the approach. The results show that the solution of uncertainties can be approximated in an analytic form by means of a Taylor series and that these techniques can be extremely useful in different practical applications. (C) , Elsevier Ltd. All rights reserved.,"Barrio, R.|Rodriguez, M.|Abad, A.|Serrano, S.",MATHEMATICAL AND COMPUTER MODELLING,taylor series method|automatic differentiation|uncertainty propagation|freeware software,10.1016/j.mcm.2011.06.036
146,WOS:000282087300021,2010,TAMkin: A Versatile Package for Vibrational Analysis and Chemical Kinetics,MOLECULAR-ORBITAL METHODS|FREE-RADICAL POLYMERIZATIONS|BLOCK HESSIAN APPROACH|FREQUENCY NORMAL-MODES|AB-INITIO CALCULATION|PHASE N-ALKANES|HARMONIC-ANALYSIS|LARGE SYSTEMS|BASIS-SET|THERMOCHEMICAL KINETICS,"TAMkin is a program for the calculation and analysis of normal modes, thermochemical properties and chemical reaction rates. At present, the output from the frequently applied software programs ADF, CHARMM, CPMD, CPK, Gaussian, Q-Chem, and VASP can be analyzed. The normal-mode analysis can be performed using a broad variety of advanced models, including the standard full Hessian, the Mobile Block Hessian, the Partial Hessian Vibrational approach, the Vibrational Subsystem Analysis with or without mass matrix correction, the Elastic Network Model, and other combinations. TAMkin is readily extensible because of its modular structure. Chemical kinetics of unimolecular and bimolecular reactions can be analyzed in a straightforward way using conventional transition state theory, including tunneling corrections and internal rotor refinements. A sensitivity analysis can also be performed, providing important insight into the theoretical error margins on the kinetic parameters. Two extensive examples demonstrate the capabilities of TAMkin: the conformational change of the biological system adenylate kinase is studied, as well as the reaction kinetics of the addition of ethene to the ethyl radical. The important feature of batch processing large amounts of data is highlighted by performing an extended level of theory study, which TAMkin can automate significantly.",,"Ghysels, An|Verstraelen, Toon|Hemelsoet, Karen|Waroquier, Michel|Van Speybroeck, Veronique",JOURNAL OF CHEMICAL INFORMATION AND MODELING,,10.1021/ci100099g
147,WOS:000403512500013,2017,"The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control",REAL-TIME MANAGEMENT|COMPUTATIONAL SCIENCE|CO2 STORAGE|FLOW|ASSIMILATION|VERIFICATION|ALGORITHMS|COMPLEXITY|TRANSPORT|SELECTION,"In the past decades, simulation frameworks have greatly increased in complexity, due to coupling of models from various disciplines into so-called integrated models. Recently, the combination with tools for uncertainty quantification, inverse modelling, optimization and control started a development towards what we call extended simulation frameworks. While there is an ongoing discussion on quality assurance and reproducibility for simulation frameworks, we have not observed a similar discussion for the extended case. Particularly for extended frameworks, the need for quality assurance is high: The overwhelming range of options and algorithms is unmanageable by a domain expert and opaque to decision makers or the public. The resulting demand for 'intelligent software' with automated configuration can lead to a blind trust in simulation results even if they are incorrect. This is a threatening scenario due to potential consequences in simulation-based engineering or political decisions. In this paper, we analyze the increasing complexity of scientific computing workflows, and discuss the corresponding problems of extended scientific simulation frameworks. We propose a paradigm that regulates the allowable properties of framework components, supports the framework configuration for complex simulations, enforces automatic self-tests of configured frameworks, and communicates automated algorithm choices, potentially critical user settings or convergence issues with adaptive detail level and urgency to the end-user. Our goal is to start transferring the quality assurance discussion in the field of integrated modeling and conventional software frameworks to the area of extended simulation frameworks. With this, we hope to increase the reliability and transparency of (extended) frameworks, framework use and of the corresponding simulation results. (C) ", Elsevier Ltd. All rights reserved.,"Wirtz, Daniel|Nowak, Wolfgang",ENVIRONMENTAL MODELLING & SOFTWARE,scientific computing|extended software frameworks|no free lunch|reproducibility|software trust,10.1016/j.envsoft.2016.10.003
148,WOS:000255770300011,2008,Tools to support a model-based methodology for emission/immission and benefit/cost/risk analysis of wastewater systems that considers uncertainty,NO. 1 RWQM1|TREATMENT PLANTS|WWTP DESIGN|QUALITY|COSTS|BENCHMARKING,"This paper presents a set of tools developed to support an innovative methodology to design and upgrade wastewater treatment systems in a probabilistic way. For the first step, data reconstruction, two different tools were developed, one for situations where data are available and another one where no data are available. The second step, modelling and simulation, implied the development of a new simulation platform and of distributed computation software to deal with the simulation load generated by the third step, uncertainty analysis, with Monte Carlo simulations of the system over one year, important dynamics and stiff behaviour. For the fourth step, evaluation of alternatives, the evaluator tool processes the results of the simulations and plots the relevant information regarding the robustness of the process against input and parameters uncertainties, as well as concentration-duration curves for the risk of non-compliance with effluent and receiving water quality limits. This paper illustrates the merits of these tools to make the innovative methodology of practical interest. The design practice should move from conventional procedures suited for the relatively fixed context of emission limits, to more advanced, transparent and cost-effective procedures appropriate to cope with the flexibility and complexity introduced by integrated water management approaches. (c) ", Elsevier Ltd. All rights reserved.,"Benedetti, Lorenzo|Bixio, Davide|Claeys, Filip|Vanrolleghem, Peter A.",ENVIRONMENTAL MODELLING & SOFTWARE,wastewater treatment plant design|cost-benefit analysis|risk|modelling and simulation|software tools|grid computing,10.1016/j.envsoft.2008.01.001
149,WOS:000250159500009,2007,Simiyu River catchment parameterization using SWAT model,INFORMATION-SYSTEM APPROACH|RAINFALL-RUNOFF MODELS|SURFACE SOIL-MOISTURE|MONSOON 90,"The paper presents advances in hydrologic modelling of the Simiyu River catchment using the soil and water assessment tool (SWAT). In this study, the SWAT model set-up and subsequent application to the catchment was based on high-resolution data such as land use from  in LandSat TM Satellite,  in Digital Elevation Model and Soil from Soil and Terrain Database for Southern Africa (SOTERSAF). The land use data were reclassified based on some ground truth maps using IDRISI Kilimanjaro software. The Soil data were also reclassified manually to represent different soil hydrologic groups, which are important for the SWAT model set-up and simulations. The SWAT application first involved analysis of parameter sensitivity, which was then used for model auto-calibration that followed hierarchy of sensitive model parameters. The analysis of sensitive parameters and auto-calibration was achieved by sensitivity analysis and auto-calibration options, which are new in the recent version of SWAT, SWAT . The paper discusses the results of sensitivity and auto -calibration analyses, and present optimum model parameters, which are important for operation and water/land management studies (e.g. rain-fed agriculture and erosion/sediment and pollutant transport) in the catchment using SWAT. The river discharge estimates from this and a previous study were compared so as to evaluate performances of the recent hydrologic simulations in the catchment. Results showed that surface water model parameters are the most sensitive and have more physical meaning especially CN (the most sensitive) and SOL-K. Simulation results showed more or less same estimate of river flow at Ndagalu gauging station. The model efficiencies (R-%) in this and in the pervious study during calibration and validation periods were, respectively, ., . and ., .. The low level of model performance achieved in these studies showed that other factors than the spatial land data are greatly important for improvement of flow estimation by SWAT in Simiyu.", (c) 2007 Published by Elsevier Ltd.,"Mulungu, Deogratlas M. M.|Munishi, Subira E.",PHYSICS AND CHEMISTRY OF THE EARTH,auto-calibration analysis|high-resolution data|parameter sensitivity analysis|simiyu river catchment|swat 2005,10.1016/j.pce.2007.07.053
152,WOS:000282320800018,2010,Effect of fracture zone on DNAPL transport and dispersion: a numerical approach,MULTIPHASE FLOW|GROUNDWATER|DISSOLUTION|MIGRATION|AQUIFER|POOLS|MEDIA,"Two numerical simulation techniques have been used to identify a suitable method to assist in the characterization of DNAPL movement within fractured porous rock aquifers. Both MODFLOW and UTCHEM software modeling suites were used to simulate different scenarios in fracture dip and hydraulic conductivities. The complexity and the physical structure of fracture characterization were shown to have a significant effect on modeling results, to the extent that fracture zone should be characterized fully before simulation models are used for DNAPL simulations. Sensitivity analysis was conducted on both the hydraulic conductivity and fracture dip values. DNAPL movement in the subsurface showed a high sensitivity to fracture dip variation.",,"Dennis, I.|Pretorius, J.|Steyl, G.",ENVIRONMENTAL EARTH SCIENCES,dnapl|fracture dip|numerical simulation|hydraulic conductivity,10.1007/s12665-010-0468-8
155,WOS:000390183000039,2016,Identifiability of sorption parameters in stirred flow-through reactor experiments and their identification with a Bayesian approach,NONEQUILIBRIUM SOLUTE TRANSPORT|CHAIN MONTE-CARLO|POROUS-MEDIA|MODELS|ADSORPTION|EQUILIBRIUM|KINETICS|VALUES,"This paper addresses the methodological conditions particularly experimental design and statistical inference ensuring the identifiability of sorption parameters from breakthrough curves measured during stirred flow-through reactor experiments also known as continuous flow stirred-tank reactor (CSTR) experiments. The equilibrium-kinetic (EK) sorption model was selected as nonequilibrium parameterization embedding the Kd approach. Parameter identifiability was studied.formally on the equations governing outlet concentrations. It was also studied numerically on  simulated CSTR experiments on a soil with known equilibrium-kinetic sorption parameters. EK sorption parameters can not be identified from a single breakthrough curve of a CSTR experiment, because K-d,K- and k(-) were diagnosed collinear. For pairs of CSTR experiments, Bayesian inference allowed to select the correct models of sorption and error among sorption alternatives. Bayesian inference was conducted with SAMCAT software (Sensitivity Analysis and Markov Chain simulations Applied to Transfer models) which launched the simulations through the embedded simulation engine GNU-MCSim, and automated their configuration and post-processing. Experimental designs consisting in varying flow rates between experiments reaching equilibrium at contamination stage were found optimal, because they simultaneously gave accurate sorption parameters and predictions. Bayesian results were comparable to maximum likehood method but they avoided convergence problems, the marginal likelihood allowed to compare all modeK and credible interval gave directly the uncertainty of sorption parameters theta. Although these findings are limited to the specific conditions studied here, in particular the considered sorption model, the chosen parameter values and error structure, they help in the conception and analysis of future CSTR experiments with radionuclides whose kinetic behaviour is suspected. (C) ", Elsevier Ltd. All rights reserved.,"Nicoulaud-Gouin, V.|Garcia-Sanchez, L.|Giacalone, M.|Attard, J. C.|Martin-Garin, A.|Bois, F. Y.",JOURNAL OF ENVIRONMENTAL RADIOACTIVITY,bayesian inference|sorption parameters|identifiability|mcmc|convergence monitoring|equilibrium kinetic model,10.1016/j.jenvrad.2016.06.008
158,WOS:000266820000004,2009,Finite element response sensitivity analysis of multi-yield-surface J(2) plasticity model by direct differentiation method,ELASTOPLASTIC SEISMIC RESPONSE|FOUNDATION-GROUND SYSTEM|STRESS-STRAIN BEHAVIOR|3-D EARTH DAMS|DYNAMIC-RESPONSE|SOFTWARE|RELIABILITY,"Finite element (FE) response sensitivity analysis is an essential tool for gradient-based optimization methods used in various sub-fields of civil engineering such as structural optimization, reliability analysis, system identification, and finite element model updating. Furthermore, stand-alone sensitivity analysis is invaluable for gaining insight into the effects and relative importance of various system and loading parameters on system response. The direct differentiation method (DDM) is a general, accurate and efficient method to compute FE response sensitivities to FE model parameters. In this paper, the DDM-based response sensitivity analysis methodology is applied to a pressure independent multi-yield-surface J() plasticity material model, which has been used extensively to simulate the nonlinear undrained shear behavior of cohesive soils subjected to static and dynamic loading conditions. The complete derivation of the DDM-based response sensitivity algorithm is presented. This algorithm is implemented in a general-purpose nonlinear finite element analysis program. The work presented in this paper extends significantly the framework of DDM-based response sensitivity analysis, since it enables numerous applications involving the use of the multi-yield-surface J() plasticity material model. The new algorithm and its software implementation are validated through two application examples, in which DDM-based response sensitivities are compared with their counterparts obtained using forward finite difference (FFD) analysis. The normalized response sensitivity analysis results are then used to measure the relative importance of the soil constitutive parameters on the system response.", (C) 2009 Elsevier B.V. All rights reserved.,"Quan Gu, |Conte, Joel P.|Elgamal, Ahmed|Yang, Zhaohui",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,nonlinear finite element analysis|response sensitivity analysis|multi-yield-surface plasticity model|direct differentiation method|soil material model,10.1016/j.cma.2009.02.030
159,WOS:000223528100004,2004,POLCAGE 1.0 - a possibilistic life-cycle assessment model for evaluating alternative transportation fuels,FUZZY OUTRANKING|SYSTEMS|ENERGY,"A composite software model for the comparative life-cycle assessment (LCA) of  different fuel options for the Philippine automotive transport sector was developed. It is based on the GREET fuel-cycle inventory model developed by the Argonne National Laboratory for the United States Department of Energy. GREET .a is linked to an impact assessment submodel using the Danish environmental design of industrial products (EDIP) method. This combined inventory-impact assessment model is enhanced further with possibilistic uncertainty propagation (PUP) and possibilistic compromise programming (PCP) features that allow the  alternatives to be ranked in the presence of multiple criteria and uncertain data. Sensitivity and scenario analysis can also be performed within the composite model. Some current and anticipated Philippine conditions, including electricity generation mix, are incorporated in the prototype's built-in database. The software model, designated as POLCAGE . (possibilistic LCA using GREET and EDIP), is coded in Microsoft Excel and Visual Basic. The model's capabilities and features are demonstrated using a case study based on its default scenario. (C) ", Elsevier Ltd. All rights reserved.,"Tan, RR|Culaba, AB|Purvis, MRI",ENVIRONMENTAL MODELLING & SOFTWARE,life-cycle assessment (lca)|decision support system (dss)|alternative fuels,10.1016/j.envsoft.2003.10.004
161,WOS:000408861800155,2017,Improving Thermal Comfort of Low-Income Housing in Thailand through Passive Design Strategies,TROPICAL HUMID REGION|BUILDINGS|STANDARDS|ADAPTATION|HOUSES,"In Thailand, the delivery of adequate low-income housing has historically been overshadowed by politics with cost and quantity being prioritised over quality, comfort and resilience. In a country that experiences hot and humid temperatures throughout the year, buildings need to be adaptable to the climate to improve the thermal comfort of inhabitants. This research is focused on identifying areas for improving the thermal performance of these housing designs. Firstly, dynamic thermal simulations were run on a baseline model using the adaptive thermal comfort model CIBSE TM for assessment. The three criteria defined in CIBSE TM were used to assess the frequency and severity of overheating in the buildings. The internal temperature of the apartments was shown to exceed the thermal comfort threshold for these criteria throughout the year. The internal operating daily temperatures of the apartment remain high, ranging from a maximum of . degrees C to a minimum of . degrees C. Based on these findings, five criteria were selected to be analysed for sensitivity to obtain the key parameters that influence the thermal performance and to suggest possible areas for improvement. The computer software package Integrated Environmental SolutionsVirtual Environment (IES-VE) was used to perform building energy simulations. Once the baseline conditions were identified, the software packages SimLab. and RStudio were used to carry out the sensitivity analysis. These results indicated that roof material and the presence of a balcony have the greatest influence on the system. Incorporating insulation into the roof reduced the mean number of days of overheating by .%. Removing the balcony increased the number of days of overheating by .% due to significant reductions in internal ventilation.",,"Bhikhoo, Nafisa|Hashemi, Arman|Cruickshank, Heather",SUSTAINABILITY,thermal comfort|low income housing|thailand|tropical climates|dynamic thermal simulations|sensitivity analysis,10.3390/su9081440
162,WOS:000186310600006,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: Part I - theory and software tools,VARIATIONAL DATA ASSIMILATION|ATMOSPHERIC CHEMISTRY PROBLEMS|GREENS-FUNCTION METHOD|NON-LINEAR SYSTEMS|STIFF ODE SOLVERS|ROSENBROCK METHOD|MODELS|IMPLEMENTATION|IMPLICIT|EXPLICIT,"The analysis of comprehensive chemical reactions mechanisms, parameter estimation techniques, and variational chemical data assimilation applications require the development of efficient sensitivity methods for chemical kinetics systems. The new release (KPP-.) of the kinetic preprocessor (KPP) contains software tools that facilitate direct and adjoint sensitivity analysis. The direct-decoupled method, built using BDF formulas, has been the method of choice for direct sensitivity studies. In this work, we extend the direct-decoupled approach to Rosenbrock stiff integration methods. The need for Jacobian derivatives prevented Rosenbrock methods to be used extensively in direct sensitivity calculations; however, the new automatic and symbolic differentiation technologies make the computation of these derivatives feasible. The direct-decoupled method is known to be efficient for computing the sensitivities of a large number of output parameters with respect to a small number of input parameters. The adjoint modeling is presented as an efficient tool to evaluate the sensitivity of a scalar response function with respect to the initial conditions and model parameters. In addition, sensitivity with respect to time-dependent model parameters may be obtained through a single backward integration of the adjoint model. KPP software may be used to completely generate the continuous and discrete adjoint models taking full advantage of the sparsity of the chemical mechanism. Flexible direct-decoupled and adjoint sensitivity code implementations are achieved with minimal user intervention. In a companion paper, we present an extensive set of numerical experiments that validate the KPP software tools for several direct/adjoint sensitivity applications, and demonstrate the efficiency of KPP-generated sensitivity code implementations. (C) ", Elsevier Ltd. All rights reserved.,"Sandu, A|Daescu, DN|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,chemical kinetics|sensitivity analysis|direct-decoupled method|adjoint model,10.1016/j.atmosenv.2003.08.019
163,WOS:000323349500001,2013,Optimal Design of Signal Controlled Road Networks Using Differential Evolution Optimization Algorithm,EQUILIBRIUM TRANSPORTATION NETWORKS|VARIATIONAL INEQUALITY CONSTRAINTS|SIMULATED ANNEALING APPROACH|AREA TRAFFIC CONTROL|GENETIC ALGORITHM|SENSITIVITY ANALYSIS|ASSIGNMENT|TIMINGS|FLOW,"This study proposes a traffic congestion minimization model in which the traffic signal setting optimization is performed through a combined simulation-optimization model. In this model, the TRANSYT traffic simulation software is combined with Differential Evolution (DE) optimization algorithm, which is based on the natural selection paradigm. In this context, the EQuilibrium Network Design (EQND) problem is formulated as a bilevel programming problem in which the upper level is the minimization of the total network performance index. In the lower level, the traffic assignment problem, which represents the route choice behavior of the road users, is solved using the Path Flow Estimator (PFE) as a stochastic user equilibrium assessment. The solution of the bilevel EQND problem is carried out by the proposed Differential Evolution and TRANSYT with PFE, the so-called DETRANSPFE model, on a well-known signal controlled test network. Performance of the proposed model is compared to that of two previous works where the EQND problem has been solved by Genetic-Algorithms- (GAs-) and Harmony-Search- (HS-) based models. Results show that the DETRANSPFE model outperforms the GA- and HS-based models in terms of the network performance index and the computational time required.",,"Ceylan, Huseyin",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2013/696374
164,WOS:000288865100006,2011,Estimating Water Budgets and Vertical Leakages for Karst Lakes in North-Central Florida (United States) Via Hydrological Modeling,RAINFALL-RUNOFF MODELS|AUTOMATIC CALIBRATION|CONDUCTANCE,"Newnans, Lochloosa, and Orange Lakes are closely hydrologically connected karst lakes located in north-central Florida, United States. The complex karst hydrology in this region poses a great challenge to the hydrological modeling that is essential to the development of Total Maximum Daily Loads for these lakes. We used a Hydrological Simulation Program - Fortran model coupled with the parallel Parameter ESTimation model calibration and uncertainty analysis software to estimate effectively the hydrological interactions between the lakes and the underlying upper Floridan aquifer and the water budgets for these three lakes. The net results of the lake-groundwater interactions in Newnans and Orange Lakes are that both lakes recharge the underlying upper Floridan aquifer, with the recharge rate of the latter one magnitude greater than that of the former. However, for Lochloosa Lake, the net lake-groundwater interaction is that the lake gains water from groundwater in a significant amount, approximately % of its total terrestrial water input. The annual average vertical leakages estimated for Newnans, Lochloosa, and Orange Lakes are . x , -. x , and . x  m, respectively. The average vertical hydraulic conductance (K(v)/b) of the units between a lake bottom and the underlying upper Floridan aquifer in this region are also estimated to be from . x - to . x - day-.",,"Lin, Zhulu",JOURNAL OF THE AMERICAN WATER RESOURCES ASSOCIATION,hspf|karst hydrology|lake water budget|optimization|surface water|groundwater interaction,10.1111/j.1752-1688.2010.00513.x
166,WOS:000230023700002,2005,Applicability of neuro-fuzzy techniques in predicting ground-water vulnerability: a GIS-based sensitivity analysis,AQUIFER VULNERABILITY|HYDRAULIC-PROPERTIES|MODELS|WATER|CLASSIFICATION|NETWORKS|FLOW|MANAGEMENT|TRANSPORT|SELECTION,"Modeling groundwater vulnerability reliably and cost effectively for non-point source (NPS) pollution at a regional scale remains a major challenge. In recent years, Geographic Information Systems (GIS), neural networks and fuzzy logic techniques have been used in several hydrological studies. However, few of these research studies have undertaken an extensive sensitivity analysis. The overall objective of this research is to examine the sensitivity of neuro-fuzzy models used to predict groundwater vulnerability in a spatial context by integrating GIS and neuro-fuzzy techniques. The specific objectives are to assess the sensitivity of neuro-fuzzy models in a spatial domain using GIS by varying (i) shape of the fuzzy sets, (ii) number of fuzzy sets, and (iii) learning and validation parameters (including rule weights). The neuro-fuzzy models were developed using NEFCLASS-J software on a JAVA platform and were loosely integrated with a GIS. Four plausible parameters which are critical in transporting contaminants through the soil profile to the groundwater, included soil hydrologic group, depth of the soil profile, soil structure (pedality points) of the A horizon, and landuse. In order to validate the model predictions, coincidence reports were generated among model inputs, model predictions, and well/spring contamination data for NO-N. A total of  neuro-fuzzy models were developed for selected sub-basins of Illinois River Watershed, AR. The sensitivity analysis showed that neuro-fuzzy models were sensitive to the shape of the fuzzy sets, number of fuzzy sets, nature of the rule weights, and validation techniques used during the learning processes. Compared to bell-shaped and triangular-shaped membership functions, the neuro-fuzzy models with a trapezoidal membership function were the least sensitive to the various permutations and combinations of the learning and validation parameters. Over all, Models  and  showed relatively higher coincidence with well contamination data than other models. The strength of this method is that it offers a means of dealing with imprecise data, therefore, is a viable option for regional and continental scale environmental modeling where imprecise data prevail. The neuro-fuzzy models, however, should only be used as a tool within a broader framework of GIS, remote sensing and solute transport modeling to assess groundwater vulnerability along with functional, mechanistic and stochastic models.", (c) 2004 Elsevier B.V. All rights reserved.,"Dixon, B",JOURNAL OF HYDROLOGY,gis|gps|no3-n|modeling|fuzzy logic|neural networks,10.1016/j.jhydrol.2004.11.010
168,WOS:000346630600011,2014,An Efficient and Very Accurate Method for Calculating Steady-State Sensitivities in Metabolic Reaction Systems,TRICARBOXYLIC-ACID CYCLE|POWER-LAW APPROXIMATION|DICTYOSTELIUM-DISCOIDEUM|MODEL,"Stability and sensitivity analyses of biological systems require the ad hoc writing of computer code, which is highly dependent on the particular model and burdensome for large systems. We propose a very accurate strategy to overcome this challenge. Its core concept is the conversion of the model into the format of biochemical systems theory (BST), which greatly facilitates the computation of sensitivities. First, the steady state of interest is determined by integrating the model equations toward the steady state and then using a Newton-Raphson method to fine-tune the result. The second step of conversion into the BST format requires several instances of numerical differentiation. The accuracy of this task is ensured by the use of a complex-variable Taylor scheme for all differentiation steps. The proposed strategy is implemented in a new software program, COSMOS, which automates the stability and sensitivity analysis of essentially arbitrary ODE models in a quick, yet highly accurate manner. The methods underlying the process are theoretically analyzed and illustrated with four representative examples: a simple metabolic reaction model; a model of aspartate-derived amino acid biosynthesis; a TCA-cycle model; and a modified TCA-cycle model. COSMOS has been deposited to https://github.com/BioprocessdesignLab/COSMOS.",,"Shiraishi, Fumihide|Yoshida, Erika|Voit, Eberhard O.",IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS,roots of nonlinear equations|algorithm design and analysis|biology and genetics|systems theory,10.1109/TCBB.2014.2338311
171,WOS:000254595200018,2008,Waste management modeling with PC-based model - EASEWASTE,SYSTEMS,"As lite-cycle-thinking becomes more integrated into waste management, quantitative tools are needed for assessing waste management systems and technologies. This article presents a decision support model to deal with integrated solid waste management planning problems at a regional or national level. The model is called EASEWASTE (environmental assessment of solid waste systems and technologies). The model consists of a number of modules (submodels), each describing a process in a real waste management system, and these modules may combine to represent a complete waste management system in a scenario. EASEWASTE generates data on emissions (inventory), which are translated and aggregated into different environmental impact categories, e.g. the global warming, acidification, and toxicity. To facilitate a ""first level"" screening evaluation, default values for process parameters have been provided, wherever possible. The EASEWASTE model for life-cycle-assessment of waste management is described and applied to a case study for illustrative purposes. The case study involving hypothetical but realistic data demonstrates the functionality, usability, and flexibilities of the model. The design and implementation of the software successfully address the substantial challenges in integrating process modeling, life-cycle inventory (M), and impact assessment (LCIA) modeling, and optimization into an interactive decision support platform."," (c) 2008 American Institute of Chemical Engineers Environ Prog, 27: 133-142, 2008.","Bhander, Gurbakhash S.|Hauschild, Michael Z.|Christensen, Thomas H.",ENVIRONMENTAL PROGRESS,life cycle assessment|waste management|modeling|easewaste model|systems analysis|waste planning|environmental assessment|waste technologies|sensitivity analysis,10.1002/ep.10250
174,WOS:000087885500003,2000,Buckling design optimization of complex built-up structures with shape and size variables,SENSITIVITY ANALYSIS,"The design optimization of buckling behaviour is studied for complex built-up structures composed of various kinds of elements and implemented within JIFEX, a general-purpose software for finite element analysis and design optimization. The direct and adjoint methods of sensitivity analysis for critical buckling loads are presented with detailed computational procedures. Particularly, the variations of prebuckling stresses and external loads have been accounted, for. The design model and solution methods presented in this paper are available for both shape and size optimization, and buckling optimization can also be combined with static, frequency and dynamic response optimization. The numerical examples show the applications of the buckling optimization method and the effectiveness of the methods and the program of this paper.",,"Gu, YX|Zhao, GZ|Zhang, HW|Kang, Z|Grandhi, RV",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,shape optimization|size optimization|buckling|natural frequency|dynamic response|built-up structures,10.1007/s001580050101
176,WOS:000355760000014,2015,Optimization of the motion control mechanism of the hatch door of airliner,FLEXIBLE MULTIBODY SYSTEMS|DESIGN SENSITIVITY-ANALYSIS|FINITE SEGMENT APPROACH|DYNAMIC-SYSTEMS|RECURSIVE FORMULATION|PARALLEL MANIPULATORS|LOOP SYSTEMS|EQUATIONS|METHODOLOGY|ALGORITHM,"This paper deals with the problem of parameter optimization of the motion control mechanism of the hatch door of ARJ-, a regional airliner of China. Motion improvement of the hatch door is implemented by two kinds of passive designs. Firstly, a single-layer optimization model for trajectory modification is developed to find the optimum size of the key parts of the control mechanism. Secondly, a novel nested bi-level optimization model is presented for the design of the size tolerance limits of the selected parts. The design objective is minimization of the total extremum deviation of the motion trajectory of the objective point of the hatch door, where the extremum deviation is obtained by solution of the inner-level size optimization problem for the fixed size tolerance limits. The optimization models for motion control of the hatch door mechanism are solved using the response surface method. Numerical examples show that the precision of the real running trajectory of the objective point of the hatch door mechanism may be improved effectively by using the methods presented. A home-made multi-body dynamics solver (THUSOLVER) and the corresponding optimization software have been developed to implement the above tasks.",,"Du, Jianbin|Huang, Zhenting|Yang, Ruizhen",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,bi-level optimization model|motion control mechanism|tolerance design|response surface method|size optimization,10.1007/s00158-014-1191-y
178,WOS:000378451500006,2016,Stiffness design of heterogeneous periodic beam by topology optimization with integration of commercial software,HOMOGENIZATION METHOD|CROSS-SECTIONS|PERMEABILITY,"A topology optimization method is developed for microstructure design of heterogeneous periodic beam structure aiming at its extreme or specified effective stiffness. The effective stiffness is calculated using a FEM formulation of asymptotic homogenization method for heterogeneous periodic beam. Sensitivity of stiffness to the density design variable is derived analytically based on the solution of unit cell problems under corresponding generalized strain fields. Implementation of optimization procedure is generalized to take full advantage of commercial FEM software capabilities, with several examples presented to demonstrate its effectiveness. It is shown here the proposed method is extendible to periodic truss beam design. (C) ", Elsevier Ltd. All rights reserved.,"Yi, Sinan|Cheng, Gengdong|Xu, Liang",COMPUTERS & STRUCTURES,periodic beam|topology optimization|sensitivity analysis|homogenization method|commercial software,10.1016/j.compstruc.2016.05.012
179,WOS:000327903400013,2013,Addressing ten questions about conceptual rainfall-runoff models with global sensitivity analyses in R,MULTIOBJECTIVE CALIBRATION|ENVIRONMENTAL-MODEL|CATCHMENT MODEL|CLIMATE-CHANGE|UNCERTAINTY|PERFORMANCE|INDEXES|PARAMETERS|AUSTRALIA|HYDROLOGY,"Sensitivity analysis (SA) is generally recognized as a worthwhile step to diagnose and remedy difficulties in identifying model parameters, and indeed in discriminating between model structures. An analysis of papers in three journals indicates that SA is a standard omission in hydrological modeling exercises. We provide some answers to ten reasonably generic questions using the Morris and Sobol SA methods, including to what extent sensitivities are dependent on parameter ranges selected, length of data period, catchment response type, model structures assumed and climatic forcing. Results presented demonstrate the sensitivity of four target functions to parameter variations of four rainfall runoff models of varying complexity (- parameters). Daily rainfall, streamflow and pan evaporation data are used from four -year data sets and from five catchments in the Australian Capital Territory (ACT) region. Similar results are obtained using the Morris and Sobol methods. It is shown how modelers can easily identify parameters that are insensitive, and how they might improve identifiability. Using a more complex objective function, however, may not result in all parameters becoming sensitive. Crucially, the results of the SA can be influenced by the parameter ranges selected. The length of data period required to characterize the sensitivities assuredly is a minimum of five years. The results confirm that only the simpler models have well-identified parameters, but parameter sensitivities vary between catchments. Answering these ten questions in other case studies is relatively easy using freely available software with the Hydromad and Sensitivity packages in R.", (C) 2013 Elsevier B.V. All rights reserved.,"Shin, Mun-Ju|Guillaume, Joseph H. A.|Croke, Barry F. W.|Jakeman, Anthony J.",JOURNAL OF HYDROLOGY,sensitivity analysis|rainfall-runoff model|identifiability,10.1016/j.jhydrol.2013.08.047
180,WOS:000323644600031,2013,Nitrous Oxide Emissions from Cropland: a Procedure for Calibrating the DayCent Biogeochemical Model Using Inverse Modelling,CARBON-DIOXIDE|SOIL|N2O|DENITRIFICATION|SIMULATIONS|COLORADO|SYSTEMS|DNDC,"DayCent is a biogeochemical model of intermediate complexity widely used to simulate greenhouse gases (GHG), soil organic carbon and nutrients in crop, grassland, forest and savannah ecosystems. Although this model has been applied to a wide range of ecosystems, it is still typically parameterized through a traditional ""trial and error"" approach and has not been calibrated using statistical inverse modelling (i.e. algorithmic parameter estimation). The aim of this study is to establish and demonstrate a procedure for calibration of DayCent to improve estimation of GHG emissions. We coupled DayCent with the parameter estimation (PEST) software for inverse modelling. The PEST software can be used for calibration through regularized inversion as well as model sensitivity and uncertainty analysis. The DayCent model was analysed and calibrated using NO flux data collected over  years at the Iowa State University Agronomy and Agricultural Engineering Research Farms, Boone, IA. Crop year  data were used for model calibration and  data were used for validation. The optimization of DayCent model parameters using PEST significantly reduced model residuals relative to the default DayCent parameter values. Parameter estimation improved the model performance by reducing the sum of weighted squared residual difference between measured and modelled outputs by up to  %. For the calibration period, simulation with the default model parameter values underestimated mean daily NO flux by  %. After parameter estimation, the model underestimated the mean daily fluxes by  %. During the validation period, the calibrated model reduced sum of weighted squared residuals by  % relative to the default simulation. Sensitivity analysis performed provides important insights into the model structure providing guidance for model improvement.",,"Rafique, Rashad|Fienen, Michael N.|Parkin, Timothy B.|Anex, Robert P.",WATER AIR AND SOIL POLLUTION,daycent model|inverse modelling|parameter estimation (pest)|nitrous oxide|sensitivity analysis|automatic calibration|validation,10.1007/s11270-013-1677-z
185,WOS:000330079500009,2014,Exploring incomplete information in maintenance materials inventory optimization,SPARE PARTS|INTERMITTENT DEMAND|STOCK CONTROL|CONTROL PERFORMANCE|REORDER POINT|PARAMETERS|SYSTEM|MODEL,"Purpose Ensuring the sufficient service level is essential for critical materials in industrial maintenance. This study aims to evaluate the use of statistically imperfect data in a stochastic simulation-based inventory optimization where items' failure characteristics are derived from historical consumption data, which represents a real-life situation in the implementation of such an optimization model. Design/methodology/approach - The risks of undesired shortages were evaluated through a service-level sensitivity analysis. The service levels were simulated within the error of margin of the key input variables by using StockOptim optimization software and real data from a Finnish steel mill. A random sample of  inventory items was selected. Findings - Service-level sensitivity is item specific, but, for many items, statistical imprecision in the input data causes significant uncertainty in the service level. On the other hand, some items seem to be more resistant to variations in the input data than others. Research limitations/implications - The case approach, with one simulation model, limits the generalization of the results. The possibility that the simulation model is not totally realistic exists, due to the model's normality assumptions. Practical implications - Margin of error in input data estimation causes a significant risk of not achieving the required service level. It is proposed that managers work to improve the preciseness of the data, while the sensitivity analysis against statistical uncertainty, and a correction mechanism if necessary, should be integrated into optimization models. Originality/value - The output limitations in the optimization, i.e. service level, are typically stated precisely, but the capabilities of the input data have not been addressed adequately. This study provides valuable insights into ensuring the availability of-critical materials.",,"Puurunen, Antti|Majava, Jukka|Kess, Pekka",INDUSTRIAL MANAGEMENT & DATA SYSTEMS,inventory optimization|maintenance materials|service-level sensitivity|spare parts|stockoptim,10.1108/IMDS-01-2013-0025
191,WOS:000325218500007,2013,Uncertainty in floodplain delineation: expression of flood hazard and risk in a Gulf Coast watershed,,"This paper investigates the development of flood hazard and flood risk delineations that account for uncertainty as improvements to standard floodplain maps for coastal watersheds. Current regulatory floodplain maps for the Gulf Coastal United States present % flood hazards as polygon features developed using deterministic, steady-state models that do not consider data uncertainty or natural variability of input parameters. Using the techniques presented here, a standard binary deterministic floodplain delineation is replaced with a flood inundation map showing the underlying flood hazard structure. Additionally, the hazard uncertainty is further transformed to show flood risk as a spatially distributed probable flood depth using concepts familiar to practicing engineers and software tools accepted and understood by regulators. A case study of the proposed hazard and risk assessment methodology is presented for a Gulf Coast watershed, which suggests that storm duration and stage boundary conditions are important variable parameters, whereas rainfall distribution, storm movement, and roughness coefficients contribute less variability. The floodplain with uncertainty for this coastal watershed showed the highest variability in the tidally influenced reaches and showed little variability in the inland riverine reaches. Additionally, comparison of flood hazard maps to flood risk maps shows that they are not directly correlated, as areas of high hazard do not always represent high risk."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Christian, Jason|Duenas-Osorio, Leonardo|Teague, Aarin|Fang, Zheng|Bedient, Philip",HYDROLOGICAL PROCESSES,flood hazard|flood risk|floodplain mapping|uncertainty analysis|coastal watersheds|storm surge,10.1002/hyp.9360
198,WOS:000301013200010,2012,"Automating calibration, sensitivity and uncertainty analysis of complex models using the R package Flexible Modeling Environment (FME): SWAT as an example",WATER ASSESSMENT-TOOL|RAINFALL-RUNOFF MODELS|HYDROLOGIC-MODELS|PARAMETER UNCERTAINTY|GLOBAL OPTIMIZATION|BAYESIAN-APPROACH|CATCHMENT MODELS|RIVER-BASIN|SOIL|PREDICTION,"Parameter optimization and uncertainty issues are a great challenge for the application of large environmental models like the Soil and Water Assessment Tool (SWAT), which is a physically-based hydrological model for simulating water and nutrient cycles at the watershed scale. In this study, we present a comprehensive modeling environment for SWAT, including automated calibration, and sensitivity and uncertainty analysis capabilities through integration with the R package Flexible Modeling Environment (FME). To address challenges (e.g., calling the model in R and transferring variables between Fortran and R) in developing such a two-language coupling framework, ) we converted the Fortran-based SWAT model to an R function (R-SWAT) using the RFortran platform, and alternatively ) we compiled SWAT as a Dynamic Link Library (DLL). We then wrapped SWAT (via R-SWAT) with FME to perform complex applications including parameter identifiability, inverse modeling, and sensitivity and uncertainty analysis in the R environment. The final R-SWAT-FME framework has the following key functionalities: automatic initialization of R, running Fortran-based SWAT and R commands in parallel, transferring parameters and model output between SWAT and R, and inverse modeling with visualization. To examine this framework and demonstrate how it works, a case study simulating streamflow in the Cedar River Basin in Iowa in the United Sates was used, and we compared it with the built-in auto-calibration tool of SWAT in parameter optimization. Results indicate that both methods performed well and similarly in searching a set of optimal parameters. Nonetheless, the R-SWAT-FME is more attractive due to its instant visualization, and potential to take advantage of other R packages (e.g., inverse modeling and statistical graphics). The methods presented in the paper are readily adaptable to other model applications that require capability for automated calibration, and sensitivity and uncertainty analysis.", Published by Elsevier Ltd.,"Wu, Yiping|Liu, Shuguang",ENVIRONMENTAL MODELLING & SOFTWARE,calibration|fme|monte carlo|r|sensitivity and uncertainty analysis|swat,10.1016/j.envsoft.2011.11.013
199,WOS:000275621700002,2010,"Controlling setup cost in (Q, r, L) inventory model with defective items",LEAD TIME|REDUCTION,"This study discusses a mixture inventory model with back orders and lost sales in which the order quantity, reorder point, lead time and setup cost are decision variables. It is assumed that an arrival order lot may contain some defective items and the number of defective items is a random variable. There are two inventory models proposed in this paper, one with normally distributed demand and another with distribution free demand. Finally we develop two computational algorithms to obtain the optimal ordering policy, A computer code using the software Matlab is developed to derive the optimal solution and present numerical examples to illustrate the models. Additionally, sensitivity analysis is conducted with respect to the various system parameters.", (C) 2009 Elsevier Inc. All rights reserved.,"Annadurai, K.|Uthayakumar, R.",APPLIED MATHEMATICAL MODELLING,setup cost|lead time|defective items|minimax distribution-free procedure|computational algorithm|optimization,10.1016/j.apm.2009.04.010
207,WOS:000328724500002,2014,SGEMS-UQ: An uncertainty quantification toolkit for SGEMS,DISTANCES|RESERVOIR|MODELS,"While algorithms and methodologies to study uncertainty in the Earth Sciences are constantly evolving, there is currently no free integrated software that allows the general practitioners access to these developments. This paper presents SGEMS-UQ a plugin for the SGEMS platform, that is used to perform distance-based uncertainty analysis on geostatistical simulations, and the resulting forward transfer function responses used in subsurface modeling and engineering. A versatile XML-derived dialect is defined for communicating with external programs that reduces the need for ad-hoc linking of codes, and a relational database system is implemented to automate many of the steps in data mining the spatial and forward model parameters. Through a graphical user interface, one can map a set of realizations and forward transfer function responses into a multidimensional scaling (MDS) space where visualization utilities, and clustering techniques are available. Once mapped in the MDS space, the user can explore linkage between simulation parameters and forward transfer function responses using a module based on a SQL database. Consideration is given to the use of software engineering paradigms and design patterns to produce a code-base that is manageable, efficient, and extensible for future applications, while being scalable to work with large datasets. Finally, we illustrate the versatility of the code-base on an application of modeling uncertainty in reservoir forecasts for an oil reservoir in the West Coast of Africa. (C) ", Elsevier Ltd. All rights reserved.,"Li, Lewis|Boucher, Alexandre|Caers, Jef",COMPUTERS & GEOSCIENCES,uncertainty quantification|software design|xml|sql database,10.1016/j.cageo.2013.09.009
208,WOS:000256243200015,2008,Sensitivity of population viability to spatial and nonspatial parameters using grip,METAPOPULATION DYNAMICS|PREDICTIVE ABILITY|PVA MODELS|MATRIX|REINTRODUCTION,"Metapopulation dynamics are influenced by spatial parameters including the amount and arrangement of suitable habitat, yet these parameters may be uncertain when deciding how to manage species or their habitats. Sensitivity analyses of population viability analysis (PVA) models can help measure relative parameter influences on predictions, identify research priorities for reducing uncertainty, and evaluate management strategies. Few spatial PVAs, however, include sensitivity analyses of both spatial and nonspatial parameters, perhaps because computationally efficient tools for such analyses are lacking or inaccessible. We developed GRIP, a program to facilitate sensitivity analysis of spatial and nonspatial input parameters for PVAs created in RAMAS Metapop, a widely applied software program. GRIP creates random sets of input files by varying parameters specified in the PVA model including vital rates and their correlations among populations, the number and configuration of populations, dispersal rates, dispersal survival, initial population abundances, carrying capacities, and the probability, intensity, and spatial extent of catastrophes, while drawing on specified parameter distributions. We evaluated GRIP's performance as a tool for sensitivity analysis of spatial PVAs and explored the consequences of varying spatial input parameters for predictions of a published PVA model of the sand lizard (Lacerta agilis). We used GRIP output to generate standardized regression coefficients (SRCs) and nonparametric correlation coefficients as indices of the relative sensitivity of predicted conservation status to input parameters. GRIP performed well; with a single analysis we were able to rank the relative influence of input parameters identified as influential by the PVA's original author, S. A. Berglind, who used three separate forms of sensitivity analysis. Our analysis, however, also underscored the value of exploring the relative influence of spatial parameters on PVA predictions; both SRCs and correlation coefficients indicated that the most influential parameters in the sand lizard model were spatial in nature. We provide annotated code so that GRIP may be modified to reflect particular species biology, customized for more complex spatial PVA models, upgraded to incorporate features added in newer versions of RAMAS Metapop, used as a template to develop similar programs, or used as it is for computationally efficient sensitivity analyses in support of conservation planning.",,"Curtis, J. M. R.|Naujokaitis-Lewis, I.",ECOLOGICAL APPLICATIONS,decision-support tool|lacerta agilis|recovery planning|sand lizard|sensitivity analysis|spatial population viability analysis|uncertainty,10.1890/07-1306.1
210,WOS:000224375900007,2004,A tool for risk-based management of surface water quality,SENSITIVITY-ANALYSIS|UNCERTAINTY|MODELS|EUTROPHICATION|PREDICTION|PHOSPHORUS|SYSTEMS|FUTURE,"Water quality Risk Analysis Tool (WaterRAT) is software for supporting decision-making in surface water quality management. The philosophy behind the software is that uncertainty in water quality model predictions is inevitably high due to model equation error, parameter error, and limited definition of boundary conditions and management objectives. Using sensitivity and uncertainty analyses based on Monte Carlo simulation and first order methods, WaterRAT allows the modeller to identify the significant uncertainties, and evaluate the degree to which they control decision-making risk. WaterRAT has a library of river and lake water quality models of varying complexity, and these can be applied at a wide range of temporal and spatial scales, allowing the model design to be responsive to both the modelling task and the data constraints. (C) ", Elsevier Ltd. All rights reserved.,"McIntyre, NR|Wheater, HS",ENVIRONMENTAL MODELLING & SOFTWARE,water quality|uncertainty|risk|decision-making,10.1016/j.envsoft.2003.12.003
213,WOS:000318057900003,2013,A model-independent Particle Swarm Optimisation software for model calibration,RAINFALL-RUNOFF MODELS|SHUFFLED COMPLEX EVOLUTION|PAMPA DEL TAMARUGAL|GLOBAL OPTIMIZATION|DIFFERENTIAL EVOLUTION|SENSITIVITY-ANALYSIS|UNCERTAINTY ASSESSMENT|REGIONAL AQUIFER|PARAMETERS|ALGORITHM,"This work presents and illustrates the application of hydroPSO, a novel multi-OS and model-independent R package used for model calibration. hydroPSO allows the modeller to perform a standard modelling work flow including, sensitivity analysis, parameter calibration, and assessment of the calibration results, using a single piece of software. hydroPSO implements several state-of-the-art enhancements and fine-tuning options to the Particle Swarm Optimisation (PSO) algorithm to meet specific user needs. hydroPSO easily interfaces the calibration engine to different model codes through simple ASCII files and/or R wrapper functions for exchanging information on the calibration parameters. Then, optimises a user-defined goodness-of-fit measure until a maximum number of iterations or a convergence criterion are met. Finally, advanced plotting functionalities facilitate the interpretation and assessment of the calibration results. The current hydroPSO version allows easy parallelization and works with single-objective functions, with multi-objective functionalities being the subject of ongoing development. We compare hydroPSO against standard algorithms (SCE_UA, DE, DREAM, SPSO-, and GML) using a series of benchmark functions. We further illustrate the application of hydroPSO in two real-world case studies: we calibrate, first, a hydrological model for the Ega River Basin (Spain) and, second, a groundwater flow model for the Pampa del Tamarugal Aquifer (Chile). Results from the comparison exercise indicate that hydroPSO is: i) effective and efficient compared to commonly used optimisation algorithms, ii) ""scalable"", i.e. maintains a high performance for increased problem dimensionality, and iii) versatile to adapt to different response surfaces of the objective function. Case study results highlight the functionality and ease of use of hydroPSO to handle several issues that are commonly faced by the modelling community such as: working on different operating systems, single or batch model execution, transient- or steady-state modelling conditions, and the use of alternative goodness-of-fit measures to drive parameter optimisation. Although we limit the application of hydroPSO to hydrological models, flexibility of the package suggests it can be implemented in a wider range of models requiring some form of parameter optimisation. (C) ", Elsevier Ltd. All rights reserved.,"Zambrano-Bigiarini, Mauricio|Rojas, Rodrigo",ENVIRONMENTAL MODELLING & SOFTWARE,global optimisation|evolutionary algorithm|surface water modelling|groundwater modelling|swat-2005|modflow-2005|r,10.1016/j.envsoft.2013.01.004
214,WOS:000222719700006,2004,Interactive software for material parameter characterization of advanced engineering constitutive models,STATE,"The development of an overall strategy to estimate the material parameters for a class of viscoplastic material models is presented. The procedure is automated through the integrated software COMPARE (Constitutive Material PARameter Estimator) that enables the determination of an 'optimum' set of material parameters by minimizing the errors between the experimental test data and the model's predicted response. The core ingredients of COMPARE are (i) primal analysis, which utilizes a finite element-based solution scheme, (ii) sensitivity analysis utilizing a direct-differentiation approach for the material response sensitivities, and (iii) a gradient-based optimization technique of an error/cost function. Now that the COMPARE core code has reached a level of maturity, a graphical user interface (GUI) was deemed necessary. Without such an interface, use of COMPARE was previously restricted to very experienced users with the additional cumbersome, and sometimes tedious, task of preparing the required input files manually. The complexity of the input containing massive amounts of data has previously placed severe limitations on the use of such optimization procedures by the general engineering community. By using C+ + and the Microsoft Foundation Classes to develop a GUI, it is believed that an advanced code such as COMPARE can now make the transition to general usability in an engineering environment. (C) ", Elsevier Ltd. All rights reserved.,"Saleeb, AF|Marks, |Wilt, TE|Arnold, SM",ADVANCES IN ENGINEERING SOFTWARE,c plus|graphical user interface|optimization|material characterization|viscoplasticity,10.1016/j.advengsoft.2004.03.010
215,WOS:000306034100006,2012,Sensitivity analysis for volcanic source modeling quality assessment and model selection,UNCERTAINTY IMPORTANCE MEASURE|AKAIKE INFORMATION CRITERION|COUPLED REACTION SYSTEMS|SURFACE DEFORMATION|RATE COEFFICIENTS|F-TEST|INDEXES|MOTION,"The increasing knowledge and understanding of volcanic sources has led to the development and implementation of sophisticated and complex mathematical models with the main goal of describing field and experimental data. Quantification of the model's ability in describing the data becomes fundamental for a realistic estimate of the model parameters. The analysis of sensitivity can help us in identifying the parameters that significantly affect the model's output and in assessing its quality factor. In this paper, we describe the Global Sensitivity Analysis (GSA) methods based both on Fourier Amplitude Sensitivity Test and on the Sobol' approach and discuss their implementation in a Mat lab software tool (GSAT). We also introduce a new criterion for model selection based on sensitivity analysis. The proposed approach is tested and applied to quantify the fitting ability of an analytic volcanic source model on a synthetic deformation data. Results show the validity of the method, against the traditional approaches, in supporting the volcanic model selection and the flexibility of the GSAT software tool in analyzing the model sensitivity. (C) ", Elsevier Ltd. All rights reserved.,"Cannavo, Flavio",COMPUTERS & GEOSCIENCES,sensitivity analysis|inverse problem|volcanic source|modeling|model selection,10.1016/j.cageo.2012.03.008
220,WOS:000256840200114,2007,Estimating uncertainty on internal dose assessments,RADON PROGENY|UNIT EXPOSURE|DOSIMETRY,"The estimation of uncertainty on doses broadly falls into three categories. () Estimating the uncertainty on prospective doses. Here, the intake is known and the uncertainties in individual parameter values must be propagated through the calculated dose. () Estimating the error or uncertainty on dose assessments made from single measurements. Here, intake, model parameter and measurement uncertainties are propagated into the measurement, but default ICRP parameter values are used to estimate the intake and dose from the measurement. ()Estimating the probability distribution of an individual's dose from a set of monitoring data. Here, Bayesian inference methods must be used to estimate the uncertainty on the estimated dose. A computer code is being developed that performs all three types of uncertainty analysis using Monte Carlo simulation. The software samples biokinetic parameters from probability density functions and then calculates doses from these parameters by calling the dosimetry code IMBA Professional Plus. A description of the methodology, together with an example application of the software, is included in this paper.",,"Puncher, M.|Birchall, A.",RADIATION PROTECTION DOSIMETRY,,10.1093/rpd/ncm361
223,WOS:000302910100010,2012,Robust design in aerodynamics using third-order sensitivity analysis based on discrete adjoint. Application to quasi-1D flows,SHAPE OPTIMIZATION|DATA ASSIMILATION|1ST,"In this paper, the second-order second moment approach, coupled with an adjoint-based steepest descent algorithm, for the solution of the so-called robust design problem in aerodynamics is proposed. Because the objective function for the robust design problem comprises first-order and second-order sensitivity derivatives with respect to the environmental parameters, the application of a gradient-based method , which requires the sensitivities of this function with respect to the design variables, calls for the computation of third-order mixed derivatives. To compute these derivatives with the minimum CPU cost, a combination of the direct differentiation and the discrete adjoint variable method is proposed. This is presented for the first time in the relevant literature and is the most efficient among other possible schemes on condition that the design variables are much more than the environmental ones; this is definitely true in most engineering design problems. The proposed approach was used for the robust design of a duct, assuming a quasi-D flow model; the coordinates of the Bezier control points parameterizing the duct shape are used as design variables, whereas the outlet Mach number and the DarcyWeisbach friction coefficient are used as environmental ones. The extension to D and D flow problems, after developing the corresponding direct differentiation and adjoint variable methods and software, is straightforward."," Copyright (C) 2011 John Wiley & Sons, Ltd.","Papoutsis-Kiachagias, E. M.|Papadimitriou, D. I.|Giannakoglou, K. C.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,robust aerodynamic shape optimization|discreteadjoint method|third-order sensitivity derivatives|method of moments,10.1002/fld.2604
224,WOS:000236011600030,2006,A standard interface between simulation programs and systems analysis software,TRANSPORT PARAMETERS|AQUATIC SYSTEMS|UNCERTAINTY|MODELS|MINIMIZATION|FLOW,"A simple interface between simulation programs and systems analytical software is proposed. This interface is designed to facilitate linkage of environmental simulation programs with systems analytical software and thus can contribute to remedying the deficiency in applying systems analytical techniques to environmental modelling studies. The proposed concept, consisting of a text file interface combined with a batch mode simulation program call, is independent of model structure, operating system and programming language. It is open for implementation by academic and commercial simulation and systems analytical software developers and is very simple to implement. Its practicability is demonstrated by implementations for three environmental simulation packages (AQUASIM, SWAT and LEACHM) and two systems analytical program packages (UNCSIM, SUR). The properties listed above and the demonstration of the ease of implementation of the approach are prerequisites for the stimulation of a widespread implementation of the proposed interface that would be beneficial for the dissemination of systems analytical techniques in the environmental and engineering sciences. Furthermore, such a development could stimulate the transfer of systems analytical techniques between different fields of application.",,"Reichert, P",WATER SCIENCE AND TECHNOLOGY,systems analytical techniques|environmental simulation programs|statistical inference|sensitivity analysis|identifiability analysis|uncertainty analysis,10.2166/wst.2006.029
227,WOS:000252516900010,2008,Self-adjoint sensitivity analysis of lossy dielectric structures with electromagnetic time-domain simulators,MICROWAVE IMAGE-RECONSTRUCTION|OPTIMAL-DESIGN METHOD|GRIDS,"We present an efficient self-adjoint approach for the computation of response derivatives in lossy inhomogencous structures with time-domain electromagnetic solvers. Our approach yields the responses and their derivatives with only one system analysis regardless of the number of optimizable parameters. The only requirement is to access the field solution at the perturbation grid points. The computation is performed as an independent post-process outside the solver. This makes our approach easy to implement as stand-alone software, which aids microwave design based on commercial computer-aided design packages. We show that our sensitivity analysis approach yields Jacobians of second-order accuracy for lossy dielectric structures. The approach is verified through -D, -D and -D examples using the time-domain field solutions obtained with solvers based on the finite-difference time-domain (FDTD) and transmission line modeling (TLM) methods."," Copyright (c) 2007 John Wiley & Sons, Ltd.","Song, Yunpeng|Li, Ying|Nikolova, Natalia K.|Bakr, Mohamed H.",INTERNATIONAL JOURNAL OF NUMERICAL MODELLING-ELECTRONIC NETWORKS DEVICES AND FIELDS,time-domain analysis|sensitivity analysis|adjoint-variable methods|tlm method|fdtd method,10.1002/jnm.659
232,WOS:000302212800003,2012,Effect of Temporal and Spatial Rainfall Resolution on HSPF Predictive Performance and Parameter Estimation,HYDROLOGICAL SIMULATION PROGRAM|AUTOMATIC CALIBRATION|MODEL PARAMETERS|VARIABILITY|RUNOFF|PRECIPITATION|RADAR|UNCERTAINTY|IMPACT|FLOW,"Watershed-scale rainfall-runoff models are used for environmental management and regulatory modeling applications, but their effectiveness is limited by predictive uncertainties associated with model input data. This study evaluated the effect of temporal and spatial rainfall resolution on the predictive performance of Hydrological Simulation Program-Fortran (HSPF) using manual and automatic calibration procedures. Furthermore, the effect of automatic parameter estimation on the physical significance of calibrated parameter values was evaluated. Temporal resolutions examined included  min,  min,  h, and  h, and spatial resolution effects evaluated included the effect of a spatially averaged network of four rain gauges and Next-Generation Radar (NEXRAD) for selected rain events. Model efficiencies ranged from . to . when individual rain gauges (RG, RG, RG, and RG) were used one at a time. Model efficiency improved and ranged from . to . when a spatially averaged network of four rain gauges was used. The effect of temporal resolution on model performance varied with rain gauge location in the watershed and with use of a single gauge or spatially averaged rain gauges for model calibration. Rainfall resolution has a strong influence on parameter estimation because, to achieve high model performance, parameter values must shift whenever the resolution of the rainfall data is changed. Despite a shift in parameter values as a result of changes in rainfall resolution, the results showed that Parameter Estimation Software (PEST)-calibrated values remained within their parameter bounds. In summary, results obtained from a medium-sized Piedmont watershed in Georgia, USA, revealed that model performance was more sensitive to spatial resolution than temporal resolution. DOI: ./(ASCE)HE.-..", (C) 2012 American Society of Civil Engineers.,"Mohamoud, Yusuf M.|Prieto, Lourdes M.",JOURNAL OF HYDROLOGIC ENGINEERING,hspf|spatial resolution|temporal resolution|parameter estimation|model performance|watershed modeling,10.1061/(ASCE)HE.1943-5584.0000457
234,WOS:000317749400002,2013,Estimation of uncertainty sources in the projections of Lithuanian river runoff,CLIMATE-CHANGE IMPACTS|FLOOD RISK-ASSESSMENT|PARAMETER|MODELS,"Particular attention is given to the reliability of hydrological modelling results. The accuracy of river runoff projection depends on the selected set of hydrological model parameters, emission scenario and global climate model. The aim of this article is to estimate the uncertainty of hydrological model parameters, to perform sensitivity analysis of the runoff projections, as well as the contribution analysis of uncertainty sources (model parameters, emission scenarios and global climate models) in forecasting Lithuanian river runoff. The impact of model parameters on the runoff modelling results was estimated using a sensitivity analysis for the selected hydrological periods (spring flood, winter and autumn flash floods, and low water). During spring flood the results of runoff modelling depended on the calibration parameters that describe snowmelt and soil moisture storage, while during the low water period-the parameter that determines river underground feeding was the most important. The estimation of climate change impact on hydrological processes in the Merkys and Neris river basins was accomplished through the combination of results from AB, A and B emission scenarios and global climate models (ECHAM and HadCM). The runoff projections of the thirty-year periods (-, -, -) were conducted applying the HBV software. The uncertainties introduced by hydrological model parameters, emission scenarios and global climate models were presented according to the magnitude of the expected changes in Lithuanian rivers runoff. The emission scenarios had much greater influence on the runoff projection than the global climate models. The hydrological model parameters had less impact on the reliability of the modelling results.",,"Kriauciuniene, Jurate|Jakimavicius, Darius|Sarauskiene, Diana|Kaliatka, Tadas",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,lithuanian rivers|climate change|hbv|model calibration|sensitivity and uncertainty analysis|susa,10.1007/s00477-012-0608-7
235,WOS:000386560600005,2016,SOFTWARE RELIABILITY GROWTH MODEL WITH TEMPORAL CORRELATION IN A NETWORK ENVIRONMENT,CHANGE-POINT|NOISE,"Increasingly software systems are developed to provide great flexibility to customers but also introduce great uncertainty for system development. The uncertain behavior of fault-detection rate has irregular fluctuation and is described as a Markovian stochastic processes (white noise). However, in many cases the white noise idealization is insufficient, and real fluctuations are always correlated and correlated fluctuations (or colored noise) are non-Markovian stochastic processes. We develop a new model to quantify the uncertainties within non-homogeneous Poisson process (NHPP) in the noisy environment. Based on a stochastic model of the software fault detection process, the environmental uncertainties collectively are treated as a noise of arbitrary distribution and correlation structure. Based on the stochastic model, the analytical solution can be derived. To validate our model, we consider five particular scenarios with distinct environmental uncertainty. Experimental comparisons with existing methods demonstrate that the new framework shows a closer fitting to actual data and exhibits a more accurately predictive power.",,"Xu, Jiajun|Yao, Shuzhen|Yang, Shunkun|Wang, Peng",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,uncertainty quantification|reliability|nhpp|noise|correlation,10.1615/Int.J.UncertaintyQuantification.2016016194
238,WOS:000240365500002,2006,Environmental and ecological hydroinformatics to support the implementation of the European Water Framework Directive for river basin management,DECISION-SUPPORT|MODELING APPROACH|SWAT MODEL|SPECIES RICHNESS|QUALITY MODELS|LANDSCAPE|INTEGRATION|DESIGN|SYSTEM|UNCERTAINTY,"Research and development in hydroinformatics can play an important role in environmental impact assessment by integrating physically-based models, data-driven models and other information and Communication Tools (ICT). An illustration is given in this paper describing the developments around the Soil and Water Assessment Tool (SWAT) to support the implementation of the EU water Framework Directive. SWAT operates on the river basin scale and includes processes for the assessment of complex diffuse pollution; it is open-source software, which allows for site-specific modifications to the source and easy linkage to other hydroinformatics tools. A crucial step in the world-wide applicability of SWAT was the integration of the model into a GIS environment, allowing for a quick model set-up using digital information on terrain elevation, land use and management, soil properties and weather conditions. Model analysis tools can be integrated with SWAT to assist in the tedious tasks of model calibration, parameter optimisation, sensitivity and uncertainty analysis and allows better understanding of the model before addressing scientific and societal questions. Finally, further linkage of SWAT to ecological assessment tools, Land Use prediction tools and tools for optimal Experimental Design shows that SWAT can play an important role in multi-disciplinary eco-environmental impact assessment studies.",,"van Griensven, A.|Breuer, L.|Di Luzio, M.|Vandenberghe, V.|Goethals, P.|Meixner, T.|Arnold, J.|Srinivasan, R.",JOURNAL OF HYDROINFORMATICS,catchment modelling|eco-hydrology|environmental hydroinformatics|eu water framework directive|model integration|swat,10.2166/hydro.2006.010
241,WOS:000088879600005,2000,Scales and similarities in runoff processes with respect to geomorphometry,SPATIAL VARIABILITY|CATCHMENT|SOIL,"Numerous investigations using various techniques have been carried out towards a more detailed understanding of relationships and interactions between catchment morphometry and rainfall-runoff processes. Recently, this research question has become more relevant through the need for accurate, yet simple, computer models simulating the water balance of large areas. Moreover, advances in the analysis of landform morphometry through the availability of high-resolution digital elevation models (DEMs) and powerful geographical information systems (GIS) have enhanced research efforts with this aim. In this study several computer techniques and models were applied to investigate the effects of geomorphometry on rainfall-runoff processes at different scales. The sensitivity of dynamic hydrological processes to comparatively static boundary conditions requires different methods for modelling, analysis and visualization of different kinds of data appropriate to different scales. Therefore an approach integrating several geocomputational concepts, including spatial analysis of different types of geodata, static modelling of spatial structures, dynamic four-dimensional modelling of hydrological processes and statistical techniques was chosen. Geomorphometric analysis of the study sites was carried out with GIS packages (including ARC/INFO and GRASS), special purpose software and self-developed tools. Soil-morphometry relationships were modelled within a GIS environment. Hydrological models (SAKE and TOPMODEL) were then used to simulate rainfall-runoff processes, and finally statistical tools and sensitivity analysis were applied to gain an insight into the hydrological significance of the various geomorphometric properties. The results demonstrate the importance of small subregions of the catchment, particularly those having low slope angles, low flow lengths and concavities. The spatial distribution of soil types significantly influences modelled runoff. Spatial distributions of soil types are partly related to morphometry and can be captured using soil-morphometry models. Further results show that catchments which differ significantly in morphometry show different runoff responses and different hydrological sensitivity to changes in boundary conditions. A crude derivation of geomorphometric-hydrological landform types could be reached. Therefore, geomorphometric classifications of catchment types could form a basis for representative hydrological modelling at the large scale. Models describing soil distribution in relation to geomorphometry could assist regionalization of spatial heterogeneity and structure of soil parameters relevant in hydrological modelling. Moreover, quantification of geomorphometric catchment structure, e.g. in terms of contributing areas, is needed to describe significant geomorphometric catchment characteristics."," Copyright (C) 2000 John Wiley & Sons, Ltd.","Schmidt, J|Hennrich, K|Dikau, R",HYDROLOGICAL PROCESSES,hydrological modelling|geomorphometry|gis|dem|soil-morphometry relationship,10.1002/1099-1085(20000815/30)14:11/12<1963::AID-HYP48>3.0.CO;2-M
245,WOS:000419231500134,2017,Passive Optimization Design Based on Particle Swarm Optimization in Rural Buildings of the Hot Summer and Warm Winter Zone of China,ENERGY PERFORMANCE|RESIDENTIAL BUILDINGS|SENSITIVITY-ANALYSIS|MULTIOBJECTIVE OPTIMIZATION|HIGHRISE BUILDINGS|THERMAL COMFORT|SIMULATION|CONSUMPTION|ENVELOPE|SYSTEM,"The development of green building is an important way to solve the environmental problems of China's construction industry. Energy conservation and energy utilization are important for the green building evaluation criteria (GBEC). The objective of this study is to evaluate the quantitative relationship between building shape parameter, envelope parameters, shading system, courtyard and the energy consumption (EC) as well as the impact on indoor thermal comfort of rural residential buildings in the hot summer and warm winter zone (HWWZ). Taking Quanzhou (Fujian Province of China) as an example, based on the field investigation, EnergyPlus is used to build the building performance model. In addition, the classical particle swarm optimization algorithm in GenOpt software is used to optimize the various factors affecting the EC. Single-objective optimization has provided guidance to the multi-dimensional optimization and regression analysis is used to find the effects of a single input variable on an output variable. Results shows that the energy saving rate of an optimized rural residence is about -% corresponding to the existing rural residence. Moreover, the payback period is about  years. A simple case study is used to demonstrate the accuracy of the proposed optimization analysis. The optimization can be used to guide the design of new rural construction in the area and the energy saving transformation of the existing rural houses, which can help to achieve the purpose of energy saving and comfort.",,"Lu, Shilei|Wang, Ran|Zheng, Shaoqun",SUSTAINABILITY,rural residence|green building|energy consumption|multidimensional optimization|particle swarm optimization|regression analysis,10.3390/su9122288
247,WOS:000392165500023,2017,Shale gas flowback water desalination: Single vs multiple-effect evaporation with vapor recompression cycle and thermal integration,PERFORMANCE EVALUATION|MEMBRANE DISTILLATION|COMPRESSION|SYSTEM|MANAGEMENT|OPTIMIZATION|DESIGN|UNCERTAINTY|RESOURCES|SELECTION,"This paper introduces a new optimization model for the single and multiple-effect evaporation (SEE/MEE) systems design, including vapor recompression cycle and thermal integration. The SEE/MEE model is specially developed for shale gas flowback water desalination. A superstructure is proposed to solve the problem, comprising several evaporation effects coupled with intermediate flashing tanks that are used to enhance thermal integration by recovering condensate vapor. Multistage equipment with intercooling is used to compress the vapor formed by flashing and evaporation. The compression cycle is driven by electricity to operate on the vapor originating from the SEE/MEE system, providing all the energy needed in the process. The mathematical model is formulated as a nonlinear programming (NLP) problem optimized under GAMS software by minimizing the total annualized cost. The SEE/MEE system application for zero liquid discharge (ZLD) is investigated by allowing brine salinity discharge near to salt saturation conditions. Additionally, sensitivity analysis is carried out to evaluate the optimal process configuration and performance under distinct feed water salinity conditions. The results highlight the potential of the proposed model to cost-effectively optimize SEE/MEE systems by producing fresh water and reducing brine discharges and associated environmental impacts. (C)  The Authors.", Published by Elsevier B.V.,"Onishi, Viviani C.|Carrero-Parreno, Alba|Reyes-Labarta, Juan A.|Ruiz-Femenia, Ruben|Salcedo-Diaz, Raquel|Fraga, Eric S.|Caballero, Jose A.",DESALINATION,shale gas|single-effect evaporation (see)|multiple-effect evaporation (mee)|mechanical vapor recompression (mvr)|thermal integration|zero liquid discharge (zld),10.1016/j.desal.2016.11.003
250,WOS:000365335000031,2016,A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes,MULTIOBJECTIVE OPTIMIZATION|EVOLUTIONARY ALGORITHMS|FLOWSHEET OPTIMIZATION|PROCESS SIMULATORS|GENETIC ALGORITHM|DESIGN|METHODOLOGY|PLANTS|PERSPECTIVES|INTEGRATION,"Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab (TM) is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented. (C) ", Elsevier Ltd. All rights reserved.,"Otte, Daniel|Lorenz, Hilke-Marie|Repke, Jens-Uwe",COMPUTERS & CHEMICAL ENGINEERING,optimization|molecular-inspired parallel tempering|multicomponent separation process|retrofit|stochastic algorithm|chemcad (tm),10.1016/j.compchemeng.2015.08.023
251,WOS:000378854600008,2016,"Towards the integration of process design, control and scheduling: Are we getting closer?",MODEL-PREDICTIVE CONTROL|INTEGER DYNAMIC OPTIMIZATION|CONSTRAINED LINEAR-SYSTEMS|OPTIMAL GRADE TRANSITION|CHEMICAL-PROCESSES|BATCH PROCESSES|UNCERTAINTY|FRAMEWORK|FLEXIBILITY|PARAMETERS,"The integration of design and control, control and scheduling and design, control and scheduling, all have been core PSE challenges. While significant progress has been achieved over the years, it is fair to say that at the moment there is not a generally accepted methodology and/or ""protocol"" for such an integration - it is also interesting to note that currently, there is not a commercially available software [or even in a prototype form] system to fully support such an activity. Here, we present the foundations for such an integrated framework and especially a software platform that enables such integration based on research developments over the last  years. In particular, we describe PAROC, a prototype software system which allows for the representation, modeling and solution of integrated design, scheduling and control problems. Its main features include: (i) a high-fidelity dynamic model representation, also involving global sensitivity analysis, parameter estimation and mixed integer dynamic optimization capabilities; (ii) a suite/toolbox of model approximation methods; (iii) a host of multi-parametric programming solvers for mixed continuous/integer problems; (iv) a state-space modeling representation capability for scheduling and control problems; and (v) an advanced toolkit for multi-parametric/explicit Model Predictive Control and moving horizon reactive scheduling problems. Algorithms that enable the integration capabilities of the systems for design, scheduling and control are presented on a case of a series of cogeneration units. (C) ", Elsevier Ltd. All rights reserved.,"Pistikopoulos, Efstratios N.|Diangelakis, Nikolaos A.",COMPUTERS & CHEMICAL ENGINEERING,multi-parametric receding horizon policies|control|design optimization|optimal scheduling|integration,10.1016/j.compchemeng.2015.11.002
254,WOS:000228401800048,2005,Investigating the dynamic behavior of biochemical networks using model families,SYSTEMS BIOLOGY|SIMULATION|CELL,"Motivation: Supporting the evolutionary modeling process of dynamic biochemical networks based on sampled in vivo data requires more than just simulation. In the course of the modeling process, the modeler is typically concerned not only with a single model but also with sequences, alternatives and structural variants of models. Powerful automatic methods are then required to assist the modeler in the organization and the evaluation of alternative models. Moreover, the structure and peculiarities of the data require dedicated tool support. Summary: To support all stages of an evolutionary modeling process, a new general formalism for the combinatorial specification of large model families is introduced. It allows for automatic navigation in the space of models and excludes biologically meaningless models on the basis of elementary flux mode analysis. An incremental usage of the measured data is supported by using splined data instead of state variables. With MMT, a versatile tool has been developed as a computational engine intended to be built into a tool chain. Using automatic code generation, automatic differentiation for sensitivity analysis and grid computing technology, a high performance computing environment is achieved. MMT supplies XML model specification and several software interfaces. The performance of MMT is illustrated by several examples from ongoing research projects.",,"Haunschild, MD|Freisleben, B|Takors, R|Wiechert, W",BIOINFORMATICS,,10.1093/bioinformatics/bti225
257,WOS:000255415000009,2008,Estimation of process parameter variations in a pre-defined process window using a Latin hypercube method,,"The aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre-defined process window. To exemplify the developed methodology, the stochastic simulation technique is used for a sheet-metal forming application. A sampling plan based on the Latin hypercube sampling method for variation of design parameters is utilized, and the thickness reduction is specified as the response. Moreover, the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value. The conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation. Consequently, it gives significant insights into the usefulness of individual design parameters. It has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters. Furthermore, the dominating design parameters can be predicated using sensitivity analysis, and this in its turn clarifies how the reliability criteria are met. Finally, the developed software can be used as an additional module for set-up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications.",,"Moshfegh, R.|Nilsson, L.|Larsson, M.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,stochastical analysis|sensitivity indicator|admissible process parameter variation|finite element method|sheet-metal forming,10.1007/s00158-007-0136-0
259,WOS:000243742700004,2007,The Data Uncertainty Engine (DUE): A software tool for assessing and simulating uncertain environmental variables,MODELING ERROR|REJECTION|BETA|PROPAGATION|PREDICTION|POISSON|GAMMA|GIS,"This paper describes a software tool for: () assessing uncertainties in environmental data; and () generating realisations of uncertain data for use in uncertainty propagation analyses: the ""Data Uncertainty Engine (DUE)"". Data may be imported into DUE from file or from a database, and are represented in DUE as objects whose positions and attribute values may be uncertain. Objects supported by DUE include spatial vectors, spatial rasters, time-series of spatial data, simple time-series and objects that are constant in space and time. Attributes supported by DUE include continuous numerical variables (e.g. rainfall), discrete numerical variables (e.g. bird counts) and categorical variables (e.g. land-cover). Once data are imported, an uncertainty model can be developed for the positional and attribute uncertainties of environmental objects. This is currently limited to probability models, but confidence intervals and scenarios will be provided in the future. Using DUE, the spatial and temporal patterns of uncertainty (autocorrelation), as well as cross-correlations between related inputs, can be incorporated into an uncertainty analysis. Alongside expert judgement, sample data may be used to help estimate uncertainties, and to reduce the uncertainty of the simulated output by ensuring each realisation reproduces the sample data. Most importantly, DUE provides a conceptual framework for structuring an uncertainty analysis, allowing users without direct experience of uncertainty methods to develop realistic uncertainty models for their data. (c) ", Elsevier Ltd. All rights reserved.,"Brown, James D.|Heuvelink, Gerard B. M.",COMPUTERS & GEOSCIENCES,uncertainty analysis|monte carlo|uncertainty propagation|java,10.1016/j.cageo.2006.06.015
261,WOS:000384855300018,2016,On ISSM and leveraging the Cloud towards faster quantification of the uncertainty in ice-sheet mass balance projections,NORTHEAST GREENLAND|MODEL|FLOW|SENSITIVITY|CREEP,"With the Amazon EC Cloud becoming available as a viable platform for parallel computing, Earth System Models are increasingly interested in leveraging its capabilities towards improving climate projections. In particular, faced with long wait periods on high-end clusters, the elasticity of the Cloud presents a unique opportunity of potentially ""infinite"" availability of small-sized clusters running on high-performance instances. Among specific applications of this new paradigm, we show here how uncertainty quantification in climate projections of polar ice sheets (Antarctica and Greenland) can be significantly accelerated using the Cloud. Indeed, small-sized clusters are very efficient at delivering sensitivity and sampling analysis, core tools of uncertainty quantification. We demonstrate how this approach was used to carry out an extensive analysis of ice-flow projections on one of the largest basins in Greenland, the North-East Greenland Glacier, using the Ice Sheet System Model, the public-domain NASA-funded ice-flow modeling software. We show how errors in the projections were accurately quantified using Monte-Carlo sampling analysis on the EC Cloud, and how a judicious mix of high-end parallel computing and Cloud use can best leverage existing infrastructures, and significantly accelerate delivery of potentially ground-breaking climate projections, and in particular, enable uncertainty quantification that were previously impossible to achieve. (C) ", Elsevier Ltd. All rights reserved.,"Larour, E.|Schlegel, N.",COMPUTERS & GEOSCIENCES,polar|ice sheet|modeling|cloud|uncertainty quantification,10.1016/j.cageo.2016.08.007
265,WOS:000265341800001,2009,GUI-HDMR - A software tool for global sensitivity analysis of complex models,STREET CANYON MODEL|ENVIRONMENTAL-MODELS|RS-HDMR|REPRESENTATIONS|UNCERTAINTY|PARAMETERS|INDEXES|OUTPUT,"The high dimensional model representation (HDMR) method is a set of tools which can be used to construct a fully functional metamodel and to calculate variance based sensitivity indices very efficiently. Extensions to the existing set of random sampling (RS)-HDMR tools have been developed in order to make the method more applicable for complex models with a large number of input parameters as often appear in environmental modelling. The HDMR software described here combines the RS-HDMR tools and its extensions in one Matlab package equipped with a graphical user interface (GUI). This makes the HDMR method easily available for all interested users. The performance of the GUI-HDMR software has been tested in this paper using two analytical test models, the Ishigami function and the Sobol' g-function. In both cases the model is highly non-linear, non-monotonic and has significant parameter interactions. The developed GUI-HDMR software copes very well with the test cases and sensitivity indices of first and second order could be calculated accurately with only low computational effort. The efficiency of the software has also been compared against other recently developed approaches and is shown to be competitive. GUI-HDMR can be applied to a wide range of applications in all fields, because in principle only one random or quasi-random set of input and output values is required to estimate all sensitivity indices up to second order. The size of the set of samples is however dependent on the problem and can be successively increased if additional accuracy is required. A brief description of its application within a range of modelling environments is given. (C) ", Elsevier Ltd. All rights reserved.,"Ziehn, T.|Tomlin, A. S.",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis|high dimensional model representation|random sampling|matlab software|graphical user interface,10.1016/j.envsoft.2008.12.002
266,WOS:000419225500038,2017,Quantifying Roughness Coefficient Uncertainty in Urban Flooding Simulations through a Simplified Methodology,POLYNOMIAL CHAOS|SWMM MODEL|INUNDATION|FLOW|CALIBRATION|PARAMETERS|CATCHMENT|SYSTEMS,"A methodology is presented which can be used in the evaluation of parametric uncertainty in urban flooding simulation. Due to the fact that such simulations are time consuming, the following methodology is proposed: (a) simplification of the description of the physical process; (b) derivation of a training data set; (c) development of a data-driven surrogate model; (d) use of a forward uncertainty propagation scheme. The simplification comprises the following steps: (a) unit hydrograph derivation using a D hydrodynamic model; (b) calculation of the losses in order to determine the effective rainfall depth; (c) flood event simulation using the principle of the proportionality and superposition. The above methodology was implemented in an urban catchment located in the city of Athens, Greece. The model used for the first step of the simplification was FLOW-RD, whereas the well-known SWMM software (US Environmental Protection Agency, Washington, DC, USA) was used for the second step of the simplification. For the training data set derivation, an ensemble of  Unit Hydrographs was derived with the FLOW-RD model. The parameters which were modified in order to produce this ensemble were the Manning coefficients in the two friction zones (residential and urban open space areas). The surrogate model used to replicate the unit hydrograph derivation, using the Manning coefficients as an input, was based on the Polynomial Chaos Expansion technique. It was found that, although the uncertainties in the derived results have to be taken into account, the proposed methodology can be a fast and efficient way to cope with dynamic flood simulation in an urban catchment.",,"Bellos, Vasilis|Kourtis, Ioannis M.|Moreno-Rodenas, Antonio|Tsihrintzis, Vassilios A.",WATER,urban flooding|swmm|flow-r2d|uncertainty|surrogate models|polynomial chaos expansion,10.3390/w9120944
267,WOS:000280656300004,2010,Simulation model for extended double-ended queueing,QUEUES|IMPATIENCE|CUSTOMERS,"The purpose of this paper is to extend traditional double-ended queuing models using a simulation approach. Traditional double-ended queuing models assume that one supply queue should satisfy one demand queue through instantaneous pairing. Inter-arrival time is assumed to follow an exponential distribution, with arrivals to the system assumed to occur just one at a time. However, this assumption is frequently violated in many real-world situations. The pairing or batch size can either be multiple or a random variable, and the pairing processing time can be greater than . Inter-arrival time may follow distributions other than exponential. In some cases bulk arrivals may come at the same time, and pairing is not always guaranteed. Because the analytical approach has enormous difficulties obtaining performance measures under these relaxed situations, a simulation approach for extended double-ended queueing processes is presented. This includes an algorithm to find state probabilities and a newly developed simulation procedure. Using this new procedure, sensitivity analyses of performance measures were performed using various input conditions implemented using ProModel and SimRumnner simulation software. A business case is studied to demonstrate the versatility of the proposed approaches. (c) ", Elsevier Ltd. All rights reserved.,"Kim, Won Kyung|Yoon, K. Paul|Mendoza, Gaston|Sedaghat, Mohammad",COMPUTERS & INDUSTRIAL ENGINEERING,double-ended queue|simulation|state probability|optimization|job placement agency,10.1016/j.cie.2010.04.002
270,WOS:000356196000011,2015,PUQ; A code for non-intrusive uncertainty propagation in computer simulations,STRENGTH|MAXIMUM|SCIENCE|CHAOS,"We present a software package for the non-intrusive propagation of uncertainties in input parameters through computer simulation codes or mathematical models and associated analysis; we demonstrate its use to drive micromechanical simulations using a phase field approach to dislocation dynamics. The PRISM uncertainty quantification framework (PUQ) offers several methods to sample the distribution of input variables and to obtain surrogate models (or response functions) that relate the uncertain inputs with the quantities of interest (QoIs); the surrogate models are ultimately used to propagate uncertainties. PUQ requires minimal changes in the simulation code, just those required to annotate the QoI(s) for its analysis. Collocation methods include Monte Carlo, Latin Hypercube and Smolyak sparse grids and surrogate models can be obtained in terms of radial basis functions and via generalized polynomial chaos. PUQ uses the method of elementary effects for sensitivity analysis in Smolyak runs. The code is available for download and also available for cloud computing in nanoHUB. PUQ orchestrates runs of the nanoPLASTICITY tool at nanoHUB where users can propagate uncertainties in dislocation dynamics simulations using simply a web browser, without downloading or installing any software. Program summary Program title: PUQ Catalogue identifier: AEWP_v_ Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEWP_v_.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT license No. of lines in distributed program, including test data, etc.:  No. of bytes in distributed program, including test data, etc.:  Distribution format: tar.gz Programming language: Python, C. Computer: Workstations. Operating system: Linux, Mac OSX. Classification: ., ., ., External routines: SciPy, Matplotlib, hpy Nature of problem: Uncertainty propagation and creation of response surfaces. Solution method: Generalized Polynomial Chaos (gPC) using Smolyak sparse grids. Running time: PUQ performs uncertainty quantification and sensitivity analysis by running a simulation multiple times using different values for input parameters. Its run time will be the product of the run time of the chosen simulation code and the number of runs required to achieve the desired accuracy.", (C) 2015 Elsevier B.V. All rights reserved.,"Hunt, Martin|Haley, Benjamin|McLennan, Michael|Koslowski, Marisol|Murthy, Jayathi|Strachan, Alejandro",COMPUTER PHYSICS COMMUNICATIONS,uncertainty quantification|surrogate model|sensitivity analysis,10.1016/j.cpc.2015.04.011
272,WOS:000329561100024,2014,A comprehensive evaluation of various sensitivity analysis methods: A case study with a hydrological model,RAINFALL-RUNOFF MODELS|IMPROVED CALIBRATION|COMPUTER EXPERIMENTS|GLOBAL OPTIMIZATION|REGRESSION|INDEXES|SYSTEMS|OUTPUT,"Sensitivity analysis (SA) is a commonly used approach for identifying important parameters that dominate model behaviors. We use a newly developed software package, a Problem Solving environment for Uncertainty Analysis and Design Exploration (PSUADE), to evaluate the effectiveness and efficiency of ten widely used SA methods, including seven qualitative and three quantitative ones. All SA methods are tested using a variety of sampling techniques to screen out the most sensitive (i.e., important) parameters from the insensitive ones. The Sacramento Soil Moisture Accounting (SAC-SMA) model, which has thirteen tunable parameters, is used for illustration. The South Branch Potomac River basin near Springfield, West Virginia in the U.S. is chosen as the study area. The key findings from this study are: () For qualitative SA methods, Correlation Analysis (CA), Regression Analysis (RA), and Gaussian Process (GP) screening methods are shown to be not effective in this example. Morris One-At-a-Time (MOAT) screening is the most efficient, needing only  samples to identify the most important parameters, but it is the least robust method. Multivariate Adaptive Regression Splines (MARS), Delta Test (DT) and Sum-Of-Trees (SOT) screening methods need about - samples for the same purpose. Monte Carlo (MC), Orthogonal Array (OA) and Orthogonal Array based Latin Hypercube (OALH) are appropriate sampling techniques for them; () For quantitative SA methods, at least  samples are needed for Fourier Amplitude Sensitivity Test (FAST) to identity parameter main effect. McKay method needs about  samples to evaluate the main effect, more than  samples to assess the two-way interaction effect. OALH and LP tau (LPTAU) sampling techniques are more appropriate for McKay method. For the Sobol' method, the minimum samples needed are  to compute the first-order and total sensitivity indices correctly. These comparisons show that qualitative SA methods are more efficient but less accurate and robust than quantitative ones. (C)  The Authors. Published by", Elsevier Ltd. All rights reserved.,"Gan, Yanjun|Duan, Qingyun|Gong, Wei|Tong, Charles|Sun, Yunwei|Chu, Wei|Ye, Aizhong|Miao, Chiyuan|Di, Zhenhua",ENVIRONMENTAL MODELLING & SOFTWARE,uncertainty quantification|sensitivity analysis|parameter screening|space-filling sampling|psuade,10.1016/j.envsoft.2013.09.031
274,WOS:000262497400004,2008,Fidelity of Network Simulation and Emulation: A Case Study of TCP-Targeted Denial of Service Attacks,,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",,"Chertov, Roman|Fahmy, Sonia|Shroff, Ness B.",ACM TRANSACTIONS ON MODELING AND COMPUTER SIMULATION,simulation|emulation|testbeds|tcp|congestion control|denial of service attacks|low-rate tcp-targeted attacks,10.1145/1456645.1456649
275,WOS:000417388800006,2017,Direct effect of atmospheric turbulence on plume rise in a neutral atmosphere,LARGE-EDDY SIMULATION|DIRECT NUMERICAL-SIMULATION|POLLUTANT DISPERSION|CROSS-FLOW|BOUNDARY-LAYERS|FINITE-ELEMENT|CFD SIMULATION|ENVIRONMENT|TERRAINS|MODELS,"The direct effect of atmospheric turbulence on plume rise in the current research work is studied through examining the turbulence intensity parameter. A hybrid unsteady Reynolds averaged Navier Stokes (RANS) and large eddy simulation (LES) numerical approach is applied with a new mixed scale sub-grid parameterization technique in the commercial ANSYS Fluent software in order to simulate the buoyant plume behavior in a turbulent crossflow. The accuracy of the simulation method is crosschecked against the wind tunnel data available in the literature. The numerical simulation results in various operating conditions are used to derive a new plume rise formula in which the direct effect of atmospheric turbulence intensity at stack height (I-Air) is explicitly introduced in the plume rise formula. Furthermore, the buoyancy parameter of the flue gas is determined at some distances upstream of the stack top surface to include the whole effects of source buoyancy on the plume rise. The value of I-Air at stack height is obtained by measuring the standard deviation of wind velocity at stack height. The sensitivity analysis showed that by increasing the atmospheric turbulence intensity, the final plume rise decreases because of the updraft and downdraft motions of turbulence and it has been found that there is a linear dependency between the plume rise and ( I-Air)(-.). The quantile-quantile plots show that the new model can predict the simulated plume rise with a deviation factor of . whereas the conventional models overestimate the final plume rise at least by a factor of ..", (C) 2017 Turkish National Committee for Air Pollution Research and Control. Production and hosting by Elsevier B.V. All rights reserved.,"Ashrafi, Khosro|Orkomi, Ali Ahmadi|Motlagh, Majid Shafipour",ATMOSPHERIC POLLUTION RESEARCH,neutral|numerical model|plume rise|rans-les method|turbulence,10.1016/j.apr.2017.01.001
277,WOS:000287575500001,2011,Laboratory and numerical modeling of water balance in a layered sloped soil cover with channel flow pathway over mine waste rock,WETTING FRONT INSTABILITY|ENGINEERED TEST COVERS|CHARACTERISTIC CURVE|UNSATURATED SOILS|OXYGEN BARRIERS|WHISTLE MINE|TAILINGS|INFILTRATION|EVAPORATION|ONTARIO,"Macropores developed in barrier layers in soil covers overlying acid-generating waste rock may produce preferential flow through the barrier layers and compromise cover performance. However, little has been published on the effects of preferential flow on water balance in soil covers. In the current study, an inclined, layered soil cover with a -cm-wide sand-filled channel pathway in a silty clay barrier layer was built over reactive waste rock in the laboratory. The channel or preferential flow pathway represented the aggregate of cracks or fissures that may occur in the barrier during compaction and/or climate-induced deterioration. Precipitation, runoff, interflow, percolation, and water content were recorded during the test. A commercial software VADOSE/W was used to simulate the measured water balance and to conduct further sensitivity analysis on the effects of the location of the channel and the saturated hydraulic conductivity of the channel material on water balance. The maximum percolation, .% of the total precipitation, was obtained when the distance between the mid-point of the channel pathway and the highest point on the slope accounted for % of the total horizontal length of the soil cover. The modeled percolation increased steadily with an increase in the hydraulic conductivity of the channel material. Percolation was found to be sensitive to the location of the channel and the saturated hydraulic conductivity of the channel material, confirming that proper cover design and construction should aim at minimizing the development of vertical preferential flow in barrier layers. The sum of percolation and interflow was relatively constant when the location of the channel changed along the slope, which may be helpful in locating preferential flow pathways and repairing the barrier.",,"Song, Qing|Yanful, Ernest K.",ENVIRONMENTAL EARTH SCIENCES,acid rock drainage|channel flow|laboratory test|soil cover|vadose/w|water balance,10.1007/s12665-010-0488-4
281,WOS:000344387000014,2014,Non-stationary extreme value analysis in a changing climate,DIFFERENTIAL EVOLUTION|MODEL PROJECTIONS|RETURN LEVELS|SIMULATIONS|EVENTS|TEMPERATURE|VARIABILITY|ENSEMBLE|IMPACTS|RECORDS,"This paper introduces a framework for estimating stationary and non-stationary return levels, return periods, and risks of climatic extremes using Bayesian inference. This framework is implemented in the Non-stationary Extreme Value Analysis (NEVA) software package, explicitly designed to facilitate analysis of extremes in the geosciences. In a Bayesian approach, NEVA estimates the extreme value parameters with a Differential Evolution Markov Chain (DE-MC) approach for global optimization over the parameter space. NEVA includes posterior probability intervals (uncertainty bounds) of estimated return levels through Bayesian inference, with its inherent advantages in uncertainty quantification. The software presents the results of non-stationary extreme value analysis using various exceedance probability methods. We evaluate both stationary and non-stationary components of the package for a case study consisting of annual temperature maxima for a gridded global temperature dataset. The results show that NEVA can reliably describe extremes and their return levels.",,"Cheng, Linyin|AghaKouchak, Amir|Gilleland, Eric|Katz, Richard W.",CLIMATIC CHANGE,,10.1007/s10584-014-1254-5
282,WOS:000254878500014,2008,In silico Biochemical Reaction Network Analysis (IBRENA): a package for simulation and analysis of reaction networks,SIGNAL-TRANSDUCTION|SOFTWARE,"We present In silico Biochemical Reaction Network Analysis (IBRENA), a software package which facilitates multiple functions including cellular reaction network simulation and sensitivity analysis (both forward and adjoint methods), coupled with principal component analysis, singular-value decomposition and model reduction. The software features a graphical user interface that aids simulation and plotting of in silico results. While the primary focus is to aid formulation, testing and reduction of theoretical biochemical reaction networks, the program can also be used for analysis of high-throughput genomic and proteomic data.",,"Liu, Gang|Neelamegham, Sriram",BIOINFORMATICS,,10.1093/bioinformatics/btn061
283,WOS:000338551800010,2014,Risk and uncertainty analysis of gas pipeline failure and gas combustion consequence,MODELS|DETONATION|METHANE,"Taking into account a general concept of risk parameters and knowing that natural gas provides very significant portion of energy, firstly, it is important to insure that the infrastructure remains as robust and reliable as possible. For this purpose, authors present available statistical information and probabilistic analysis related to failures of natural gas pipelines. Presented historical failure data is used to model age-dependent reliability of pipelines in terms of Bayesian methods, which have advantages of being capable to manage scarcity and rareness of data and of being easily interpretable for engineers. The performed probabilistic analysis enables to investigate uncertainty and failure rates of pipelines when age-dependence is significant and when it is not relevant. The results of age-dependent modeling and analysis of gas pipeline reliability and uncertainty are applied to estimate frequency of combustions due to natural gas release when pipeline failure occurs. Estimated age-dependent combustion frequency is compared and proposed to be used instead of conservative and age-independent estimate. The rupture of a high-pressure natural gas pipeline can lead to consequences that can pose a significant threat to people and property in the close vicinity to the pipeline fault location. The dominant hazard is combustion and thermal radiation from a sustained fire. The second purpose of the paper is to present the combustion consequence assessment and application of probabilistic uncertainty analysis for modeling of gas pipeline combustion effects. The related work includes performance of the following tasks: to study gas pipeline combustion model, to identify uncertainty of model inputs noting their variation range, and to apply uncertainty and sensitivity analysis for results of this model. The performed uncertainty analysis is the part of safety assessment that focuses on the combustion consequence analysis. Important components of such uncertainty analysis are qualitative and quantitative analysis that identifies the most uncertain parameters of combustion model, assessment of uncertainty, analysis of the impact of uncertain parameters on the modeling results, and communication of the results' uncertainty. As outcome of uncertainty analysis the tolerance limits and distribution function of thermal radiation intensity are given. The measures of uncertainty and sensitivity analysis were estimated and outcomes presented applying software system for uncertainty and sensitivity analysis. Conclusions on the importance of the parameters and sensitivity of the results are obtained using a linear approximation of the model under analysis. The outcome of sensitivity analysis confirms that distance from the fire center has the greatest influence on the heat flux caused by gas pipeline combustion.",,"Alzbutas, Robertas|Iesmantas, Tomas|Povilaitis, Mantas|Vitkute, Jurate",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,risk parameters|natural gas pipelines|age-dependent reliability|bayesian inference|gas combustion|uncertainty and sensitivity analysis,10.1007/s00477-013-0845-4
288,WOS:000259895700004,2008,Design of sustainable chemical processes: Systematic retrofit analysis generation and evaluation of alternatives,EFFICIENCY|NETWORKS,"The objective of this paper is to present a generic and systematic methodology for identifying the feasible retrofit design alternatives of any chemical process. The methodology determines a set of mass and energy indicators from steady-state process data, establishes the operational and design targets, and through a sensitivity-based analysis, identifies the design alternatives that can match a set of design targets. The significance of this indicator-based method is that it is able to identify alternatives, where one or more performance criteria (factors) move in the same direction thereby eliminating the need to identify tradeoff-based solutions. These indicators are also able to reduce (where feasible) a set of safety indicators. An indicator sensitivity analysis algorithm has been added to the methodology to define design targets and to generate sustainable process alternatives. A computer-aided tool has been developed to facilitate the calculations needed for the application of the methodology. The application of the indicator-based methodology and the developed software are highlighted through a process flowsheet for the production of vinyl chlorine monomer (VCM). (C)  The Institution of Chemical Engineers.", Published by Elsevier B.V. All rights reserved.,"Carvalho, Ana|Gani, Rafiqul|Matos, Henrique",PROCESS SAFETY AND ENVIRONMENTAL PROTECTION,process design|sustainability metrics|mass and energy indicators|safety index|indicator sensitivity algorithm,10.1016/j.psep.2007.11.003
289,WOS:000249895700008,2007,Automatic concept model generation for optimisation and robust design of passenger cars,,"A fully automated method of structural optimisation for the body in white structure is presented. The body in white is a technical term for the car body without windows and closures. The iterations in the optimisation loop comprise the following steps: fully parameterised design creation, automated meshing and model assembly, parallel computation and evaluation. For this purpose several free and commercially available software applications were combined, including: SFE concept, Hypermesh, Perl, Matlab, and Radioss. The optimisation was conducted using Genetic Algorithms (GA), which are ideally suited to solve problems with solution spaces that are too large to be exhaustively searched. The viability of the method is demonstrated for a vehicle component model of a front bumper system utilizing both material and geometry related properties as design variables. (c) ", Elsevier Ltd. and Civil-Comp Ltd. All rights reserved.,"Hilmann, J.|Paas, M.|Haenschke, A.|Vietor, T.",ADVANCES IN ENGINEERING SOFTWARE,vehicle engineering|structural optimisation|sfe concept|genetic algorithms|finite element method|parametric modelling|sensitivity analysis,10.1016/j.advengsoft.2006.08.031
291,WOS:000344530400002,2014,An SMT Based Method for Optimizing Arithmetic Computations in Embedded Software Code,PROGRAMS|EXAMPLES,"We present a new method for optimizing the source code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of the satisfiability modulo theory (SMT) solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on a new inductive synthesis procedure, which is guaranteed to find a valid solution as long as such solution exists. Furthermore, we propose an incremental optimization procedure, which applies the synthesis procedure only to small code regions-one at a time-as opposed to the entire program, which is crucial for scaling the method up to programs of realistic size and complexity. We have implemented our new method in a software tool based on the Clang/LLVM compiler frontend and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and digital signal processing applications, show that the method is both effective and efficient in optimizing arithmetic computations in embedded software code.",,"Eldib, Hassan|Wang, Chao",IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS,fixed point arithmetic|inductive program synthesis|satisfiability modulo theory (smt) solver|superoptimization,10.1109/TCAD.2014.2341931
293,WOS:000241005500006,2006,Distance-based and stochastic uncertainty analysis for multi-criteria decision analysis in Excel using Visual Basic for Applications,SENSITIVITY-ANALYSIS|GENETIC ALGORITHMS|WATER-RESOURCES|MANAGEMENT|SOFTWARE|SUPPORT,"A program has been developed in Excel and written in Visual Basic for Applications, which enables a decision maker to examine the robustness of a solution obtained when using multi-criteria decision analysis (MCDA). The distance-based and stochastic uncertainty analysis approaches contained in the program allow a decision to be made with confidence that the alternative chosen is the best performing alternative under the range of probable circumstances. The uncertainty analysis methodology overcomes the limitations of existing sensitivity analysis techniques for MCDA by enabling all of the input parameters to be varied simultaneously within their expected ranges. The Weighted Sum Method (WSM) and PROMETHEE are the MCDA techniques available for the user to select in the program. The program is illustrated by applying it to a sustainable water resource development problem in the Northern Adelaide Plains, South Australia. (c) ", Elsevier Ltd. All fights reserved.,"Hyde, K. M.|Maier, H. R.",ENVIRONMENTAL MODELLING & SOFTWARE,multi-criteria decision analysis|visual basic|uncertainty analysis|excel|water resources,10.1016/j.envsoft.2005.08.004
295,WOS:000174593500001,2002,Use of the most likely failure point method for risk estimation and risk uncertainty analysis,,"The most likely failure point (MLFP) method, developed within the field of structural reliability analysis (where it is known as the FORM/SORM method) is a technique for estimating the risk (probability) that a calculated quantity Q exceeds a set limit Q(lim) when some or all of the inputs to the calculation are uncertain. It can be used as an efficient stand-alone method for this type of risk calculation. However, for application within the field of toxic hazards, it is proposed as a means for performing sensitivity analyses, possibly in parallel with a risk calculation carried out by conventional methods. The basis of the method is outlined and its use is demonstrated by means of an example calculation of the risk arising froth an installation containing chlorine. The calculation uses, as a consequence model, commercial software for the prediction of dense gas transport. The risk estimate is shown to be acceptably close to that obtained by the Monte Carlo method. The use of a proposed screening procedure utilising the sensitivity formulas that the method provides, in order to identify the most significant uncertainties, is demonstrated. The identification of a single set of input values containing sufficient information to summarise (at least approximately) the entire risk analysis is considered to be an important feature of the method and is proposed as the basis of a means for assessing the validity of the consequence model.", (C) 2002 Elsevier Science B.V All rights reserved.,"Mitchell, B",JOURNAL OF HAZARDOUS MATERIALS,risk|toxic|hazard|uncertainty|sensitivity,10.1016/S0304-3894(01)00378-8
305,WOS:000388092400006,2016,"ALBANY: USING COMPONENT-BASED DESIGN TO DEVELOP A FLEXIBLE, GENERIC MULTIPHYSICS ANALYSIS CODE",EMBEDDED ANALYSIS CAPABILITIES|MANAGING SOFTWARE COMPLEXITY|FINITE-ELEMENT|PARALLEL|SIMULATION|FRAMEWORK|EQUATIONS|SYSTEMS|LIBRARY,"Albany is a multiphysics code constructed by assembling a set of reusable, general components. It is an implicit, unstructured grid finite element code that hosts a set of advanced features that are readily combined within a single analysis run. Albany uses template-based generic programming methods to provide extensibility and flexibility; it employs a generic residual evaluation interface to support the easy addition and modification of physics. This interface is coupled to powerful automatic differentiation utilities that are used to implement efficient nonlinear solvers and preconditioners, and also to enable sensitivity analysis and embedded uncertainty quantification capabilities as part of the forward solve. The flexible application programming interfaces in Albany couple to two different adaptive mesh libraries; it internally employs generic integration machinery that supports tetrahedral, hexahedral, and hybrid meshes of user specified order. We present the overall design of Albany, and focus on the specifics of the integration of many of its advanced features. As Albany and the components that form it are openly available on the internet, it is our goal that the reader might find some of the design concepts useful in their own work. Albany results in a code that enables the rapid development of parallel, numerically efficient multiphysics software tools. In discussing the features and details of the integration of many of the components involved, we show the reader the wide variety of solution components that are available and what is possible when they are combined within a simulation capability.",,"Salinger, Andrew G.|Bartlett, Roscoe A.|Bradley, Andrew M.|Chen, Qiushi|Demeshko, Irina P.|Gao, Xujiao|Hansen, Glen A.|Mota, Alejandro|Muller, Richard P.|Nielsen, Erik|Ostien, Jakob T.|Pawlowski, Roger P.|Perego, Mauro|Phipps, Eric T.|Sun, WaiChing|Tezaur, Irina K.",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,partial differential equations|finite element analysis|template-based generic programming,10.1615/IntJMultCompEng.2016017040
306,WOS:000291785400002,2011,Concurrent Decisions on Design Concept and Material Using Analytical Hierarchy Process at the Conceptual Design Stage,MATERIALS SELECTION|SYSTEM,"There is an increased study for considering the precise decisions on the design concept (DC) and material concurrently at the early stage of development of product. Inappropriate decisions on DC and material always lead to huge cost involvement and ultimately drive toward premature component or product failure. To overcome this problem, concurrent engineering (CE) is an approach which allows designers to consider early decision making (EDM) need to be implemented. To illustrate the use of CE principle at the early stage of design process, a concept selection framework called concurrent DC selection and materials selection (CDCSMS) was proposed. In order to demonstrate the proposed CDCSMS framework, eight DC s and six different types of composite materials of automotive bumper beam have been considered. Both of these decisions were then verified by performing various scenarios of sensitivity analysis by using analytical hierarchy process through utilizing Expert Choice software.",,"Hambali, A.|Sapuan, S. M.|Rahim, A. S.|Ismail, N.|Nukman, Y.",CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS,concurrent engineering|analytical hierarchy process|early decision making,10.1177/1063293X11408138
307,WOS:000257041800013,2008,Selection of infectious medical waste disposal firms by using the analytic hierarchy process and sensitivity analysis,DELPHI,"While Taiwanese hospitals dispose of large amounts of medical waste to ensure sanitation and personal hygiene, doing so inefficiently creates potential environmental hazards and increases operational expenses. However, hospitals lack objective criteria to select the most appropriate waste disposal firm and evaluate its performance, instead relying on their own subjective judgment and previous experiences. Therefore, this work presents an analytic hierarchy process (AHP) method to objectively select medical waste disposal firms based on the results of interviews with experts in the field, thus reducing overhead costs and enhancing medical waste management. An appropriate weight criterion based on AHP is derived to assess the effectiveness of medical waste disposal firms. The proposed AHP-based method offers a more efficient and precise means of selecting medical waste firms than subjective assessment methods do, thus reducing the potential risks for hospitals. Analysis results indicate that the medical sector selects the most appropriate infectious medical waste disposal firm based on the following rank: matching degree, contractor's qualifications, contractor's service capability, contractor's equipment and economic factors, By providing hospitals with an effective means of evaluating medical waste disposal firms, the proposed AHP method can reduce overhead costs and enable medical waste management to understand the market demand in the health sector. Moreover, performed through use of Expert Choice software, sensitivity analysis can survey the criterion weight of the degree of influence with an alternative hierarchy. (C) ", Elsevier Ltd. All rights reserved.,"Hsu, Pi-Fang|Wu, Cheng-Ru|Li, Ya-Ting",WASTE MANAGEMENT,,10.1016/j.wasman.2007.05.016
308,WOS:000397072800012,2017,On the variational data assimilation problem solving and sensitivity analysis,LAPLACE TRANSFORM INVERSION|COVARIANCE MATRICES|CONDITION NUMBER|REGULARIZATION|IMPLEMENTATION,"We consider the Variational Data Assimilation (VarDA) problem in an operational framework, namely, as it results when it is employed for the analysis of temperature and salinity variations of data collected in closed and semi closed seas. We present a computing approach to solve the main computational kernel at the heart of the VarDA problem, which outperforms the technique nowadays employed by the oceanographic operative software. The new approach is obtained by means of Tikhonov regularization. We provide the sensitivity analysis of this approach and we also study its performance in terms of the accuracy gain on the computed solution. We provide validations on two realistic oceanographic data sets.", (C) 2017 Elsevier Inc. All rights reserved.,"Arcucci, Rossella|D'Amore, Luisa|Pistoia, Jenny|Toumi, Ralf|Murli, Almerico",JOURNAL OF COMPUTATIONAL PHYSICS,data assimilation|sensitivity analysis|inverse problem,10.1016/j.jcp.2017.01.034
309,WOS:000186661200005,2003,Development and application of computer simulation tools for ecological risk assessment,FOOD WEBS|EXPOSURE|MODEL|WATER|CONTAMINANTS|CHEMICALS|TOXICITY|ROUTES|SOIL,"Based on a review of available models for ecological risk estimation, most are site-specific and their applications are limited. However, general models, which can be easily adapted to other sites, remain few, in addition, they are simple and associated with significant uncertainties. In this paper, an approach is introduced for an ecological risk assessment ( ERA) model that can be modified for site-specific conditions. Using computer simulation as a screening tool for ecological risk assessment can assist environmental managers and policy decision-makers in the planning and implementation of potentially highly focused assessments and remediation, should the ERA dictate the need. The model was integrated with a Windows-based interface and interactive database management system (DBMS) as a user-friendly software package. In addition, based on trophic sources, a food web has been integrated into the framework of the DBMS. In an effort to evaluate the model, a case study was implemented to characterize the effects on an ecosystem of replacing electroplated chromium coatings with sputtered tantalum at U. S. Army Yuma and Aberdeen Proving Grounds. Potential exposure pathways included ingestion, inhalation, and dermal absorption for terrestrial animals; root and foliar uptake for plants; and direct absorption for aquatic species. Overall, results showed that the most significant exposure resulted from molybdenum and hexavalent chromium, which posed higher risks to select aquatic and terrestrial species at both sites. On the other hand, tantalum ( with vanadium as the surrogate) resulted in the least risk to all receptors within the studied areas. A sensitivity analysis demonstrated that soil-water distribution coefficients have a significant impact on the results. Based on the results, neither molybdenum nor chromium are recommended as a coating in gun barrels, and further study would be essential to address any affected firing range area. Tantalum is recommended for use, although for those species receiving a slight adverse risk, field investigations that include receptor sampling maybe necessary once soil/sediment and water sampling validates projected concentrations.",,"Lu, HY|Axe, L|Tyson, TA",ENVIRONMENTAL MODELING & ASSESSMENT,ecological risk assessment|exposure model|heavy metals|food web,10.1023/B:ENMO.0000004585.85305.3d
310,WOS:000343083600028,2014,MCMC_CLIB-an advanced MCMC sampling package for ODE models,,"A Summary: We present a new C implementation of an advanced Markov chain Monte Carlo (MCMC) method for the sampling of ordinary differential equation (ODE) model parameters. The software MCMC_CLIB uses the simplified manifold Metropolis-adjusted Langevin algorithm (SMMALA), which is locally adaptive; it uses the parameter manifold's geometry (the Fisher information) to make efficient moves. This adaptation does not diminish with MC length, which is highly advantageous compared with adaptive Metropolis techniques when the parameters have large correlations and/or posteriors substantially differ from multivariate Gaussians. The software is standalone (not a toolbox), though dependencies include the GNU scientific library and sundials libraries for ODE integration and sensitivity analysis.",,"Kramer, Andrei|Stathopoulos, Vassilios|Girolami, Mark|Radde, Nicole",BIOINFORMATICS,,10.1093/bioinformatics/btu429
311,WOS:000264918900016,2009,Modelling of anaerobic treatment of evaporator condensate (EC) from a sulphite pulp mill using the IWA anaerobic digestion model no. 1 (ADM1),WASTE-WATER|MOLASSES|REACTOR,"This paper presents the application of the ADM model to simulate the dynamic behaviour of an anaerobic reactor treating the condensate effluent (EC) generated in a sulphite pulp mill. The model was implemented in the simulation software AQUASIM (R) .d and its predictions were compared to experimental data obtained in lab-scale semi-continuous assays treating the industrial effluent. Sensitivity analysis revealed high influence of kinetic parameters on the process behaviour, which were further estimated: maximum specific uptake rate (k(m) = . d(-)) and half-saturation constant (K-s = . kg COD m(-)). The accuracy of the optimised parameters was assessed against experimental data from a second lab-scale reactor treating EC effluent with an additional carbon source (molasses). It was concluded that the model predicted reasonably the dynamic behaviour of the anaerobic reactor under different loading rates. In addition, simulations successfully predicted a better stability and performance of the process (lower VIA accumulation and higher COD removal and methane production) for the EC treatment when an external carbon source is added to the reactor, specifically at high organic loads ( kg COD m(-) d(-) or higher). The model was not able to describe adequately the reactor behaviour at high organic loads when molasses was not added, thus application of the model for the anaerobic treatment of EC effluent needs to be further evaluated.", (C) 2008 Elsevier B.V. All rights reserved.,"Silva, F.|Nadais, H.|Prates, A.|Arroja, L.|Capela, I.",CHEMICAL ENGINEERING JOURNAL,dynamic simulation|adm1|anaerobic reactor|acetic acid|sulphite pulping process|semi-continuous assay,10.1016/j.cej.2008.09.002
312,WOS:000276075900006,2010,Object-oriented design of process line simulation and optimization-A case study in papermaking,MULTIDISCIPLINARY DESIGN|PAPER|FRAMEWORK|SYSTEM|FLOW,"Simulation-based optimization for industrial process lines is discussed in this paper. Our approach combines multidisciplinary modeling, modern sensitivity analysis methodology as well as multiobjective optimization by means of object-oriented software design principles. As a result, a simulation and optimization approach that can be extended and modified due to users' needs can be developed. Our approach is illustrated by a real-world example from papermaking industry.",,"Madetoja, Elina|Tarvainen, Pasi",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,process line optimization|multiobjective optimization|multidisciplinary modeling|object-oriented programming,10.1007/s00158-009-0451-8
313,WOS:000265341800006,2009,Estimating storm discharge and water quality data uncertainty: A software tool for monitoring and modeling applications,AGRICULTURAL WATERSHEDS|SAMPLE PRESERVATION|REACTIVE PHOSPHORUS|INORGANIC-PHOSPHATE|RISK-ASSESSMENT|RIVER WATER|NITROGEN|STRATEGIES|SEDIMENT|SORPTION,"Uncertainty estimates corresponding to measured hydrologic and water quality data can contribute to improved monitoring design, decision-making, model application, and regulatory formulation. With these benefits in mind, the Data Uncertainty Estimation Tool for Hydrology and Water Quality (DUET-H/WQ) was developed from an existing uncertainty estimation framework for small watershed discharge, sediment, and N and P data. Both the software and its framework-basis utilize the root mean square error propagation methodology to provide uncertainty estimates instead of more rigorous approaches requiring detailed statistical information, which is rarely available. DUET-H/WQ lists published uncertainty information for data collection procedures to assist the user in assigning appropriate data-specific uncertainty estimates and then calculates the uncertainty for individual discharge, concentration, and load values. Results of DUET-H/WQ application in several studies indicated that substantial uncertainty can be contributed by each procedural category (discharge measurement, sample collection, sample preservation/storage, laboratory analysis, and data processing and management). For storm loads, the uncertainty was typically least for discharge (+/- -%), greater for sediment (+/- -%) and dissolved N and P (+/- -%) loads, and greater yet for total N and P (+/- -%). When these uncertainty estimates for individual values were aggregated within study periods (i.e. total discharge, average concentration, and total load), uncertainties followed the same pattern (Q < TSS < dissolved N and P < total N and P). This rigorous demonstration of uncertainty in discharge and water quality data illustrates the importance of uncertainty analysis and the need for appropriate tools. It is our hope that DUET-H/WQ contributes to making uncertainty estimation a routine data collection and reporting procedure and thus enhances environmental monitoring, modeling, and decision-making. Hydrologic and water quality data are too important for scientists to continue to ignore the inherent uncertainty.", Published by Elsevier Ltd.,"Harmel, R. D.|Smith, D. R.|King, K. W.|Slade, R. M.",ENVIRONMENTAL MODELLING & SOFTWARE,error propagation|data collection|hydrology|nutrients|watershed models,10.1016/j.envsoft.2008.12.006
316,WOS:000399586700038,2017,Accelerating Monte Carlo estimation with derivatives of high-level finite element models,SENSITIVITY DERIVATIVES|CHAOS,In this paper we demonstrate the ability of a derivative-driven Monte Carlo estimator to accelerate the propagation of uncertainty through two high-level non-linear finite element models. The use of derivative information amounts to a correction to the standard Monte Carlo estimation procedure that reduces the variance under certain conditions. We express the finite element models in variational form using the high-level Unified Form Language (UFL). We derive the tangent linear model automatically from this high-level description and use it to efficiently calculate the required derivative information. To study the effectiveness of the derivative-driven method we consider two stochastic PDEs; a one-dimensional Burgers equation with stochastic viscosity and a three-dimensional geometrically non-linear Mooney-Rivlin hyperelastic equation with stochastic density and volumetric material parameter. Our results show that for these problems the first-order derivative-driven Monte Carlo method is around one order of magnitude faster than the standard Monte Carlo method and at the cost of only one extra tangent linear solution per estimation problem. We find similar trends when comparing with a modern non-intrusive multi-level polynomial chaos expansion method. We parallelise the task of the repeated forward model evaluations across a cluster using the ipyparallel and mpipy software tools. A complete working example showing the solution of the stochastic viscous Burgers equation is included as supplementary material., (C) 2017 Published by Elsevier B.V.,"Hauseux, Paul|Hale, Jack S.|Bordas, Stephane P. A.",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,monte carlo methods|uncertainty propagation|tangent linear models|partially intrusive methods|polynomial chaos expansion|parallel computing,10.1016/j.cma.2017.01.041
320,WOS:000374602000009,2016,Assessing minimum environmental flows in nonpermanent rivers: The choice of thresholds,INSTANTANEOUS PEAK FLOW|MEAN DAILY FLOW|WATERSHED SCALE|SPAIN|MODEL|BASIN|REQUIREMENTS|VARIABILITY|SOFTWARE|SURFACES,"The criteria used in the computation of the minimum environmental flow regime and flow cessation periods in nonpermanent rivers are often left to open criteria. This study proposes a stochastic approach for evaluating the choice of local thresholds in the characterization of minimum environmental flows through both the Monte Carlo technique and local hydrological relationships. This approach is applied to four regimes obtained by hydrologic and hydraulic habitat modeling in a Mediterranean watershed. The operationality, defined as the probability of the calculated environmental regime being satisfied by the natural regime over  years, was assessed for eight different scenarios. Two monthly minimum environmental flow regimes were then generated, with  and % operationality levels. This analysis allows the generation of minimum flow regime prescriptions from a strictly hydrologic point of view. The methodology proposed constitutes a useful tool for the implementation of uncertainty analysis of environmental flows in water resource management. (C) ", Elsevier Ltd. All rights reserved.,"Aguilar, Cristina|Jose Polo, Maria",ENVIRONMENTAL MODELLING & SOFTWARE,operationality|minimum environmental flows|minimum environmental flow regime|flow cessation periods|semiarid systems|wimmed,10.1016/j.envsoft.2016.02.003
322,WOS:000238960100008,2006,Variance-based sensitivity analysis of the probability of hydrologically induced slope instability,STABILITY MODEL|UNSATURATED SOILS|UNCERTAINTY|DESIGN,"Analysis of the sensitivity of predictions of slope instability to input data and model uncertainties provides a rationale for targeted site investigation and iterative refinement of geotechnical models. However, sensitivity methods based on local derivatives do not reflect model behaviour over the whole range of input variables. whereas methods based on standardised regression or correlation coefficients cannot detect non-linear and non-monotonic relationships between model input and output. Variance-based sensitivity analysis (VBSA) provides a global, model-independent sensitivity measure. The approach is demonstrated using the Combined Hydrology and Stability Model (CHASM) and is applicable to a wide variety of computer models. The method of Sobol', assuming independence between input variables, was used to identify interactions between model input variables, whilst replicated Latin Hypercube Sampling (LHS) is used to investigate the effects of statistical dependence between the input variables. The SIMLAB software was used, both to generate the input sample and to calculate the sensitivity indices. The analysis provided quantified evidence of well-known sensitivities as well demonstrating how uncertainty in slope failure during rainfall is, for the examples tested here. more attributable to uncertainty in the soil strength than to uncertainty in the rainfall. (c) ", Elsevier Ltd. All rights reserved.,"Hamm, N. A. S.|Hall, J. W.|Anderson, M. G.",COMPUTERS & GEOSCIENCES,site investigation|slope stability analysis|statistical analysis|sensitivity analysis|uncertainty analysis,10.1016/j.cageo.2005.10.007
326,WOS:000170761700007,2001,Integration of topology and shape optimization for design of structural components,,"This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach.",,"Tang, PS|Chang, KH",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,cad|design sensitivity analysis|fea|shape optimization|topology optimization,10.1007/PL00013282
327,WOS:000306043900002,2012,MVC3: A MATLAB graphical interface toolbox for third-order multivariate calibration,PARALLEL FACTOR-ANALYSIS|TRILINEAR LEAST-SQUARES|RESIDUAL TRILINEARIZATION|CURVE RESOLUTION|FOLIC-ACID|4-WAY CALIBRATION|MASS-SPECTROMETRY|2ND-ORDER|METHOTREXATE|SAMPLES,"A new MATIAB graphical interface toolbox for implementing third-order multivariate calibration methodologies is discussed. Multivariate calibration  (MVC) is a sequel of the already described first-order (MVC) and second-order (MVC) toolboxes. MVC accepts a variety of ASCII data for input, depending on whether the third-order data are vectorized or matricized. If required, data for sample sets are arranged into four-way arrays for processing with several quadrilinear and non-quadrilinear algorithms. Quadrilinear decomposition techniques and latent structured models based on partial least-squares regression and residual trilinearization are included in the software. Appropriate working sensor regions in the three data dimensions can be selected. Model development and its subsequent application to unknown samples are straightforward from the interface. Prediction results are provided along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (C) 2012 Elsevier B.V. All rights reserved.,"Olivieri, Alejandro C.|Wu, Hai-Long|Yu, Ru-Qin",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,third-order multivariate calibration|matlab program|graphical interface|figures of merit,10.1016/j.chemolab.2012.03.018
331,WOS:000363949300025,2015,Feasibility analysis of a hybrid off-grid wind-DG-battery energy system for the eco-tourism remote areas,POWER PINCH ANALYSIS|MALAYSIA|PERFORMANCE|GENERATION|BANGLADESH|SIMULATION|STORAGE|DESIGN|PLANT,"The electrification process of the remote areas and decentralized areas is being a vital fact for the improvement of its eco-tourism issues such as the Cameron Highland of Malaysia. Renewable energy (RE) resources can be used extensively to support and fulfill the demand of the expected loads of these areas. This article presents an analysis of a complete off-grid wind-diesel-battery hybrid RE model. The main objective of the present analysis is to visualize the optimum volume of systems capable of fulfilling the requirements of  kWh/day primary load in coupled with . kW peak for  residential hotels of Cameron Highlands. The hybrid power system can be effective for the tourists of that area as it is a decentralized region of Malaysia. The main motto of this analysis is to minimize the electricity unit cost and ensure the most reliable and feasible system to fulfill the requirements of the desired or expected energy system using HOMER software. From the simulation result, it can be seen that  wind turbines ( kW),  diesel generator ( kW), and  battery (Hoppecke  OPzS) hybrid RE system is the most economically feasible and lowest cost of energy is nearing USD ./kWh and net present cost is USD , . The decrement of the CO emissions also can be identified from the simulation results using that most feasible RE system including the renewable fraction value which is about ., . % capacity shortage and . % electricity as storage as compared to the other energy system.",,"Shezan, S. K. A.|Saidur, R.|Ullah, K. R.|Hossain, A.|Chong, W. T.|Julai, S.",CLEAN TECHNOLOGIES AND ENVIRONMENTAL POLICY,renewable energy|wind energy|wind turbines homer|diesel generator|sensitivity analysis|optimization|hybrid model,10.1007/s10098-015-0983-0
336,WOS:000266765700013,2009,Nitritation performance and biofilm development of co- and counter-diffusion biofilm reactors: Modeling and experimental comparison,MEMBRANE-AERATED BIOFILM|AUTOTROPHIC NITROGEN REMOVAL|WASTE-WATER TREATMENT|PARTIAL NITRIFICATION|NITRIFYING BIOFILM|DENITRIFICATION|BIOREACTOR|AMMONIA|STRATIFICATION|OXIDATION,"A comparative study was conducted on the start-up performance and biofilm development in two different biofilm reactors with aim of obtaining partial nitritation. The reactors were both operated under oxygen limited conditions, but differed in geometry. While substrates (O-, NH) co-diffused in one geometry, they counter-diffused in the other. Mathematical simulations of these two geometries were implemented in two -D multispecies biofilm models using the AQUASIM software. Sensitivity analysis results showed that the oxygen mass transfer coefficient (K-i) and maximum specific growth rate of ammonia-oxidizing (AOB) and nitrite-oxidizing bacteria (NOB) were the determinant parameters in nitrogen conversion simulations. The modeling simulations demonstrated that Ki had stronger effects on nitrogen conversion at lower (- m d(-)) than at the higher values (>  m d(-)). The experimental results showed that the counter-diffusion biofilms developed faster and attained a larger maximum biofilm thickness than the co-diffusion biofilms. Under oxygen limited condition (DO < . mg L-) and high pH (.-.), nitrite accumulation was triggered more significantly in co-diffusion than counter-diffusion biofilms by increasing the applied ammonia loading from . to . g NH+-N L- d(-). The co- and counter-diffusion biofilms displayed very different spatial structures and population distributions after  days of operation. AOB were dominant throughout the biofilm depth in co-diffusion biofilms, while the counter-diffusion biofilms presented a stratified structure with an abundance of AOB and NOB at the base and putative heterotrophs at the surface of the biofilm, respectively. (C) ", Elsevier Ltd. All rights reserved.,"Wang, Rongchang|Terada, Akihiko|Lackner, Susanne|Smets, Barth F.|Henze, Mogens|Xia, Siqing|Zhao, Jianfu",WATER RESEARCH,nitritation|co-diffusion|counter-diffusion|biofilm development|fluorescence in situ hybridization|membrane-aerated biofilm reactor,10.1016/j.watres.2009.03.017
343,WOS:000314802700001,2013,An educational model for ensemble streamflow simulation and uncertainty analysis,CLIMATE-CHANGE|SENSITIVITY|HYDROLOGY|FUTURE|TOOL,"This paper presents the hands-on modeling toolbox, HBV-Ensemble, designed as a complement to theoretical hydrology lectures, to teach hydrological processes and their uncertainties. The HBV-Ensemble can be used for in-class lab practices and homework assignments, and assessment of students' understanding of hydrological processes. Using this modeling toolbox, students can gain more insights into how hydrological processes (e.g., precipitation, snowmelt and snow accumulation, soil moisture, evapotranspiration and runoff generation) are interconnected. The educational toolbox includes a MATLAB Graphical User Interface (GUI) and an ensemble simulation scheme that can be used for teaching uncertainty analysis, parameter estimation, ensemble simulation and model sensitivity. HBV-Ensemble was administered in a class for both in-class instruction and a final project, and students submitted their feedback about the toolbox. The results indicate that this educational software had a positive impact on students understanding and knowledge of uncertainty in hydrological modeling.",,"AghaKouchak, A.|Nakhjiri, N.|Habib, E.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-445-2013
344,WOS:000367774700005,2015,Chaospy: An open source tool for designing methods of uncertainty quantification,,"The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables. (C)  The Authors.", Published by Elsevier B.V.,"Feinberg, Jonathan|Langtangen, Hans Petter",JOURNAL OF COMPUTATIONAL SCIENCE,uncertainty quantification|polynomial chaos expansions|monte carlo simulation|rosenblatt transformations|python package,10.1016/j.jocs.2015.08.008
347,WOS:000371777100004,2015,Sensitivity of algorithm parameters and objective function scaling in multi-objective optimisation of water distribution systems,OF-THE-ART|DISTRIBUTION NETWORKS|GENETIC ALGORITHMS|EVOLUTIONARY ALGORITHMS|OPTIMAL OPERATION|NSGA-II|DECISION-MAKING|TOTAL-COST|DESIGN|QUALITY,"This paper presents an extensive analysis of the sensitivity of multi-objective algorithm parameters and objective function scaling tested on a large number of parameter setting combinations for a water distribution system optimisation problem. The optimisation model comprises two operational objectives minimised concurrently, the pump energy costs and deviations of constituent concentrations as a water quality measure. This optimisation model is applied to a regional non-drinking water distribution system, and solved using the optimisation software GANetXL incorporating the NSGA-II linked with the network analysis software EPANet. The sensitivity analysis employs a set of performance metrics, which were designed to capture the overall quality of the computed Pareto fronts. The performance and sensitivity of NSGA-II parameters using those metrics is evaluated. The results demonstrate that NSGA-II is sensitive to different parameter settings, and unlike in the single-objective problems, a range of parameter setting combinations appears to be required to reach a Pareto front of optimal solutions. Additionally, inadequately scaled objective functions cause the NSGA-II bias towards the second objective. Lastly, the methodology for performance and sensitivity analysis may be used for calibration of algorithm parameters.",,"Mala-Jetmarova, Helena|Barton, Andrew|Bagirov, Adil",JOURNAL OF HYDROINFORMATICS,algorithm parameters|multi-objective optimisation|performance metrics|scaling|sensitivity|water distribution systems,10.2166/hydro.2015.062
350,WOS:000361906900013,2015,A bootstrap method for estimating uncertainty of water quality trends,LOAD ESTIMATION|REGRESSION|STREAM|VARIABLES|MODELS|TESTS,"Estimation of the direction and magnitude of trends in surface water quality remains a problem of great scientific and practical interest. The Weighted Regressions on Time, Discharge, and Season (WRTDS) method was recently introduced as an exploratory data analysis tool to provide flexible and robust estimates of water quality trends. This paper enhances the WRTDS method through the introduction of the WRTDS Bootstrap Test (WBT), an extension of WRTDS that quantifies the uncertainty in WRTDS-estimates of water quality trends and offers various ways to visualize and communicate these uncertainties. Monte Carlo experiments are applied to estimate the Type I error probabilities for this method. WBT is compared to other water-quality trend-testing methods appropriate for data sets of one to three decades in length with sampling frequencies of - observations per year. The software to conduct the test is in the EGRETci R-package.", Published by Elsevier Ltd.,"Hirsch, Robert M.|Archfield, Stacey A.|De Cicco, Laura A.",ENVIRONMENTAL MODELLING & SOFTWARE,water quality|bootstrap|trend|uncertainty analysis,10.1016/j.envsoft.2015.07.017
351,WOS:000175645300003,2002,Automatic versus manual model differentiation to compute sensitivities and solve non-linear inverse problems,,"Emerging tools for automatic differentiation (AD) of computer programs should be of great benefit for the implementation of many derivative-based numerical methods such as those used for inverse modeling. The Odyssee software, one such tool for Fortran  codes, has been tested on a sample model that solves a D non-linear diffusion-type equation. Odyssee offers both the forward and the reverse differentiation modes, that produce the tangent and the cotangent models, respectively. The two modes have been implemented on the sample application. A comparison is made with a manually-produced differentiated code for this model (MD), obtained by solving the adjoint equations associated with the model's discrete state equations. Following a presentation of the methods and tools and of their relative advantages and drawbacks, the performances of the codes produced by the manual and automatic methods are compared, in terms of accuracy and of computing efficiency (CPU and memory needs). The perturbation method (finite-difference approximation of derivatives) is also used as a reference. Based on the test of Taylor, the accuracy of the two AD modes proves to be excellent and as high as machine precision permits, a good indication of Odyssee's capability to produce error-free codes. In comparison, the manually-produced derivatives (MD) sometimes appear to be slightly biased, which is likely due to the fact that a theoretical model (state equations) and a practical model (computer program) do not exactly coincide, while the accuracy of the perturbation method is very uncertain. The MD code largely outperforms all other methods in computing efficiency, a subject of current research for the improvement of AD tools. Yet these tools can already be of considerable help for the computer implementation of many numerical methods, avoiding the tedious task of hand-coding the differentiation of complex algorithms.", (C) 2002 Published by Elsevier Science Ltd.,"Elizondo, D|Cappelaere, B|Faure, C",COMPUTERS & GEOSCIENCES,code differentiation|optimization|adjoint state|data assimilation|sensitivity analysis|odyssee,10.1016/S0098-3004(01)00048-6
352,WOS:000320010900007,2013,UNCERTAINTY QUANTIFICATION IN DAMAGE MODELING OF HETEROGENEOUS MATERIALS,POLYMER COMPOSITES|FAILURE ANALYSIS|MATRIX CRACKING|CARBON-FIBER|FATIGUE|CALIBRATION|BEHAVIOR,"This manuscript investigates the use of Bayesian statistical methods for calibration and uncertainty quantification in rate-dependent damage modeling of composite materials. The epistemic and aleatory uncertainties inherent in the model prediction due to model parameter uncertainty, model form error, solution approximations, and measurement errors are investigated. Gaussian process surrogate models are developed to replace expensive finite element models in the analysis. A viscous damage model is employed with a solution algorithm designed for implementation within a commercial finite element software package (Abaqus). Experimental results from a suite of monotonic load tests conducted on unidirectional glass fiber reinforced epoxy composite samples at multiple strain rates and strain orientations are used to quantify the uncertainty in the prediction of the composite response within a Bayesian framework.",,"Bogdanor, Michael J.|Mahadevan, Sankaran|Oskay, Caglar",INTERNATIONAL JOURNAL FOR MULTISCALE COMPUTATIONAL ENGINEERING,composite materials|bayesian calibration|rate-dependent damage|gaussian process surrogate model,10.1615/IntJMultCompEng.2013005821
354,WOS:000331776000033,2014,Characterisation factors for life cycle impact assessment of sound emissions,ROAD TRAFFIC NOISE|SENSITIVITY-ANALYSIS|LCA|FRAMEWORK,"Noise is a serious stressor affecting the health of millions of citizens. It has been suggested that disturbance by noise is responsible for a substantial part of the damage to human health. However, no recommended approach to address noise impacts was proposed by the handbook for life cycle assessment (LCA) of the European Commission, nor are characterisation factors (CFs) and appropriate inventory data available in commonly used databases. This contribution provides CFs to allow for the quantification of noise impacts on human health in the LCA framework. Noise propagation standards and international reports on acoustics and noise impacts were used to define the model parameters. Spatial data was used to calculate spatially-defined CFs in the form of -by--km maps. The results of this analysis were combined with data from the literature to select input data for representative archetypal situations of emission (e.g. urban day with a frequency of  Hz, rural night at  Hz, etc.). A total of  spatial and  archetypal CFs were produced to evaluate noise impacts at a European level (i.e. EU). The possibility of a user-defined characterisation factor was added to support the possibility of portraying the situation of full availability of information, as well as a highly-localised impact analysis. A Monte Carlo-based quantitative global sensitivity analysis method was applied to evaluate the importance of the input factors in determining the variance of the output. The factors produced are ready to be implemented in the available LCA databases and software. The spatial approach and archetypal approach may be combined and selected according to the amount of information available and the life cycle under study. The framework proposed and used for calculations is flexible enough to be expanded to account for impacts on target subjects other than humans and to continents other than Europe.", (C) 2013 Elsevier B.V. All rights reserved.,"Cucurachi, S.|Heijungs, R.",SCIENCE OF THE TOTAL ENVIRONMENT,noise|noise impacts|life cycle|lcia|lca|annoyance,10.1016/j.scitotenv.2013.07.080
355,WOS:000245766800002,2007,Methods and object-oriented software for FE reliability and sensitivity analysis with application to a bridge structure,OPTIMIZATION,"This paper addresses the growing demand for finite-element software with capabilities to incorporate uncertainty in the input parameters. Reliability and response sensitivity algorithms are implemented in the general-purpose finite-element software OpenSees, which employs an object-oriented programming approach to achieve a sustainable software with focus on maintainability and extensibility. The product is a comprehensive and freely available library of software tools for finite-element reliability and response sensitivity analysis. A numerical example involving a detailed model of a highway bridge with inelastic material behavior and  random variables is presented to demonstrate features of the methodology and the software. Importance vectors are employed to rank the input parameters according to their relative influence on the structural reliability. The required response sensitivities are obtained by an extensive implementation of the direct differentiation method.",,"Haukaas, Terje|Kiureghian, Armen Der",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2007)21:3(151)
359,WOS:000264171200019,2009,A Life Cycle Comparison of Alternative Cheese Packages,,"A comparative life cycle assessment (LCA) between three different cheese packages (P: completely polypropylene (PP), P: tin and polyethylene (PE), and P: carton and PE) has been carried out for the production, distribution and waste disposal (% landfill) phase. A package for  kg of cheese was selected as the functional unit. SimaPro software (PReConsultants, The Netherlands) was used for the LCA study. The EcoIndicator  method was selected for comparison of the packages. The comparisons show that the total environmental performance of the cheese package types in order from worst to best is P, P, and P. This conclusion was supported by a sensitivity analysis, which was conducted by using different impact assessment methods.",,"Banar, Muefide|Cokaygil, Zerrin",CLEAN-SOIL AIR WATER,ecoindicator 99|food technology|landfilling|life cycle assessment|packaging|simapro7,10.1002/clen.200700185
361,WOS:000243927200008,2007,Reliability-based multiobjective optimization for automotive crashworthiness and occupant safety,,"This paper presents a methodology for reliability-based multiobjective optimization of large-scale engineering systems. This methodology is applied to the vehicle crashworthiness design optimization for side impact, considering both structural crashworthiness and occupant safety, with structural weight and front door velocity under side impact as objectives. Uncertainty quantification is performed using two first order reliability method-based techniques: approximate moment approach and reliability index approach. Genetic algorithm-based multiobjective optimization software GDOT, developed in-house, is used to come up with an optimal pareto front in all cases. The technique employed in this study treats multiple objective functions separately without combining them in any form. It shows that the vehicle weight can be reduced significantly from the baseline design and at the same time reduce the door velocity. The obtained pareto front brings out useful inferences about optimal design regions. A decision-making criterion is subsequently invoked to select the ""best"" subset of solutions from the obtained nondominated pareto optimal solutions. The reliability, thus computed, is also checked with Monte Carlo simulations. The optimal solution indicated by knee point on the optimal pareto front is verified with LS-DYNA simulation results.",,"Sinha, Kaushik",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,reliability-based multiobjective optimization|uncertainty quantification|form|nondominated points|gdot|pareto optimal solution|knee point|automotive crashworthiness|occupant safety|side impact|monte carlo simulation,10.1007/s00158-006-0050-x
362,WOS:000389785300007,2017,An automated decision support system for aided assessment of variogram models,GLOBAL SENSITIVITY-ANALYSIS|ENVIRONMENTAL-MODELS|VALIDATION|FRAMEWORK|NETWORKS|ROBUST|SOIL,"In the present paper, an extensive cross-validation procedure, based on the analysis of numerical indices and graphical tools, is described and discussed. The procedure has been implemented in a software application designed to support practitioners in the variogram model assessment. It provides an extensive report, which summarizes a large post-processing stage and suggests how to interpret the performed analysis to rate the model to be validated. Besides classical accuracy indices, two new integrated tools based on the variogram of residuals are introduced, which take the spatial nature of the dataset into account. Finally, inspecting the summary report, the user can decide whether the considered model is satisfactory for his/her goals or it needs to be improved. Finally, a case study is presented related to the variogram assessment of groundwater level measured in a porous shallow aquifer of the Apulia Region (South -Italy). (C) ", Elsevier Ltd. All rights reserved.,"Barca, Emanuele|Porcu, Emilio|Bruno, Delia|Passarella, Giuseppe",ENVIRONMENTAL MODELLING & SOFTWARE,geostatistics|extensive cross-validation|rank coefficient of spatial correlation|decision-support system,10.1016/j.envsoft.2016.11.004
364,WOS:000401878000001,2017,Minmax regret combinatorial optimization problems with investments,INTERVAL DATA|ALGORITHM,"A new minmax regret optimization model in a system with uncertain parameters is proposed. In this model it is allowed to make investments before a minmax regret solution is implemented in order to modify the source or the nature of the existing uncertainty. Therefore, it is allowed to spend resources in order to change the basic cost structure of the system and take advantage of the modified system to find a robust solution. Some properties of this model allow us to have proper Mathematical Programming formulations that can be solved by standard optimization packages. As a practical application we consider the shortest path problem in a network in which it is possible to modify the uncertainty intervals for the arc costs by investing in the system. We also give an approximate algorithm and generalize some existing results on constant factor approximations. (C) ", Elsevier Ltd. All rights reserved.,"Conde, Eduardo|Leal, Marina",COMPUTERS & OPERATIONS RESEARCH,minmax regret models|robustness and sensitivity analysis|shortest path problem,10.1016/j.cor.2017.03.007
365,WOS:000165465600002,2000,Generating probabilistic spatially-explicit individual and population exposure estimates for ecological risk assessments,LANDSCAPES|DYNAMICS|PREY,"Exposure to chemical contaminants in various media must be estimated when performing ecological risk assessments. Exposure estimates are often based on the th-percentile upper confidence limit on the mean concentration of all samples, calculated without regard to critical ecological and spatial information about the relative relationship of receptors, their habitats, and contaminants. This practice produces exposure estimates that are potentially unrepresentative of the ecology of the receptor. This article proposes a habitat area and quality conditioned exposure estimator, E[HQ], that requires consideration of these relationships. It describes a spatially explicit ecological exposure model to facilitate calculation of E[HQ]. The model provides () a flexible platform for investigating the effect of changes in habitat area, habitat quality, foraging area, and population size on exposure estimates, and () a tool for calculating E[HQ] for use in actual risk assessments. The inner loop of a Visual Basic(R) program randomly walks a receptor over a multicelled landscape-each cell of which contains values for cell area, habitat area, habitat quality, and concentration-accumulating an exposure estimate until the total area foraged is less than or equal to a given foraging area. An outer loop then steps through foraging areas of increasing size. This program is iterated by Monte Carlo software, with the number of iterations representing the population size. Results indicate that () any single estimator may over- or underestimate exposure, depending on foraging strategy and spatial relationships of habitat and contamination, and () changes in exposure estimates in response to changes in foraging and habitat area are not linear.",,"Hope, BK",RISK ANALYSIS,ecological risk|spatially explicit exposures|populations|uncertainty analysis,10.1111/0272-4332.205053
368,WOS:000354547700014,2015,Identification of Critical Erosion Watersheds for Control Management in Data Scarce Condition Using the SWAT Model,LEAST-SQUARES REGRESSION|CRITICAL SUBWATERSHEDS|SENSITIVITY-ANALYSIS|SEDIMENT YIELD|SOIL-EROSION|PRONE AREAS|CLIMATE|CATCHMENT|INDIA|REQUIREMENTS,"Identification of critical watersheds prone to soil erosion has been performed by using a hydrological model in data scarce Damodar River catchment, located in Jharkhand state of India. Model is calibrated and validated for two watersheds, i.e.,()Nagwan, .km; and ()Banikdih, .km, nested within the catchment. The achieved R values of predicted monthly runoff and sediment yield varies, respectively, .-. and .-., for both the watersheds during calibration and validation period. Calibration and validation results revealed that model is predicting monthly runoff and sediment yield satisfactory for the two watersheds of the Damodar River catchment. The validated model parameters were then up-scaled to the whole catchment and model was run from - to identify the critical watersheds. Model was successfully used for prioritization of  watersheds delineated using the computer software model within the catchment. In delineation process, the boundaries of  watersheds matched exactly with the watersheds delineated manually by Damodar Valley Corporation. Out of these  watersheds, erosion classes of  exactly matched with the manually described erosion class. For remaining  watersheds, priority of  watersheds was either one class higher or lower, whereas eight watersheds showed complete mismatch. Overall results showed that the hydrological model used in this paper may be helpful in prioritization of management strategies to manage the resources where availability of data is a big concern. Such approach for managing resources is particularly needed in developing countries for better utilization of limited resources.",,"Kumar, Sanjeet|Mishra, Ashok|Raghuwanshi, Narendra Singh",JOURNAL OF HYDROLOGIC ENGINEERING,soil and water assessment tool (swat)|calibration|validation|watershed|critical erosion|identification|prioritization,10.1061/(ASCE)HE.1943-5584.0001093
369,WOS:000301688100020,2012,Generalized estimating equations and regression diagnostics for longitudinal controlled clinical trials: A case study,DELETION DIAGNOSTICS|INFLUENTIAL OBSERVATIONS|LINEAR-MODELS|GEE|SOFTWARE,"Generalized estimating equations (GEE) were proposed for the analysis of correlated data. They are popular because regression parameters can be consistently estimated even if only the mean structure is correctly specified. GEE have been extended in several ways, including regression diagnostics for outlier detection. However, GEE have rarely been used for analyzing controlled clinical trials. The SB-LOT trial, a double-blind placebo-controlled randomized multicenter trial in which the oedema-protective effect of a vasoactive drug was investigated in patients suffering from chronic insufficiency was re-analyzed using the GEE approach. It is demonstrated that the autoregressive working correlation structure is the most plausible working correlation structure in this study. The effect of the vasoactive drug is a difference in lower leg volume of . ml per week (p=., % confidence interval .-. ml per week), making a difference of  ml at the end of the study. Deletion diagnostics are used for identification of outliers and influential probands. After exclusion of the most influential patients from the analysis, the overall conclusion of the study is not altered. At the same time, the goodness of fit as assessed by half-normal plots increases substantially. In summary, the use of GEE in a longitudinal clinical trial is an alternative to the standard analysis which usually involves only the last follow-up. Both the GEE and the regression diagnostic techniques should accompany the GEE analysis to serve as sensitivity analysis.", (C) 2011 Elsevier B.V. All rights reserved.,"Vens, Maren|Ziegler, Andreas",COMPUTATIONAL STATISTICS & DATA ANALYSIS,chronic venous insufficiency|cook statistic|deletion diagnostics|independence estimating equations|half-normal plot,10.1016/j.csda.2011.04.010
370,WOS:000090149600007,2000,An object-oriented structural optimization program,SHAPE OPTIMIZATION,"In this paper, implementation concepts of a structural optimization software using object-oriented programming (OOP) in CS ++ is presented. A brief mathematical formulation of structural optimization and continuum-based sensitivity analysis is presented. The requirements of a computational optimization environment are derived from this formulation. The OOP characteristics are analysed and this paradigm is employed in the implementation of design variables, structural performance functionals, velocity fields, design model and mathematical programming algorithms using CI-Jr. Finally, the program obtained is applied to D linear elastic examples of sizing and shape optimization.",,"Silva, CAC|Bittencourt, ML",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,structural optimization|sensitivity analysis|c plus|object-oriented programming|linear elasticity,10.1007/s001580050146
372,WOS:000283340800004,2010,A DECISION SUPPORT TOOL FOR IRRIGATION INFRASTRUCTURE INVESTMENTS,OPTIMAL ALLOCATION|WATER-RESOURCES|SYSTEM|MODEL|MANAGEMENT|IMPACTS|FARM,"Increasing water scarcity, climate change and pressure to provide water for environmental flows urge irrigators to be more efficient. In Australia, ongoing water reforms and most recently the National Water Security Plan offer incentives to irrigators to adjust their farming practices by adopting water-saving Irrigation infrastructures to match soil, crop and climatic conditions. Water Works is a decision support tool to facilitate irrigators to make long- and short-term irrigation infrastructure investment decisions at the farm level. It helps irrigators to improve the economic efficiency, water use efficiency and environmental performance of their farm businesses. Water Works has been tested, validated and accepted by the irrigation community and researchers in NSW, Australia. The interface of Water Works is user-friendly and flexible. The simulation and optimisation module in Water Works provides an opportunity to evaluate Infrastructure investment decisions to suit their seasonal or long-term water availability. The sensitivity analysis allows substantiation of the impact of major variables Net present value, internal rate of return, benefit cost ratio and payback period are used to analyse the costs and benefits of modern irrigation technology. Application of Water Works using a whole farm-level case study indicates its effectiveness in making long- and short-term investment decisions Water Works can be easily integrated into commercial software such as spreadsheets, GIS, real-time data acquisition and control systems to further enhance its usability. Water Works can also be used in regional development planning."," Copyright (C) 2009 John Wiley & Sons, Ltd.","Khan, Shahbaz|Mushtaq, Shahbaz|Chen, Charlie",IRRIGATION AND DRAINAGE,decision support tool|water management|seasonal and long-term investment|optimisation|simulation|benefit-cost analysis|whole farm|water trading|water saving,10.1002/ird.501
374,WOS:000250352400010,2007,Parallel computing techniques for sensitivity analysis in optimum structural design,FINITE-ELEMENT-ANALYSIS|OPTIMIZATION|SYSTEMS|ENVIRONMENT|LOADS,"Among different activities of the optimum structural design using the gradient-based optimization approaches, design sensitivity analysis is the most time-consuming computational process. By introducing parallel computing techniques for sensitivity computation, significant speedup has been obtained in optimum structural design. Computation of design sensitivities is characteristically uncoupled, thus opening the door to parallelization. In this paper, two types of approaches viz. single-level and multilevel parallelisms are pursued for design sensitivities. The design sensitivities are computed using analytical and finite-difference methods. Numerical studies show that the performance of the parallel algorithms for design sensitivities on message passing systems is very good. Good speedups have been achieved in parallel multilevel sensitivity calculation. The parallel algorithms for design sensitivity analysis have been implemented on message passing parallel systems within the software platform of Parallel Computer Adaptive Language.",,"Umesha, P. K.|Venuraju, M. T.|Hartmann, D.|Leimbach, K. R.",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1016/(ASCE)0887-3801(2007)21:6(463)
376,WOS:000344676000001,2014,Kinetic Study of Nonequilibrium Plasma-Assisted Methane Steam Reforming,DIELECTRIC-BARRIER DISCHARGE|CONVERSION|TRANSPORT|MECHANISM|REACTOR,"To develop a detailed reaction mechanism for plasma-assisted methane steam reforming, a comprehensive numerical and experimental study of effect laws on methane conversion and products yield is performed at different steam to methane molar ratio (S/C), residence time s, and reaction temperatures. A CHEMKIN-PRO software with sensitivity analysis module and path flux analysis module was used for simulations. A set of comparisons show that the developed reaction mechanism can accurately predict methane conversion and the trend of products yield in different operating conditions. Using the developed reaction mechanism in plasma-assisted kinetic model, the reaction path flux analysis was carried out. The result shows that CH recombination is the limiting reaction for CO production and O is the critical species for CO production. Adding wt.% Ni/SiO in discharge region has significantly promoted the yield of H-, CO, or CO in dielectric packed bed (DPB) reactor. Plasma catalytic hybrid reforming experiment verifies the reaction path flux analysis tentatively.",,"Zheng, Hongtao|Liu, Qian",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/938618
378,WOS:000330491600049,2014,Improvement of the R-SWAT-FME framework to support multiple variables and multi-objective functions,MISSISSIPPI RIVER-BASIN|RAINFALL-RUNOFF MODELS|AUTOMATIC CALIBRATION|PARAMETER-ESTIMATION|UNCERTAINTY ANALYSIS|SENSITIVITY-ANALYSIS|BAYESIAN-APPROACH|CATCHMENT MODELS|LAND-USE|POLLUTION,"Application of numerical models is a common practice in the environmental field for investigation and prediction of natural and anthropogenic processes. However, process knowledge, parameter identifiability, sensitivity, and uncertainty analyses are still a challenge for large and complex mathematical models such as the hydrological/water quality model, Soil and Water Assessment Tool (SWAT). In this study, the previously developed R program language-SWAT-Flexible Modeling Environment (R-SWAT-FME) was improved to support multiple model variables and objectives at multiple time steps (i.e., daily, monthly, and annually). This expansion is significant because there is usually more than one variable (e.g., water, nutrients, and pesticides) of interest for environmental models like SWAT. To further facilitate its easy use, we also simplified its application requirements without compromising its merits, such as the user-friendly interface. To evaluate the performance of the improved framework, we used a case study focusing on both streamflow and nitrate nitrogen in the Upper Iowa River Basin (above Marengo) in the United States. Results indicated that the R-SWAT-FME performs well and is comparable to the built-in auto-calibration tool in multi-objective model calibration. Overall, the enhanced R-SWAT-FME can be useful for the SWAT community, and the methods we used can also be valuable for wrapping potential R packages with other environmental models.", Published by Elsevier B.V.,"Wu, Yiping|Liu, Shuguang",SCIENCE OF THE TOTAL ENVIRONMENT,calibration|fme|monte carlo|r|sensitivity and uncertainty analyses|swat,10.1016/j.scitotenv.2013.07.048
380,WOS:000297595300005,2011,Shape optimisation of preform design for precision close-die forging,FINITE-ELEMENT METHOD|TOPOLOGY OPTIMIZATION|STRUCTURAL OPTIMIZATION|EVOLUTIONARY PROCEDURE|SENSITIVITY-ANALYSIS|HOMOGENIZATION|SIMULATION|ALGORITHM|BLADE,"Preform design is an essential stage in forging especially for parts with complex shapes. In this paper, based on the evolutionary structural optimisation (ESO) concept, a topological optimisation method is developed for preform design. In this method, a new criterion for element elimination and addition on the workpiece boundary surfaces is proposed to optimise material distribution. To improve the quality of the boundary after element elimination, a boundary smoothing technique is developed using B-spline curve approximation. The developed methods are programmed using C# code and integrated with DEFORM D software package. Two D case problems including forging of an aerofoil shape and forging of rail wheel are evaluated using the developed method. The results suggest that the developed topology optimisation method is an efficient approach for preform design optimisation.",,"Lu, Bin|Ou, Hengan|Cui, Z. S.",STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION,preform design|topology|optimisation|forging,10.1007/s00158-011-0668-1
382,WOS:000345254600006,2014,Quantification of the uncertainties related to velocity-area streamgauging data,,"Estimating the contribution of the different error sources in a given streamgauging offers a practical tool to improve the measurement strategy. To address the limitations of the method proposed by the ISO  standard, a generalized approach is introduced for computing the uncertainty associated with velocity-area discharge measurements. Direct computation methods are suggested for estimating the uncertainty components related to the vertical integration of velocity and to the transversal integration of velocity and depth. Discharge extrapolations to the edges and in the top/bottom layers are explicitly taken into account, as well as the distribution of the verticals throughout the cross-section. The new uncertainty analysis method was applied to streamgauging data which are representative of varied site conditions and field procedures. The new method appears to be more versatile than the ISO  method, and better suited to the diversity of streamgauging procedures. It is possible to implement it in discharge computation software such as BAREME.",,"Le Coz, Jerome|Bechon, Pierre-Marie|Camenen, Benoit|Dramais, Guillaume",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,gauging|uncertainty|velocity area|procedure,10.1051/lhb/2014047
385,WOS:000326685400009,2013,Uncertainty analysis in urban drainage modelling: should we break our back for normally distributed residuals?,PARAMETER-ESTIMATION|CALIBRATION,"This study presents results on the assessment of the application of a Bayesian approach to evaluate the sensitivity and uncertainty associated with urban rainfall-runoff models. The software MICA was adopted, in which the prior information about the parameters is updated to generate the parameter posterior distribution. The likelihood function adopted in MICA assumes that the residuals between the measured and modelled values have a normal distribution. This is a trait of many uncertainty/sensitivity procedures. This study compares the results from three different scenarios: (i) when normality of the residuals was checked but if they were not normal then nothing was done (unverified); (ii) normality assumption was checked, verified (using data transformations) and a weighting strategy was used that gives more importance to high flows; and (iii) normality assumption was checked and verified, but no weights were applied. The modelling implications of such scenarios were analysed in terms of model efficiency, sensitivity and uncertainty assessment. The overall results indicated that verifying the normality assumption required the models to fit a wider portion of the hydrograph, allowing a more detailed inspection of parameters and processes simulated in both models. Such an outcome provided important information about the advantages and limitations of the models' structure.",,"Dotto, C. B. S.|Deletic, A.|McCarthy, D. T.",WATER SCIENCE AND TECHNOLOGY,bayesian approach|normality assumption|uncertainty analysis|urban drainage models,10.2166/wst.2013.360
388,WOS:000355262900012,2015,A meta-heuristic solution for automated refutation of complex software systems specified through graph transformations,ANT COLONY OPTIMIZATION|FLY MODEL CHECKING|SEARCH,"One of the best approaches for verifying software systems (especially safety critical systems) is the model checking in which all reachable states are generated from an initial state. All of these states are searched for errors or desirable patterns. However, the drawback for many real and complex systems is the state space explosion in which model checking cannot generate all the possible states. In this situation, designers can use refutation to check refusing a property rather than proving it. In refutation, it is very important to handle the state space for finding errors efficiently. In this paper, we propose an efficient solution to implement refutation in complex systems modeled by graph transformation. Since meta-heuristic algorithms are efficient solutions for searching in the problems with very large state spaces, we use them to find errors (e.g., deadlocks) in systems which cannot be verified through existing model checking approaches due to the state space explosion. To do so, we employ a Particle Swarm Optimization (PSO) algorithm to consider only a subset of states (called population) in each step of the algorithm. To increase the accuracy, we propose a hybrid algorithm using PSO and Gravitational Search Algorithm (GSA). The proposed approach is implemented in GROOVE, a toolset for designing and model checking graph transformation systems. The experiments show improved results in terms of accuracy, speed and memory usage in comparison with other existing approaches.", (C) 2015 Elsevier B.V. All rights reserved.,"Rafe, Vahid|Moradi, Maryam|Yousefian, Rosa|Nikanjam, Amin",APPLIED SOFT COMPUTING,model checking|refutation|pso|gsa|graph transformation system|state space explosion,10.1016/j.asoc.2015.04.032
391,WOS:000343415200006,2013,UNCERTAINTY IN THE DEVELOPMENT AND USE OF EQUATION OF STATE MODELS,,"In this paper we present the results from a series of focus groups on the visualization of uncertainty in equation-of-state (EOS) models. The initial goal was to identify the most effective ways to present EOS uncertainty to analysts, code developers, and material modelers. Four prototype visualizations were developed to present EOS surfaces in a three-dimensional, thermodynamic space. Focus group participants, primarily from Sandia National Laboratories, evaluated particular features of the various techniques for different use cases and discussed their individual workflow processes, experiences with other visualization tools, and the impact of uncertainty on their work. Related to our prototypes, we found the D presentations to be helpful for seeing a large amount of information at once and for a big-picture view; however, participants also desired relatively simple, two-dimensional graphics for better quantitative understanding and because these plots are part of the existing visual language for material models. In addition to feedback on the prototypes, several themes and issues emerged that are as compelling as the original goal and will eventually serve as a starting point for further development of visualization and analysis tools. In particular, a distributed workflow centered around material models was identified. Material model stakeholders contribute and extract information at different points in this workflow depending on their role, but encounter various institutional and technical barriers which restrict the flow of information. An effective software tool for this community must be cognizant of this workflow and alleviate the bottlenecks and barriers within it. Uncertainty in EOS models is defined and interpreted differently at the various stages of the workflow. In this context, uncertainty propagation is difficult to reduce to the mathematical problem of estimating the uncertainty of an output from uncertain inputs.",,"Weirs, V. Gregory|Fabian, Nathan|Potter, Kristin|McNamara, Laura|Otahal, Thomas",INTERNATIONAL JOURNAL FOR UNCERTAINTY QUANTIFICATION,materials|uncertainty quantification|representation of uncertainty|model validation and verification|continnum mechanics,10.1615/Int.J.UncertaintyQuantification.2012003960
393,WOS:000355932400009,2015,"Parameter sensitivity analysis and optimization of Noah land surface model with field measurements from Huaihe River Basin, China",MESOSCALE ETA-MODEL|WATERSHED MODEL|ENVIRONMENTAL-MODELS|UNCERTAINTY|CALIBRATION|IMPLEMENTATION|HYDROLOGY|SYSTEMS|IMPACT|EVAPORATION,"This study aims to identify the parameters that are most important in controlling the Noah land surface model (LSM), the analysis of parameter interactions, and the evaluation of the performance of parameter optimization using the parameter estimation software PEST. We found it necessary to analyze parameter sensitivity in order to properly simulate hydrological variables such as latent heat flux in the Huaihe River Basin, China. The parameters under study in the Noah LSM link thermodynamic and hydrological parts into a complete model. To our knowledge, this parameter interaction in the Noah LSM has never been studied before. There are, however, several studies concerning the influence of vegetation types and climate conditions on parameter sensitivity of the Noah LSM. Three sensitivity analysis methods, the including local sensitivity analysis method SENSAN, regional sensitivity analysis, and Sobol's method, were tested. Five experimental sites in the Huaihe River Basin were chosen to perform the simulations. The results show that the Noah LSM parameter sensitivities were impacted by the choice of the analysis method. The local method SENSAN often produced significant differences in results compared to the two global methods. The parameter interactions investigated made a significant contribution towards elucidating how one process influences another in the Noah LSM. The results show that parameters were not transferable solely based on vegetation types but also rely on climate conditions. According to the sensitivity analysis results, four sensitive parameters were chosen to be optimized using the PEST method. PEST is a widely used method for estimating parameters in models. Root-mean-square error was used to evaluate the effect of the optimization. Generally in all sites, the optimized parameters values perform better than the original parameter values.",,"Hou, Ting|Zhu, Yonghua|Lu, Haishen|Sudicky, Edward|Yu, Zhongbo|Ouyang, Fen",STOCHASTIC ENVIRONMENTAL RESEARCH AND RISK ASSESSMENT,noah lsm|huaihe river|sensitivity analysis|rsa|sobol's method|pest,10.1007/s00477-015-1033-5
394,WOS:000289865900008,2011,Automatic differentiation strategy for the local sensitivity analysis of a one-dimensional hydraulic model,,"In this paper, automatic differentiation (AD) techniques are introduced and applied in the local sensitivity analysis of the state function handled by the one-dimensional hydraulic model, Mage. We have proposed the different steps to easily compute automatic derivatives of a given numerical model. More specifically, Tapenade software, in the tangent linear mode (TLM), has been used to calculate derivatives of the model outputs (discharge and water level) with respect to the bottom friction expressed in terms of Strickler relation. We have shown the independent contribution of the main stream and floodplain Strickler coefficients on discharges and water levels. Furthermore, numerical comparison has shown that derivatives computed using the AD tool are more accurate than those using the forward divided differences scheme."," Copyright (C) 2010 John Wiley & Sons, Ltd.","Souhar, O.|Faure, J. -B.",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS,automatic differentiation|divided differences|hydraulic models|sensitivity analysis,10.1002/fld.2263
395,WOS:000267193100001,2009,Evaluating uncertainty in integrated environmental models: A review of concepts and tools,RAINFALL-RUNOFF MODELS|GENERALIZED POLYNOMIAL CHAOS|WATER-QUALITY MODELS|SENSITIVITY-ANALYSIS|RISK-ASSESSMENT|AUTOMATIC CALIBRATION|GLOBAL OPTIMIZATION|RELIABILITY METHODS|HYDROLOGIC-MODELS|DIFFERENTIAL-EQUATIONS,"This paper reviews concepts for evaluating integrated environmental models and discusses a list of relevant software-based tools. A simplified taxonomy for sources of uncertainty and a glossary of key terms with ""standard'' definitions are provided in the context of integrated approaches to environmental assessment. These constructs provide a reference point for cataloging  different model evaluation tools. Each tool is described briefly (in the auxiliary material) and is categorized for applicability across seven thematic model evaluation methods. Ratings for citation count and software availability are also provided, and a companion Web site containing download links for tool software is introduced. The paper concludes by reviewing strategies for tool interoperability and offers guidance for both practitioners and tool developers.",,"Matott, L. Shawn|Babendreier, Justin E.|Purucker, S. Thomas",WATER RESOURCES RESEARCH,,10.1029/2008WR007301
397,WOS:000253663600007,2008,A computational algorithm for the multiple generation of nonlinear mathematical models and stability study,,"This paper presents an algorithm that generates families of mathematical models with nonlinear parameters, and includes the study of linear models, based on the experimental data of the intervening variables. The implementation of this algorithm has been named poly-model and is based on the application of the Gauss-Newton algorithm for obtaining the parameters of nonlinear models [Verdu F. Un Algoritmo para la construccion multiple de modelos matematicos no lineales y el estudio de su estabilidad. Doctoral Tesis. Universidad de Alicante, ]. One of its characteristics is a search among different nonlinear models within the parameters; unlike the methods found in the scientific literature [Camacho Rosales J. Estadistica con SPSS para windows. Ed. Ra-Ma, ; Mathsoft Inc. Splus-. Guide to Statistics. Seattle, ], the user does not intervene in their generation. A pruning criteria has also been introduced that is based on the stability analysis of models generated from perturbations, applying studies carried out by the authors and published in [Verdu F, Villacampa Y. A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data. Advances in Engineering Software, ]. Object-oriented Pascal has been used in Delphi .", (c) 2007 Published by Elsevier Ltd.,"Verdu, F.|Villacampa, Y.",ADVANCES IN ENGINEERING SOFTWARE,modelling|nonlinear regression|sensitivity analysis,10.1016/j.advengsoft.2007.03.004
398,WOS:000400594100014,2017,Uncertainty Estimation in Flood Inundation Mapping: An Application of Non-parametric Bootstrapping,MONTHLY STREAMFLOW PREDICTION|ARTIFICIAL NEURAL-NETWORKS|CONFIDENCE-INTERVALS|MODEL CALIBRATION|DESIGN FLOODS|RUNOFF|RISK|PRECIPITATION|OPTIMIZATION|PARAMETERS,"Disaster prevention planning is affected in a significant way by a lack of in-depth understanding of the numerous uncertainties involved with flood delineation and related estimations. Currently, flood inundation extent is represented as a deterministic map without in-depth consideration of the inherent uncertainties associated with variables such as precipitation, streamflow, topographic representation, modelling parameters and techniques, and geospatial operations. The motivation of this study is to estimate uncertainties in flood inundation mapping based on a non-parametric bootstrapping method. The uncertainty is addressed through the application of non-parametric bootstrap sampling to the hydrodynamic modelling software, HEC-RAS, integrated with Geographic Information System (GIS). This approach was used to simulate different water levels and flow rates corresponding to different return periods from the available database. The study area was the Langat River Basin in Malaysia. The results revealed that the inundated land and infrastructure are subject to a flooding hazard of high-frequency events and that the flood damage potential is increasing significantly for residential areas and valuable land-use classes with higher return periods. The proposed methodology, as well as the study outcomes, of this paper could be beneficial to policymakers, water resources managers, insurance companies and other flood-related stakeholders."," Copyright (c) 2017 John Wiley & Sons, Ltd.","Faghih, M.|Mirzaei, M.|Adamowski, J.|Lee, J.|El-Shafie, A.",RIVER RESEARCH AND APPLICATIONS,flood mapping|uncertainty analysis|non-parametric bootstrap sampling|generalized extreme value distribution,10.1002/rra.3108
400,WOS:000321088500001,2013,A review of Bayesian belief networks in ecosystem service modelling,COLUMBIA RIVER BASIN|LAND MANAGEMENT ALTERNATIVES|DECISION-SUPPORT TOOLS|GROUNDWATER CONTAMINATION|UNCERTAINTY ANALYSIS|ADAPTIVE MANAGEMENT|RESOURCE MANAGEMENT|EXPERT KNOWLEDGE|AUSTRALIA|SYSTEMS,"A wide range of quantitative and qualitative modelling research on ecosystem services (ESS) has recently been conducted. The available models range between elementary, indicator-based models and complex process-based systems. A semi-quantitative modelling approach that has recently gained importance in ecological modelling is Bayesian belief networks (BBNs). Due to their high transparency, the possibility to combine empirical data with expert knowledge and their explicit treatment of uncertainties, BBNs can make a considerable contribution to the ESS modelling research. However, the number of applications of BBNs in ESS modelling is still limited. This review discusses a number of BBN-based ESS models developed in the last decade. A SWOT analysis highlights the advantages and disadvantages of BBNs in ESS modelling and pinpoints remaining challenges for future research. The existing BBN models are suited to describe, analyse, predict and value ESS. Nevertheless, some weaknesses have to be considered, including poor flexibility of frequently applied software packages, difficulties in eliciting expert knowledge and the inability to model feedback loops. (c) ", Elsevier Ltd. All rights reserved.,"Landuyt, Dries|Broekx, Steven|D'hondt, Rob|Engelen, Guy|Aertsens, Joris|Goethals, Peter L. M.",ENVIRONMENTAL MODELLING & SOFTWARE,bayesian belief networks|ecosystem services|expert based systems|graphical models,10.1016/j.envsoft.2013.03.011
401,WOS:000348756700001,2015,Pi 4U: A high performance computing framework for Bayesian uncertainty quantification of complex models,LIQUID WATER|EVOLUTIONARY STRATEGIES|PROBABILISTIC APPROACH|MARGINAL LIKELIHOOD|DYNAMICAL-SYSTEMS|INVERSE PROBLEMS|UPDATING MODELS|SIMULATION|OPTIMIZATION|RELIABILITY,"We present Pi U,() an extensible framework, for non-intrusive Bayesian Uncertainty Quantification and Propagation (UQ+P) of complex and computationally demanding physical models, that can exploit massively parallel computer architectures. The framework incorporates Laplace asymptotic approximations as well as stochastic algorithms, along with distributed numerical differentiation and task-based parallelism for heterogeneous clusters. Sampling is based on the Transitional Markov Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization tasks associated with the asymptotic approximations are treated via the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified subset simulation method is used for posterior reliability measurements of rare events. The framework accommodates scheduling of multiple physical model evaluations based on an adaptive load balancing library and shows excellent scalability. In addition to the software framework, we also provide guidelines as to the applicability and efficiency of Bayesian tools when applied to computationally demanding physical models. Theoretical and computational developments are demonstrated with applications drawn from molecular dynamics, structural dynamics and granular flow.", (C) 2014 Elsevier Inc. All rights reserved.,"Hadjidoukas, P. E.|Angelikopoulos, P.|Papadimitriou, C.|Koumoutsakos, P.",JOURNAL OF COMPUTATIONAL PHYSICS,uncertainty quantification|parallel computing|distributed computing|bayesian inference|reliability,10.1016/j.jcp.2014.12.006
402,WOS:000300128000003,2012,Spatio-temporal uncertainty in Spatial Decision Support Systems: A case study of changing land availability for bioenergy crops in Mozambique,AGENT-BASED MODEL|COVER CHANGE|BIO-ENERGY|DYNAMICS|VALIDATION|SCENARIOS|GIS|SCIENCE|EXAMPLE|TOOL,"Spatial Decision Support Systems (SDSSs) often include models that can be used to assess the impact of possible decisions. These models usually simulate complex spatio-temporal phenomena, with input variables and parameters that are often hard to measure. The resulting model uncertainty is, however, rarely communicated to the user, so that current SDSSs yield clear, but therefore sometimes deceptively precise outputs. Inclusion of uncertainty in SDSSs requires modeling methods to calculate uncertainty and tools to visualize indicators of uncertainty that can be understood by its users, having mostly limited knowledge of spatial statistics. This research makes an important step towards a solution of this issue. It illustrates the construction of the PCRaster Land Use Change model (PLUC) that integrates simulation, uncertainty analysis and visualization. It uses the PCRaster Python framework, which comprises both a spatio-temporal modeling framework and a Monte Carlo analysis framework that together produce stochastic maps, which can be visualized with the Aguila software, included in the PCRaster Python distribution package. This is illustrated by a case study for Mozambique in which it is evaluated where bioenergy crops can be cultivated without endangering nature areas and food production now and in the near future, when population and food intake per capita will increase and thus arable land and pasture areas are likely to expand. It is shown how the uncertainty of the input variables and model parameters effects the model outcomes. Evaluation of spatio-temporal uncertainty patterns has provided new insights in the modeled land use system about, e.g., the shape of concentric rings around cities. In addition, the visualization modes give uncertainty information in an comprehensible way for users without specialist knowledge of statistics, for example by means of confidence intervals for potential bioenergy crop yields. The coupling of spatio-temporal uncertainty analysis to the simulation model is considered a major step forward in the exposure of uncertainty in SDSSs. (C) ", Elsevier Ltd. All rights reserved.,"Verstegen, Judith Anne|Karssenberg, Derek|van der Hilst, Floor|Faaij, Andre",COMPUTERS ENVIRONMENT AND URBAN SYSTEMS,spatial decision support systems|uncertainty|spatial modeling|visualization|land use change|bioenergy,10.1016/j.compenvurbsys.2011.08.003
406,WOS:000239466700009,2006,The comparison of four dynamic systems-based software packages: Translation and sensitivity analysis,WETNESS,"Dynamic model development for describing complex ecological systems continues to grow in popularity. For both academic research and project management, understanding the benefits and limitations of systems-based software could improve the accuracy of results and enlarge the user audience. A Surface Wetness Energy Balance (SWEB) model for canopy surface wetness has been translated into four software packages and their strengths and weaknesses were compared based on 'novice' user interpretations. We found expression-based models such as Simulink and GoldSim with Expressions were able to model the SWEB more accurately; however, stock and flow-based models such as STELLA, Madonna, and GoldSim with Flows provided the user a better conceptual understanding of the ecologic system. Although the original objective of this study was to identify an 'appropriate' software package for predicting canopy surface wetness using SWEB, our outcomes suggest that many factors must be considered by the stakeholders when selecting a model because the modeling software becomes part of the model and of the calibration process. These constraints may include user demographics, budget limitations, built-in sensitivity and optimization tools, and the preference of user friendliness vs. computational power. Furthermore, the multitude of closed proprietary software may present a disservice to the modeling community, creating model artifacts that originate somewhere deep inside the undocumented features of the software, and masking the underlying properties of the model. (c) ", Elsevier Ltd. All rights reserved.,"Rizzo, Donna M.|Mouser, Paula J.|Whitney, David H.|Mark, Charles D.|Magarey, Roger D.|Voinov, Alexey A.",ENVIRONMENTAL MODELLING & SOFTWARE,model comparison|dynamic simulation|system-based models|canopy surface energy balance,10.1016/j.envsoft.2005.07.009
411,WOS:000248617800003,2007,Automatic sensitivity analysis of a finite volume model for two-dimensional shallow water flows,DENSE URBAN AREA,"Given a numerical model for solving two-dimensional shallow water equations, we are interested in the robustness of the simulation by identifying the rate of change of the water depths and discharges with respect to a change in the bottom friction coefficients. Such a sensitivity analysis can be carried out by computing the corresponding derivatives. Automatic differentiation (AD) is an efficient numerical method, free of approximation errors, to evaluate derivatives of the objective function specified by the computer program, Rubar for example. In this paper AD software tool Tapenade is used to compute forward derivatives. Numerical tests were done to show the robustness of the model and to demonstrate the efficiency of these AD-derivatives.",,"Souhar, O.|Faure, J.-B.|Paquier, A.",ENVIRONMENTAL FLUID MECHANICS,automatic differentiation|hydraulic model|sensitivity analysis|uncertainty propagation|steady flow,10.1007/s10652-007-9028-5
412,WOS:000374807600014,2016,Towards uncertainty quantification and parameter estimation for Earth system models in a component-based modeling framework,EQUIFINALITY,"Component-based modeling frameworks make it easier for users to access, configure, couple, run and test numerical models. However, they do not typically provide tools for uncertainty quantification or data-based model verification and calibration. To better address these important issues, modeling frameworks should be integrated with existing, general-purpose toolkits for optimization, parameter estimation and uncertainty quantification. This paper identifies and then examines the key issues that must be addressed in order to make a component-based modeling framework interoperable with general-purpose packages for model analysis. As a motivating example, one of these packages, DAKOTA, is applied to a representative but nontrivial surface process problem of comparing two models for the longitudinal elevation profile of a river to observational data. Results from a new mathematical analysis of the resulting nonlinear least squares problem are given and then compared to results from several different optimization algorithms in DAKOTA. (C) ", Elsevier Ltd. All rights reserved.,"Peckham, Scott D.|Kelbert, Anna|Hill, Mary C.|Hutton, Eric W. H.",COMPUTERS & GEOSCIENCES,model uncertainty|modeling frameworks|component-based modeling|optimization|inverse problems|nonlinear least squares|parameter estimation|longitudinal river elevation profiles,10.1016/j.cageo.2016.03.005
414,WOS:000312813800011,2012,Ecological Risk-O-Meter: a risk assessor and manager software tool for better decision making in ecosystems,SECURITY METER|QUANTIFY|MODEL,"Increased awareness of environmental issues and their effects on ecological systems and human health drive an interest in developing computational methods to reduce detrimental consequences. For example, there are concerns regarding chlorofluorocarbons and their impact on stratospheric ozone, radon and its effect on human health, coal mining and effects on habitat loss, as well as numerous other issues. However, these issues do not exist in a vacuum nor occur just one at a time. There is a need to assess social and ecological risks comprehensively and account for numerous, inter-related potential risks. Given limited funds available for addressing these issues, how can spending for purposes of environmental and ecological mitigation be optimized? What is the magnitude of overall ecological risk for a given region? Novel software, the Ecological Risk-o-Meter, addresses these questions and concerns. The software tool not only assesses the current environmental and ecological risks, but also takes into account potential solutions and provides guidance as to how spending can be optimized to reducing overall environmental risk. We demonstrate this new tool and show how to optimize the costs of risk reduction in recursive cycles based on feedbacks."," Copyright (c) 2012 John Wiley & Sons, Ltd.","Sahinoglu, Mehmet|Simmons, Susan J.|Cahoon, Lawrence B.|Morton, Scott",ENVIRONMETRICS,ecological systems|vulnerability|threat|countermeasure|risk-o-meter,10.1002/env.2186
416,WOS:000240794000013,2006,Sensitivity analysis of differential-algebraic equations and partial differential equations,ADAPTIVE MESH REFINEMENT|SYSTEMS|SOFTWARE|OPTIMIZATION|ALGORITHMS,"Sensitivity analysis generates essential information for model development, design optimization, parameter estimation, optimal control, model reduction and experimental design. In this paper we describe the forward and adjoint methods for sensitivity analysis, and outline some of our recent work on theory, algorithms and software for sensitivity analysis of differential-algebraic equation (DAE) and time-dependent partial differential equation (PDE) systems. (c) ", Elsevier Ltd. All rights reserved.,"Petzold, Linda|Li, Shengtai|Cao, Yang|Serban, Radu",COMPUTERS & CHEMICAL ENGINEERING,sensitivity analysis|differential-algebraic equations|adjoint method,10.1016/j.compchemeng.2006.05.015
417,WOS:000331341400011,2014,Adaptive stochastic Galerkin FEM,GENERALIZED POLYNOMIAL CHAOS|FINITE-ELEMENT-METHOD|ELLIPTIC SPDES|CONVERGENCE|PDES,"A framework for residual-based a posteriori error estimation and adaptive mesh refinement and polynomial chaos expansion for general second order linear elliptic PDEs with random coefficients is presented. A parametric, deterministic elliptic boundary value problem on an infinite-dimensional parameter space is discretized by means of a Galerkin projection onto finite generalized polynomial chaos (gpc) expansions, and by discretizing each gpc coefficient by a FEM in the physical domain. An anisotropic residual-based a posteriori error estimator is developed. It contains bounds for both contributions to the overall error: the error due to gpc discretization and the error due to Finite Element discretization of the gpc coefficients in the expansion. The reliability of the residual estimator is established. Based on the explicit form of the residual estimator, an adaptive refinement strategy is presented which allows to steer the polynomial degree adaptation and the dimension adaptation in the stochastic Galerkin discretization, and, embedded in the gpc adaptation loop, also the Finite Element mesh refinement of the gpc coefficients in the physical domain. Asynchronous mesh adaptation for different gpc coefficients is permitted, subject to a minimal compatibility requirement on meshes used for different gpc coefficients. Details on the implementation with the open-source software framework ALEA are presented; it is generic, and is based on available stiffness and mass matrices of a FEM for the deterministic, nonparametric nominal problem evaluated in the FEniCS environment. Preconditioning of the resulting matrix equation and iterative solution are discussed. Numerical experiments in two spatial dimensions for membrane and plane stress boundary value problems on polygons are presented. They indicate substantial savings in total computational complexity due to FE mesh coarsening in high gpc coefficients.", (C) 2013 Elsevier B.V. All rights reserved.,"Eigel, Martin|Gittelson, Claude Jeffrey|Schwab, Christoph|Zander, Elmar",COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING,uncertainty quantification|stochastic finite element methods|operator equations|alea|fenics|adaptive methods,10.1016/j.cma.2013.11.015
419,WOS:000323981900016,2013,Analyzing the effects of geological and parameter uncertainty on prediction of groundwater head and travel time,MULTIPLE-POINT STATISTICS|GLUE METHODOLOGY|CAPTURE ZONE|FLOW|SIMULATION|MODEL|HETEROGENEITY|INCOHERENCE|CALIBRATION|DENMARK,"Uncertainty of groundwater model predictions has in the past mostly been related to uncertainty in the hydraulic parameters, whereas uncertainty in the geological structure has not been considered to the same extent. Recent developments in theoretical methods for quantifying geological uncertainty have made it possible to consider this factor in groundwater modeling. In this study we have applied the multiple-point geostatistical method (MPS) integrated in the Stanford Geostatistical Modeling Software (SGeMS) for exploring the impact of geological uncertainty on groundwater flow patterns for a site in Denmark. Realizations from the geostatistical model were used as input to a groundwater model developed from Modular three-dimensional finite-difference ground-water model (MODFLOW) within the Groundwater Modeling System (GMS) modeling environment. The uncertainty analysis was carried out in three scenarios involving simulation of groundwater head distribution and travel time. The first scenario implied  stochastic geological models all assigning the same hydraulic parameters for the same geological units. In the second scenario the same  geological models were subjected to model optimization, where the hydraulic parameters for each of them were estimated by calibration against observations of hydraulic head and stream discharge. In the third scenario each geological model was run with  randomized sets of parameters. The analysis documented that the uncertainty on the conceptual geological model was as significant as the uncertainty related to the embedded hydraulic parameters.",,"He, X.|Sonnenborg, T. O.|Jorgensen, F.|Hoyer, A. -S.|Moller, R. R.|Jensen, K. H.",HYDROLOGY AND EARTH SYSTEM SCIENCES,,10.5194/hess-17-3245-2013
420,WOS:000351458000015,2015,Risk Analysis of Water Demand for Agricultural Crops under Climate Change,OPTIMIZATION HBMO ALGORITHM|RESERVOIR OPERATION|DESIGN|UNCERTAINTY|DISCRETE|NETWORKS|STRATEGY|IMPACTS|SYSTEM,"This paper assesses the risk of increase in water demand for a wide range of irrigated crops in an irrigation network located downstream of the Aidoghmoush Dam in East Azerbaijan by considering climate change conditions for the period -. Atmosphere-ocean global circulation models (AOGCMs) are used to simulate climatic variables such as temperature and precipitation. The Bayesian approach is used to consider uncertainties of AOGCMs. Climate change scenarios of climatic variables are first weighted by using the mean observed temperature-precipitation (MOTP) method, and related probability distribution functions are produced. Outputs of AOGCMs are used as input to water requirement models. Then, produced by using the Monte Carlo method,  samples (discrete values) from the probability distribution functions of monthly downscaled temperature and precipitation in the study area are extracted by using a software for sensitivity and uncertainty analysis. Time series of climatic variables in future periods are then generated (temperature variable to calculate potential evapotranspiration and rainfall variable to calculate effective rainfall). To estimate crop water requirements, crop evapotranspiration (from the product of potential evapotranspiration in the previous step and coefficient of crop computed) and effective precipitation (from time series of the previous step) are calculated. The Food and Agricultural Organization of the United Nations (FAO) methods, FAO- and Penman-Monteith, were used to compute crop and potential evapotranspiration, respectively. Because of lack of required data, potential evapotranspiration in future periods is computed through the relationship of temperature and potential evapotranspiration in the baseline period; the same procedure is conducted for temperature. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering  monthly time series of downscaled temperature and precipitation in future periods. The results indicate that risk of changes in crop water requirements increases by approximately % for a % risk, approximately % for a % risk, and approximately % for a % risk. Also, based on the current cultivated area, on average, the volume of water demand only for the aforementioned crops will be approximately .(() m()/year) with a risk of %, approximately (() m()/year) with a risk of %, and approximately (() m()/year) with a risk of %. Wheat and barley are more resistant and less sensitive to climate change than other crops considered.", (C) 2014 American Society of Civil Engineers.,"Ashofteh, Parisa-Sadat|Bozorg-Haddad, Omid|Marino, Miguel A.",JOURNAL OF HYDROLOGIC ENGINEERING,climate change|net water requirement|risk|uncertainty|monte carlo,10.1061/(ASCE)HE.1943-5584.0001053
422,WOS:000313918200054,2013,Stochastic approach to municipal solid waste landfill life based on the contaminant transit time modeling using the Monte Carlo (MC) simulation,EARTHEN BARRIERS|COMPACTED CLAY|SATURATED SOIL|TRANSPORT|MIGRATION|DIFFUSION|SORPTION|SITES|DECAY,"The paper is concerned with application and benefits of MC simulation proposed for estimating the life of a modern municipal solid waste (MSW) landfill. The software Crystal Ball (R) (CB), simulation program that helps analyze the uncertainties associated with Microsoft (R) Excel models by MC simulation, was proposed to calculate the transit time contaminants in porous media. The transport of contaminants in soil is represented by the one-dimensional (D) form of the advection-dispersion equation (ADE). The computer program CONTRANS written in MATLAB language is foundation to simulate and estimate the thickness of landfill compacted clay liner. In order to simplify the task of determining the uncertainty of parameters by the MC simulation, the parameters corresponding to the expression Z taken from this program were used for the study. The tested parameters are: hydraulic gradient (HG), hydraulic conductivity (HC), porosity (POROS), linear thickness (TH) and diffusion coefficient (EDC). The principal output report provided by CB and presented in the study consists of the frequency chart, percentiles summary and statistics summary. Additional CB options provide a sensitivity analysis with tornado diagrams. The data that was used include available published figures as well as data concerning the Mittal Steel Poland (MSP) S.A. in Krakow, Poland. This paper discusses the results and show that the presented approach is applicable for any MSW landfill compacted clay liner thickness design.", (C) 2012 Elsevier B.V. All rights reserved.,"Bieda, Boguslaw",SCIENCE OF THE TOTAL ENVIRONMENT,poland|mc simulation|cb (r)|sensitivity analysis|advection dispersion equation|stochastic process,10.1016/j.scitotenv.2012.10.032
424,WOS:000270236100003,2009,Estimating stream metabolism from oxygen concentrations: Effect of spatial heterogeneity,ORGANIC-MATTER|ECOSYSTEM METABOLISM|GAS-EXCHANGE|PRIMARY PRODUCTIVITY|SENSITIVITY-ANALYSIS|RIVER|RESPIRATION|REAERATION|VARIABILITY|TURNOVER,"Rivers are heterogeneous at various scales. River metabolism estimators based on oxygen time series provide average estimates of net oxygen production at the scale of a river reach. These estimators are derived for homogeneous river reaches. For this reason, they cannot be used to analyze how exactly they average over longitudinal variations in net production, reaeration, oxygen saturation concentration and flow velocity. We try to fill this gap by using a general analytical solution of the transport-reaction equation to () demonstrate how downstream oxygen concentration is affected by upstream concentration and (possible) longitudinally varying values of net production, reaeration, oxygen saturation concentration and flow velocity within a reach, and () derive how the net production estimate depends on varying upstream river parameters. In addition, we derive a new net production estimator that extends previously suggested estimators. The equations derived in this paper provide a general framework for understanding the assumptions underlying net production estimators. They are used to derive recommendations on the use of single station or two stations measurement layouts to get accurate river metabolism estimates. The estimator is implemented in the freely available statistics and graphics software package R (http://www.r-project.org). This makes it easily applicable to observed oxygen time series. Empirical evidence of the significance of heterogeneity in rivers is demonstrated by applying the estimator to four subsequent reaches of a river using oxygen measurements from the ends of all reaches.",,"Reichert, Peter|Uehlinger, Urs|Acuna, Vicenc",JOURNAL OF GEOPHYSICAL RESEARCH-BIOGEOSCIENCES,,10.1029/2008JG000917
426,WOS:000258484000001,2008,Software framework for parameter updating and finite-element response sensitivity analysis,INELASTIC STRUCTURES|RELIABILITY|SERVICES|PROGRAM,"The finite-element software framework OpenSees is extended with parameter updating and response sensitivity capabilities to support client applications such as reliability, optimization, and system identification. Using software design patterns, member properties, applied loadings, and nodal coordinates can be identified and repeatedly updated in order to create customized finite-element model updating applications. Parameters are identified using a Chain of Responsibility software pattern, where objects in the finite-element model forward a parameterization request to component objects until the request is handled. All messages to identify and update parameters are passed through a Facade that decouples client applications from the finite-element domain of OpenSees. To support response sensitivity analysis, the Strategy design pattern facilitates multiple approaches to evaluate gradients of the structural response, whereas the Visitor pattern ensures that objects in the finite-element domain make the proper contributions to the equations that govern the response sensitivity. Examples demonstrate the software design and the steps taken by representative finite-element model updating and response sensitivity applications.",,"Scott, Michael H.|Haukaas, Terje",JOURNAL OF COMPUTING IN CIVIL ENGINEERING,,10.1061/(ASCE)0887-3801(2008)22:5(281)
427,WOS:000364452300007,2015,Uncertainties propagations in 1D hydraulic modeling,,"Numerical modeling tools, like Crue the D modeling software developed by CNR, are widely used to analyze hydraulic and hydrological behavior of rivers. Those tools are based on input parameters, with physical or numerical meaning; theses inputs are generally known with some uncertainties. The tool Promethee, developed by IRSN, is able to realize uncertainties propagations, and two kinds of sensibility analysis: the first one, a determinist method (Morris) based on screening, is able to identify factors which influenced outputs variability; the second one, a probabilistic method (FAST) based on variance analysis of outputs regarding inputs variances, performs inputs ranking in function of outputs sensibilities. Uncertainties propagations studies require an important computational capacity; to do so; the Promethee/Crue coupling is used. The coupled tool is able to parameter Crue files for the hydraulic computations, to run lots of computation, and then to analyze results with statistic tools. This coupled tool gives the possibility to realize sensitivity studies by probabilistic method, to parameter realistic and complex model rivers, and to study the influence of several inputs variations.",,"Nguyen, Tra-mi|Richet, Yann|Balayn, Pierre|Bardet, Lise",HOUILLE BLANCHE-REVUE INTERNATIONALE DE L EAU,promethee|crue9|sensitivity analysis|fast|morris,10.1051/lhb/20150055
429,WOS:000330574400011,2014,Computer algebra systems coming of age: Dynamic simulation and optimization of DAE systems in Mathematica (TM),SENSITIVITY-ANALYSIS|CONSISTENT INITIALIZATION|PATH CONSTRAINTS|EQUATIONS|EFFICIENT|SOFTWARE|CRITERIA,"In this article, DAE parser and SQPsolver, new Computer Algebra System packages specialized in Differential-Algebraic Equations for Mathematical (TM) are presented. These packages joint capabilities for dynamical system analysis, simulation and dynamic optimization through the direct sequential approach are presented with examples and case studies highlighting applications of practical interest to chemical engineers. An overview of the relevant theoretical topics to each of the features of the packages are presented as well as implementation insights. This work paves the way for innovative R&D platforms both capable of solving practical problems of interest as well as offer seamless computational workflow. (C) ", Elsevier Ltd. All rights reserved.,"Navarro, A. K. W.|Vassiliadis, V. S.",COMPUTERS & CHEMICAL ENGINEERING,differential algebraic equations|simulation|parametric sensitivity|dynamic optimizationa,10.1016/j.compchemeng.2013.11.004
430,WOS:000256053900010,2008,A comparative life cycle analysis of two different juice packages,,"Packaging wastes have a portion of -% in total municipal solid waste (MSW) in Turkey, and they have to be evaluated from production to final disposal from the environmental point of view. The concern about the environmental impacts of packages has been dealt with using several approaches in environmental management, such as risk assessment, environmental impact assessment, environmental auditing, energy analysis, material flow analysis, and life cycle analysis (LCA). The main purpose of this research was to investigate the life cycle environmental impact of glass bottles and beverage cartons. This LCA study was performed by using SimaPro (PRe Consultants, The Netherlands) software. Individual and comparative life cycle analysis of two packages was performed depending on a consumer who lives in Eskisehir city. For that aim, SimaPro was used, and the data to run the software was gathered from package producers, the database of the software, and the literature. Life cycle comparisons of the two juice packages among themselves and also within themselves were carried out by using EcoIndicator  on the basis of climate change, ecotoxicity, acidification/eutrophication, and fossil fuels. Sensitivity analysis was performed to evaluate the effects of transportation. According to comparison figures, the environmental load of glass bottles is higher than beverage carton's load for all the impact categories. This result is also supported by the sensitivity analysis.",,"Banar, Mufide|Cokaygil, Zerrin",ENVIRONMENTAL ENGINEERING SCIENCE,lca|glass bottle|beverage carton|packaging waste|simapro7,10.1089/ees.2007.0079
431,WOS:000244915000009,2007,Topology optimization of material-nonlinear continuum structures by the element connectivity parameterization,DESIGN SENSITIVITY-ANALYSIS|ELASTOPLASTIC STRUCTURES|COMPLIANT MECHANISMS|PLASTIC-DEFORMATION|CONTACT|CRASHWORTHINESS,"The application of the element density-based topology optimization method to nonlinear continuum structures is limited to relatively simple problems such as bilinear elastoplastic material problems. Furthermore, it is very difficult to use analytic sensitivity when a commercial nonlinear finite element code is used. As an alternative to the element density formulation, the element connectivity parameterization (ECP) formulation is developed for the topology optimization of isotropic-hardening elastoplastic or hyperelastic continua by using commercial software. ECP varies the stiffness of zero-length linear elastic links that connect design domain-discretizing finite elements. Unloading was not considered. But the advantages of ECP in material-nonlinear problems were demonstrated: considerably simple analytic sensitivity calculation using a commercial code and simple link stiffness penalization regardless of nonlinear material behaviour."," Copyright (c) 2006 John Wiley & Sons, Ltd.","Yoon, Gil Ho|Kim, Yoon Young",INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING,topology optimization|material-nonlinearity|element connectivity parameterization,10.1002/nme.1843
432,WOS:000334003800020,2014,Environmental impact assessment based on dynamic fuzzy simulation,MODELS|SYSTEMS,"A new ""quick scan"" method for an expert-/stakeholder-based impact assessment approach is introduced. This approach aims to reduce the complexity of models, to simulate and visualize the system dynamics and to provide a basis for guided discussion with stakeholders. The approach is based on dynamic fuzzy models that can be understood easily and developed by experts and understood and adapted by stakeholders (""white box models""). This open modeling process also forms the basis of the credibility of the simulation results. The quick scan approach is supported by an interactive simulation tool that includes optimization and uncertainty analysis as open source software. (C) ", Elsevier Ltd. All rights reserved.,"Wieland, Ralf|Gutzler, Carsten",ENVIRONMENTAL MODELLING & SOFTWARE,quick scan|environmental impact assessment|fuzzy modeling|dynamic fuzzy simulation,10.1016/j.envsoft.2014.02.001
433,WOS:000324007200020,2013,Robust optimal dynamic production/pricing policies in a closed-loop system,INVENTORY CONTROL|OPTIMIZATION APPROACH|PRODUCT|RETURNS|MODEL|ENVIRONMENT|EXCHANGE|DEMAND|COSTS,"Hybrid manufacturing/remanufacturing systems play a key role in implementing closed-loop production systems which have been considered due to increasingly environmental concerns and latent profit of used products. Manufacturing and remanufacturing rates, selling price of new products, and acquisition price of used products are the most critical variables to optimize in such hybrid systems. In this paper, we develop a dynamic production/pricing problem, in which decisions should be made in each period confronting with uncertain demand and return. The manufacturer is able to control the demand and return by adjusting selling price and acquisition price respectively, also she can stock inventories of used and new products to deal with uncertainties. Modeling a nominal profit maximization problem, we go through robust optimization approach to reformulate it for the uncertain case. Final robust optimization model is obtained as a quadratic programming model over discrete periods which can be solved by optimization packages of QP. A numerical example is defined and sensitivity analysis is performed on both basic parameters and parameters associated with uncertainty to create managerial views.", (C) 2013 Elsevier Inc. All rights reserved.,"Mahmoudzadeh, Mahdi|Sadjadi, Seyed Jafar|Mansour, Saeed",APPLIED MATHEMATICAL MODELLING,closed-loop supply chain|robust optimization|production-pricing|dynamic pricing|quadratic programming|remanufacturing,10.1016/j.apm.2013.03.008
435,WOS:000395108800010,2017,Pattern Mixture Models for Quantifying Missing Data Uncertainty in Longitudinal Invariance Testing,STRUCTURAL EQUATION MODELS|FACTORIAL INVARIANCE|DROP-OUT|PSYCHOLOGICAL-RESEARCH|NONIGNORABLE DROPOUT|SENSITIVITY-ANALYSIS|CLINICAL-TRIALS|INCOMPLETE DATA|MIXED MODELS|GROWTH,"Many psychology applications assess measurement invariance of a construct (e.g., depression) over time. These applications are often characterized by few time points (e.g., ), but high rates of dropout. Although such applications routinely assume that the dropout mechanism is ignorable, this assumption may not always be reasonable. In the presence of nonignorable dropout, fitting a conventional longitudinal factor model (LFM) to assess longitudinal measurement invariance can yield misleading inferences about the level of invariance, along with biased parameter estimates. In this article we develop pattern mixture longitudinal factor models (PM-LFMs) for quantifying uncertainty in longitudinal invariance testing due to an unknown, but potentially nonignorable, dropout mechanism. PM-LFMs are a kind of multiple group model wherein observed missingness patterns define groups, LFM parameters can differ across these pattern-groups subject to identification constraints, and marginal inference about longitudinal invariance is obtained by pooling across pattern-groups. When dropout is nonignorable, we demonstrate via simulation that conventional LFMs can indicate longitudinal noninvariance, even when invariance holds in the overall population; certain PM-LFMs are shown to ameliorate this problem. On the other hand, when dropout is ignorable, PM-LFMs are shown to provide results comparable to conventional LFMs. Additionally, we contrast PM-LFMs to a latent mixture approach for accommodating nonignorable dropoutwherein missingness patterns can differ across latent groups. In an empirical example assessing longitudinal invariance of a harsh parenting construct, we employ PM-LFMs to assess sensitivity of results to assumptions about nonignorable missingness. Software implementation and recommendations for practice are discussed.",,"Sterba, Sonya K.",STRUCTURAL EQUATION MODELING-A MULTIDISCIPLINARY JOURNAL,longitudinal factor model|longitudinal invariance|nonignorable missing data|pattern mixture model,10.1080/10705511.2016.1250635
436,WOS:000409151600011,2017,"Simulation, identification and statistical variation in cardiovascular analysis (SISCA) - A software framework for multi-compartment lumped modeling",SENSITIVITY-ANALYSIS|BLOOD-FLOW|ARTERIAL|WINDKESSEL|PRESSURE|SYSTEM|HEART,"It has not yet been possible to obtain modeling approaches suitable for covering a wide range of real world scenarios in cardiovascular physiology because many of the system parameters are uncertain or even unknown. Natural variability and statistical variation of cardiovascular system parameters in healthy and diseased conditions are characteristic features for understanding cardiovascular diseases in more detail. This paper presents SISCA, a novel software framework for cardiovascular system modeling and its MATLAB implementation. The framework defines a multi-model statistical ensemble approach for dimension reduced, multi-compartment models and focuses on statistical variation, system identification and patient-specific simulation based on clinical data. We also discuss a data-driven modeling scenario as a use case example. The regarded dataset originated from routine clinical examinations and comprised typical pre and post surgery clinical data from a patient diagnosed with coarctation of aorta. We conducted patient and disease specific pre/post surgery modeling by adapting a validated nominal multi-compartment model with respect to structure and parametrization using metadata and MRI geometry. In both models, the simulation reproduced measured pressures and flows fairly well with respect to stenosis and stent treatment and by pre-treatment cross stenosis phase shift of the pulse wave. However, with post-treatment data showing unrealistic phase shifts and other more obvious inconsistencies within the dataset, the methods and results we present suggest that conditioning and uncertainty management of routine clinical data sets needs significantly more attention to obtain reasonable results in patient-specific cardiovascular modeling.",,"Huttary, Rudolf|Goubergrits, Leonid|Schutte, Christof|Bernhard, Stefan",COMPUTERS IN BIOLOGY AND MEDICINE,windkessel elements|lumped models|od modeling|multi-compartment modeling|cardiovascular simulation|distributed parameter modeling|clinical data set|coarctation of aorta|patient-specific models|disease-specific models|multiscale modeling,10.1016/j.compbiomed.2017.05.021
437,WOS:000278967500018,2010,SensSB: a software toolbox for the development and sensitivity analysis of systems biology models,,"SensSB (Sensitivity Analysis for Systems Biology) is an easy to use, MATLAB-based software toolbox, which integrates several local and global sensitivity methods that can be applied to a wide variety of biological models. In addition to addressing the sensitivity analysis problem, SensSB aims to cover all the steps involved during the modeling process. The main features of SensSB are: (i) derivative and variance-based global sensitivity analysis, (ii) pseudo-global identifiability analysis, (iii) optimal experimental design (OED) based on global sensitivities, (iv) robust parameter estimation, (v) local sensitivity and identifiability analysis, (vi) confidence intervals of the estimated parameters and (vii) OED based on the Fisher Information Matrix (FIM). SensSB is also able to import models in the Systems Biology Mark-up Language (SBML) format. Several examples from simple analytical functions to more complex biological pathways have been implemented and can be downloaded together with the toolbox. The importance of using sensitivity analysis techniques for identifying unessential parameters and designing new experiments is quantified by increased identifiability metrics of the models and decreased confidence intervals of the estimated parameters.",,"Rodriguez-Fernandez, Maria|Banga, Julio R.",BIOINFORMATICS,,10.1093/bioinformatics/btq242
440,WOS:000303381300019,2012,Chloride migration in groundwater for a tannery belt in Southern India,NATURAL RECHARGE|AQUIFER PARAMETERS|SOLUTE TRANSPORT|TAMIL-NADU|TERRAIN|BASIN|WATER|SITE,"Groundwater in a tannery belt in Southern India is being polluted by the discharge of untreated effluents from  operating tanneries. Total dissolved solids and chloride (Cl-) measurements in open wells in the tannery cluster vary from , to , and , to , mg/l, respectively. A mass transport model was constructed using Visual MODFLOW Premium . software to investigate the chloride migration in an area of . km(). Input to the chloride migration model was a groundwater flow model that considered steady and transient conditions. This model was calibrated with field observations; and sensitivity analysis was carried out whereby model parameters, viz., conductivity, dispersivity, and source concentration were altered slightly, and the effect on calibration statistics was evaluated. Results indicated that hydraulic conductivity played a more sensitive role than did dispersivity. The Cl- migration was mainly through advection rather than dispersion. It was found that even if the pollutant load reduced to % of the present level, the Cl- concentration in groundwater, even after  years, would not be reduced to the permissible limit of drinking water in the tannery belt.",,"Mondal, N. C.|Singh, V. P.",ENVIRONMENTAL MONITORING AND ASSESSMENT,shallow aquifer|tannery industry|groundwater pollution|chloride migration|southern india,10.1007/s10661-011-2156-x
441,WOS:000356741300001,2015,Multi-objective model auto-calibration and reduced parameterization: Exploiting gradient-based optimization tool for a hydrologic model,AUTOMATIC CALIBRATION|MULTICRITERIA METHODS|SENSITIVITY-ANALYSIS|GLOBAL OPTIMIZATION|SOIL-MOISTURE|SWAT|MODULE|INFORMATION|VALIDATION|ALGORITHM,"Multi-objective model optimization methods have been extensively studied based on evolutionary algorithms, but less on gradient-based algorithms. This study demonstrates a framework for multi-objective model calibration/optimization using gradient-based optimization tools. Model-independent software Parameter ESTimation (PEST) was used to auto-calibrate ISWAT, a modified version of the distributed hydrologic model Soil and Water Assessment Tool (SWAT), in the Shenandoah River watershed. The time-series processor TSPROC was used to combine multiple objectives into the auto-calibration process. Two sets of roughness coefficients for main channels, one assigned and calibrated according on soil types and one determined via empirical equations, were examined for stream discharge simulation. Five different weighting alternatives were investigated for their effects on ISWAT calibrations. Results showed that using Manning's roughness coefficients obtained from empirical equations improves simulation results and calibration efficiency. Applying a two-step weighting alternative to different observation groups would provide the best calibration results. (C) ", Elsevier Ltd. All rights reserved.,"Wang, Yan|Brubaker, Kaye",ENVIRONMENTAL MODELLING & SOFTWARE,pest|swat|tsproc|multi-objectives|auto-calibration,10.1016/j.envsoft.2015.04.001
443,WOS:000418736700067,2017,"A Practical, Robust Methodology for Acquiring New Observation Data Using Computationally Expensive Groundwater Models",EMPIRICAL ORTHOGONAL FUNCTIONS|BAYESIAN EXPERIMENTAL-DESIGN|REDUCED-ORDER MODEL|PREDICTIVE UNCERTAINTY|PARAMETER-IDENTIFICATION|GENETIC ALGORITHM|NETWORK DESIGN|DATA-WORTH|REDUCTION|FLOW,"Regional groundwater flow models play an important role in decision making regarding water resources; however, the uncertainty embedded in model parameters and model assumptions can significantly hinder the reliability of model predictions. One way to reduce this uncertainty is to collect new observation data from the field. However, determining where and when to obtain such data is not straightforward. There exist a number of data-worth and experimental design strategies developed for this purpose. However, these studies often ignore issues related to real-world groundwater models such as computational expense, existing observation data, high-parameter dimension, etc. In this study, we propose a methodology, based on existing methods and software, to efficiently conduct such analyses for large-scale, complex regional groundwater flow systems for which there is a wealth of available observation data. The method utilizes the well-established d-optimality criterion, and the minimax criterion for robust sampling strategies. The so-called Null-Space Monte Carlo method is used to reduce the computational burden associated with uncertainty quantification. And, a heuristic methodology, based on the concept of the greedy algorithm, is proposed for developing robust designs with subsets of the posterior parameter samples. The proposed methodology is tested on a synthetic regional groundwater model, and subsequently applied to an existing, complex, regional groundwater system in the Perth region of Western Australia. The results indicate that robust designs can be obtained efficiently, within reasonable computational resources, for making regional decisions regarding groundwater level sampling. Plain Language Summary Water supply for the public, industry, and the environment can be heavily reliant on groundwater resources. Therefore, decision makers must be able to make predictions about how a groundwater system will respond to management options. These predictions often contain a significant degree of uncertainty. This uncertainty must be reduced in order for decision makers to make optimal use of groundwater resources with minimal risk to the environment. One way to reduce this uncertainty is to obtain more information about the nature of the groundwater system by collecting new measurement data from the study site. However, it is often not clear where and when to collect this data. This study proposes a new methodology for collecting data in an optimal fashion so that the information acquired is maximized. The method incorporates any existing information, examines the characteristics of uncertainty, and alleviates the high computing costs associated with conducting the necessary calculations. The procedure is applied to a regional groundwater system in the Perth area of Western Australia. The results are consistent with the hydrogeologic conceptualization of the Perth system, and provide important insight into where new observation wells could be constructed to obtain information about the hydraulic nature of faults.",,"Siade, Adam J.|Hall, Joel|Karelse, Robert N.",WATER RESOURCES RESEARCH,groundwater modeling|uncertainty assessment|experimental design|calibration|monitoring network,10.1002/2017WR020814
444,WOS:000379138500008,2016,A probabilistic projection of the transient flow equations with random system parameters and internal boundary conditions,FREQUENCY-RESPONSE METHOD|POLYNOMIAL CHAOS|WATER-HAMMER|PIPELINES|DESIGN,"This paper presents a novel probabilistic approach based on the polynomial chaos expansion that can model the uncertainty propagation from the beginning of a waterhammer simulation and not as an afterthought. Uncertainties are considered in pipe diameter, friction coefficient, and wave speed, as well as internal boundary conditions of leaks and blockages. The polynomial chaos expansion solver results are in an excellent agreement with those calculated by using a model employing the traditional method of characteristics. The probabilistic polynomial chaos approach has the advantage of being robust and more efficient than other non-intrusive methods such as Monte Carlo simulation, which requires thousands of iterations for sharp solutions. The polynomial chaos approach is further extended to solve for randomness in frequency domain using the transfer matrix method with results of comparable accuracy. With further developments, this probabilistic approach can be integrated within existing network modelling software for practical hydraulic engineering problems.",,"Sattar, Ahmed M. A.",JOURNAL OF HYDRAULIC RESEARCH,blockages|leaks|pipelines|polynomial chaos expansion|probabilistic analysis|random variable|transient flow|waterhammer equations,10.1080/00221686.2016.1140682
447,WOS:000304453300004,2012,Standardized uncertainty analysis for hydrometry: a review of relevant approaches and implementation examples,,"The water-centric community has continuously made efforts to identify, assess and implement rigorous uncertainty analyses for routine hydrological measurements. This paper reviews some of the most relevant efforts and subsequently demonstrates that the Guide to the expression of uncertainty in measurement (GUM) is a good candidate for estimation of uncertainty intervals for hydrometry. The demonstration is made by implementing the GUM to typical hydrometric applications and comparing the analysis results with those obtained using the Monte Carlo method. The results show that hydrological measurements would benefit from the adoption of the GUM as the working standard, because of its soundness, the availability of software for practical implementation and potential for extending the GUM to hydrological/hydraulic numerical simulations.",,"Muste, Marian|Lee, Kyutae|Bertrand-Krajewski, Jean-Luc",HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES,uncertainty analysis|hydrometric measurements|monte carlo|uncertainty estimation|sensitivity analysis,10.1080/02626667.2012.675064
450,WOS:000375818700005,2016,River-to-sea pressure retarded osmosis: Resource utilization in a full-scale facility,POWER-GENERATION|SALINITY GRADIENTS|REVERSE ELECTRODIALYSIS|OSMOTIC POWER|SEAWATER DESALINATION|ENERGY EFFICIENCY|RO-PRO|WATER|SYSTEM|PERFORMANCE,"Pressure retarded osmosis (PRO) is a technology that could be utilized to recover energy from the mixing of freshwater with seawater. This source of renewable energy is sizeable and in the past decade several investigations analyzed its potential. The vast majority of studies focused on mass transfer problems across the membrane in order to improve membrane productivity and just recently studies started to look at membrane module efficiencies and parasitic loads within the PRO facility. In this article, the net specific energy production from a facility-scale PRO system was determined and optimized by using a novel simulation method that integrates parasitic loads and efficiencies of the PRO facility components and combines the model with an optimization software in a linked system optimization scheme. It was found that the overall net specific energy that may be recovered by a river-to-sea PRO facility is approximately . kWh per m() of permeate. Furthermore, a sensitivity analysis was performed to elucidate the relationship between net specific energy and power density as functions of membrane area, flow rates, and operating pressures. In general, in order to maximize resource recovery, a low power density, thus a low membrane productivity, must be accepted.", (C) 2016 Elsevier B.V. All rights reserved.,"O'Toole, Galen|Jones, Lori|Coutinho, Chris|Hayes, Corey|Napoles, Monica|Achilli, Andrea",DESALINATION,renewable energy|salinity gradient power|pressure retarded osmosis|net specific energy|power density|facility analysis,10.1016/j.desal.2016.01.012
454,WOS:000186310600007,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: II - Numerical validation and applications,VARIATIONAL DATA ASSIMILATION|CHEMISTRY DATA ASSIMILATION|AIR-QUALITY MODEL|OZONE|IMPLEMENTATION|CODE,"The Kinetic PreProcessor KPP was extended to generate the building blocks needed for the direct and adjoint sensitivity analysis of chemical kinetic systems. An overview of the theoretical aspects of sensitivity calculations and a discussion of the KPP software tools is presented in the companion paper. In this work the correctness and efficiency of the KPP generated code for direct and adjoint sensitivity studies are analyzed through an extensive set of numerical experiments. Direct-decoupled Rosenbrock methods are shown to be cost-effective for providing sensitivities at low and medium accuracies. A validation of the discrete-adjoint evaluated gradients is performed against the finite difference estimates. The accuracy of the adjoint gradients is measured using a reference gradient value obtained with a standard direct-decoupled method. The accuracy is studied for both constant step size and variable step size integration of the forward/adjoint model and the consistency between the discrete and continuous adjoint models is analyzed. Applications of the KPP-. software package to direct and adjoint sensitivity studies, variational data assimilation, and parameter identification are considered for the comprehensive chemical mechanism SAPRC-. (C) ", Elsevier Ltd. All rights reserved.,"Daescu, DN|Sandu, A|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,sensitivity analysis|data assimilation|parameter identification|optimization,10.1016/j.atmosenv.2003.08.020
455,WOS:000312654600026,2013,SPSens: a software package for stochastic parameter sensitivity analysis of biochemical reaction networks,COUPLED CHEMICAL-REACTIONS|SYSTEMS,"SPSens is a software package for the efficient computation of stochastic parameter sensitivities of biochemical reaction networks. Parameter sensitivity analysis is a valuable tool that can be used to study robustness properties, for drug targeting, and many other purposes. However its application to stochastic models has been limited when Monte Carlo methods are required due to extremely high computational costs. SPSens provides efficient, state of the art sensitivity analysis algorithms in a single software package so that sensitivity analysis can be easily performed on stochastic models of biochemical reaction networks. SPSens implements the algorithms in C and estimates sensitivities with respect to both infinitesimal and finite perturbations to system parameters, in many cases reducing variance by orders of magnitude compared to basic methods. Included among the features of SPSens are serial and parallel command line versions, an interface with Matlab, and several example problems.",,"Sheppard, Patrick W.|Rathinam, Muruhan|Khammash, Mustafa",BIOINFORMATICS,,10.1093/bioinformatics/bts642
456,WOS:000412192800042,2017,Dynamic optimization of beer fermentation: Sensitivity analysis of attainable performance vs. product flavour constraints,VERTICAL ELECTRICAL FURNACE|PERLITE GRAIN EXPANSION|BATCH DISTILLATION|OPTIMAL OPERATION|FORMULATION|SIMULATION|MODEL,"The declining alcohol industry in the UK and the concurrent surge in supply and variety of beer products has created extremely competitive environment for breweries, many of which are pursuing the benefits of process intensification and optimization. To gain insight into the brewing process, an investigation into the influence of by-product threshold levels on obtainable fermentation performance has been performed, by computing optimal operating temperature profiles for a range of constraint levels on by-product concentrations in the final product. The DynOpt software package has been used, converting the continuous control vector optimization problem into nonlinear programming (NLP) form via collocation on finite elements, which has then been solved with an interior point algorithm. This has been performed for increasing levels of time discretization, by means of a range of initializing solution profiles, for a wide spectrum of imposed by-product flavour constraints. Each by-product flavour threshold affects process performance in a unique way. Results indicate that the maximum allowable diacetyl concentration in the final product has very strong influence on batch duration, with lower limits requiring considerably longer batches. The maximum allowable ethyl acetate concentration is shown to dictate the attainable ethanol concentration, and lower limits adversely affect the desired high alcohol content in the final product. (C) ", Elsevier Ltd. All rights reserved.,"Rodman, Alistair D.|Gerogiorgis, Dimitrios I.",COMPUTERS & CHEMICAL ENGINEERING,beer fermentation|dynamic optimization|multi-objective optimization|orthogonal collocation on finite elements|sensitivity analysis|flavour constraints,10.1016/j.compchemeng.2017.06.024
459,WOS:000275555800009,2010,Benchmarking multidisciplinary design optimization algorithms,COLLABORATIVE OPTIMIZATION,"A comparison of algorithms for multidisciplinary design optimization (MDO) is performed with the aid of a new software framework. This framework, pyMDO, was developed in Python and is shown to be an excellent platform for comparing the performance of the various MDO methods. pyMDO eliminates the need for reformulation when solving a given problem using different MDO methods: once a problem has been described, it can automatically be cast into any method. In addition, the modular design of pyMDO allows rapid development and benchmarking of new methods. Results generated from this study provide a strong foundation for identifying the performance trends of various methods with several types of problems.",,"Tedford, Nathan P.|Martins, Joaquim R. R. A.",OPTIMIZATION AND ENGINEERING,multidisciplinary design optimization|decomposition algorithms|nonlinear programming|sensitivity analysis,10.1007/s11081-009-9082-6
460,WOS:000322557200002,2013,Comparison of sediment transport computations using hydrodynamic versus hydrologic models in the Simiyu River in Tanzania,BED-LOAD TRANSPORT|SENSITIVITY-ANALYSIS|PARAMETERS|SWAT|TOOL,"This paper presents the results of a study that compares the sediment routing of the Simiyu River using the hydrologic model, Soil and Water Assessment Tool (SWAT) and the D hydrodynamic simulation software for Rivers and Estuaries (SOBEK-RE) model. Routing in SWAT is completed using the simplified Bagnold's equation and in the SOBEK-RE model is undertaken using the Saint Venant equation. The upstream boundary conditions for the routing modules were derived from the subcatchments sediment yields that were estimated by SWAT using the Modified Universal Soil Loss Equation (MUSLE). The sediment loads extrapolated or interpolated from the sediment rating curve for the catchment outlet were used for calibration and validation purposes. The SWAT model predicted an erosion rate of . Mt/yr. The total sediment load transported to the main outlet of the catchment simulated by the SWAT and SOBEK-RE models was equal to . and . Mt/yr, respectively. Thus the models computed a net erosion in the channels of . Mt/yr (SWAT) and . Mt/yr (SOBEK-RE). When comparing the results of the models for the different reaches of the main channel and main tributaries, the models showed different results both in magnitude and in sign (erosion/deposition). However, in a situation where data is scarce (such as grain size, channel geometry), the more complex hydrodynamic model does not necessarily lead to more reliable results. (c) ", Elsevier Ltd. All rights reserved.,"van Griensven, Ann|Popescu, Loana|Abdelhamid, M. R.|Ndomba, Preksedis Marco|Beevers, Lindsay|Betrie, Getnet D.",PHYSICS AND CHEMISTRY OF THE EARTH,sediment routing|sediment transport|swat|sober-re|simiyu river basin,10.1016/j.pce.2013.02.003
461,WOS:000362848700009,2015,A Multi-Attribute Decision Analysis for Decommissioning Offshore Oil and Gas Platforms,UTILITY MEASUREMENT,"The  oil and gas platforms off the coast of southern California are reaching the end of their economic lives. Because their decommissioning involves large costs and potential environmental impacts, this became an issue of public controversy. As part of a larger policy analysis conducted for the State of California, we implemented a decision analysis as a software tool (PLATFORM) to clarify and evaluate decision strategies against a comprehensive set of objectives. Key options selected for in-depth analysis are complete platform removal and partial removal to  feet below the water line, with the remaining structure converted in place to an artificial reef to preserve the rich ecosystems supported by the platform's support structure. PLATFORM was instrumental in structuring and performing key analyses of the impacts of each option (e.g., on costs, fishery production, air emissions) and dramatically improved the team's productivity. Sensitivity analysis found that disagreement about preferences, especially about the relative importance of strict compliance with lease agreements, has much greater effects on the preferred option than does uncertainty about specific outcomes, such as decommissioning costs. It found a near-consensus of stakeholders in support of partial removal and ""rigs-to-reefs"" program. The project's results played a role in the decision to pass legislation enabling an expanded California ""rigs-to-reefs"" program that includes a mechanism for sharing cost savings between operators and the state.", (C) 2015 SETAC,"Henrion, Max|Bernstein, Brock|Swamy, Surya",INTEGRATED ENVIRONMENTAL ASSESSMENT AND MANAGEMENT,decision analysis|decommissioning|multi-attribute utility|oil and gas platforms|rigs-to-reefs,10.1002/ieam.1693
462,WOS:000303082000011,2012,An environmental and economic analysis for geotube coastal structures retaining dredge material,EROSION,"This paper investigates the environmental and economic sensitivity of coastal structures for two different construction methods: a traditional rubble mound structure and a geotube coastal structure using dredged material. The analysis is undertaken for two projects: a small scale coastal protection project using a revetment and a medium size capital harbour expansion using a breakwater. This work provides further insight into previously published work by Sheehan et al. () on the economic aspects of geotube technology and identifies the optimum method of construction for each type of coastal structure. An economic sensitivity analysis is undertaken on the key logistical parameters involved in the construction of these coastal structures. An environmental sensitivity analysis focuses on the CO emissions produced from the construction of the coastal structures for both construction methods. These sensitivity analyses are undertaken using a decision support software program (DMMAP), developed to assist users at the planning stages of a project to achieve sustainable dredge material management. The key logistical parameters are analysed to generate environmental and economic ranking tables. The analyses highlight that the size of the structure and the distance to the source of the quarry material are crucial factors in determining the optimum construction method. This work shows that geotubes are a viable alternative to traditional rubble mound coastal structures. It also shows that traditional construction methods may be more economical than geotube structures when considering small coastal structures. In general, the larger the scale of the project the greater the potential savings in CO emissions and cost that can be achieved through the use of geotube technology. Geotubes, with the use of dredge material, may provide a sustainable beneficial use for dredge material and offer a serious economic and environmental alternative to traditional rubble mound structures.", (C) 2012 Elsevier B.V. All rights reserved.,"Sheehan, C.|Harrington, J.",RESOURCES CONSERVATION AND RECYCLING,geotube|revetment|breakwater|coastal structures|dredging|beneficial use,10.1016/j.resconrec.2012.01.011
464,WOS:000348201000009,2015,Sensitivity analyses and simulations of a full-scale experimental membrane bioreactor system using the activated sludge model No. 3 (ASM3),RETENTION TIME|OPERATION|WATER,"An ASM-based model was implemented in the numerical software MATHEMATICA where sensitivity analyses and simulations of a membrane bioreactor (MBR) system were carried out. These results were compared with those obtained using the commercial simulator WEST. Predicted values did not show significant variations between both software and simulations showed that the most influential operational conditions were influent flow rate and concentrations and bioreactor volumes. On the other hand, sensitivity analyses were carried out with both software programs for the same five outputs: COD, ammonium and nitrate concentrations in the effluent, total suspended solids concentration and oxygen uptake rate in the aerobic bioreactor. Similar results were in general obtained in both cases and according to these analyses, the most significant inputs over the model predictions were growth and storage heterotrophic biomass yields and decay coefficient. Other parameters related to the hydrolysis process or to the autotrophic biomass also significantly influenced model outputs.",,"Ruiz, L. M.|Rodelas, P.|Perez, J. I.|Gomez, M. A.",JOURNAL OF ENVIRONMENTAL SCIENCE AND HEALTH PART A-TOXIC/HAZARDOUS SUBSTANCES & ENVIRONMENTAL ENGINEERING,simulation|modeling|sensitivity analysis|mbr|asm3,10.1080/10934529.2015.981122
467,WOS:000318057900004,2013,A long-term sensitivity analysis of the denitrification and decomposition model,NITROUS-OXIDE EMISSIONS|DRAINED ILLINOIS AGROECOSYSTEMS|GREENHOUSE-GAS EMISSIONS|SOIL ORGANIC-CARBON|DNDC MODEL|AGRICULTURAL SOILS|N2O EMISSIONS|COMPUTATIONAL EXPERIMENTS|MECHANISTIC MODEL|RAINFALL EVENTS,"Although sensitivity analysis (SA) was conducted on the DeNitrification-DeComposition (DNDC) model, a global SA over a long period of time is lacking. We used a method of Bayesian analysis of computer code outputs (BACCO) with the Gaussian emulation machine for sensitivity analysis software (GEM-SA) to conduct a long-term SA of DNDC for predicting the annual change of soil organic carbon (dSOC), nitrous oxide emission (NO) and grain yield of spring wheat. Twenty seven non-weather input parameters with wide ranges were selected for SA using weather data recorded from Three Hills, Alberta over  years (-). The SA had two steps: ) a preliminary BACCO GEM-SA was conducted to identify a more accurate emulator sampling method and to screen out parameters with insignificant influence on model outcomes; and ) final BACCO GEM-SA was conducted with optimal input design set for emulator training runs varying only the significant input parameters. Results indicated that the Maximin Latin Hypercube sampling method outperformed the LP-x method with higher emulator accuracy. Most of the  input parameters contributed little to the three outputs by the first step BACCO GEM-SA. In the second step of BACCO GEM-SA there were only three (in the case of dSOC) and six (in the cases of NO and yield) input parameters whose influence contributed to more than % of the total output variances by their total effects. Among the selected parameters, initial soil organic carbon and clay content are very important and were important in determining results for all three outputs. Sensitivities of some parameters, such as clay content and urea fertilizer amount changed dramatically over the years. This indicates that a single year SA may overestimate or underestimate a long-term parameter effect on the model prediction. The two-step procedure with the BACCO GEM-SA method improved the accuracy of SA and provided important information for model validation and parameterization.", Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.,"Qin, Xiaobo|Wang, Hong|Li, Yu'e|Li, Yong|McConkey, Brian|Lemke, Reynald|Li, Changsheng|Brandt, Kelsey|Gao, Qingzhu|Wan, Yunfan|Liu, Shuo|Liu, Yuntong|Xu, Chao",ENVIRONMENTAL MODELLING & SOFTWARE,dndc|long-term|global sensitivity analysis|bacco gem-sa,10.1016/j.envsoft.2013.01.005
172,WOS:000308971400030,2012,An integrated assessment tool to define effective air quality policies at regional scale,POLLUTION|MODEL|PM10|STRATEGIES|VALIDATION|LONDON|EUROPE,"In this paper, the Integrated Assessment of air quality is dealt with at regional scale. First the paper describes the main challenges to tackle current air pollution control, including economic aspects. Then it proposes a novel approach to manage the problem, presenting its mathematical formalization and describing its practical implementation into the Regional Integrated Assessment Tool (RIAT). The main features of the software system are described and some preliminary results on a domain in Northern Italy are illustrated. The novel features in RIAT are then compared to the state-of-the-art in integrated assessment of air quality, for example the ability to handle nonlinearities (instead of the usual linear approach) and the multi-objective framework (alternative to cost-effectiveness and scenario analysis). Then the lessons learned during the RIAT implementation are discussed, focusing on the locality, flexibility and openness of the tool. Finally the areas for further development of air quality integrated assessment are highlighted, with a focus on sensitivity analysis, structural and non technical measures, and the application of parallel computing concepts. (C) ", Elsevier Ltd. All rights reserved.,"Carnevale, Claudio|Finzi, Giovanna|Pisoni, Enrico|Volta, Marialuisa|Guariso, Giorgio|Gianfreda, Roberta|Maffeis, Giuseppe|Thunis, Philippe|White, Les|Triacchini, Giuseppe",ENVIRONMENTAL MODELLING & SOFTWARE,integrated assessment modeling|model reduction|air quality modeling|multi-objective optimization|decision support,10.1016/j.envsoft.2012.07.004
