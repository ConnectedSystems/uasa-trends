,id,year,title,keywords,abstract,copyright,AU,SO,DE,DOI
0,WOS:000399845800057,2017,"Risk assessment of pesticides and other stressors in bees: Principles, data gaps and perspectives from the European Food Safety Authority",HONEYBEES APIS-MELLIFERA HORNET VESPA-VELUTINA YELLOW-LEGGED HORNET NEONICOTINOID INSECTICIDES ECOSYSTEM SERVICES POLLEN CONSUMPTION AETHINA-TUMIDA COLONY FAILURE EXPOSURE IMPACTS,"Current approaches to risk assessment in bees do not take into account co-exposures from multiple stressors. The European Food Safety Authority (EFSA) is deploying resources and efforts to move towards a holistic risk assessment approach of multiple stressors in bees. This paper describes the general principles of pesticide risk assessment in bees, including recent developments at EFSA dealing with risk assessment of single and multiple pesticide residues and biological hazards. The EFSA Guidance Document on the risk assessment of plant protection products in bees highlights the need for the inclusion of an uncertainty analysis, other routes of exposures and multiple stressors such as chemical mixtures and biological agents. The EFSA risk assessment on the survival, spread and establishment of the small hive beetle, Aethina tumida, an invasive alien species, is provided with potential insights for other bee pests such as the Asian hornet, Vespa velutina. Furthermore, data gaps are identified at each step of the risk assessment, and recommendations are made for future research that could be supported under the framework of Horizon . Finally, the recent work conducted at EFSA is presented, under the over-arching MUST-B project (""EU efforts towards the development of a holistic approach for the risk assessment on MUltiple STressors in Bees"") comprising a toolbox for harmonised data collection under field conditions and a mechanistic model to assess effects from pesticides and other stressors such as biological agents and beekeeping management practices, at the colony level and in a spatially complex landscape. Future perspectives at EFSA include the development of a data model to collate high quality data to calibrate and validate the model to be used as a regulatory tool. Finally, the evidence collected within the framework of MUST-B will support EFSA's activities on the development of a holistic approach to the risk assessment of multiple stressors in bees. In conclusion, EFSA calls for collaborative action at the EU level to establish a common and open access database to serve multiple purposes and different stakeholders. (C)  The Authors.", Published by Elsevier B.V.,"Rortais, A|Arnold, G|Dorne, JL|More, SJ|Sperandio, G|Streissl, F|Szentes, C|Verdonck, F",SCIENCE OF THE TOTAL ENVIRONMENT,multiple stressors honeybee colony health modelling indicator data collection research needs,10.1016/j.scitotenv.2016.09.127
6,WOS:000340951200003,2014,Numerical and intelligent modeling of triaxial strength of anisotropic jointed rock specimens,FUZZY INFERENCE SYSTEM INTACT ROCKS CONTINUUM MASSES,"The strength of anisotropic rock masses can be evaluated through either theoretical or experimental methods. The latter is more precise but also more expensive and time-consuming especially due to difficulties of preparing high-quality samples. Numerical methods, such as finite element method (FEM), finite difference method (FDM), distinct element method (DEM), etc. have been regarded as precise and low-cost theoretical approaches in different fields of rock engineering. On the other hand, applicability of intelligent approaches such as fuzzy systems, neural networks and decision trees in rock mechanics problems has been recognized through numerous published papers. In current study, it is aimed to theoretically evaluate the strength of anisotropic rocks with through-going discontinuity using numerical and intelligent methods. In order to do this, first, strength data of such rocks are collected from the literature. Then FlAC, a commercially well-known software for FDM analysis, is applied to simulate the situation of triaxial test on anisotropic jointed specimens. Reliability of this simulation in predicting the strength of jointed specimens has been verified by previous researches. Therefore, the few gaps of the experimental data are filled by numerical simulation to prevent unexpected learning errors. Furthermore, a sensitivity analysis is carried out based on the numerical process applied herein. Finally, two intelligent methods namely feed forward neural network and a newly developed fuzzy modeling approach are utilized to predict the strength of above-mentioned specimens. Comparison of the results with experimental data demonstrates that the intelligent models result in desirable prediction accuracy.",,"Asadi, M|Bagheripour, MH",EARTH SCIENCE INFORMATICS,numerical modeling artificial neural networks fuzzy systems strength anisotropy jointed rock,10.1007/s12145-013-0137-z
11,WOS:000231782200001,2005,Scenario-based simulation of runoff-related pesticide entries into small streams on a landscape level,SURFACE WATER FIELD,"The prediction of runoff-related pesticide entry into surface waters on a landscape level usually requires considerable efforts with regard to input data, time, and personnel. Therefore, the need for an easy to use simulation tool with easily accessible input data, for example from already existing public sources, is obvious. In this paper, we present a simulation tool for the simulation of pesticide entry from arable land into adjacent streams. Our aim was to develop a tool applicable on the landscape level using ""real world data"" from numerous sites and for the simulation of parameter case studies concerning particular parameters at single sites. We used the ratio of exposure to toxicity (REXTOX) model proposed by the OECD, which had been successfully validated in the study area as part of a previous study and which was extended to calculate pesticide concentrations in adjacent streams. We simulated the pesticide entry on the landscape level at  sites in small streams situated in the central lowland of Germany with winter wheat, barley, and sugar beat as the main agricultural. crops. A sensitivity analysis indicated that the most significant model parameters were the width of the no-application zone and the degree of plant interception. The simulation was carried out for the  most frequently detected substances found in the study area using eight different environmental scenarios, covering variation of the width of the no-application zone, climate, and seasonal scenarios. The highest in-stream concentrations were predicted for a scenario using no ( m) buffer zone in conjunction with increased precipitation. According to the predicted concentrations, the risk for the aquatic communities was estimated based on standard toxicity tests and the application of a safety factor. The simulation results are presented both by means of risk maps for the study area showing the simulated pesticide concentration and the resulting ecological risk for numerous sites under varying scenarios and by case study diagrams with focus on the model behavior under the influence of single parameters. Risk maps confirmed the importance of no-application (buffer) zones for the levels of pesticide input. They also indicated the importance of the existing no-application zones for certain compounds and in some cases the need for a further evaluation of these regulations. The simulation tool was implemented as a standard PC software combining the REXTOX model with a geographical information system and can be used on any current personal computer. All input data was taken from public sources of German authorities. With little effort the tool should be applicable for other areas with similar data quality.", (C) 2005 Elsevier Inc. All rights reserved.,"Probst, M|Berenzen, N|Lentzen-Godding, A|Schulz, R",ECOTOXICOLOGY AND ENVIRONMENTAL SAFETY,risk assessment pesticides runoff buffer zones simulation modeling landscape level climate change risk mitigation,10.1016/j.ecoenv.2005.04.012
14,WOS:000308971400008,2012,Stochastic cost optimization of DNAPL remediation - Method description and sensitivity study,GROUNDWATER MONITORING DESIGN AQUIFER REMEDIATION GENETIC ALGORITHM TRANSPORT UNCERTAINTY MANAGEMENT MODEL NETWORK SYSTEMS PUMP,"A modeling approach is described for optimizing the design and operation of groundwater remediation at DNAPL sites that considers uncertainty in site and remediation system characteristics, performance and cost model limitations, and measurement uncertainties that affect predictions of remediation performance and cost. The performance model simulates performance and costs for thermal source zone treatment and enhanced bioremediation with statistical compliance rules and real-time operational system monitoring. An inverse solution is employed to estimate model parameters, parameter covariances, and residual prediction error from site data and a stochastic cost optimization algorithm determines design and operation variables that minimize expected net present value cost over Monte Carlo realizations. The method is implemented in the program SCOToolkit. A series of applications to a hypothetical problem yielded expected cost reductions for site remediation as much as % compared to conventional non-optimized approaches, while also increasing the probability of achieving ""no further action"" status in a specified timeframe by more than %. Optimizing monitoring frequency for compliance wells used to make no further action determinations as well as operational monitoring used to make decisions on individual remediation system components reveals tradeoffs between increased direct costs for sampling and analysis versus decreased construction and operating costs that arise because more data increases decision reliability. Optimizing protocols for operational monitoring and heating unit shutdown protocols for thermal source treatment (incremental versus all-or-none shutdown, soil versus groundwater sampling, number and frequency of samples) produced cost savings of more than %. Defining compliance based on confidence limits of a moving time window regression decreased expected cost and lowered failure probability compared to using measured extreme values over a lookback period. Uncertainty in DNAPL source delineation was found to have a large effect on the cost and probability of achieving remediation objectives for thermal source remediation. (C) ", Elsevier Ltd. All rights reserved.,"Parker, J|Kim, U|Kitanidis, P|Cardiff, M|Liu, XY|Beyke, G",ENVIRONMENTAL MODELLING & SOFTWARE,stochastic optimization uncertainty analysis dnapl model calibration thermal source treatment enhanced bioremediation remediation cost,10.1016/j.envsoft.2012.05.002
27,WOS:000382049400008,2016,"Parameterization, sensitivity analysis, and inversion: an investigation using groundwater modeling of the surface-mined Tivoli-Guidonia basin (Metropolitan City of Rome, Italy)",WATER MODELS UNCERTAINTY IMPORTANCE FLOW TRANSPORT VALIDATION SYSTEM SITE USA,"With respect to model parameterization and sensitivity analysis, this work uses a practical example to suggest that methods that start with simple models and use computationally frugal model analysis methods remain valuable in any toolbox of model development methods. In this work, ground-water model calibration starts with a simple parameterization that evolves into a moderately complex model. The model is developed for a water management study of the TivoliGuidonia basin (Rome, Italy) where surface mining has been conducted in conjunction with substantial dewatering. The approach to model development used in this work employs repeated analysis using sensitivity and inverse methods, including use of a new observation-stacked parameter importance graph. The methods are highly parallelizable and require few model runs, which make the repeated analyses and attendant insights possible. The success of a model development design can be measured by insights attained and demonstrated model accuracy relevant to predictions. Example insights were obtained: () A long-held belief that, except for a few distinct fractures, the travertine is homogeneous was found to be inadequate, and () The dewatering pumping rate is more critical to model accuracy than expected. The latter insight motivated additional data collection and improved pumpage estimates. Validation tests using three other recharge and pumpage conditions suggest good accuracy for the predictions considered. The model was used to evaluate management scenarios and showed that similar dewatering results could be achieved using  % less pumped water, but would require installing newly positioned wells and cooperation between mine owners.",,"La Vigna, F|Hill, MC|Rossetto, R|Mazza, R",HYDROGEOLOGY JOURNAL,stochastic optimization uncertainty analysis dnapl model calibration thermal source treatment enhanced bioremediation remediation cost,10.1007/s10040-016-1393-z
35,WOS:000407603800012,2017,Sensitivity analysis of DEM prediction for sliding wear by single iron ore particle,DISCRETE ELEMENT METHOD BALL MILLS SIMULATION MODEL PERFORMANCE CONTACT MECHANISMS MOTION STEEL LIFE,"Purpose - Sliding wear is a common phenomenon in the iron ore handling industry. Large-scale handling of iron ore bulk-solids causes a high amount of volume loss from the surfaces of bulk-solids-handling equipment. Predicting the sliding wear volume from equipment surfaces is beneficial for efficient maintenance of worn equipment. Recently, the discrete element method (DEM) simulations have been utilised to predict the wear by bulk-solids. However, the sensitivity of wear prediction subjected to DEM parameters has not been systemically investigated at single particle level. To ensure the wear predictions by DEM are accurate and stable, this study aims to conduct the sensitivity analysis at the single particle level. Design/methodology/approach - In this research, pin-on-disc wear tests are modelled to predict the sliding wear by individual iron ore particles. The Hertz-Mindlin ( no slip) contact model is implemented to simulate interactions between particle ( pin) and geometry ( disc). To quantify the wear from geometry surface, a sliding wear equation derived from Archard's wear model is adopted in the DEM simulations. The accuracy of the pin-on-disc wear test simulation is assessed by comparing the predicted wear volume with that of the theoretical calculation. The stability is evaluated by repetitive tests of a reference case. At the steady-state wear, the sensitivity analysis is done by predicting sliding wear volumes using the parameter values determined by iron ore-handling conditions. This research is carried out using the software EDEM (R) ... Findings - Numerical errors occur when a particle passes a joint side of geometry meshes. However, this influence is negligible compared to total wear volume of a wear revolution. A reference case study demonstrates that accurate and stable results of sliding wear volume can be achieved. For the sliding wear at steady state, increasing particle density or radius causes more wear, whereas, by contrast, particle Poisson's ratio, particle shear modulus, geometry mesh size, rotating speed, coefficient of restitution and time step have no impact on wear volume. As expected, increasing indentation force results in a proportional increase. For maintaining wear characteristic and reducing simulation time, the geometry mesh size is recommended. To further reduce simulation time, it is inappropriate using lower particle shear modulus. However, the maximum time step can be increased to % T-R without compromising simulation accuracy. Research limitations/implications - The applied coefficient of sliding wear is determined based on theoretical and experimental studies of a spherical head of iron ore particle. To predict realistic volume loss in the iron ore-handling industry, this coefficient should be experimentally determined by taking into account the non-spherical shapes of iron ore particles. Practical implications - The effects of DEM parameters on sliding wear are revealed, enabling the selections of adequate values to predict sliding wear in the iron ore-handling industry. Originality/value - The accuracy and stability to predict sliding wear by using EDEM (R) .. are verified. Besides, this research accelerates the calibration of sliding wear prediction by DEM.",,"Chen, GM|Schott, DL|Lodewijks, G",ENGINEERING COMPUTATIONS,discrete element method pin-on-disc bulk-solids-handling wear prediction,10.1108/EC-07-2016-0265
55,WOS:000417388800006,2017,Direct effect of atmospheric turbulence on plume rise in a neutral atmosphere,LARGE-EDDY SIMULATION DIRECT NUMERICAL-SIMULATION POLLUTANT DISPERSION CROSS-FLOW BOUNDARY-LAYERS FINITE-ELEMENT CFD SIMULATION ENVIRONMENT TERRAINS MODELS,"The direct effect of atmospheric turbulence on plume rise in the current research work is studied through examining the turbulence intensity parameter. A hybrid unsteady Reynolds averaged Navier Stokes (RANS) and large eddy simulation (LES) numerical approach is applied with a new mixed scale sub-grid parameterization technique in the commercial ANSYS Fluent software in order to simulate the buoyant plume behavior in a turbulent crossflow. The accuracy of the simulation method is crosschecked against the wind tunnel data available in the literature. The numerical simulation results in various operating conditions are used to derive a new plume rise formula in which the direct effect of atmospheric turbulence intensity at stack height (I-Air) is explicitly introduced in the plume rise formula. Furthermore, the buoyancy parameter of the flue gas is determined at some distances upstream of the stack top surface to include the whole effects of source buoyancy on the plume rise. The value of I-Air at stack height is obtained by measuring the standard deviation of wind velocity at stack height. The sensitivity analysis showed that by increasing the atmospheric turbulence intensity, the final plume rise decreases because of the updraft and downdraft motions of turbulence and it has been found that there is a linear dependency between the plume rise and ( I-Air)(-.). The quantile-quantile plots show that the new model can predict the simulated plume rise with a deviation factor of . whereas the conventional models overestimate the final plume rise at least by a factor of ..", (C) 2017 Turkish National Committee for Air Pollution Research and Control. Production and hosting by Elsevier B.V. All rights reserved.,"Ashrafi, K|Orkomi, AA|Motlagh, MS",ATMOSPHERIC POLLUTION RESEARCH,neutral numerical model plume rise rans-les method turbulence,10.1016/j.apr.2017.01.001
56,WOS:000331688500003,2014,Parallel flow routing in SWMM 5,URBAN DRAINAGE SYSTEMS SENSITIVITY-ANALYSIS MODEL PERFORMANCE IMPACT,"The hydrodynamic rainfall-runoff and urban drainage simulation model SWMM (Storm Water Management Model) is a state of the art software tool applied likewise in research and practice. In order to reduce the computational burden of long simulation runs and to use the extra power of modern multicore computers, a parallel version of SWMM is presented herein. The challenge has been to modify the software in such minimal way that the resulting code enhancement may find its way into the commercial and non-commercial software tools that depend on SWMM for its calculation engine. A pragmatic approach to identify and enhance only the critical parts of the software in terms of run-time was chosen in order to keep the code changes as low as possible. The enhanced software was first tested for coherence against the original code and then benchmarked on four different input scenarios ranging from a very small village to a medium sized urban area. For the investigated sewer systems a speedup of six to ten times on a twelve core system was realized, thus decreasing the execution time to an acceptable level even for tedious system analysis. (C) ", Elsevier Ltd. All rights reserved.,"Burger, G|Sitzenfrei, R|Kleidorfer, M|Rauch, W",ENVIRONMENTAL MODELLING & SOFTWARE,multi-core openmp parallel computing storm water management model urban drainage modeling,10.1016/j.envsoft.2013.11.002
68,WOS:000239980800018,2006,"Series of experiments for empirical validation of solar gain modeling in building energy simulation codes - Experimental setup, test cell characterization, specifications and uncertainty analysis",SPACE ANALYSIS TOOLS PROGRAMS,"Empirical validation of building energy simulation codes is an important component in understanding the capacity and limitations of the software. Within the framework of Task /Annex  of the International Energy Agency (IEA), a series of experiments was performed in an outdoor test cell. The objective of these experiments was to provide a high-quality data set for code developers and modelers to validate their solar gain models for windows with and without shading devices. A description of the necessary specifications for modeling these experiments is provided in this paper, which includes information about the test site location, experimental setup, geometrical and thermophysical cell properties including estimated uncertainties. Computed overall thermal cell properties were confirmed by conducting a steady-state experiment without solar gains. A transient experiment, also without solar gains, and corresponding simulations from four different building energy simulation codes showed that the provided specifications result in accurate thermal cell modeling. A good foundation for the following experiments with solar gains was therefore accomplished. (c) ", Elsevier Ltd. All rights reserved.,"Manz, H|Loutzenhiser, P|Frank, T|Strachan, PA|Bundi, R|Maxwell, G",BUILDING AND ENVIRONMENT,building energy simulation empirical validation test cell specification,10.1016/j.buildenv.2005.07.020
70,WOS:000262565100007,2009,More efficient PEST compatible model independent model calibration,GLOBAL OPTIMIZATION METHODS RAINFALL-RUNOFF MODELS WATERSHED MODEL MULTIOBJECTIVE CALIBRATION AUTOMATIC CALIBRATION PARAMETER-ESTIMATION SENSITIVITY CATCHMENT SCALE HSPF,"This article describes some of the capabilities encapsulated within the Model Independent Calibration and Uncertainty Analysis Toolbox (MICUT), which was written to support the popular PEST model independent interface. We have implemented a secant version of the Levenberg-Marquardt (LM) method that requires far fewer model calls for local search than the PEST LM methodology. Efficiency studies on three distinct environmental model structures (HSPF, FASST and GSSHA) show that we can find comparable local minima with -% fewer model calls than a conventional model independent LM application. Using the secant LM method for local search, MICUT also supports global optimization through the use of a slightly modified version of a stochastic global search technique called Multi-Level Single Linkage [Rinnooy Kan, A.H.G., Timmer, G., a. Stochastic global optimization methods, part : clustering methods. Math. Program. , -; Rinnooy Kan, A.H.G., Timmer, G., b. Stochastic global optimization methods, part ii: multi level methods. Math. Program. , -.]. Comparison studies with three environmental models suggest that the stochastic global optimization algorithm in MICUT is at least as, and sometimes more efficient and reliable than the global optimization algorithms available in PEST.", Published by Elsevier Ltd.,"Skahill, BE|Baggett, JS|Frankenstein, S|Downer, CW",ENVIRONMENTAL MODELLING & SOFTWARE,calibration efficiency secant version of levenberg-marquardt multi-level single linkage,10.1016/j.envsoft.2008.09.011
71,WOS:000306043900002,2012,MVC3: A MATLAB graphical interface toolbox for third-order multivariate calibration,PARALLEL FACTOR-ANALYSIS TRILINEAR LEAST-SQUARES RESIDUAL TRILINEARIZATION CURVE RESOLUTION FOLIC-ACID 4-WAY CALIBRATION MASS-SPECTROMETRY 2ND-ORDER METHOTREXATE SAMPLES,"A new MATIAB graphical interface toolbox for implementing third-order multivariate calibration methodologies is discussed. Multivariate calibration  (MVC) is a sequel of the already described first-order (MVC) and second-order (MVC) toolboxes. MVC accepts a variety of ASCII data for input, depending on whether the third-order data are vectorized or matricized. If required, data for sample sets are arranged into four-way arrays for processing with several quadrilinear and non-quadrilinear algorithms. Quadrilinear decomposition techniques and latent structured models based on partial least-squares regression and residual trilinearization are included in the software. Appropriate working sensor regions in the three data dimensions can be selected. Model development and its subsequent application to unknown samples are straightforward from the interface. Prediction results are provided along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (C) 2012 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,third-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2012.03.018
82,WOS:000236131200002,2006,Discrete element representation of manure products,WHEAT EN-MASSE SIMULATION MODELS PARAMETERS FLOW,"To simulate the machine-product interactions taking place in land application equipment, models of manure products must first be developed and validated. Several parameters must be defined to appropriately represent organic fertilizers in the discrete element method (DEM) framework. The work reported herein was aimed at determining the properties of the virtual product that would allow mimicking the behaviour of manure in the DEM software PFCD. A procedure was developed to generate an assembly of particles within the domain under investigation according to a user-defined particle size distribution, as would be measured by screening. The results generated by this procedure in terms of granulometry of the assembly of particles were very close to the user specifications with errors on the number of particles and on their size averaging .% and .%, respectively. A procedure was also developed to create clusters of particles randomly oriented and located within the modeled domain. The cluster-generation code was tested for clusters made of up to six particles, but could be expanded to include more particles. A calibration procedure based on a virtual direct shear test was developed to define the properties of the resulting virtual manure. A sensitivity analysis was performed to study the influence of parameters defining the linear and Hertz-Mindlin contact constitutive models. The simulations were based on experimental results obtained for pig manure at a total solids (TS) concentration of %. The results showed that numerous parameters have an influence on the behaviour of the virtual product in the direct shear test. Implementing the measured particle size distribution for pig manure at % TS, a friction coefficient of . and a Young's modulus value of . MPa allowed reaching an angle of internal friction of . degrees and an apparent cohesion value of . kPa that favourably compared to the . degrees and . kPa values measured experimentally.", (c) 2005 Elsevier B.V. All rights reserved.,"Landry, H|Lague, C|Roberge, M",COMPUTERS AND ELECTRONICS IN AGRICULTURE,discrete element method dem constitutive models input parameters numerical modeling manure organic fertilizers,10.1016/j.compag.2005.10.004
88,WOS:000277498900014,2010,Sediment and pollutant load modelling using an integrated urban drainage modelling toolbox: an application of City Drain,COMBINED SEWER SYSTEMS UNCERTAINTY ANALYSIS COPPER LOADS DATA SETS CALIBRATION RUNOFF EROSION PREDICTION TRANSPORT SURFACE,"Numerical and computational modelling of flow and pollutant dynamics in urban drainage systems is becoming more and more integral to planning and design. The main aim of integrated flow and pollutant models is to quantify the efficiency of different measures at reducing the amount of pollutants discharged into receiving water bodies and minimise the consequent negative water quality impact. The open source toolbox CITY DRAIN developed in the Matlab/Simulink environment, which was designed for integrated modelling of urban drainage systems, is used in this work. The goal in this study was to implement and test computational routines for representing sediment and pollutant loads in order to evaluate catchment surface pollution. Tested models estimate the accumulation, erosion and transport of pollutants-aggregately-on urban surfaces and in sewers. The toolbox now includes mathematical formulations for accumulation of pollutants during dry weather period and their wash-off during rainfall events. The experimental data acquired in a previous research project carried out by the Environmental Engineering Research Centre (CIIA) at the Universidad de los Andes in Bogota (Colombia) was used for the calibration of the models. Different numerical approaches were tested for their ability to calibrate to the sediment transport conditions. Initial results indicate, when there is more than one peak during the rainfall event duration, wash-off processes probably can be better represented using a model based on the flow instead of the rainfall intensity. Additionally, it was observed that using more detailed models (compared with an instantaneous approach) for representing pollutant accumulation do not necessarily lead to better results.",,"Rodriguez, JP|Achleitner, S|Moderl, M|Rauch, W|Maksimovic, C|McIntyre, N|Diaz-Granados, MA|Rodriguez, MS",WATER SCIENCE AND TECHNOLOGY,bogota city build-up and wash-off processes calibration and uncertainty analysis city drain toolbox sediment and pollutant load modelling,10.2166/wst.2010.139
89,WOS:000400594100014,2017,Uncertainty Estimation in Flood Inundation Mapping: An Application of Non-parametric Bootstrapping,MONTHLY STREAMFLOW PREDICTION ARTIFICIAL NEURAL-NETWORKS CONFIDENCE-INTERVALS MODEL CALIBRATION DESIGN FLOODS RUNOFF RISK PRECIPITATION OPTIMIZATION PARAMETERS,"Disaster prevention planning is affected in a significant way by a lack of in-depth understanding of the numerous uncertainties involved with flood delineation and related estimations. Currently, flood inundation extent is represented as a deterministic map without in-depth consideration of the inherent uncertainties associated with variables such as precipitation, streamflow, topographic representation, modelling parameters and techniques, and geospatial operations. The motivation of this study is to estimate uncertainties in flood inundation mapping based on a non-parametric bootstrapping method. The uncertainty is addressed through the application of non-parametric bootstrap sampling to the hydrodynamic modelling software, HEC-RAS, integrated with Geographic Information System (GIS). This approach was used to simulate different water levels and flow rates corresponding to different return periods from the available database. The study area was the Langat River Basin in Malaysia. The results revealed that the inundated land and infrastructure are subject to a flooding hazard of high-frequency events and that the flood damage potential is increasing significantly for residential areas and valuable land-use classes with higher return periods. The proposed methodology, as well as the study outcomes, of this paper could be beneficial to policymakers, water resources managers, insurance companies and other flood-related stakeholders."," Copyright (c) 2017 John Wiley & Sons, Ltd.","Faghih, M|Mirzaei, M|Adamowski, J|Lee, J|El-Shafie, A",RIVER RESEARCH AND APPLICATIONS,flood mapping uncertainty analysis non-parametric bootstrap sampling generalized extreme value distribution,10.1002/rra.3108
91,WOS:000281772900010,2010,INTEGRATION OF UNCERTAINTIES INTO INTERNAL CONTAMINATION MONITORING,IDEAS GUIDELINES DOSIMETRY,"Potential internal contaminations of workers are monitored by periodic bioassays interpreted in terms of intake and committed effective dose through biokinetic and dosimetric models. After a prospective evaluation of exposure at a workplace, a suitable monitoring program can be defined by the choice of measurement techniques and frequency of measurements. However, the actual conditions of exposure are usually not well defined and the measurements are subject to errors. In this study we took into consideration the uncertainties associated with a routine monitoring program in order to evaluate the minimum intake and dose detectable for a given level of confidence. Major sources of uncertainty are the contamination time, the size distribution and absorption into blood of the incorporated particles, and the measurement errors. Different assumptions may be applied to model uncertain knowledge, which lead to different statistical approaches. The available information is modeled here by classical or Bayesian probability distributions. These techniques are implemented in the OPSCI software under development. This methodology was applied to the monitoring program of workers in charge of plutonium purification at the AREVA NC reprocessing facility (La Hague, France). A sensitivity analysis was carried out to determine the important parameters for the minimum detectable dose. The methods presented here may be used for assessment of any other routine monitoring program through the comparison of the minimum detectable dose for a given confidence level with dose constraints. Health Phys. ():-; ",,"Davesne, E|Casanova, P|Chojnacki, E|Paquet, F|Blanchardon, E",HEALTH PHYSICS,computer calculations contamination internal effective dose plutonium,10.1097/HP.0b013e3181cd3d47
102,WOS:000270369500007,2009,Probabilistic Assessment Study of Channels Downstream Slopes Erosion in the Maritime Environment,,"This research deals with the probabilistic simulation and assessment of erosion in the downstream maritime slopes in hop ports (ports with deep approach channels to be able to accommodate the recent vessels generations) with natural side slopes. The study concentrated on the liquefaction effect in the erosion factor, which is the main controllable parameter for this phenomena. The probability of failure for the limit state function represents the erosion factor, which has a liable representation by a normal distribution with parameters mu = . and sigma = ., as a representative limit state function. This research deals with a maritime channel with certain dimensions as an example. The probabilistic simulations for downstream slope erosion were carried out using the Monte Carlo technique by using a probabilistic model. The generated probabilistic histograms of the erosion factor based on one run and different numbers of simulated random samples were determined. Based on these reliability simulation results, the erosion volumes per unit width of the channel were evaluated. Validation and sensitivity analyses were also carried out to ensure more reliability for this research. The study produced a group of guiding regression models for estimates and the determined conclusions related to the evaluated erosion volumes we carefully examined by considering calculation conditions based on a % confidence level with different assumptions. Then preliminary estimates for the eroded volumes (m()/m) in the downstream slope of the channel were evaluated and so used to determine the relevant regression models. These distributions were determined based on a group of assumed realistic conditions, which include variable berm depths and constant downstream slope angles in one simulated group with erosion volumes against downstream slopes depths heights variation and constant berm depths and variable slope angles in another with erosion volumes against downstream slopes angles variation. The limit state functions representing the erosion volumes variation behavior under the different conditions were also determined by using reliable statistical goodness-of-fit software. The research results are presented in a graphical form for the purpose of improving the current application capabilities in the subject and providing practical usage for the unprotected maritime navigation channel, trenches, and maritime downstream slopes.",,"Khalifa, MA|El Ganainy, MA|Nasr, RI",JOURNAL OF COASTAL RESEARCH,maritime downstream slopes hop ports approach channels downstream erosion factor downstream slopes liquefaction probabilistic simulations monte carlo simulation probabilistic slope failure sensitivity analysis downstream erosion volumes limit state functions,10.2112/08-1074.1
103,WOS:000382269000125,2016,Irrigation water demand of selected agricultural crops in Germany between 1902 and 2010,LEAF-AREA INDEX CLIMATE-CHANGE IMPACT ASSESSMENT NORTHERN GERMANY HUMID CLIMATE REQUIREMENTS EUROPE AVAILABILITY ENGLAND MODEL,"Irrigation water demand (IWD) is increasing worldwide, including in regions such as Germany that are characterized with low precipitation levels, yet grow water-demanding crops such as sugar beets, potatoes, and vegetables. This study aimed to calculate and analyze the spatial and temporal changes in the IWD of four crops spring barley, oat, winter wheat, and potato between  and  in Germany by using the modeling software AgroHyd Farmmodel. Climatic conditions in Germany continued to change over the investigation period, with an increase in temperature of . K/yr and an increase in precipitation of  mm/yr. Nevertheless, no significant increasing or decreasing trend in IWD was noted in the analysis. The IWD for the investigated crops in the area of the current ""Federal Republic of Germany"" over the  years was  mm/yr, varying between  and  mm/yr. Changes in cropping pattern and cultivated area over the last century caused large differences in the IWD calculated for each administrative district. The mean annual IWD of over the study period (which was divided into  parts) varied between , Mm()/yr in the earliest period (-) and  Mm()/yr in the latest period (-). Policy and management measures to adapt to climate change are currently being debated in Germany. The presented results suggest that the effects of the choice of crops (in this case, changes in cropping pattern in the German nation states) had a stronger influence on regional water resources than those of climate variability. Thus, the influence of climate change on water resources is relativized which brings an important input into the debate.", (C) 2016 Elsevier BM. All rights reserved.,"Drastig, K|Prochnow, A|Libra, J|Koch, H|Rolinski, S",SCIENCE OF THE TOTAL ENVIRONMENT,in igation water demand inigation trend agrohyd fanninodel,10.1016/j.scitotenv.2016.06.206
104,WOS:000245786200002,2007,Stormwater pollutant loads modelling: epistemological aspects and case studies on the influence of field data sets on calibration and verification,REGRESSION-MODELS,"In urban drainage, stormwater quality models have been used by researchers and practitioners for more than  years. Most of them were initially developed for research purposes, and have been later on implemented in commercial software packages devoted to operational needs. This paper presents some epistemological problems and difficulties with practical consequences in the application of stormwater quality models, such as simplified representation of reality, scaling-up, over-parameterisation, transition from calibration to verification and prediction, etc. Two case studies (one to estimate pollutant loads at the outlet of a catchment, one to design a detention tank to reach a given pollutant interception efficiency), with simple and detailed stormwater quality models, illustrate some of the above problems. It is hard to find, if not impossible, an ""optimum"" or ""best"" unique set of parameters values. Model calibration and verification appear to dramatically depend on the data sets used for their calibration and verification. Compared to current practice, collecting more and reliable data is absolutely necessary.",,"Bertrand-Krajewski, JL",WATER SCIENCE AND TECHNOLOGY,calibration epistemology field data modelling sensitivity analysis separate and combined sewers stormwater verification,10.2166/wst.2007.090
105,WOS:000269046900012,2009,IPH-TRIM3D-PCLake: A three-dimensional complex dynamic model for subtropical aquatic ecosystems,,"This paper presents IPH-TRIMD-PCLake, a three-dimensional complex dynamic model for subtropical aquatic ecosystems. It combines a spatially explicit hydrodynamic model with a water-quality and biotic model of ecological interactions. The software, which is freely available for research purposes, has a graphical user-friendly interface and a flexible design that allows the user to vary the complexity of the model. It also has built-in analysis tools such as Monte Carlo sensitivity analysis, a genetic algorithm for calibration, and plotting tools. (C) ", Elsevier Ltd. All rights reserved.,"Fragoso, CR|van Nes, EH|Janse, JH|Marques, DD",ENVIRONMENTAL MODELLING & SOFTWARE,aquatic ecosystem model cascading trophic effects subtropical ecosystems 3d model,10.1016/j.envsoft.2009.05.006
106,WOS:000309267300007,2012,Comparison of empirical and numerical methods in tunnel stability analysis,,"The stability of a tunnel can be evaluated using mathematical solutions, empirical methods, or numerical modelling. Mathematical solutions are precise methods; however the need to conduct mathematical calculations usually decreases the user's desire to use this method. Empirical methods are based on the experience gathered by researchers in various parts of the world whereas numerical modelling utilises computing power and, using various modelling techniques, can be a precise way of solving very complex problems. In this method the environment and the geometry can be set by the user. This method allows the user to conduct sensitivity analysis. In this article, empirical methods and numerical modelling using UDEC software were used to conduct a stability analysis of the access tunnel at the Shahriar dam crest, which was one of the most important tunnels of this project. In addition, numerical modelling was used to predict the stresses and deformations around the perimeter of the tunnel, and select the most suitable ground support system. The results obtained from both methods were compared for selection of the best suited support system. The results indicated that the empirical methods presented similar results to the results of numerical modelling at the first stages of tunnel design in jointed rocks. Therefore, in the absence of sufficient information for numerical analysis, the results of the empirical method can be used for this project.",,"Rahmani, N|Nikbakhtan, B|Ahangari, K|Apel, D",INTERNATIONAL JOURNAL OF MINING RECLAMATION AND ENVIRONMENT,mathematical analyses empirical methods numerical modelling tunnel udec,10.1080/17480930.2011.611615
108,WOS:000253663600007,2008,A computational algorithm for the multiple generation of nonlinear mathematical models and stability study,,"This paper presents an algorithm that generates families of mathematical models with nonlinear parameters, and includes the study of linear models, based on the experimental data of the intervening variables. The implementation of this algorithm has been named poly-model and is based on the application of the Gauss-Newton algorithm for obtaining the parameters of nonlinear models [Verdu F. Un Algoritmo para la construccion multiple de modelos matematicos no lineales y el estudio de su estabilidad. Doctoral Tesis. Universidad de Alicante, ]. One of its characteristics is a search among different nonlinear models within the parameters; unlike the methods found in the scientific literature [Camacho Rosales J. Estadistica con SPSS para windows. Ed. Ra-Ma, ; Mathsoft Inc. Splus-. Guide to Statistics. Seattle, ], the user does not intervene in their generation. A pruning criteria has also been introduced that is based on the stability analysis of models generated from perturbations, applying studies carried out by the authors and published in [Verdu F, Villacampa Y. A computer program for a Monte Carlo analysis of sensitivity in equations of environmental modelling obtained from experimental data. Advances in Engineering Software, ]. Object-oriented Pascal has been used in Delphi .", (c) 2007 Published by Elsevier Ltd.,"Verdu, F|Villacampa, Y",ADVANCES IN ENGINEERING SOFTWARE,modelling nonlinear regression sensitivity analysis,10.1016/j.advengsoft.2007.03.004
114,WOS:000224365000004,2004,MVC1: an integrated MatLab toolbox for first-order multivariate calibration,PARTIAL LEAST-SQUARES ORTHOGONAL SIGNAL CORRECTION WAVELENGTH SELECTION PROPAGATION ALGORITHM ERROR PLS,"Multivariate calibration  (MVC), a MatLab(R) toolbox for implementing up to  different first-order calibration methodologies through easily managed graphical user interfaces, is presented. The toolbox accepts different input data formats (either arranged as matrices or vectors contained in raw data files or in already existing MatLab variables) and incorporates many preprocessing algorithms in order to improve prediction capabilities. The development and validation of each model and its subsequent application to unknown samples are straightforward. Prediction results are produced along analytical figures of merit and standard errors calculated by uncertainty propagation. Moreover, the toolbox allows one to manually select working sensor regions, or to automatically find which region provides the minimum error. It also generates many different plots regarding model performance, including outliers detection, facilitating both model evaluation and interpretation.", (C) 2004 Elsevier B.V. All rights reserved.,"Oliveri, AC|Goicoechea, HC|Inon, FA",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,,10.1016/j.chemolab.2004.03.004
115,WOS:000245063400009,2007,Numerical and visual evaluation of hydrological and environmental models using the Monte Carlo analysis toolbox,RAINFALL-RUNOFF MODELS REGIONALIZED SENSITIVITY ANALYSIS WATER-QUALITY IMPROVED CALIBRATION PHOSPHORUS TRANSFER CONCEPTUAL MODELS UNCERTAINTY CATCHMENT IDENTIFICATION SYSTEMS,"The detailed evaluation of mathematical models and the consideration of uncertainty in the modeling of hydrological and environmental systems are of increasing importance, and are sometimes even demanded by decision makers. At the same time, the growing complexity of models to represent real-world systems makes it more and more difficult to understand model behavior, sensitivities and uncertainties. The Monte Carlo Analysis Toolbox (MCAT) is a Matlab library of visual and numerical analysis tools for the evaluation of hydrological and environmental models. Input to the MCAT is the result of a Monte Carlo or population evolution based sampling of the parameter space of the model structure under investigation. The MCAT can be used off-line, i.e. it does not have to be connected to the evaluated model, and can thus be used for any model for which an appropriate sampling can be performed. The MCAT contains tools for the evaluation of performance, identitiability, sensitivity, predictive uncertainty and also allows for the testing of hypotheses with respect to the model structure used. In addition to research applications, the MCAT can be used as a teaching tool in courses that include the use of mathematical models. (c) ", Elsevier Ltd. All rights reserved.,"Wagener, T|Kollat, J",ENVIRONMENTAL MODELLING & SOFTWARE,evaluation uncertainty analysis sensitivity analysis identifiability hypothesis testing model diagnostics matlab visualization,10.1016/j.envsoft.2006.06.017
116,WOS:000414818700006,2017,Assessment of environmental impacts and operational costs of the implementation of an innovative source-separated urine treatment,WASTE-WATER TREATMENT LIFE-CYCLE ASSESSMENT TREATMENT PLANTS NUTRIENT MANAGEMENT REMOVAL ALTERNATIVES SYSTEM FOCUS,"Innovative treatment technologies and management methods are necessary to valorise the constituents of wastewater, in particular nutrients from urine (highly concentrated and can have significant impacts related to artificial fertilizer production). The FP project, ValuefromUrine, proposed a new two-step process (called VFU) based on struvite precipitation and microbial electrolysis cell (MEC) to recover ammonia, which is further transformed into ammonium sulphate. The environmental and economic impacts of its prospective implementation in the Netherlands were evaluated based on life cycle assessment (LCA) methodology and operational costs. In order to tackle the lack of stable data from the pilot plant and the complex effects on wastewater treatment plant (WWTP), process simulation was coupled with LCA and costs assessment using the Python programming language. Additionally, particular attention was given to the propagation and analysis of inputs uncertainties. Five scenarios of VFU implementation were compared to the conventional treatment of  m() of wastewater. Inventory data were obtained from SUMO software for the WWTP operation. LCA was based on Brightway software (using ecoinvent database and ReCiPe method). The results, based on  iterations sampled from inputs distributions (foreground parameters, ecoinvent background data and market prices), showed a significant advantage of VFU technology, both at a small and decentralized scale and at a large and centralized scale (% confidence intervals not including zero values). The benefits mainly concern the production of fertilizers, the decreased efforts at the WWTP, the water savings from toilets flushing, as well as the lower infrastructure volumes if the WWTP is redesigned (in case of significant reduction of nutrients load in wastewater). The modelling approach, which could be applied to other case studies, improves the representativeness and the interpretation of results (e.g. complex relationships, global sensitivity analysis) but requires additional efforts (computing and engineering knowledge, longer calculation time). Finally, the sustainability assessment should be refined in the future with the development of the technology at larger scale to update these preliminary conclusions before its commercialization. (C) ", Elsevier Ltd. All rights reserved.,"Igos, E|Besson, M|Gutierrez, TN|de Faria, ABB|Benetto, E|Barna, L|Ahmadi, A|Sperandio, M",WATER RESEARCH,source-separated urine treatment process simulation sustainability assessment innovative technology integrated modelling,10.1016/j.watres.2017.09.016
118,WOS:000286284700004,2011,A methodology for the design and development of integrated models for policy support,RIVER-BASIN MANAGEMENT 10 ITERATIVE STEPS LAND-USE PATTERNS PLANNING-SUPPORT OPTIMIZATION METHODOLOGY ORGANIZATIONAL-CHANGE ENVIRONMENTAL-MODELS SENSITIVITY-ANALYSIS CELLULAR-AUTOMATA SCENARIO ANALYSIS,"The development of Decision Support Systems (DSS) to inform policy making has been increasing rapidly. This paper aims to provide insight into the design and development process of policy support systems that incorporate integrated models. It will provide a methodology for the development of such systems that attempts to synthesize knowledge and experience gained over the past - years from developing a suite of these DSSs for a number of users in different geographical contexts worldwide. The methodology focuses on the overall iterative development process that includes policy makers, scientists and IT-specialists. The paper highlights important tasks in model integration and system development and illustrates these with some practical examples from DSS that have dynamic, spatial and integrative attributes. Crucial integrative features of modelling systems that aim to provide support to policy processes, and to which we refer as integrated Decision Support Systems, are: Synthesis of relevant drivers, processes and characteristics of the real world system at relevant spatial and temporal scales. An integrated approach linking economic, environmental and social domains. Connection to the policy context, interest groups and end-users. Engagement with the policy process. Ability to provide added value to the current decision-making practice. With this paper we aim to provide a methodology for the design and development of these integrated Decision Support Systems that includes the 'hard' elements of model integration and software development as well as the 'softer' elements related to the user-developer interaction and social learning of all groups involved in the process. (C) ", Elsevier Ltd. All rights reserved.,"van Delden, H|Seppelt, R|White, R|Jakeman, AJ",ENVIRONMENTAL MODELLING & SOFTWARE,decision support system (dss) model integration design and development process iterative process social learning policy support,10.1016/j.envsoft.2010.03.021
121,WOS:000168591400008,2001,MCE-RISK: integrating multicriteria evaluation and CIS for risk decision-making in natural hazards,GEOGRAPHICAL INFORMATION-SYSTEMS,"During the past two decades there have been a wide range of applications for decision-making linking multicriteria evaluation (MCE) and geographic information systems (GIS). However, limited literature reports the development of MCE-GIS software, and the comparison of various MCE-GIS approaches. This paper introduces an MCE-GIS program called MCE-RISK for risk-based decision-making. It consists of a series of modules for data standardisation, weighting, MCE-GIS methods. and sensitivity analysis. The program incorporates different MCE-GIS methods. including weighted linear combination (WLC), the technique for order preference by similarity to ideal solution (TOPSIS), and compromise programming (CP), enabling comparisons between different methods for the same decision problem to be made. An example of decision-making for determining priority areas for a bushfire hazard reduction burning is examined. After implementing the alternative MCE-GIS methods, and comparing final outputs and the computational difficulty involved in the analysis, WLC is recommended. Some caveats on using MCE-GIS methods art: also dis cussed. Although the development of MCE-RISK and its application reported in this paper are specific to risk-based decisionmaking in natural hazards, the program can be used for other environmental decision applications. such as environmental impact assessment and land-use planning."," (C) 2001 Elsevier Science Ltd, All rights reserved.","Chen, KP|Blong, R|Jacobson, C",ENVIRONMENTAL MODELLING & SOFTWARE,risk decision-making multicriteria evaluation cis bushfire prescribed burning,10.1016/S1364-8152(01)00006-8
128,WOS:000308971400030,2012,An integrated assessment tool to define effective air quality policies at regional scale,POLLUTION MODEL PM10 STRATEGIES VALIDATION LONDON EUROPE,"In this paper, the Integrated Assessment of air quality is dealt with at regional scale. First the paper describes the main challenges to tackle current air pollution control, including economic aspects. Then it proposes a novel approach to manage the problem, presenting its mathematical formalization and describing its practical implementation into the Regional Integrated Assessment Tool (RIAT). The main features of the software system are described and some preliminary results on a domain in Northern Italy are illustrated. The novel features in RIAT are then compared to the state-of-the-art in integrated assessment of air quality, for example the ability to handle nonlinearities (instead of the usual linear approach) and the multi-objective framework (alternative to cost-effectiveness and scenario analysis). Then the lessons learned during the RIAT implementation are discussed, focusing on the locality, flexibility and openness of the tool. Finally the areas for further development of air quality integrated assessment are highlighted, with a focus on sensitivity analysis, structural and non technical measures, and the application of parallel computing concepts. (C) ", Elsevier Ltd. All rights reserved.,"Carnevale, C|Finzi, G|Pisoni, E|Volta, M|Guariso, G|Gianfreda, R|Maffeis, G|Thunis, P|White, L|Triacchini, G",ENVIRONMENTAL MODELLING & SOFTWARE,integrated assessment modeling model reduction air quality modeling multi-objective optimization decision support,10.1016/j.envsoft.2012.07.004
130,WOS:000183565000005,2003,Calibration and sensitivity analysis of a river water quality model under unsteady flow conditions,SYSTEMS QUASAR UNCERTAINTY PREDICTION SIMULATION CHANNEL OUSE,"Water quality models generally require a relatively large number of parameters to define their functional relationships, and since prior information on parameter values is limited, these are commonly defined by fitting the model to observed data. In this paper, the identifiability of water quality parameters and the associated uncertainty in model simulations are investigated. A modification to the water quality model `Quality Simulation Along River Systems' is presented in which an improved flow component is used within the existing water quality model framework. The performance of the model is evaluated in an application to the Bedford Ouse river, UK, using a Monte-Carlo analysis toolbox. The essential framework of the model proved to be sound, and calibration and validation performance was generally good. However some supposedly important water quality parameters associated with algal activity were found to be completely insensitive, and hence non-identifiable, within the model structure, while others (nitrification and sedimentation) had optimum values at or close to zero, indicating that those processes were not detectable from the data set examined.", (C) 2003 Elsevier Science B.V. All rights reserved.,"Sincock, AM|Wheater, HS|Whitehead, PG",JOURNAL OF HYDROLOGY,bedford ouse nitrate do bod quality simulation along river systems water quality modelling,10.1016/S0022-1694(03)00127-6
134,WOS:000249622700001,2007,An integrated framework for multipollutant air quality management and its application in georgia,SOUTHEASTERN UNITED-STATES SOURCE APPORTIONMENT TIME-SERIES POLLUTION OZONE MODEL EMISSIONS MORTALITY HEALTH PM2.5,"Air protection agencies in the United States increasingly confront non-attainment of air quality standards for multiple pollutants sharing interrelated emission origins. Traditional approaches to attainment planning face important limitations that are magnified in the multipollutant context. Recognizing those limitations, the Georgia Environmental Protection Division has adopted an integrated framework to address ozone, fine particulate matter, and regional haze in the state. Rather than applying atmospheric modeling merely as a final check of an overall strategy, photochemical sensitivity analysis is conducted upfront to compare the effectiveness of controlling various precursor emission species and source regions. Emerging software enables the modeling of health benefits and associated economic valuations resulting from air pollution control. Photochemical sensitivity and health benefits analyses, applied together with traditional cost and feasibility assessments, provide a more comprehensive characterization of the implications of various control options. The fuller characterization both informs the selection of control options and facilitates the communication of impacts to affected stakeholders and the public. Although the integrated framework represents a clear improvement over previous attainment-planning efforts, key remaining shortcomings are also discussed.",,"Cohan, DS|Boylan, JW|Marmur, A|Khan, MN",ENVIRONMENTAL MANAGEMENT,air pollution control cost-benefit analysis ozone fine particulate matter state implementation plans attainment,10.1007/s00267-006-0228-4
135,WOS:000402819500015,2017,Investigation on the effect of geometrical and geotechnical parameters on elongated offshore piles using fuzzy inference systems,PLATFORMS BEHAVIOR,"Among numerous offshore structures used in oil extraction, jacket platforms are still the most favorable ones in shallow waters. In such structures, log piles are used to pin the substructure of the platform to the seabed. The pile's geometrical and geotechnical properties are considered as the main parameters in designing these structures. In this study, ANSYS was used as the FE modeling software to study the geometrical and geotechnical properties of the offshore piles and their effects on supporting jacket platforms. For this purpose, the FE analysis has been done to provide the preliminary data for the fuzzy-logic post-process. The resulting data were implemented to create Fuzzy Inference System (FIS) classifications. The resultant data of the sensitivity analysis suggested that the orientation degree is the main factor in the pile's geometrical behavior because piles which had the optimal operational degree of about A degrees are more sustained. Finally, the results showed that the related fuzzified data supported the FE model and provided an insight for extended offshore pile designs.",,"Aminfar, A|Mojtahedi, A|Ahmadi, H|Aminfar, MH",CHINA OCEAN ENGINEERING,pile soil fem offshore jacket platform pile-soil interaction fuzzy-logic fuzzification,10.1007/s13344-017-0044-z
140,WOS:000280656300004,2010,Simulation model for extended double-ended queueing,QUEUES IMPATIENCE CUSTOMERS,"The purpose of this paper is to extend traditional double-ended queuing models using a simulation approach. Traditional double-ended queuing models assume that one supply queue should satisfy one demand queue through instantaneous pairing. Inter-arrival time is assumed to follow an exponential distribution, with arrivals to the system assumed to occur just one at a time. However, this assumption is frequently violated in many real-world situations. The pairing or batch size can either be multiple or a random variable, and the pairing processing time can be greater than . Inter-arrival time may follow distributions other than exponential. In some cases bulk arrivals may come at the same time, and pairing is not always guaranteed. Because the analytical approach has enormous difficulties obtaining performance measures under these relaxed situations, a simulation approach for extended double-ended queueing processes is presented. This includes an algorithm to find state probabilities and a newly developed simulation procedure. Using this new procedure, sensitivity analyses of performance measures were performed using various input conditions implemented using ProModel and SimRumnner simulation software. A business case is studied to demonstrate the versatility of the proposed approaches. (c) ", Elsevier Ltd. All rights reserved.,"Kim, WK|Yoon, KP|Mendoza, G|Sedaghat, M",COMPUTERS & INDUSTRIAL ENGINEERING,double-ended queue simulation state probability optimization job placement agency,10.1016/j.cie.2010.04.002
142,WOS:000321439500015,2013,Application of Bayesian Networks in Quantitative Risk Assessment of Subsea Blowout Preventer Operations,OFFSHORE SAFETY ASSESSMENT RELIABILITY-ANALYSIS ORGANIZATIONAL-FACTORS FAULT-TREES SYSTEMS METHODOLOGY FACILITIES ACCIDENTS SECURITY PLATFORM,"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.",,"Cai, BP|Liu, YH|Liu, ZK|Tian, XJ|Zhang, YZ|Ji, RJ",RISK ANALYSIS,bayesian networks quantitative risk assessment subsea blowout preventer,10.1111/j.1539-6924.2012.01918.x
145,WOS:000356741300007,2015,A Matlab toolbox for Global Sensitivity Analysis,IDENTIFICATION UNCERTAINTIES MODELS,"Global Sensitivity Analysis (GSA) is increasingly used in the development and assessment of environmental models. Here we present a Matlab/Octave toolbox for the application of GSA, called SAFE (Sensitivity Analysis For Everybody). It implements several established GSA methods and allows for easily integrating others. All methods implemented in SAFE support the assessment of the robustness and convergence of sensitivity indices. Furthermore, SAFE includes numerous visualisation tools for the effective investigation and communication of GSA results. The toolbox is designed to make GSA accessible to non-specialist users, and to provide a fully commented code for more experienced users to complement their own tools. The documentation includes a set of workflow scripts with practical guidelines on how to apply GSA and how to use the toolbox. SAFE is open source and freely available for academic and non-commercial purpose. Ultimately, SAFE aims at contributing towards improving the diffusion and quality of GSA practice in the environmental modelling community. (C)  The Authors.", Published by Elsevier Ltd.,"Pianosi, F|Sarrazin, F|Wagener, T",ENVIRONMENTAL MODELLING & SOFTWARE,global sensitivity analysis matlab octave open-source software,10.1016/j.envsoft.2015.04.009
146,WOS:000266225700018,2009,MVC2: A MATLAB graphical interface toolbox for second-order multivariate calibration,TRILINEAR DECOMPOSITION ALGORITHM PARALLEL FACTOR-ANALYSIS PARTIAL LEAST-SQUARES CURVE RESOLUTION ADVANTAGE BILINEARIZATION SENSITIVITY PREDICTION,"This work reports the release of Multivariate Calibration  (MVC), a MATLAB graphical interface toolbox for implementing several second-order multivariate calibration methodologies. The toolbox accepts a variety of input data formats, arranged in either matrices or vectors (i.e., unfolded matrices), and contained in ASCII files. It allows one to manually select working sensor regions and plot landscapes for selected samples. The development of each model and its subsequent application to unknown samples is straightforward. Prediction results are produced along with analytical figures of merit and standard concentration errors, as calculated by modern concepts of uncertainty propagation.", (c) 2009 Elsevier B.V. All rights reserved.,"Olivieri, AC|Wu, HL|Yu, RQ",CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS,second-order multivariate calibration matlab program graphical interface figures of merit,10.1016/j.chemolab.2009.02.005
149,WOS:000186310600007,2003,Direct and adjoint sensitivity analysis of chemical kinetic systems with KPP: II - Numerical validation and applications,VARIATIONAL DATA ASSIMILATION CHEMISTRY DATA ASSIMILATION AIR-QUALITY MODEL OZONE IMPLEMENTATION CODE,"The Kinetic PreProcessor KPP was extended to generate the building blocks needed for the direct and adjoint sensitivity analysis of chemical kinetic systems. An overview of the theoretical aspects of sensitivity calculations and a discussion of the KPP software tools is presented in the companion paper. In this work the correctness and efficiency of the KPP generated code for direct and adjoint sensitivity studies are analyzed through an extensive set of numerical experiments. Direct-decoupled Rosenbrock methods are shown to be cost-effective for providing sensitivities at low and medium accuracies. A validation of the discrete-adjoint evaluated gradients is performed against the finite difference estimates. The accuracy of the adjoint gradients is measured using a reference gradient value obtained with a standard direct-decoupled method. The accuracy is studied for both constant step size and variable step size integration of the forward/adjoint model and the consistency between the discrete and continuous adjoint models is analyzed. Applications of the KPP-. software package to direct and adjoint sensitivity studies, variational data assimilation, and parameter identification are considered for the comprehensive chemical mechanism SAPRC-. (C) ", Elsevier Ltd. All rights reserved.,"Daescu, DN|Sandu, A|Carmichael, GR",ATMOSPHERIC ENVIRONMENT,sensitivity analysis data assimilation parameter identification optimization,10.1016/j.atmosenv.2003.08.020
153,WOS:000334003800020,2014,Environmental impact assessment based on dynamic fuzzy simulation,MODELS SYSTEMS,"A new ""quick scan"" method for an expert-/stakeholder-based impact assessment approach is introduced. This approach aims to reduce the complexity of models, to simulate and visualize the system dynamics and to provide a basis for guided discussion with stakeholders. The approach is based on dynamic fuzzy models that can be understood easily and developed by experts and understood and adapted by stakeholders (""white box models""). This open modeling process also forms the basis of the credibility of the simulation results. The quick scan approach is supported by an interactive simulation tool that includes optimization and uncertainty analysis as open source software. (C) ", Elsevier Ltd. All rights reserved.,"Wieland, R|Gutzler, C",ENVIRONMENTAL MODELLING & SOFTWARE,quick scan environmental impact assessment fuzzy modeling dynamic fuzzy simulation,10.1016/j.envsoft.2014.02.001
157,WOS:000392568100009,2017,Uncertainty quantification in littoral erosion,SHALLOW-WATER FLOWS GEOMETRIC CHARACTERIZATION SHAPE OPTIMIZATION SENSITIVITY SPACES EVOLUTION SCHEME RISK,We aim at quantifying the impact of flow state uncertainties in littoral erosion to provide confidence bounds on deterministic predictions of bottom morphodynamics. Two constructions of the bathymetry standard deviation are discussed. The first construction involves directional quantile-based extreme scenarios using what is known on the flow state Probability Density Function (PDF) from on site observations. We compare this construction to a second cumulative one using the gradient by adjoint of a functional involving the energy of the system. These ingredients are illustrated for two models for the interaction between a soft bed and a flow in a shallow domain. Our aim is to keep the computational complexity comparable to the deterministic simulations taking advantage of what already available in our simulation toolbox. (C) , Elsevier Ltd. All rights reserved.,"Mohammadi, B",COMPUTERS & FLUIDS,backward propagation quantile uncertainty littoral morphodynamics shallow water equations sensitivity analysis worst-case analysis,10.1016/j.compfluid.2016.10.017
164,WOS:000260920100004,2008,ON THE SENSITIVITY OF DESIRABILITY FUNCTIONS FOR MULTIRESPONSE OPTIMIZATION,,"Desirability functions have been one of the most important multiresponse optimization technique since the early eighties. Main reasons for this popularity might be counted as the convenience of the implementation of the method and it's availability in many experimental design software packages. Technique itself involves somehow subjective parameters such as the importance coefficients between response characteristics that are used to calculate overall desirability, weights used in determining the shape of each individual response and the size of the specification band of the response. However, the impact of these sensitive parameters on the solution set is mostly uninvestigated. This paper proposes a procedure to analyze the sensitivity of the important characteristic parameters of desirability functions and their impact on pareto-optimal solution set. The proposed procedure uses the experimental design tools on the solution space and estimates a prediction equation on the overall desirability to identify the sensitive parameters. For illustration, a classical desirability example is selected from the literature and results are given along with the discussion.",,"Aksezer, CS",JOURNAL OF INDUSTRIAL AND MANAGEMENT OPTIMIZATION,desirability functions parametric sensitivity analysis multiresponse optimization,10.3934/jimo.2008.4.685
167,WOS:000242724500019,2006,Uncertainty analysis for regional-scale reserve selection,SITE SELECTION SPECIES DISTRIBUTION CONSERVATION DESIGN BIODIVERSITY NETWORKS MODELS PERSISTENCE PROBABILITIES CONNECTIVITY,"Methods for reserve selection and conservation planning often ignore uncertainty. For example, presence-absence observations and predictions of habitat models are used as inputs but commonly assumed to be without error We applied information-gap decision theory to develop uncertainty analysis methods for reserve selection. Our proposed method seeks a solution that is robust in achieving a given conservation target, despite uncertainty in the data. We maximized robustness in reserve selection through a novel method, ""distribution discounting,"" in which the site- and species-specific measure of conservation value (related to species-specific occupancy probabilities) was penalized by an error measure (in our study, related to accuracy of statistical prediction). Because distribution discounting can be implemented as a modification of input files, it is a computationally efficient solution for implementing uncertainty analysis into reserve selection. Thus, the method is particularly useful for high-dimensional decision problems characteristic of regional conservation assessment. We implemented distribution discounting in the zonation reserve-selection algorithm that produces a hierarchy of conservation priorities throughout the landscape. We applied it to reserve selection for seven priority fauna in a landscape in New South Wales, Australia. The distribution discounting method can be easily adapted for use with different kinds of data (e.g., probability of occurrence or abundance) and different landscape descriptions (grid or patch based) and incorporated into other reserve-selection algorithms and software.",,"Moilanen, A|Wintle, BA|Elith, J|Burgman, M",CONSERVATION BIOLOGY,distribution discounting distribution smoothing information-gap decision theory reserve-network design site-selection algorithm spatial reserve design zonation,10.1111/j.1523-1739.2006.00560.x
169,WOS:000330675000001,2014,Intelligent Platform for Model Updating in a Structural Health Monitoring System,PARAMETER SELECTION DYNAMICS,"The main aim of this study is to develop an automated smart software platform to improve the time-consuming and laborious process of model updating. We investigate the key techniques of model updating based on intelligent optimization algorithms, that is, accuracy judgment methods for basic finite element model, parameter choice theory based on sensitivity analysis, commonly used objective functions and their construction methods, particle swarm optimization, and other intelligent optimization algorithms. An intelligent model updating prototype software framework is developed using the commercial software systems ANSYS and MATLAB. A parameterized finite element modeling technique is proposed to suit different bridge types and different model updating requirements. An objective function library is built to fit different updating targets. Finally, two case studies are conducted to verify the feasibility of the techniques used by the proposed software platform.",,"Dan, DH|Yang, T|Gong, JX",MATHEMATICAL PROBLEMS IN ENGINEERING,,10.1155/2014/628619
175,WOS:000383298800002,2016,A software framework for probabilistic sensitivity analysis for computationally expensive models,STOCHASTIC PREDICTIONS POLYMERIC NANOCOMPOSITES CORRELATED PARAMETERS OPTIMIZATION SIMULATIONS UNCERTAINTY PROPAGATION VARIABLES INDEXES DESIGN,"We provide a sensitivity analysis toolbox consisting of a set of Matlab functions that offer utilities for quantifying the influence of uncertain input parameters on uncertain model outputs. It allows the determination of the key input parameters of an output of interest. The results are based on a probability density function (PDF) provided for the input parameters. The toolbox for uncertainty and sensitivity analysis methods consists of three ingredients: () sampling method, () surrogate models, () sensitivity analysis (SA) method. Numerical studies based on analytical functions associated with noise and industrial data are performed to prove the usefulness and effectiveness of this study. (C) ", Elsevier Ltd. All rights reserved.,"Vu-Bac, N|Lahmer, T|Zhuang, X|Nguyen-Thoi, T|Rabczuk, T",ADVANCES IN ENGINEERING SOFTWARE,uncertainty quantification random sampling penalized spline regression sensitivity analysis matlab toolbox,10.1016/j.advengsoft.2016.06.005
184,WOS:000262497400004,2008,Fidelity of Network Simulation and Emulation: A Case Study of TCP-Targeted Denial of Service Attacks,,"In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",,"Chertov, R|Fahmy, S|Shroff, NB",ACM TRANSACTIONS ON MODELING AND COMPUTER SIMULATION,simulation emulation testbeds tcp congestion control denial of service attacks low-rate tcp-targeted attacks,10.1145/1456645.1456649
