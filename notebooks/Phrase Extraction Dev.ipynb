{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "text = 'Over-parameterisation is a well-known and often described problem in hydrological models, especially for distributed models. Therefore, methods to reduce the number of parameters via sensitivity analysis are important for the efficient use of these models. This paper describes a novel sampling strategy that is a combination of latin-hypercube and one-factor-at-a-time sampling that allows a global sensitivity analysis for a long list of parameters with only a limited number of model runs. The method is illustrated with an application of the water flow and water quality parameters of the distributed water quality program SWAT, considering flow, suspended sediment, total nitrogen, total phosphorus, nitrate and ammonia outputs at several locations in the Upper North Bosque River catchment in Texas and the Sandusky River catchment in Ohio. The application indicates that the methodology works successfully. The results also show that hydrologic parameters are dominant in controlling water quality predictions. Finally, the sensitivity results are not transferable between basins and thus the analysis needs to be conducted separately for each study catchment.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "candidate_patterns = \"\"\"\n",
    "NP:\n",
    "    {(<NN.*>+)|(<NN.*>+<JJ.*>?)|(<JJ.*>?<NN.*>+)}\n",
    "    {<NN.*|JJ>*<NN.*>}\n",
    "    {(<JJ.>|<NN.>)*<IN>?(<JJ.>|<NN.>)*<NN.>}\n",
    "    {<PRP>?<JJ.*>*<NN.*>+}\n",
    "    {<DT|PP$>?<JJ>*<NN.*>+}\n",
    "    {(<\\w+DT>)?(<\\w+JJ>)*(<\\w+>(<NN|NP|PRN>))}\n",
    "    {(<NN.*>+<JJ.*>?)|(<JJ.*>?<NN.*>+)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# corpus = text\n",
    "# # corpus = \"cognitive agent specification language\"\n",
    "\n",
    "# cp = nltk.RegexpParser(candidate_patterns)\n",
    "# sent_tokenize_list = nltk.sent_tokenize(corpus)\n",
    "\n",
    "# for sent in sent_tokenize_list:\n",
    "#     tagged_words = nltk.word_tokenize(sent)\n",
    "#     tagged_tokens = nltk.tag.pos_tag(tagged_words)\n",
    "#     identified = cp.parse(tagged_tokens)\n",
    "    \n",
    "#     sent_len = len(sent.split(\" \"))\n",
    "#     if sent_len % 2 == 0:\n",
    "#         central_pos = int((sent_len - 1) / 2)\n",
    "#     else:\n",
    "#         central_pos = int(sent_len / 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specification\n",
      "cognitive\n",
      "Agent\n",
      "specification\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(candidate_patterns)\n",
    "\n",
    "sent_corpora = [\"cognitive agent specification language and verification\", \n",
    "                \"we apply a cognitive agent specification language\",\n",
    "                \"cognitive Agent specification language\",\n",
    "                \"specification language\"\n",
    "               ]\n",
    "\n",
    "sent_corpora = [[sent, 1] for sent in sent_corpora]\n",
    "\n",
    "for idx, corpus_score in enumerate(sent_corpora):\n",
    "    \n",
    "    corpus, score = corpus_score\n",
    "\n",
    "    sent_tokenize_list = nltk.sent_tokenize(corpus)\n",
    "    for sent in sent_tokenize_list:\n",
    "        tagged_words = nltk.word_tokenize(sent)\n",
    "        tagged_tokens = nltk.tag.pos_tag(tagged_words)\n",
    "        identified = cp.parse(tagged_tokens)\n",
    "\n",
    "        split_sent = sent.split(\" \")\n",
    "\n",
    "        sent_len = len(split_sent)\n",
    "        if sent_len % 2 == 0:\n",
    "            central_pos = int((sent_len - 1) / 2)\n",
    "        else:\n",
    "            central_pos = int(sent_len / 2)\n",
    "\n",
    "        root = split_sent[central_pos]\n",
    "        print(root)\n",
    "\n",
    "        # contains_root = [s for ins, s in enumerate(corpora) if root in s and ins != idx]\n",
    "        \n",
    "        for candidate_sentence in sent_corpora:\n",
    "            candidate = candidate_sentence[0]\n",
    "            if (corpus == candidate) or (root not in candidate):\n",
    "                continue\n",
    "\n",
    "            candidate_score = sent_corpora[idx][1]\n",
    "            corpus_score[1] = candidate_score + (fuzz.token_sort_ratio(sent, candidate) / 100.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cognitive agent specification language and verification',\n",
       "  3.1999999999999997],\n",
       " ['we apply a cognitive agent specification language', 2.68],\n",
       " ['cognitive Agent specification language', 1],\n",
       " ['specification language', 2.92]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Lets try a full example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import wosis\n",
    "import metaknowledge as mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# corpora = [\"in order to use environmental models effectively for management and decisionmaking it is vital to establish an appropriate level of confidence in their performance this paper reviews techniques available across various fields for characterising the performance of environmental models with focus on numerical graphical and qualitative methods general classes of direct value comparison coupling real and modelled values preserving data patterns indirect metrics based on parameter values and data transformations are discussed in practice environmental modelling requires the use and implementation of workflows that combine several methods tailored to the model purpose and dependent upon the data and information available a fivestep procedure for performance evaluation of models is suggested with the key elements including i reassessment of the models aim scale and scope ii characterisation of the data for calibration and testing iii visual and other analysis to detect under or nonmodelled behaviour and to gain an overview of overall performance iv selection of basic performance criteria and v consideration of more advanced methods to handle problems such as systematic divergence between modelled and observed values c  elsevier ltd all rights reserved\",\n",
    "#            \"in  morris proposed an effective screening sensitivity measure to identify the few important factors in models with many factors the method is based on computing for each input a number of incremental ratios namely elementary effects which are then averaged to assess the overall importance of the input despite its value the method is still rarely used and instead local analyses varying one factor at a time around a baseline point are usually employed in this piece of work we propose a revised version of the elementary effects method improved in terms of both the definition of the measure and the sampling strategy in the present form the method shares many of the positive qualities of the variancebased techniques having the advantage of a lower computational cost as demonstrated by the analytical examples the method is employed to assess the sensitivity of a chemical reaction model for dimethylsulphide dms a gas involved in climate change results of the sensitivity analysis open up the ground for model reconsideration some model components may need a more thorough modelling effort while some others may need to be simplified c  elsevier ltd all rights reserved\",\n",
    "#            \"a terminology and typology of uncertainty is presented together with a framework for the modelling process its interaction with the broader water management process and the role of uncertainty at different stages in the modelling processes brief reviews have been made of  different partly complementary methods commonly used in uncertainty assessment and characterisation data uncertainty engine due error propagation equations expert elicitation extended peer review inverse modelling parameter estimation inverse modelling predictive uncertainty monte carlo analysis multiple model simulation nusap quality assurance scenario analysis sensitivity analysis stakeholder involvement and uncertainty matrix the applicability of these methods has been mapped according to purpose of application stage of the modelling process and source and type of uncertainty addressed it is concluded that uncertainty assessment is not just something to be added after the completion of the modelling work instead uncertainty should be seen as a red thread throughout the modelling study starting from the very beginning where the identification and characterisation of all uncertainty sources should be performed jointly by the modeller the water manager and the stakeholders c  elsevier ltd all rights reserved\",\n",
    "#            \"This paper analyzes the application of global sensitivity analysis (GSA) to the improvement of processes using various case studies. First, a brief description of the methods applied is given, and several case studies are examined to show how GSA can be applied to the study to improve the processes. The case studies include the identification of processes; comparisons of the Sobol, E-FAST and Morris GSA methods; a comparison of GSA with local sensitivity analysis; an examination of the effect of uncertainty levels and the type of distribution function on the input factors; and the application of GSA to the improvement of a copper flotation circuit. We conclude that GSA can be a useful tool in the analysis, comparison, design and characterization of separation circuits. In addition, we conclude that using the stage's recoveries of each species as input factors is a suitable choice for the GSA of a flotation plant. (C) 2014 Elsevier Ltd. All rights reserved.\"\n",
    "#           ]\n",
    "\n",
    "config = wosis.load_config('../config/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1799 records with no DOIs\n"
     ]
    }
   ],
   "source": [
    "query_id = \"756d39801152fe5f5f4ad3a3df9b6a30\"\n",
    "RC = mk.RecordCollection(\"tmp/{}.txt\".format(query_id))\n",
    "\n",
    "# Remove publications with no DOI\n",
    "corpora_df = wosis.rc_to_df(RC)\n",
    "corpora_df, removed_pubs = wosis.constrain.remove_empty_DOIs(corpora_df)\n",
    "\n",
    "# Create a new metaknowledge collection\n",
    "filtered_corpora = wosis.extract_recs(corpora_df.loc[:, 'id'], RC, name='Filtered Corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<metaknowledge.RecordCollection object Filtered Corpora>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df = wosis.rc_to_df(filtered_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "corpora = df[0:5]['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE 2013: Sustainability of countries updated (2014)\n",
      "Sustainability Assessment by Fuzzy Evaluation (SAFE) is a model that measures the overall sustainability of countries by combining indicators of ecological sustainability and human sustainability. The model provides country rankings and performs sensitivity analysis which reveals key indicators that each country should improve. This note presents the most recent country rankings and policy recommendations based on the most recent data. A comparison with past rankings shows the relative progress of each country over the last decade. (C) \n",
      "\n",
      "\n",
      "                                                                                                            text  \\\n",
      "0  This note presents the most recent country rankings and policy recommendations based on the most recent data.   \n",
      "\n",
      "   score  \n",
      "0  0.97   \n",
      "\n",
      "=================\n",
      "\n",
      "Integration of topology and shape optimization for design of structural components (2001)\n",
      "This paper presents an integrated approach that supports the topology optimization and CAD-based shape optimization. The main contribution of the paper is using the geometric reconstruction technique that is mathematically sound and error bounded for creating solid models of the topologically optimized structures with smooth geometric boundary. This geometric reconstruction method extends the integration to -D applications. In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process. The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces. The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure. The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed. Both -D plane stress and -D solid examples are presented to demonstrate the proposed approach.\n",
      "\n",
      "\n",
      "                                                                                                                                                                                                                                                        text  \\\n",
      "0  The integration is carried out by first converting the geometry of the topologically optimized structure into smooth and parametric B-spline curves and surfaces.                                                                                           \n",
      "1  The B-spline curves and surfaces are then imported into a parametric CAD environment to build solid models of the structure.                                                                                                                                \n",
      "2  In addition, commercial Computer-Aided Design (CAD), finite element analysis (FEA), optimization, and application software tools are incorporated to support the integrated optimization process.                                                           \n",
      "3  The control point movements of the B-spline curves or surfaces are defined as design variables for shape optimization, in which CAD-based design velocity field computations, design sensitivity analysis (DSA), and nonlinear programming are performed.   \n",
      "\n",
      "   score  \n",
      "0  3.78   \n",
      "1  3.72   \n",
      "2  3.01   \n",
      "3  0.52   \n",
      "\n",
      "=================\n",
      "\n",
      "Optimization and Simulation Modelling for Managing the Problems of Water Resources (2013)\n",
      "Waterlogging and secondary salinization have become a serious problem in the canal irrigated areas of arid and semi-arid regions worldwide. In this study, a unique and simple technique was evolved in which a linear programming (LP) optimization model was first developed that allocates available land and water resources in order to maximize net annual returns by mitigating the waterlogging problems. A finite-difference two-dimensional simulation model was then used to evaluate the long-term impacts of various water management strategies on the groundwater table with the optimal land and water use parameters which were obtained through the optimization model. The model was used to combat the waterlogging and salinity problem of an area located in Haryana State of India. The calibration, validation, sensitivity analysis, and error analysis of the model was performed before it was used to study the impact of various water management scenarios on the long-term groundwater level. Based on the model results a change in cropping pattern with reduced rice area is suggested. Groundwater withdrawal should be increased by - % in the various nodes. It is concluded from the analysis of various scenarios that implementing multiple approaches simultaneously are more effective in controlling waterlogging problems as compared to individual interventions.\n",
      "\n",
      "\n",
      "                                                                                                                                                                                                                                                                      text  \\\n",
      "0  Waterlogging and secondary salinization have become a serious problem in the canal irrigated areas of arid and semi-arid regions worldwide.                                                                                                                               \n",
      "1  Based on the model results a change in cropping pattern with reduced rice area is suggested.                                                                                                                                                                              \n",
      "2  A finite-difference two-dimensional simulation model was then used to evaluate the long-term impacts of various water management strategies on the groundwater table with the optimal land and water use parameters which were obtained through the optimization model.   \n",
      "3  The calibration, validation, sensitivity analysis, and error analysis of the model was performed before it was used to study the impact of various water management scenarios on the long-term groundwater level.                                                         \n",
      "4  Groundwater withdrawal should be increased by - % in the various nodes.                                                                                                                                                                                                   \n",
      "\n",
      "   score  \n",
      "0  2.98   \n",
      "1  2.77   \n",
      "2  2.48   \n",
      "3  1.64   \n",
      "4  0.32   \n",
      "\n",
      "=================\n",
      "\n",
      "Uncertainty and Sensitivity in the Carbon Footprint of Shopping Bags (2011)\n",
      "P>Carbon footprints for several shopping bag alternatives (polyethylene, paper, cotton, biodegradable modified starch, and recycled polyethylene) were compared with life cycle assessment. Stochastic uncertainty analysis was used to study the sensitivity of the comparison to scenario and parameter uncertainty. On the basis of the results, we could give only a few robust conclusions without choosing a waste treatment scenario or limiting the parameter space. Given the scenario of current waste infrastructure in Finland, recycled polyethylene bags seem to be the most preferable (- to  g CO() eq./bag) and biodegradable bags the least preferable ( to  g CO() eq./bag) option. In each analyzed waste treatment scenario, a few parameters dominated the uncertainty of results. Most of these parameters were downstream of the shopping bag manufacturing (consumer behavior, landfill conditions, method of waste combustion, etc.). The choice of waste treatment scenario had a greater effect on the ranking of bags than parameter uncertainty within scenarios. This result highlights the importance of including several scenarios in comparative life cycle assessments.\n",
      "\n",
      "\n",
      "                                                                                                                                                                                          text  \\\n",
      "0  In each analyzed waste treatment scenario, a few parameters dominated the uncertainty of results.                                                                                             \n",
      "1  Most of these parameters were downstream of the shopping bag manufacturing (consumer behavior, landfill conditions, method of waste combustion, etc.).                                        \n",
      "2  P>Carbon footprints for several shopping bag alternatives (polyethylene, paper, cotton, biodegradable modified starch, and recycled polyethylene) were compared with life cycle assessment.   \n",
      "\n",
      "   score  \n",
      "0  3.81   \n",
      "1  1.44   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  0.57   \n",
      "\n",
      "=================\n",
      "\n",
      "Diffusion of methylene blue in glass fibers - Application of the shrinking core model (2009)\n",
      "Diffusion of mass in a solid cylinder with concentration dependent diffusivity (or temperature-dependent thermal conductivity in case of heat diffusion) does not admit of an analytical solution except in special cases. The 'shrinking core model' has been used to develop an approximate analytical solution in certain circumstances. The model, generally useful to describe heterogeneous solid-fluid reactions, is applied to theoretically analyze the adsorption-diffusion phenomena of methylene blue dye in a glass fiber in the present work. Theoretical equations have been derived for the case of diffusivity as an exponential function of concentration. The diffusivity parameters are evaluated by global minimization of the error between the experimental and the theoretical concentration history. Other forms of diffusivity, namely constant diffusivity and diffusivity varying linearly with concentration are found to involve larger errors. A parametric sensitivity analysis of the error has been done. The shrinking core model could satisfactorily interpret the experimental dye concentration profile in the substrate.\n",
      "\n",
      "\n",
      "                                                                                                                                                                                                                         text  \\\n",
      "0  The diffusivity parameters are evaluated by global minimization of the error between the experimental and the theoretical concentration history.                                                                             \n",
      "1  The shrinking core model could satisfactorily interpret the experimental dye concentration profile in the substrate.                                                                                                         \n",
      "2  The model, generally useful to describe heterogeneous solid-fluid reactions, is applied to theoretically analyze the adsorption-diffusion phenomena of methylene blue dye in a glass fiber in the present work.              \n",
      "3  Diffusion of mass in a solid cylinder with concentration dependent diffusivity (or temperature-dependent thermal conductivity in case of heat diffusion) does not admit of an analytical solution except in special cases.   \n",
      "4  A parametric sensitivity analysis of the error has been done.                                                                                                                                                                \n",
      "\n",
      "   score  \n",
      "0  3.27   \n",
      "1  2.81   \n",
      "2  2.67   \n",
      "3  2.47   \n",
      "4  2.43   \n",
      "\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "for c_idx, corpus in enumerate(corpora):\n",
    "    sent_tokenize_list = nltk.sent_tokenize(corpus)\n",
    "    # print(sent_tokenize_list)\n",
    "    sent_corpora = [[sent, 0.0] for sent in sent_tokenize_list]\n",
    "\n",
    "    sent_score = pd.DataFrame({'text': sent_tokenize_list})\n",
    "    sent_score['score'] = 0.0\n",
    "    \n",
    "    print(df.iloc[c_idx]['title'], \"({})\".format(df.iloc[c_idx]['year']))\n",
    "    if len(corpus) == 0:\n",
    "        print(\"    No abstract, skipping...\\n\")\n",
    "        continue\n",
    "\n",
    "    print(corpus)\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "    for idx, sent in enumerate(sent_tokenize_list):\n",
    "        # tagged_words = nltk.word_tokenize(sent)\n",
    "        # tagged_tokens = nltk.tag.pos_tag(tagged_words)\n",
    "        # identified = cp.parse(tagged_tokens)\n",
    "\n",
    "        split_sent = sent.split(\" \")\n",
    "\n",
    "        sent_len = len(split_sent)\n",
    "        if sent_len % 2 == 0:\n",
    "            central_pos = int((sent_len - 1) / 2)\n",
    "        else:\n",
    "            central_pos = int(sent_len / 2)\n",
    "\n",
    "        root = split_sent[central_pos]\n",
    "\n",
    "        # contains_root = [s for ins, s in enumerate(corpora) if root in s and ins != idx]\n",
    "        for candidate_sentence in sent_corpora:\n",
    "            candidate = candidate_sentence[0]\n",
    "            if (sent == candidate) or (root not in candidate):\n",
    "                continue\n",
    "\n",
    "            current_score = sent_score.loc[idx, 'score']\n",
    "            sent_score.loc[idx, 'score'] = current_score + (fuzz.token_set_ratio(sent, candidate) / 100.0)\n",
    "\n",
    "    sent_score = sent_score[sent_score['score'] > 0.0]\n",
    "    sent_score = sent_score.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    tmp = sent_score[0:5]\n",
    "    print(tmp)\n",
    "    print('\\n=================\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE 2013: Sustainability of countries updated (2014)\n",
      "AU\n",
      "Empty DataFrame\n",
      "Columns: [text, score]\n",
      "Index: []\n",
      "\n",
      "=================\n",
      "\n",
      "Integration of topology and shape optimization for design of structural components (2001)\n",
      "DE\n",
      "Empty DataFrame\n",
      "Columns: [text, score]\n",
      "Index: []\n",
      "\n",
      "=================\n",
      "\n",
      "Optimization and Simulation Modelling for Managing the Problems of Water Resources (2013)\n",
      "DOI\n",
      "Empty DataFrame\n",
      "Columns: [text, score]\n",
      "Index: []\n",
      "\n",
      "=================\n",
      "\n",
      "Uncertainty and Sensitivity in the Carbon Footprint of Shopping Bags (2011)\n",
      "SO\n",
      "Empty DataFrame\n",
      "Columns: [text, score]\n",
      "Index: []\n",
      "\n",
      "=================\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1e35d04729ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwosis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_corpora\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwosis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentify_phrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\userdata\\takuyai\\owncloud\\projects\\wosis\\wosis\\analysis\\constrain.py\u001b[0m in \u001b[0;36midentify_phrases\u001b[1;34m(corpora)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0msent_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mdoc_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\UserData\\takuyai\\Miniconda3\\envs\\biblio\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\UserData\\takuyai\\Miniconda3\\envs\\biblio\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2102\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\UserData\\takuyai\\Miniconda3\\envs\\biblio\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "df = wosis.rc_to_df(filtered_corpora)\n",
    "wosis.analysis.constrain.identify_phrases(df[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "An efficient algorithm for building locally refined hp - adaptive H-PCFE: Application to uncertainty quantification\n",
    "Hybrid polynomial correlated function expansion (H-PCFE) is a novel metamodel formulated by coupling polynomial correlated function expansion (PCFE) and Kriging. Unlike commonly available metamodels, H-PCFE performs a bi-level approximation and hence, yields more accurate results. However, till date, it is only applicable to medium scaled problems. In order to address this apparent void, this paper presents an improved H-PCFE, referred to as locally refined hp - adaptiveH-PCFE. The proposed framework computes the optimal polynomial order and important component functions of PCFE, which is an integral part of H-PCFE, by using global variance based sensitivity analysis. Optimal number of training points are selected by using distribution adaptive sequential experimental design. Additionally, the formulated model is locally refined by utilizing the prediction error, which is inherently obtained in H-PCFE. Applicability of the proposed approach has been illustrated with two academic and two industrial problems. To illustrate the superior performance of the proposed approach, results obtained have been compared with those obtained using hp - adaptivePCFE. It is observed that the proposed approach yields highly accurate results. Furthermore, as compared to hp - adaptivePCFE, significantly less number of actual function evaluations are required for obtaining results of similar accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "r = rake_nltk.Rake(min_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "r.get_ranked_phrases()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python (Biblio)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
